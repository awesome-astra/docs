{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"pages/cheatsheet/","title":"Cheatsheet","text":""},{"location":"pages/cheatsheet/#executable-code-replit","title":"Executable Code (REPLIT)","text":"<p>Change values <code>ASTRA_DB_TOKEN</code>, <code>ASTRA_DB_ID</code>, <code>ASTRA_DB_REGION</code>, <code>ASTRA_DB_KEYSPACE</code> in the code below and execute with</p> <p></p>"},{"location":"pages/cheatsheet/#external-code-block","title":"External code block","text":"AstraDriver4x.java<pre><code>package com.datastax.astra;\nimport java.nio.file.Paths;\nimport com.datastax.oss.driver.api.core.CqlSession;\npublic class AstraDriver4x {\npublic static void main(String[] args) {\ntry (CqlSession cqlSession = CqlSession.builder()\n.withCloudSecureConnectBundle(Paths.get(\"/tmp/secure-connect-bundle-db-demo.\"))\n.withAuthCredentials(\"client_id\",\"client_secret\")\n.withKeyspace(\"keyspace_demo\")\n.build()) {\nSystem.out.println(\"Connected to \" + cqlSession.getKeyspace().get());\n}\n}\n}\n</code></pre>"},{"location":"pages/cheatsheet/#mermaids","title":"Mermaids","text":""},{"location":"pages/cheatsheet/#1-flow","title":"1\ufe0f\u20e3 Flow","text":"<p>Cassandra</p> GraphCode <pre><code>graph LR\n    user&gt;fa:fa-user Developer]-- Create Database --&gt; cassandra[(fa:fa-database Cassandra)]\n\n    user-- Design --&gt;usecase{{fa:fa-cube Use Case}}\n    usecase-- Workflow --&gt;queries[fa:fa-bezier-curve queries]\n    usecase-- MCD --&gt;entities[fa:fa-grip-vertical entities]\n    queries-- Chebotko modelization --&gt;schema[fa:fa-list schema]\n    entities-- Chebotko modelization --&gt;schema[fa:fa-list schema]\n    schema[fa:fa-list  schema]-- Inject --&gt;cassandra[(fa:fa-database Cassandra)]\n\n    user-- prepare --&gt;dataset{{fa:fa-coings DataSet}}\n    dataset-- input --&gt;dsbulk-- load data --&gt;cassandra\n\n    user-- Create Token --&gt;token{{fa:fa-key Token}}\n    usecase--&gt;API\n\n    API--&gt;Request\n    token--&gt;Request\n    schema--&gt;Request\n    Request-- invoke --&gt;cassandra</code></pre> <pre><code>graph LR\n    user&gt;fa:fa-user Developer]-- Create Database --&gt; cassandra[(fa:fa-database Cassandra)]\nuser-- Design --&gt;usecase{{fa:fa-cube Use Case}}\nusecase-- Workflow --&gt;queries[fa:fa-bezier-curve queries]\nusecase-- MCD --&gt;entities[fa:fa-grip-vertical entities]\nqueries-- Chebotko modelization --&gt;schema[fa:fa-list schema]\nentities-- Chebotko modelization --&gt;schema[fa:fa-list schema]\nschema[fa:fa-list  schema]-- Inject --&gt;cassandra[(fa:fa-database Cassandra)]\nuser-- prepare --&gt;dataset{{fa:fa-coings DataSet}}\ndataset-- input --&gt;dsbulk-- load data --&gt;cassandra\n\nuser-- Create Token --&gt;token{{fa:fa-key Token}}\nusecase--&gt;API\n\nAPI--&gt;Request\n    token--&gt;Request\n    schema--&gt;Request\n    Request-- invoke --&gt;cassandra\n</code></pre> <p>Example #1</p> OutputMarkdown <pre><code>graph TD;\n  A--&gt;B;\n  A--&gt;C;\n  B--&gt;D;\n  C--&gt;D;</code></pre> <pre><code>   ```mermaid\n   graph TD;\n     A--&gt;B;\n     A--&gt;C;\n     B--&gt;D;\n     C--&gt;D;\n   ```\n</code></pre> <p>Example3</p> OutputMarkdown <pre><code>graph TD\n  A[Hard] --&gt;|Text| B(Round)\n  B --&gt; C{Decision}\n  C --&gt;|One| D[Result 1]\n  C --&gt;|Two| E[Result 2]</code></pre> <pre><code>   ```mermaid\n   graph TD\n     A[Hard] --&gt;|Text| B(Round)\n     B --&gt; C{Decision}\n     C --&gt;|One| D[Result 1]\n     C --&gt;|Two| E[Result 2]\n   ```\n</code></pre>"},{"location":"pages/cheatsheet/#2-sequence","title":"2\ufe0f\u20e3 Sequence","text":"OutputMarkdown <pre><code>sequenceDiagram\nAlice-&gt;&gt;John: Hello John, how are you?\nloop Healthcheck\n    John-&gt;&gt;John: Fight against hypochondria\nend\nNote right of John: Rational thoughts!\nJohn--&gt;&gt;Alice: Great!\nJohn-&gt;&gt;Bob: How about you?\nBob--&gt;&gt;John: Jolly good!</code></pre> <pre><code>   ```mermaid\n   sequenceDiagram\n   Alice-&gt;&gt;John: Hello John, how are you?\n   loop Healthcheck\n       John-&gt;&gt;John: Fight against hypochondria\n   end\n   Note right of John: Rational thoughts!\n   John--&gt;&gt;Alice: Great!\n   John-&gt;&gt;Bob: How about you?\n   Bob--&gt;&gt;John: Jolly good!\n\n   ```\n</code></pre>"},{"location":"pages/cheatsheet/#3-gantt","title":"3\ufe0f\u20e3 Gantt","text":"OutputMarkdown <pre><code>gantt\nsection Section\nCompleted :done,    des1, 2014-01-06,2014-01-08\nActive        :active,  des2, 2014-01-07, 3d\nParallel 1   :         des3, after des1, 1d\nParallel 2   :         des4, after des1, 1d\nParallel 3   :         des5, after des3, 1d\nParallel 4   :         des6, after des4, 1d</code></pre> <pre><code>   ```mermaid\n   gantt\n   section Section\n   Completed :done,    des1, 2014-01-06,2014-01-08\n   Active        :active,  des2, 2014-01-07, 3d\n   Parallel 1   :         des3, after des1, 1d\n   Parallel 2   :         des4, after des1, 1d\n   Parallel 3   :         des5, after des3, 1d\n   Parallel 4   :         des6, after des4, 1d\n\n   ```\n</code></pre>"},{"location":"pages/cheatsheet/#4-class","title":"4\ufe0f\u20e3 Class","text":"OutputMarkdown <pre><code>classDiagram\nClass01 &lt;|-- AveryLongClass : Cool\n&lt;&lt;interface&gt;&gt; Class01\nClass09 --&gt; C2 : Where am i?\nClass09 --* C3\nClass09 --|&gt; Class07\nClass07 : equals()\nClass07 : Object[] elementData\nClass01 : size()\nClass01 : int chimp\nClass01 : int gorilla\nclass Class10 {\n &lt;&lt;service&gt;&gt;\n int id\n size()\n}</code></pre> <pre><code>   ```mermaid\n   classDiagram\n   Class01 &lt;|-- AveryLongClass : Cool\n   &lt;&lt;interface&gt;&gt; Class01\n   Class09 --&gt; C2 : Where am i?\n   Class09 --* C3\n   Class09 --|&gt; Class07\n   Class07 : equals()\n   Class07 : Object[] elementData\n   Class01 : size()\n   Class01 : int chimp\n   Class01 : int gorilla\n   class Class10 {\n    &lt;&lt;service&gt;&gt;\n    int id\n    size()\n   }\n\n   ```\n</code></pre>"},{"location":"pages/cheatsheet/#5-state","title":"5\ufe0f\u20e3 State","text":"OutputMarkdown <pre><code>stateDiagram-v2\n[*] --&gt; Still\nStill --&gt; [*]\nStill --&gt; Moving\nMoving --&gt; Still\nMoving --&gt; Crash\nCrash --&gt; [*]</code></pre> <pre><code>   ```mermaid\n   stateDiagram-v2\n   [*] --&gt; Still\n   Still --&gt; [*]\n   Still --&gt; Moving\n   Moving --&gt; Still\n   Moving --&gt; Crash\n   Crash --&gt; [*]\n\n   ```\n</code></pre>"},{"location":"pages/cheatsheet/#6-pie","title":"6\ufe0f\u20e3 Pie","text":"OutputMarkdown <pre><code>pie\n\"Dogs\" : 386\n\"Cats\" : 85\n\"Rats\" : 15</code></pre> <pre><code>   ```mermaid\n   pie\n   \"Dogs\" : 386\n   \"Cats\" : 85\n   \"Rats\" : 15\n\n   ```\n</code></pre>"},{"location":"pages/cheatsheet/#7-journey","title":"7\ufe0f\u20e3 Journey","text":"OutputMarkdown <pre><code>journey\n  title My working day\n  section Go to work\n    Make tea: 5: Me\n    Go upstairs: 3: Me\n    Do work: 1: Me, Cat\n  section Go home\n    Go downstairs: 5: Me\n    Sit down: 3: Me</code></pre> <pre><code>   ```mermaid\n   journey\n    title My working day\n    section Go to work\n      Make tea: 5: Me\n      Go upstairs: 3: Me\n      Do work: 1: Me, Cat\n    section Go home\n      Go downstairs: 5: Me\n      Sit down: 3: Me\n\n   ```\n</code></pre>"},{"location":"pages/cheatsheet/#8-er","title":"8\ufe0f\u20e3 ER","text":"OutputMarkdown <pre><code>erDiagram\n  CUSTOMER ||--o{ ORDER : places\n  ORDER ||--|{ LINE-ITEM : contains\n  CUSTOMER }|..|{ DELIVERY-ADDRESS : uses</code></pre> <pre><code>   ```mermaid\n   erDiagram\n     CUSTOMER ||--o{ ORDER : places\n     ORDER ||--|{ LINE-ITEM : contains\n     CUSTOMER }|..|{ DELIVERY-ADDRESS : uses\n   ```\n</code></pre>"},{"location":"pages/cheatsheet/#sample-blocs","title":"Sample Blocs","text":"<p>THis is a note</p> <p>my note</p> <p>abstract</p> <p>my note</p> <p>info</p> <p>info</p> Sample tip  <p>tip</p> How to add plugins to the Docker image? <p>Import Stuff</p> <p>Success</p> <p>my note</p> <p>Sample warning</p> <p>This is so cool.</p> <p>failure</p> <p>my note</p> <p>danger</p> <p>danger</p> <p>bug</p> <p>bug</p> Sample example <p>example</p> <p>Sample warning</p> <p>warning</p> Sample wuote <p>quote</p>"},{"location":"pages/cheatsheet/#tooltip","title":"Tooltip","text":"<pre><code>wanna a tooltip ? # (1)!\n</code></pre> <ol> <li> <p>Cedrick rock</p> <pre><code>mkdocs serve\n</code></pre> </li> </ol>"},{"location":"pages/cheatsheet/#icons","title":"Icons","text":""},{"location":"pages/cheatsheet/#material","title":"Material","text":"<p>HERE is the full list</p>"},{"location":"pages/cheatsheet/#font-awesome","title":"Font Awesome","text":"<p>HTML</p> <li> = fa-camera-retro</li> <p>MARKDOWN </p> <p>HERE is the full list</p>"},{"location":"pages/cheatsheet/#opticons","title":"Opticons","text":"<p> Sample</p>"},{"location":"pages/cheatsheet/#adding-buttons","title":"Adding buttons","text":"<p>In order to render a link as a button, suffix it with curly braces and add the <code>.md-button</code> class selector to it. The button will receive the selected [primary color] and [accent color] if active.</p> Button<pre><code>[Subscribe to our newsletter](#){ .md-button }\n</code></pre> <p>[Subscribe to our newsletter][demo]{ .md-button }</p> <p>[Subscribe to our newsletter][demo]{ .md-button .md-button--primary }</p>"},{"location":"pages/aiml/","title":"Overview","text":"<p>Welcome to your ultimate solution for AI/ML workloads: DataStax Astra. As the demand for robust and scalable artificial intelligence (AI) and machine learning (ML) applications continues to grow, Astra DB provides the power, efficiency, and scalability your systems need to deliver real-time results.</p> <p>In the ever-evolving landscape of AI, Astra is perfect for meeting the high data demands and complex workload patterns associated with these applications. It's not just about the volume of data; it's about the speed and flexibility with which you can manipulate and interpret it. That's where Astra shines. Its cloud-native, scalable database service is designed to manage your AI/ML data effortlessly, no matter the scale or complexity.</p> <p>In the upcoming sections, we'll show you the essential tools and knowledge to kickstart your journey with Astra DB for your AI applications.</p> <p>Get ready to dive into the world of AI/ML with DataStax Astra, which doesn't just meet your data needs - it exceeds them. With Astra DB's robust infrastructure, you'll have everything you need to harness the true power of generative AI and other AI/ML applications, delivering solutions that are not just innovative, but transformative.</p>"},{"location":"pages/aiml/#generative-ai-llms","title":"Generative AI / LLMs","text":"<p>Let's focus on one of the most promising and fascinating realms of AI: Generative AI. This cutting-edge technology uses advanced ML models to generate new, original content, requiring the power, speed, and reliability that Astra provides.</p> <p>A very important topic in Generative AI is that of LLMs, or \"Large Language Models\". These are essentially free-form \"text in, text out\" functions. Efficient interaction with these models is best done through frameworks that abstract away most of the low-level operations.</p>"},{"location":"pages/aiml/#vector-search-capabilities","title":"Vector Search capabilities","text":"<p>Astra DB is now equipped with a feature that is very important for advanced language-model-related tasks: namely, Vector Search capability. (Note: at the time of writing, this feature is being added to OSS Cassandra as well.)</p> <p>The practical implication of this is that Astra DB can store, and efficiently retrieve, pieces of text based on \"semantic similarity\": this enables many powerful workloads such as question-answering over a (possibly very large) knowledge base, retrieval of relevant parts of a long past conversation, or a caching system that is oblivious of the exact phrase used to express a concept.</p> What's in a Vector? <p>The general idea is that at storing time a piece of unstructured data (the phrase) is made into a \"vector\" (a list of numbers), which is then used at retrieval time to assess how \"semantically similar\" each item is to a given search \"term\" (a vector itself).</p> <p>It may be not immediately clear what exactly the data in a vector is there to represent. How does a piece of data, e.g. a paragraph of text, get converted into a vector, and why do we want that in the first place? </p> <p>When we have data in the form of a vector, it's computationally simple to compare two pieces of data for similarity by measuring how close vectors are to each other. Which means that if we know how to represent a particular piece of data as a vector, then we have a way to find similar data based on the vector proximity. So, the utility is pretty apparent, but we still haven't answered \"how\".</p> <p>The process by which data is converted into a vector is called \"embedding\". There are various embedding libraries for all kinds of different data types and purposes. An embedding library analyzes a datum in the context of the data type that it was created for, and outputs a vector representation. Input that is substantially similar will produce output vectors that are geometrically close, and inputs that bear no similarity will be geometrically far apart. So, in particular, two sentences that express the same concept with different words and constructions will correspond to two vectors that are very close to each other.</p>"},{"location":"pages/aiml/#integrations","title":"Integrations","text":"<p>The Astra team is working hard to provide Astra-specific integrations with more and more GenAI-oriented frameworks. Pick the tile corresponding to your stack to learn more:</p> <p> </p>"},{"location":"pages/aiml/#traditional-ml-workloads","title":"Traditional ML workloads","text":"<p>Many ML applications, especially those around supervised learning, take advantage of specialized infrastructure, such as Feature Stores.</p> <p>Here is a collection of the main integrations with Astra that can power your ML/Real-Time AI application. Pick the solution you are interested in:</p> <p> </p>"},{"location":"pages/aiml/aws/aws-bedrock/","title":"Amazon Bedrock","text":""},{"location":"pages/aiml/aws/aws-bedrock/#overview","title":"Overview","text":"<p>Amazon Bedrock is a fully managed, fully serverless service offering access to foundation models from various AI companies in a single API, together with additional capabilities such as facilities for fine-tuning and agent frameworks.</p> <p>Amazon Bedrock aims at simplifying development, especially for Generative AI, while maintaining privacy and security. Experimenting with foundation models is easy, and models can be further customized in a privacy-aware fashion within Bedrock.</p> <p>Foundation models from Amazon Bedrock can be easily integrated with Astra DB, most notably with its Vector capabilities, thereby providing a solid framework for developing Generative AI and other ML-centered applications.</p> <p>In the following example, you will be able to run a standard RAG (retrieval-augmented generation) application that makes use of AI models from Amazon Bedrock and uses Astra DB as a Vector Store.</p> <p>The integration is built with the LangChain framework, which conveniently offers native support for Amazon Bedrock as well as Astra DB. Using LangChain is a popular and well-established choice -- but certainly not the only one. See the references at the end of the page for further options.</p>"},{"location":"pages/aiml/aws/aws-bedrock/#prerequisites","title":"Prerequisites","text":"<p>To run the integration demo notebook, you need:</p> <ul> <li>An Amazon account with access to Amazon Bedrock and Amazon SageMaker Studio. In particular, you will be asked to provide a set of credentials for programmatic access (i.e. <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code> and <code>AWS_SESSION_TOKEN</code>);</li> <li>Alternatively, if you run the notebook from within SageMaker Studio, it is sufficient to add the Bedrock policy to your SageMaker role instead of explicitly providing the above secrets. Please refer to this link for details.</li> <li>An Astra account with a Serverless Cassandra with Vector Search database. You will need the Database ID and an Access Token with role \"Database Administrator\".</li> </ul>"},{"location":"pages/aiml/aws/aws-bedrock/#run-the-example","title":"Run the example","text":"<p>The integration example is a Python 3.8+ runnable notebook. The suggested method is to import the notebook in your Amazon SageMaker Studio and run it from there on a standardized environment, which also makes the necessary AWS access credentials readily available. As a side note, however, the code can be executed on any environment able to run Python notebooks.</p> <p> </p> <p>Step 1. Download the notebook from this repository at this link and save it to your local computer. (You can also view it on your browser.)</p> <p></p> <p>Step 2. Open your Amazon SageMaker console and click the \"Studio\" item in the left navbar. Do not choose \"Studio Lab\", which is a different thing.</p> <p></p> <p>Step 3. Click the \"Open Studio\" button after choosing a user profile: this will bring you to the Studio interface. You may need to create a Studio instance (\"SageMaker domain\") if you don't have one already.</p> <p></p> <p>Step 4. In the left toolbox of Sagemaker Studio, make sure you select the \"File Browser\" view and locate the \"Upload\" button: use it to upload the notebook file you previously saved. The notebook will be shown in the file browser.</p> <p></p> <p>Step 5. If you double-click on it, the notebook will be opened in Studio. In order to run it, you will be asked to start a \"notebook environment\" (essentially, a Python runtime). Choose the \"Data Science 3.0\" image and a \"Python 3\" kernel and hit \"Select\".</p> <p></p> <p>Step 6. Once the kernel has fully started, you can run each cell in sequence by clicking on them and pressing Shift+Enter. You will be asked for the secrets during execution.</p> <p></p>"},{"location":"pages/aiml/aws/aws-bedrock/#cleanup","title":"Cleanup","text":"<p>During the above steps, some resources are created, which you may want to cleanly dispose of after you are done.</p> <p>These have been stared to run the notebook itself. You can shut them down from within SageMaker Studio: select the \"Running Terminals and Kernels\" view on the left toolbar (see picture below) and click the \"shut down\" icon next to all instances, apps and sessions associated to the notebook you just ran.</p> <p></p>"},{"location":"pages/aiml/aws/aws-bedrock/#additional-information","title":"Additional information","text":"<p>More info on Amazon Bedrock</p> <p>Accessing Amazon Bedrock from Python</p> <p>Using Astra DB for your Generative AI applications</p>"},{"location":"pages/aiml/aws/aws-sagemaker/","title":"Amazon SageMaker","text":""},{"location":"pages/aiml/aws/aws-sagemaker/#overview","title":"Overview","text":"<p>Amazon SageMaker is a managed service to deploy and use Machine Learning models for a variety of purposes. Its offering includes infrastructure provisioning, tooling and end-to-end workflow setup.</p> <p>With Amazon SageMaker it is possible not only to deploy ready-made models, but also to manage training data and perform custom training or fine-tuning steps. The easiest way to begin using SageMaker is to deploy and use many well-established foundation models that are made available through \"SageMaker JumpStart\".</p> <p>Models deployed within Amazon SageMaker can be integrated with Astra DB, making it possible to build enterprise-grade Generative AI applications with minimal infrastructural effort, while retaining desirable features such as the usage of secure, dedicated instances.</p> <p>In the following example, you will provision and execute a simple end-to-end application implementing the RAG (retrieval-augmented generation) flow, with AI models from Amazon SageMaker JumpStart and Astra DB as the backend for the Vector Store.</p> <p>The example makes use of the LangChain framework (which has great support for both SageMaker and Astra DB); however, other, lower-level ways to interact programmatically with SageMaker are available -- as a matter of fact you will get a chance to experiment with them in an Appendix at the end of the integration example.</p> <p>For more information and other options to interface with SageMaker, please consult the suggested readings at the end of this page.</p>"},{"location":"pages/aiml/aws/aws-sagemaker/#prerequisites","title":"Prerequisites","text":"<p>To run the integration demo notebook, you need:</p> <ul> <li>An Amazon account with access to Amazon SageMaker. In particular, you will need to acces SageMaker Studio and run the example in there: besides ensuring a standardized runtime and environment, SageMaker Studio provides automated access to a ready-made set of Amazon IAM roles that grant your account all necessary permissions to deploy the \"JumpStart\" models in SageMaker (see below for details);</li> <li>An Astra account and a Vector database created in it. To connect to it, you need the API Endpoint and an Application Token, which you can obtain from the database main dashboard on Astra. You may optionally need to specify a Namespace for the connection if you are not targeting the default one.</li> </ul>"},{"location":"pages/aiml/aws/aws-sagemaker/#setup","title":"Setup","text":"<p>The sample application needs an embedding model and a large language model (LLM), both in SageMaker.</p> <p>The models can be deployed either through the SageMaker UI or programmatically with the Python SageMaker SDK. You can choose your preferred method: the integration example will adapt to your choice.</p>"},{"location":"pages/aiml/aws/aws-sagemaker/#deploy-models-programmatically","title":"Deploy models programmatically","text":"<p>The code for automated deployment of the models is included in the example: you can skip to \"Run the example\" below and start with the code.</p>"},{"location":"pages/aiml/aws/aws-sagemaker/#deploy-models-from-sagemaker-ui","title":"Deploy models from SageMaker UI","text":"<p>If you prefer to use the SageMaker UI to deploy the models, here are all steps.</p> <p> <p>Choice of the models</p> <p>This example is tailored to run using exactly the models we suggest in the following.</p> <p>Please pay attention to deploying the right models for a smooth quickstart experience. Changes could impact the performance or the results or even not work altogether, since the signature of the inputs and outputs varies between models.</p> <p>On the other hand, the reason why this method is described in some details is precisely because it is what you may need when using your own models -- possibly created, trained or fine-tuned outside of the JumpStart experience.</p> <p>If you plan on using other models, remember that you will have to adapt parts of the code (such as the signature of the input parameters, the serializers/deserializers and the \"context handlers\" passed to the LangChain SageMaker objects) your model's specific needs.</p> <p></p> <p></p> <p>Step 1. Open your Amazon SageMaker console and click the \"Studio\" item in the left navbar. Do not choose \"Studio Lab\", which is a different thing.</p> <p></p> <p>Step 2. Click the \"Open Studio\" button after choosing a user profile: this will bring you to the Studio interface. You may need to create a Studio instance (\"SageMaker domain\") if you don't have one already. Creation of a new domain may take about five minutes.</p> <p></p> <p></p> <p>Step 3. Make sure you are on the \"Home\" view (click on the home icon in the left navbar) and pick the \"JumpStart\" button under Prebuilt and automated solutions. Clicking on it will bring the \"SageMaker JumpStart\" listing in the main view, with the entries grouped by \"Hub\". Notice the search box at the top of the listing.</p> <p></p> <p>Step 4. Search for the embedding model <code>GPT-J 6B Embedding</code> with the search box of JumpStart. Click on it in the search results: you will be brought to the model's details. Click the \"Deploy\" button on the top right to configure and start the model deployment.</p> <p></p> <p>Step 5. Check the endpoint name (you can keep the default suggested one) and select <code>ml.g5.24xlarge</code> for the instance type that will host the model. (Note: you might need to check with your Service Quotas to make sure you have capacity to deploy this instance.)</p> <p>Step 6. Click \"Deploy\" (bottom right) to start the process. This might take between five and ten minutes. The endpoint you are going to use has been given a final name (visible also as the title of this page) obtained by prepending <code>\"jumpstart-dft-\"</code> to the name chosen earlier. Keep this endpoint name handy: you will be asked to provide it later when running the example code.</p> <p></p> <p>Step 7. While you wait, you can check the status of the deploy in two ways: (a) from the Amazon SageMaker UI, choosing the \"Inference / Endpoints\" entry in the left navbar, and then refreshing periodically; or (b) from within SageMaker Studio, by selecting the \"Deployment/Endpoints\" entry in the left navbar. You have to periodically click the \"Refresh\" button until you see the status of your model changing from \"Creating\" to \"In service\".</p> <p> Checking deploy status, screenshots <p>From the SageMaker UI:  From within SageMaker Studio: </p> <p></p> <p></p> <p></p> <p>Step 8. When the deploy has finished, you will see its status being reported as \"In Service\". You can run a quick test with a handy Playground-like interface available in SageMaker Studio: click on the endpoint name in the list (reachable through \"Deployments/Endpoints\" on the left navbar), then select the \"Test inference\" tab to open it. Try with a JSON payload such as <code>{\"text_inputs\": [\"I am here!\", \"So do I.\"]}</code>: paste this text in the \"JSON Payload\" text field, then hit \"Send Request\" (bottom right): the endpoint response should resemble the one shown in the screenshot, with the two embedding vectors under the <code>body =&gt; embedding</code> list-of-lists entry.</p> <p>Now the embedding model is deployed and ready to be used in the example code.</p> <p>Step 9. Repeat steps 4 through 7, this time to deploy the LLM: search in JumpStart for the model named \"Llama 2 70B Chat\" and choose <code>ml.g5.48xlarge</code> as instance type. (Note: this model, made available by Meta, requires acceptance of its \"End User License Agreement\" and \"Acceptable Use Policy\" prior to use: you will be presented with a dialog to do so at deploy time. Also, keep in mind that the deploy time for this model can well exceed twenty minutes in some cases.)</p> <p>Step 10. Similarly as for the embedding model, you can test the LLM in the UI through the model's \"Test inference\" tab. Try with a payload like the following: <code>{\"inputs\": \"Write a short poem about the late Paleocene.\", \"parameters\": {\"max_new_tokens\": 256, \"temperature\": 0.6, \"top_p\": 0.9}}</code>. Hit the \"Send Request\" button (bottom right), then check the results in the right-hand panel. (Note: the \"Test inference\" tab can also show Python code to use that particular endpoint through <code>boto3</code> invocations. This may come handy, as in most cases exemplifies more advanced usages such as how to encode a past exchange between system/assistant/user roles in the text generation request.)</p> <p>Deploys are now complete. You can now start the example notebook as outlined in the next section.</p>"},{"location":"pages/aiml/aws/aws-sagemaker/#run-the-example","title":"Run the example","text":"<p>The integration example is a Python 3.8+ runnable notebook. The notebook is designed to run within Amazon SageMaker Studio, so as to receive, through its usage of the AWS <code>boto3</code> library, an AWS identity equipped with the special permissions required to programmatically deploy the SageMaker JumpStart models.</p> <p>As mentioned earlier, you will be able to either supply the endpoint names for the AI models (in case you went for UI-based deploys as outlined earlier), or have the SageMaker SDK take care of them programmatically.</p> <p>The main flow of the notebook features usage of the models through the corresponding SageMaker-specific LangChain plugins: however, the model endpoints can be accessed in other ways - something that is demonstrated in an Appendix at the end of the notebook.</p> <p> </p> <p>Step 1. Download the notebook from this repository at this link and save it to your local computer. (You can also view it on your browser.) Make sure you are in SageMaker Studio (check out steps 1 and 2 of the UI deploy steps above for how to open it).</p> <p></p> <p></p> <p>Step 2. You need a \"JupyterLab space\", i.e. an Amazon-provided filesystem with compute resources on top of it, ready to run Jupyter kernels and effectively execute your notebooks. You find JupyterLab in the \"Applications\" icon group at the top of the left-hand sidebar; then, if you don't have one yet, click the \"Create JupyterLab space\" button, give it a name and hit \"Create space\". You should now see the space's dashboard -- and notice that it has been created, but it is not running yet.</p> <p></p> <p>Step 3. Check the compute instance type that will support the execution of your code (you can stick with a <code>ml.t3.medium</code> instance for this example) and click \"Run space\" to get your space actually running. Wait a minute or so until you see the space's status change from \"Starting\" to \"Running\".</p> <p>Step 4. Once the JupyterLab space is Running, you can click \"Open JupyterLab\": a new browser tab will be brought up. There is a toolbox on the left, which you will momentarily use to upload the notebook file. (Note: you can stop and re-start the space whenever you want, in order to optimize your resource usage. The notebooks stored in the space's file system will be persisted.)</p> <p></p> <p>Step 5. In the left toolbox of your JupyterLab space, make sure you select the \"File Browser\" view and locate the \"Upload\" button: click on it and pick the notebook file from your computer. The notebook will be shown in the file browser.</p> <p></p> <p>Step 6. If you double-click on it, the notebook will be opened in Studio, with a Jupyter kernel behind id, ready to execute its code.</p> <p></p> <p>Step 7. You can now run each cell in sequence by clicking on them and pressing Shift+Enter. You will be asked for the secrets and the connection details during execution.</p>"},{"location":"pages/aiml/aws/aws-sagemaker/#cleanup","title":"Cleanup","text":"<p>During the above steps, some resources are created, which you may want to cleanly dispose of after you are done:</p> <ul> <li>endpoints deployed in SageMaker (i.e. the Embedding and the LLM models). You can delete them from the \"Endpoints\" view, reachable through the \"Deployments/Endpoints\" entry in SageMaker's left-hand navbar (once deleted, you can click the Refresh button in the listing, to make sure they are not displayed anymore); (Please note some endpoints can be deleted only after having manually deleted the Models that are part of the endpoint. The SageMaker UI would guide you through the process.)</li> <li>to delete the JupyterLab you created earlier, navigate to \"Applications/JupyterLab\" and select the JupyterLab space. First you need to hit \"Stop space\" and acknowledge the warning about additional resources (endpoints, buckets); wait until the space is \"Stopped\" (about 10 seconds), then you can open the \"...\" menu on the upper right and pick the \"Delete space\" menu entry.</li> <li>Deletion of a JupyterLab space will not delete the associated S3 bucket created with it (called something like <code>sagemaker-studio-01234567890-9abc8defg7h</code>) automatically: you need to navigate to S3, identify the bucket and delete it separately;</li> <li>you may want to even delete entirely the \"SageMaker domain\" you created. This requires several steps, outlined here. Do not forget you will have to manually destroy an associated S3 bucket as well;</li> <li>a Collection in your Astra DB instance. The notebook provides a way to delete it programmatically; alternatively, you can do so through the \"Data Explorer\" tab in the Astra DB dashboard for the database.</li> </ul>"},{"location":"pages/aiml/aws/aws-sagemaker/#additional-information","title":"Additional information","text":"<p>What is Amazon SageMaker?</p> <p>Documentation for the Python SageMaker SDK</p> <p>Using Astra DB for your Generative AI applications</p> <p>Getting Started with SageMaker JumpStart</p> <p>Llama 2 foundation models from Meta available in SageMaker JumpStart</p> <p>Accessing SageMaker from boto3 with Python</p>"},{"location":"pages/aiml/llm/cassio/","title":"CassIO","text":""},{"location":"pages/aiml/llm/cassio/#overview","title":"Overview","text":"<p>CassIO is the ultimate solution for seamlessly integrating Astra DB and Apache Cassandra\u00ae with generative artificial intelligence and other machine learning workloads. This powerful Python library simplifies the complicated process of accessing the advanced features of the Cassandra database, including vector search capabilities. With CassIO, developers can fully concentrate on designing and perfecting their AI systems without any concerns regarding the complexities of integration with Cassandra.</p> <p>While CassIO can be used directly in your application, most often the library is used as the core engine powering integration with third-party LLM-oriented frameworks, such as LangChain or LlamaIndex.</p> <p>To find out more, stay up to date, and get plenty of code examples and demos, please head over to cassio.org.</p> <p></p>"},{"location":"pages/aiml/llm/langchain/","title":"LangChain","text":""},{"location":"pages/aiml/llm/langchain/#overview","title":"Overview","text":"<p>LangChain is a popular and rapidly evolving framework to automate most of the management of, and interaction with, large language models (LLMs): among its features are support for memory, vector-based similarity search, an advanced prompt templating abstraction and much more.</p> <p>LangChain comes with a Python and a Javascript implementation. This section targets the Python version.</p> <p>Reference documentation:</p> <ul> <li>\u2139\ufe0f LangChain documentation</li> <li>\u2139\ufe0f LangChain Python docs</li> <li>\u2139\ufe0f CassIO</li> </ul>"},{"location":"pages/aiml/llm/langchain/#benefits-of-langchain","title":"Benefits of LangChain","text":"<p>Essentially, interacting with a LLM amounts to this: some free-form text is sent to it, and the model \"responds\" by producing other text. That's it.</p> <p>This is the starting point for a wide variety of tasks, from translations to question answering, from code completion to chatbot assistants, and so on. But how exactly are all those more complex usages constructed out of the simple \"text in, text out\" building block?</p> <p>This is where the power of LangChain comes in. The framework makes it easier to manage the handling of typical LLM-oriented tasks such as constructing prompts programmatically, parsing the output back to a structured form, providing memory of past interactions, and so on.</p> <p>Further, LangChain's modularity means it can work regardless of the actual service providing the core LLM functionality (OpenAI, Google PaLM, Azure OpenAI, AWS SageMaker LLMs ...). LangChain also offers a modular set of abstractions, so that one can stack them on top of each other and design complex LLM applications with minimal boilerplate.</p>"},{"location":"pages/aiml/llm/langchain/#astra-db-langchain","title":"Astra DB + LangChain","text":"<p>There are several tasks that benefit from the Astra integration for LangChain, as there are many different reasons to augment the power of an LLM with the kind of persistent storage Astra DB can offer.</p> <p>Generally speaking, the Astra integration for LangChain builds on top of the open-source <code>cassIO</code> library, which provides a set of standardized facilities to interact with Astra DB (and Cassandra) through the patterns typically needed by ML/LLM applications. The <code>cassIO</code> library is framework-agnostic: it is in turn used by the Astra-specific extensions of any specific framework (such as LangChain).</p> <p>The integration takes advantage of Astra DB's Vector Search capabilities, so that it is possible to run advanced LLM workloads, based on semantic similarity, without leaving your Astra DB storage backend.</p>"},{"location":"pages/aiml/llm/langchain/#usage-examples","title":"Usage Examples","text":"<p>The LangChain integration for Astra DB is documented in detail in the LangChain section of the <code>cassIO</code> website, with complete tutorials and sample applications.</p> <p>Have a look at what this integration enables ... and to learn more, and stay up to date, check out the <code>cassIO</code> homepage!</p>"},{"location":"pages/aiml/llm/langchain/#question-answering","title":"Question answering","text":"<p>With the help of Vector Search, applications such as natural-language question answering over documents are made easy. Once the input documents are loaded and indexed,</p> <pre><code>index_creator = VectorstoreIndexCreator(\nvectorstore_cls=Cassandra,\nembedding=myEmbedding,\nvectorstore_kwargs={\n'session': session,\n'keyspace': keyspace,\n},\n)\nloader = ...\nindex = index_creator.from_loaders([loader])\n</code></pre> <p>a Q&amp;A session is just a single line of code:</p> <pre><code>index.query(\"How do I restore a deleted file?\", llm=llm)\n</code></pre> <p>Vector Store document retrieval from Astra DB in LangChain also support metadata filtering. Check out cassio.org for more details.</p>"},{"location":"pages/aiml/llm/langchain/#memory","title":"Memory","text":"<p>Give LLMs a memory of past interactions stored in an Astra table and leave it to LangChain to retrieve previous exchanges and store the new ones as the conversation proceeds:</p> <pre><code>message_history = CassandraChatMessageHistory(\nsession_id='my-session-id',\nsession=session,\nkeyspace=keyspace,\n)\ncassBuffMemory = ConversationBufferMemory(\nchat_memory=message_history,\n)\nconversation = ConversationChain(\nllm=llm, \nmemory=cassBuffMemory,\n)\nconversation.predict(input=\"Hello, what can you tell me about rainforests?\")\n</code></pre> <p>The base <code>CassandraChatMessageHistory</code> works as well when you ask LangChain to keep a separate \"summary\" of the whole past conversation, which is automatically updated and injected into each new interaction:</p> <pre><code>memory = ConversationSummaryBufferMemory(\nllm=llm,\nchat_memory=message_history,\nmax_token_limit=180,\n)\nsummaryConversation = ConversationChain(\nllm=llm, \nmemory=memory,\n)\n</code></pre> <p>Alternatively, you can have a semantically aware memory element, able to pick the most relevant exchanges occurred and make the LLM aware of them regardless of how far back they took place:</p> <pre><code>cassVStore = Cassandra(\nsession=session,\nkeyspace=keyspace,\nembedding=myEmbedding,\n)\nretriever = cassVStore.as_retriever(search_kwargs={'k': 3})\nsemanticMemory = VectorStoreRetrieverMemory(retriever=retriever)\nsemanticMemoryTemplateString = ...\nmemoryPrompt = PromptTemplate(\ninput_variables=[\"history\", \"input\"],\ntemplate=semanticMemoryTemplateString\n)\nconversationWithVectorRetrieval = ConversationChain(\nllm=llm, \nprompt=memoryPrompt,\nmemory=semanticMemory,\n)\nconversationWithVectorRetrieval.predict(\ninput=\"Do you remember what I told you about rainforests?\"\n)\n</code></pre>"},{"location":"pages/aiml/llm/langchain/#caching","title":"Caching","text":"<p>Save on latencies and token costs by using Astra DB as a cache for the responses to frequently-used prompts:</p> <pre><code>langchain.llm_cache = CassandraCache(\nsession=session,\nkeyspace=keyspace,\n)\nllm(\"What is the best way to peel a tomato?\")       # 1-2 seconds\n...\nllm(\"What is the best way to peel a tomato?\")       # milliseconds\n</code></pre> <p>You can make the cache semantically-aware as well:</p> <pre><code>langchain.llm_cache = CassandraSemanticCache(\nsession=session,\nkeyspace=keyspace,\nembedding=myEmbedding,\n)\nllm(\"What is the best way to peel a tomato?\")       # 1-2 seconds\n...\nllm(\"Tell me how do I best peel tomatoes.\")         # milliseconds\n</code></pre>"},{"location":"pages/aiml/llm/langchain/#prompt-management","title":"Prompt management","text":"<p>You can attach one or more Astra DB tables to a prompt template, so that at \"rendering time\" the DB is queried and the relevant values are injected into the prompt with minimal boilerplate:</p> <pre><code>ctemplate0 = \"\"\"\nYou are helpful a tech support chatbot providing assistance to a human user.\nThe user's name is {user_name}, from this city: {user_city}.\nPlease provide an answer to the user's question below.\nUSER: {user_question}\nYOUR ANSWER:\"\"\"\ncassPrompt = createCassandraPromptTemplate(\nsession=session,\nkeyspace=keyspace,\ntemplate=ctemplate0,\ninput_variables=['user_id', 'user_question'],\nfield_mapper={\n# template-variable: (table-name, column-name)\n'user_name': ('users', 'u_name'),\n'user_city': ('users', 'u_city'),\n},\n)\n...\nfinalPrompt = cassPrompt.format(\nuser_id='fc4ab05',\nuser_question='How do I ...',\n)\n</code></pre>"},{"location":"pages/aiml/llm/langchain/#find-out-more","title":"Find out more!","text":"<p>You just caught a glimpse of what is possible with LangChain and Astra DB.</p> <p>Do you want to know more? Visit cassio.org for full tutorials, setup guides and other resources.</p>"},{"location":"pages/aiml/llm/llamaindex/","title":"LlamaIndex","text":""},{"location":"pages/aiml/llm/llamaindex/#overview","title":"Overview","text":"<p>LlamaIndex, formerly GPT Index, is a Python data framework designed to manage and structure LLM-based applications, with a particular emphasis on storage, indexing and retrieval of data.</p> <p>LlamaIndex provides a complete set of tools to automate tasks such as data ingestion from heterogeneous sources (PDF files, Web pages, ...) and retrieval-augmented generation (RAG); it also features a rich ecosystem of plugins that make it possible to connect with third-party components, from vector stores to data readers.</p> <p>Reference documentation:</p> <ul> <li>\u2139\ufe0f LlamaIndex documentation</li> <li>\u2139\ufe0f CassIO</li> </ul>"},{"location":"pages/aiml/llm/llamaindex/#reasons-for-using-lamaindex","title":"Reasons for using LamaIndex","text":"<p>When working with LLMs, one often needs to augment the power of a model (which comes equipped with general knowledge already) by supplying domain-specific, possibly proprietary data, such as internal reports, a corpus of PDF presentations, and so on.</p> <p>Effective, reproducible management of this domain-specific data needed to augment the LLMs requires a fair amount of machinery, which is precisely what LlamaIndex offers: the framework supports several indexing and retrieval techniques, ranging from simple to quite sophisticated, and offers plugin for seamless integration with a variety of third-party products.</p>"},{"location":"pages/aiml/llm/llamaindex/#astra-db-llamaindex","title":"Astra DB + LlamaIndex","text":"<p>One of the important abstractions in LlamaIndex is that of the \"vector store\", a general interface which allows usage of most vector-capable storage systems within LlamaIndex.</p> <p>Recently, support for Astra DB and Apache Cassandra\u00ae, as one of the available vector stores, was introduced. This makes it possible to run advanced LLM workloads natively in LlamaIndex while at the same time using Astra DB as the storage backend.</p> <p>The Astra integration for LlamaIndex is powered by the open-source <code>cassIO</code> library, which provides a set of standardized facilities to interact with Astra DB (and Cassandra) through the patterns typically needed by ML/LLM applications.</p>"},{"location":"pages/aiml/llm/llamaindex/#vector-store","title":"Vector Store","text":"<p>Creating a vector store in Astra with the LlamaIndex abstractions is as simple as these few lines of Python code:</p> <pre><code>from llama_index.vector_stores import CassandraVectorStore\nvector_store = CassandraVectorStore(\nsession=session,\nkeyspace=keyspace,\ntable=table_name,\nembedding_dimension=vector_dimension,\n)\n</code></pre> <p>You can then use the store within most LlamaIndex native higher-level abstractions, such as <code>VectorStoreIndex</code>, and within complex LLM-based pipelines, such as the \"query engines\" to run question-answering interactions, in just a few lines of code. Metadata filtering is also supported for vector-based document retrieval.</p> <p>For full working examples, and more, please head over to the CassIO pages dedicated to LlamaIndex.</p>"},{"location":"pages/aiml/llm/llamaindex/#future-integrations","title":"Future integrations","text":"<p>Other LlamaIndex integrations with Astra DB / Cassandra are planned, such as a \"table reader\" for seamless use within the rest of the LlamaIndex ecosystem.</p>"},{"location":"pages/aiml/llm/llamaindex/#find-out-more","title":"Find out more!","text":"<p>You just caught a glimpse of what is possible with LlamaIndex and Astra DB.</p> <p>Do you want to know more? Visit cassio.org for full tutorials, setup guides and other resources.</p>"},{"location":"pages/aiml/llm/vector_demos/","title":"Vector Demos","text":"<p>Vector Search and GenAI, a curated collection of demo notebooks and apps.</p>"},{"location":"pages/aiml/llm/vector_demos/#notebooks","title":"Notebooks","text":"<p>This is a collection of quickstarts and tutorials, available either as stand-alone Jupyter or Google Colab notebooks (usually both), and involving Astra DB and generative AI -- specifically, vector-search-powered use cases.</p> Overview Prerequisites Links Learn about using Vector Search to find content related to a query, and then pass that to an LLM to understand how the RAG pattern works for AI powered chatbots. You will need an Astra account with a Serverless Cassandra with Vector Search database. Moreover, an OpenAI API Key is required.  Or, download the notebook. Learn how to use Vector Similarity Search to find images based on natural language descriptions using CLIP. You will need an Astra account with a Serverless Cassandra with Vector Search database.  Or, download the notebook. Try a simple Question-Answering demo powered by a vector-capable database instance. You will use the Astra integration for LangChain and OpenAI for the embeddings and the LLM (Large-Language-Model). You will need an Astra account with a Serverless Cassandra with Vector Search database. Moreover, an OpenAI API Key is required.  Or, download the notebook. Understand Vector Search and the RAG pattern by building a simple generator of \"philosophical quotes\" which uses Astra for storage and OpenAI for retrieval+generation. You will need an Astra account with a Serverless Cassandra with Vector Search database. Moreover, an OpenAI API Key is required. Notebooks (including Colab links) hosted by openai-cookbook. <p>If you open the notebooks in Colab, and would like to make changes to them, choose \"Save a copy in Drive\" from the File menu in Colab.</p>"},{"location":"pages/aiml/llm/vector_demos/#sample-applications","title":"Sample applications","text":"<p>Check this section for full applications making use of Vector Search, Astra DB and other GenAI technologies.</p> <ul> <li>AI-powered \"Hotel search demo\" (uses Vector Search and GenAI personalization; has a one-click \"open in Gitpod\" button)</li> <li>AI-powered \"FLARE QA on PDF files\" application (vector-search-based question-answering client+API setup; has a one-click \"open in Gitpod\" button)</li> </ul>"},{"location":"pages/astra/","title":"Getting Started With Astra","text":""},{"location":"pages/astra/#greetings-developers","title":"Greetings Developers !","text":"<p>Datastax created Astra which provides Apache Cassandra databases in the cloud. You can start for free with no credit card and no time limits.</p> <p>A lot of effort has been put to ease the usage of the database with some developer friendly apis.</p> <p> <p></p> <p></p>"},{"location":"pages/astra/#whats-next","title":"What's NEXT ?","text":"<p>With your database running, you may want to learn more about Apis exposed by running Quick Start guides.</p> <p>info</p> <p>The Getting started guides for each Api are being created as we speak, please come back in a week to get them.</p> <p>Then when you feel confident, pick the language of your choice in friendly apis and start building amazing apps.</p>"},{"location":"pages/astra/astra-cli/","title":"\u2023 Astra CLI","text":""},{"location":"pages/astra/astra-cli/#overview","title":"Overview","text":"<p>CLI Latest version : </p> <p>Astra CLI is a command-line interface (CLI) tool that enables users to interact with and manage the  Astra database-as-a-service platform. It provides developers with a streamlined way to create, configure, and  manipulate databases, making it easier to integrate Astra into their applications and workflows.</p> <p>Astra CLI goes beyond basic database management by facilitating the seamless integration of external tools like dsbulk,  cqlsh, and pulshar-shell or by providing useful shortcuts like dotenv file generation.</p>"},{"location":"pages/astra/astra-cli/#1-installation","title":"1. Installation","text":"<p>This software has been developed and packaged as a Java application. As such, it can be installed on any machine with a JVM. </p> <p>But it has also been compiled as native executables with GraalVM and  native executables for <code>Linux</code> and <code>osx</code> are available.</p>"},{"location":"pages/astra/astra-cli/#11-prerequisites","title":"1.1. Prerequisites","text":"Setup Datastax <code>Astra DB</code> <ul> <li> Create your DataStax Astra account: </li> </ul> <p>Sign Up</p> <ul> <li> Create an Astra Token</li> </ul> <p>An astra token acts as your credentials, it holds the different permissions. The scope of a token is the whole organization (tenant) but permissions can be edited to limit usage to a single database.</p> <p>To create a token, please follow this guide</p> <p>The Token is in fact three separate strings: a <code>Client ID</code>, a <code>Client Secret</code> and the <code>token</code> proper. You will need some of these strings to access the database, depending on the type of access you plan. Although the Client ID, strictly speaking, is not a secret, you should regard this whole object as a secret and make sure not to share it inadvertently (e.g. committing it to a Git repository) as it grants access to your databases.</p> <pre><code>{\n\"ClientId\": \"ROkiiDZdvPOvHRSgoZtyAapp\",\n\"ClientSecret\": \"fakedfaked\",\n\"Token\":\"AstraCS:fake\"\n}\n</code></pre> Third-party tools Requirements <ul> <li>To run the vast majority of the commands you do not need special software but some commands   require external tools like <code>dsbulk</code>, <code>cqlsh</code>, or <code>pulsar-shell</code>. If you want to use those commands, you need to    install relevant dependencies:</li> </ul> Command Tools Dependencies <code>db cqlsh</code> <code>cqlsh</code> Python version 3.6+ <code>db load,unload,count</code> <code>dsbulk</code> Java version 8+ <code>streaming pulsar-shell</code> <code>pulsar-shell</code> Java version 8+"},{"location":"pages/astra/astra-cli/#12-installation-on-linux","title":"1.2. Installation on <code>Linux</code>","text":"Installation or update <ul> <li> Installation and update use the same procedure. </li> </ul> <pre><code>curl -Ls \"https://dtsx.io/get-astra-cli\" | bash\n</code></pre> <ul> <li> <p>The CLI is installed in <code>~/.astra</code> and the binary is in <code>~/.astra/bin/astra</code>. </p> </li> <li> <p>The <code>~/.astra/bin</code> folder is added to the <code>PATH</code> environment variable. The folder contains the  script allowing bash auto-completion.</p> </li> <li> <p>The configuration of the CLI is stored in <code>~/.astrarc</code> - It is not lost on updates.</p> </li> </ul> Uninstall <ul> <li> Remove folder where the files are installed.</li> </ul> <pre><code>rm -R ~/.astra\n</code></pre> <ul> <li> Remove the configuration file.</li> </ul> <pre><code>rm ~/.astrarc\n</code></pre> <ul> <li> Edit <code>~/.bash_profile</code> or <code>~/.zhrc</code> to remove reference to CLI</li> </ul> <pre><code>#THIS MUST BE AT THE END OF THE FILE FOR ASTRA_CLI TO WORK!!!\nexport ASTRADIR=\"/Users/&lt;your_user&gt;/.astra/cli\"\n[[ -s \"/Users/&lt;your_user&gt;/.astra/cli/astra-init.sh\" ]] &amp;&amp; source \"/Users/&lt;your_user&gt;/.astra/cli/astra-init.sh\"\n</code></pre>"},{"location":"pages/astra/astra-cli/#13-installation-on-mac-osx","title":"1.3. Installation on MAC <code>osx</code>","text":"Installation or update <p>Homebrew is the recommended solution to manage Astra CLI on <code>osX</code>. It provides convenient ways to install, update, and uninstall. It will install tools required for external components as well like proper Java and python versions.</p> <ul> <li> Install with</li> </ul> <pre><code>brew install datastax/astra-cli/astra-cli\n</code></pre> <ul> <li> <p><code>osx</code> is based on a linux kernel and can run shell scripts.  The Linux installation procedure also work on mac machines.</p> </li> <li> <p>Files are installed in <code>/usr/local/Cellar/astra-cli/&lt;version&gt;</code> and linked with a symbolic link to <code>/usr/local/bin/astra</code>.</p> </li> <li> <p>The configuration of the CLI is stored in <code>~/.astrarc</code> - **It is not lost on updates</p> </li> <li> <p> LinuxBrew is not supported: The binary installed by the brew fomulae is a native executable for <code>osx</code>  and will not work on <code>Linux</code>. To install on linux please use the  Linux installation procedure.</p> </li> </ul> Updates <ul> <li> Update with</li> </ul> <pre><code>brew upgrade datastax/astra-cli/astra-cli\n</code></pre> Uninstall <ul> <li> Remove CLI files with:</li> </ul> <pre><code>brew uninstall datastax/astra-cli/astra-cli\n</code></pre> <ul> <li> Remove the configuration file.</li> </ul> <pre><code>rm ~/.astrarc\n</code></pre>"},{"location":"pages/astra/astra-cli/#14-installation-on-windows","title":"1.4. Installation on <code>Windows</code>","text":"Installation <p>To install the CLI on <code>Windows</code> you will leverage the WSL or Windows Subsystem for Linux.   Enable the windows subsystem for Linux option in settings.</p> <ul> <li> <p> Go to Start. Search for \"Turn Windows features on or off.</p> </li> <li> <p> Check the option Windows Subsystem for Linux.</p> </li> </ul> <p></p> <ul> <li> Open a terminal and run the linux command</li> </ul> <pre><code>curl -Ls \"https://dtsx.io/get-astra-cli\" | bash\n</code></pre>"},{"location":"pages/astra/astra-cli/#15-installation-with-java","title":"1.5. Installation with <code>Java</code>","text":"Installation <ul> <li> <p> Download the latest version of the CLI from the release page.         For example for the version <code>0.4</code> you can download the file <code>astra-cli-0.4.zip</code> from:   <pre><code>https://github.com/datastax/astra-cli/releases/download/0.4/astra-cli-0.4.zip\n</code></pre></p> </li> <li> <p> Validate that Java 17 is installed.If not, download and install it from Oracle</p> </li> </ul> <pre><code>java -version\n</code></pre> <ul> <li> Unzip the archive in destination folder. </li> </ul> <p>You should find a file called <code>astra-cli.jar</code>. It is a fat jar with all the dependencies.</p> <ul> <li> Run the CLI with the following command.</li> </ul> <pre><code>java -jar ./astra-cli.jar\n</code></pre> <ul> <li> You can use all options as suffix on this command.</li> </ul> <pre><code>java -jar ./astra-cli.jar db list --token AstraCS:...\n</code></pre>"},{"location":"pages/astra/astra-cli/#16-docker-image","title":"1.6. Docker Image","text":"Execution <p>Prerequisite: you need to have docker installed on your machine. If not, please follow the instructions on docker website</p> <ul> <li> Run the CLI with the command <code>docker run</code> and the image <code>clunven/astra-cli</code>.</li> </ul> <pre><code>docker run clunven/astra-cli ?\n</code></pre> <ul> <li> Run a command  to a organization: As the container is stateless you need to provide a token as an input </li> </ul> <pre><code>docker run clunven/astra-cli db list --token AstraCS:...\n</code></pre> <ul> <li> You can also mount a volume to store the configuration file.</li> </ul> <pre><code>docker run -v ~/.astrarc:/work/?/.astrarc clunven/astra-cli db list\n</code></pre>"},{"location":"pages/astra/astra-cli/#2-getting-started","title":"2. Getting Started","text":""},{"location":"pages/astra/astra-cli/#21-setup","title":"2.1. Setup","text":"Make sur to have <code>astra</code> in your path <p>After installation you need to open a new terminal for <code>astra</code> to be in your path.</p> <p>\u2705 Issue <code>setup</code> command and provide your token when prompted. It must start by <code>AstraCS:...</code>. Make sure to have the <code>Organization Administrator</code> role to avoid any permission limitations later on.</p> <pre><code>astra setup\n</code></pre> \ud83d\udda5\ufe0f <code>astra setup</code> command output <pre><code> _____            __                  \n/  _  \\   _______/  |_____________    \n/  /_\\  \\ /  ___/\\   __\\_  __ \\__  \\  \n/    |    \\\\___ \\  |  |  |  | \\ //__ \\_ \n\\____|__  /____  &gt; |__|  |__|  (____  /\n        \\/     \\/                   \\/ \n\n                        Version: 0.2.2\n\n-----------------------\n---      SETUP      ---\n-----------------------\n\n$ Enter an Astra token:\n</code></pre> Skip interactive mode <p>This can also be done non-interactively (for example, using a script) by providing token as a flag. Do do it, run:</p> <pre><code>astra setup --token AstraCS:******\n</code></pre> <p>\u2705 Display your local configuration list, validating setup is complete. </p> <pre><code>astra config list\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>+-----------------------------------------+\n| configuration                           |\n+-----------------------------------------+\n| default (cedrick.lunven@datastax.com)   |\n| cedrick.lunven@datastax.com             |\n+-----------------------------------------+\n</code></pre> <p>You can work with multiple organizations and swap from one to another. Creating and managing extra configurations is covered in Advanced Configuration chapter.</p> Scope of Astra security tokens <p>The security tokens are created for an organization only. If you need to work with multiple organizations then multiple tokens are required. You limit the scope of a token to a single database.</p>"},{"location":"pages/astra/astra-cli/#22-autocompletion","title":"2.2. Autocompletion","text":"<p>The cli provides bash autocompletion for <code>bash</code> and <code>zsh</code> shells. Use <code>TAB</code> key twice to get a list of available options.</p> <p>\u2705 Autocomplete</p> <pre><code>astra &lt;TAB&gt; &lt;TAB&gt;\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>--no-color  config      db          help        role        setup       shell       user  \n</code></pre>"},{"location":"pages/astra/astra-cli/#23-documentation","title":"2.3. Documentation","text":"<p>The better documentation of the code is the code itself. This page will provide you samples but where you are not sure use the <code>astra help &lt;my_command&gt;</code></p> <p>\u2705 Display main help</p> <pre><code>astra help\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>usage: astra &lt;command&gt; [ &lt;args&gt; ]\n\nCommands are:\n    ?           Display this help version\n    help        Display help information\n    setup       Initialize configuration file\n    config      Manage configuration file\n    db          Manage databases\n    org         Display Organization Info\n    role        Manage roles\n    streaming   Manage Streaming tenants\n    token       Manage tokens\n    user        Manage users\n\nSee 'astra help &lt;command&gt;' for more information on a specific command.\n</code></pre> <p>\u2705 Display help for a command group <code>astra db</code></p> <pre><code>astra help db\n</code></pre> <p>\u2705 Display help for unitary command <code>astra db list</code></p> <pre><code>astra help db list\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>NAME\n        astra db list - Display the list of Databases in an organization\n\nSYNOPSIS\n        astra db list [ {-conf | --config} &lt;CONFIG_SECTION&gt; ]\n                [ --config-file &lt;CONFIG_FILE&gt; ] [ --log &lt;LOG_FILE&gt; ]\n                [ --no-color ] [ {-o | --output} &lt;FORMAT&gt; ]\n                [ --token &lt;AUTH_TOKEN&gt; ] [ {-v | --verbose} ]\n\nOPTIONS\n        -conf &lt;CONFIG_SECTION&gt;, --config &lt;CONFIG_SECTION&gt;\n            Section in configuration file (default = ~/.astrarc)\n\n        --config-file &lt;CONFIG_FILE&gt;\n            Configuration file (default = ~/.astrarc)\n\n        --log &lt;LOG_FILE&gt;\n            Logs will go in the file plus on console\n\n        --no-color\n            Remove all colors in output\n\n        -o &lt;FORMAT&gt;, --output &lt;FORMAT&gt;\n            Output format, valid values are: human,json,csv\n\n        --token &lt;AUTH_TOKEN&gt;\n            Key to use authenticate each call.\n\n        -v, --verbose\n            Verbose mode with log in console\n</code></pre>"},{"location":"pages/astra/astra-cli/#24-important-options","title":"2.4. Important Options","text":"<p>Each commands has some specific parameters but all commands share have the following options.</p> Name Option Description verbose <code>-v</code> Make the output more verbose, debug remove colors <code>--no-color</code> Remove colors, ease parsing and display json output <code>-o json</code> To ease parsing output can be json csv output <code>-o csv</code> To ease export output can be CSV override token <code>--token ...</code> Enforce token for this command override config <code>--config ...</code> Change section use in <code>~/.astrarc</code> for the command override config-file <code>--config-file ...</code> Do not use <code>~/.astrarc</code> for the command"},{"location":"pages/astra/astra-cli/#25-configuration","title":"2.5 Configuration","text":"<p>If you work with multiple organizations, it could be useful to switch from one configuration to another, one token to another. The CLI provides a configuration management solution to handle this use case.</p> <p>\u2705 2.6.a - List available configuration</p> <pre><code>astra config list\n</code></pre> <p>\u2705 2.6.b - Create a new section</p> <pre><code>astra config create dev --token &lt;token_of_org_2&gt;\n</code></pre> <p>\u2705 2.6.c - Use your section config anywhere</p> <p>You can use any organization anytime with <code>--config &lt;onfig_name&gt;</code>.</p> <pre><code>astra user list --config dev\n</code></pre> <p>\u2705 2.6.d - Select a section as defaul</p> <ul> <li>Change the current org</li> </ul> <pre><code>astra config use dev\n</code></pre> <ul> <li>See your new list </li> </ul> <pre><code>astra config list\n</code></pre> <p>\u2705 1e - Delete a section</p> <p>You can delete any organization. If you delete the selected organization you will have to pick a new one.</p> <ul> <li> <p>Delete you config <pre><code>astra config delete dev\n</code></pre></p> </li> <li> <p>See the new list</p> </li> </ul> <pre><code>astra config list\n</code></pre>"},{"location":"pages/astra/astra-cli/#3-astra-db","title":"3. Astra DB","text":"Synchronous/Asynchronous <p>Some commands can take a while like a DB creation or the creation of a new region and data replication. Default behaviour is to be <code>synchronous</code> and <code>wait</code> until the operation is done. You can change this behaviour by using the option <code>--no-wait</code></p>"},{"location":"pages/astra/astra-cli/#31-db-commands-glossary","title":"3.1. DB commands glossary","text":"<p>\u2705 Display available commands for DB</p> <p>The documentation is the tool itself with the following command:</p> <p><pre><code>astra help db\n</code></pre> Still, for ease of use here is a glossary of the commands:</p> Command Purpose <code>count &lt;DB&gt;</code> Count records in a table <code>cqlsh &lt;DB&gt;</code> Setup and run <code>cqlsh</code>: interactive, <code>-e</code> and <code>-f</code> all supported <code>create &lt;DB&gt;</code> Create a database <code>create-cdc &lt;DB&gt;</code> Create Change Data Capture to Pulsar <code>create-dotenv &lt;DB&gt;</code> Create environment file <code>.env</code> <code>create-keyspace &lt;DB&gt;</code> Create a keyspace <code>create-region &lt;DB&gt;</code> Expand database to a rew region (multi-region) <code>delete &lt;DB&gt;</code> Delete a database <code>delete-cdc &lt;DB&gt;</code> Delete a change data capture <code>delete-keyspace &lt;DB&gt;</code> Delete a keyspace <code>delete-region &lt;DB&gt;</code> Remove a region from a database <code>describe &lt;DB&gt;</code> Describe metadata of a database <code>download-scb &lt;DB&gt;</code> Download the secure connect bundle for database region <code>get &lt;DB&gt;</code> Describe metadata of a database <code>list</code> List databases in the organization <code>list-cdc &lt;DB&gt;</code> List Change Data Captures <code>list-clouds</code> List clouds available to deploy db <code>list-keyspaces &lt;DB&gt;</code> List keyspaces for a DB <code>list-regions &lt;DB&gt;</code> List regions (datacenters) for a DB <code>list-regions-classic</code> List available regions for classic <code>list-regions-serverless</code> List available regions for serverless <code>load &lt;DB&gt;</code> Load a CSV into a table <code>playground &lt;DB&gt;</code> Show GraphQL Playground URL <code>resume &lt;DB&gt;</code> Resume DB that was hibernated <code>status &lt;DB&gt;</code> Show DB Status <code>swagger &lt;DB&gt;</code> Show swagger url <code>unload &lt;DB&gt;</code> Leverage DSbulk to dump data"},{"location":"pages/astra/astra-cli/#32-list","title":"3.2. List","text":"<p>\u2705 3.2.a - list</p> <p>To get the list of non-terminated databases in your organization, use the command <code>list</code> in the group <code>db</code>.</p> <pre><code>astra db list\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>+---------------------+--------------------------------------+---------------------+----------------+\n| Name                | id                                   | Default Region      | Status         |\n+---------------------+--------------------------------------+---------------------+----------------+\n| mtg                 | dde308f5-a8b0-474d-afd6-81e5689e3e25 | eu-central-1        | ACTIVE         |\n| workshops           | 3ed83de7-d97f-4fb6-bf9f-82e9f7eafa23 | eu-west-1           | ACTIVE         |\n| sdk_tests           | 06a9675a-ca62-4cd0-9b94-aefaf395922b | us-east-1           | ACTIVE         |\n| test                | 7677a789-bd57-455d-ab2c-a3bdfa35ba68 | eu-central-1        | ACTIVE         |\n| demo                | 071d7059-d55b-4cdb-90c6-41c26da1a029 | us-east-1           | ACTIVE         |\n| ac201               | 48c7178c-58cb-4657-b3d2-8a9e3cc89461 | us-east-1           | ACTIVE         |\n+---------------------+--------------------------------------+---------------------+----------------+\n</code></pre> <p>\u2705 3.2.b - Get Help</p> <p>To get help on a command, always prefix with <code>astra help XXX</code></p> <pre><code>astra help db list\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>NAME\n        astra db list - Display the list of Databases in an organization\n\nSYNOPSIS\n        astra db list [ {-conf | --config} &lt;CONFIG_SECTION&gt; ]\n                [ --config-file &lt;CONFIG_FILE&gt; ] [ --log &lt;LOG_FILE&gt; ]\n                [ --no-color ] [ {-o | --output} &lt;FORMAT&gt; ]\n                [ --token &lt;AUTH_TOKEN&gt; ] [ {-v | --verbose} ]\n\nOPTIONS\n        -conf &lt;CONFIG_SECTION&gt;, --config &lt;CONFIG_SECTION&gt;\n            Section in configuration file (default = ~/.astrarc)\n\n        --config-file &lt;CONFIG_FILE&gt;\n            Configuration file (default = ~/.astrarc)\n\n        --log &lt;LOG_FILE&gt;\n            Logs will go in the file plus on console\n\n        --no-color\n            Remove all colors in output\n\n        -o &lt;FORMAT&gt;, --output &lt;FORMAT&gt;\n            Output format, valid values are: human,json,csv\n\n        --token &lt;AUTH_TOKEN&gt;\n            Key to use authenticate each call.\n\n        -v, --verbose\n            Verbose mode with log in console\n</code></pre> <p>\u2705 3.2.c - Change output</p> <pre><code>astra db list -o csv\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>Name,id,Default Region,Status\nmtg,dde308f5-a8b0-474d-afd6-81e5689e3e25,eu-central-1,ACTIVE\nworkshops,3ed83de7-d97f-4fb6-bf9f-82e9f7eafa23,eu-west-1,ACTIVE\nsdk_tests,06a9675a-ca62-4cd0-9b94-aefaf395922b,us-east-1,ACTIVE\ntest,7677a789-bd57-455d-ab2c-a3bdfa35ba68,eu-central-1,ACTIVE\ndemo,071d7059-d55b-4cdb-90c6-41c26da1a029,us-east-1,ACTIVE\nac201,48c7178c-58cb-4657-b3d2-8a9e3cc89461,us-east-1,ACTIVE\n</code></pre> <pre><code>astra db list -o json\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>  {\n\"code\" : 0,\n\"message\" : \"astra db list -o json\",\n\"data\" : [ {\n\"Status\" : \"ACTIVE\",\n\"Default Region\" : \"eu-central-1\",\n\"id\" : \"dde308f5-a8b0-474d-afd6-81e5689e3e25\",\n\"Name\" : \"mtg\"\n}, {\n\"Status\" : \"ACTIVE\",\n\"Default Region\" : \"eu-west-1\",\n\"id\" : \"3ed83de7-d97f-4fb6-bf9f-82e9f7eafa23\",\n\"Name\" : \"workshops\"\n}, {\n\"Status\" : \"ACTIVE\",\n\"Default Region\" : \"us-east-1\",\n\"id\" : \"06a9675a-ca62-4cd0-9b94-aefaf395922b\",\n\"Name\" : \"sdk_tests\"\n}, {\n\"Status\" : \"ACTIVE\",\n\"Default Region\" : \"eu-central-1\",\n\"id\" : \"7677a789-bd57-455d-ab2c-a3bdfa35ba68\",\n\"Name\" : \"test\"\n}, {\n\"Status\" : \"ACTIVE\",\n\"Default Region\" : \"us-east-1\",\n\"id\" : \"071d7059-d55b-4cdb-90c6-41c26da1a029\",\n\"Name\" : \"demo\"\n}, {\n\"Status\" : \"ACTIVE\",\n\"Default Region\" : \"us-east-1\",\n\"id\" : \"48c7178c-58cb-4657-b3d2-8a9e3cc89461\",\n\"Name\" : \"ac201\"\n} ]\n}\n</code></pre>"},{"location":"pages/astra/astra-cli/#33-create-database","title":"3.3. Create database","text":"<p>\u2705 3.3.a - Create Database </p> <p>If not provided, the region will be the default free region, and the keyspace will be the database name, but you can change them with the <code>-r</code> and <code>-k</code> flags, respectively.</p> <pre><code>astra db create demo\n</code></pre> <p>\u2705 3.3.b - Options <code>--if-not-exist</code> and <code>--wait</code> </p> <ul> <li> <p>The database name does not ensure unicity (the database id does). As such, if you issue the command multiple times, you will end up with multiple instances. To change this behavior, you can use <code>--if-not-exist</code></p> </li> <li> <p>Database creation is an asynchronous operation. In some situations, such as during your CI/CD, you will most likely want the db to be <code>ACTIVE</code> before moving forward. The option <code>--wait</code> will trigger a blocking command until the db is ready</p> </li> <li> <p>On the free tier, after a period of inactivity, the database moves to a <code>HIBERNATED</code> state. The creation command will resume the db when needed.</p> </li> </ul> <pre><code>astra db create demo -k ks2 --if-not-exist --wait\n</code></pre> <p>\u2705 3.3.c - Get help </p> <p>To show help, enter the following command: </p> <pre><code>astra help db create\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>NAME\n        astra db create - Create a database with cli\n\nSYNOPSIS\n        astra db create [ {-cf | --config-file} &lt;CONFIG_FILE&gt; ]\n                [ {-conf | --config} &lt;CONFIG_SECTION&gt; ]\n                [ {--if-not-exist | --if-not-exists} ]\n                [ {-k | --keyspace} &lt;KEYSPACE&gt; ] [ --no-color ]\n                [ {-o | --output} &lt;FORMAT&gt; ] [ {-r | --region} &lt;DB_REGION&gt; ]\n                [ --timeout &lt;timeout&gt; ] [ --token &lt;AUTH_TOKEN&gt; ]\n                [ {-v | --verbose} ] [ --wait ] [--] &lt;DB&gt;\n\nOPTIONS\n        -cf &lt;CONFIG_FILE&gt;, --config-file &lt;CONFIG_FILE&gt;\n            Configuration file (default = ~/.astrarc)\n\n        -conf &lt;CONFIG_SECTION&gt;, --config &lt;CONFIG_SECTION&gt;\n            Section in configuration file (default = ~/.astrarc)\n\n        --if-not-exist, --if-not-exists\n            will create a new DB only if none with same name\n\n        -k &lt;KEYSPACE&gt;, --keyspace &lt;KEYSPACE&gt;\n            Default keyspace created with the Db\n\n        --no-color\n            Remove all colors in output\n\n        -o &lt;FORMAT&gt;, --output &lt;FORMAT&gt;\n            Output format, valid values are: human,json,csv\n\n        -r &lt;DB_REGION&gt;, --region &lt;DB_REGION&gt;\n            Cloud provider region to provision\n\n        --timeout &lt;timeout&gt;\n            Provide a limit to the wait period in seconds, default is 300s.\n\n        --token &lt;AUTH_TOKEN&gt;\n            Key to use authenticate each call.\n\n        -v, --verbose\n            Verbose mode with log in console\n\n        --wait\n            Will wait until the database become ACTIVE\n\n        --\n            This option can be used to separate command-line options from the\n            list of arguments (useful when arguments might be mistaken for\n            command-line options)\n\n        &lt;DB&gt;\n            Database name (not unique)\n</code></pre>"},{"location":"pages/astra/astra-cli/#34-resume-database","title":"3.4. Resume database","text":"<p>In the free tier, after 23H of inactivity, your database will be hibernated. To wake up the db, you can use the <code>resume</code> command.</p> <p>\u2705 3.4.a - Resuming </p> <ul> <li>Assuming you have an hibernating database.</li> </ul> <pre><code>astra db list\n</code></pre> <pre><code>+---------------------+--------------------------------------+---------------------+----------------+\n| Name                | id                                   | Default Region      | Status         |\n+---------------------+--------------------------------------+---------------------+----------------+\n| hemidactylus        | 643c6bb8-2336-4649-97d5-39c33491f5c1 | eu-central-1        | HIBERNATED     |\n+---------------------+--------------------------------------+---------------------+----------------+\n</code></pre> <ul> <li>Trigger an explicit resuming with:</li> </ul> <pre><code>astra db resume hemidactylus\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>+---------------------+--------------------------------------+---------------------+----------------+\n| Name                | id                                   | Default Region      | Status         |\n+---------------------+--------------------------------------+---------------------+----------------+\n| hemidactylus        | 643c6bb8-2336-4649-97d5-39c33491f5c1 | eu-central-1        | RESUMING       |\n+---------------------+--------------------------------------+---------------------+----------------+\n\nAnd after a few time\n+---------------------+--------------------------------------+---------------------+----------------+\n| Name                | id                                   | Default Region      | Status         |\n+---------------------+--------------------------------------+---------------------+----------------+\n| hemidactylus        | 643c6bb8-2336-4649-97d5-39c33491f5c1 | eu-central-1        | ACTIVE         |\n+---------------------+--------------------------------------+---------------------+----------------+\n</code></pre>"},{"location":"pages/astra/astra-cli/#35-get-database-details","title":"3.5. Get database details","text":"<p>\u2705 3.5.a. To get general information or details on an entity use the command <code>get</code>.</p> <pre><code>astra db get demo\n</code></pre> <p>In the output, you specially see the list of keyspaces available and the different regions.</p> \ud83d\udda5\ufe0f Sample output <pre><code>+------------------------+-----------------------------------------+\n| Attribute              | Value                                   |\n+------------------------+-----------------------------------------+\n| Name                   | demo                                    |\n| id                     | 071d7059-d55b-4cdb-90c6-41c26da1a029    |\n| Status                 | ACTIVE                                  |\n| Default Cloud Provider | AWS                                     |\n| Default Region         | us-east-1                               |\n| Default Keyspace       | demo                                    |\n| Creation Time          | 2022-07-26T15:41:18Z                    |\n|                        |                                         |\n| Keyspaces              | [0] demo                                |\n|                        |                                         |\n| Regions                | [0] us-east-1                           |\n+------------------------+-----------------------------------------+\n</code></pre> <p>\u2705 3.5.b. To get a special property, you can add the option <code>--key</code>. Multiple keys are available: <code>id</code>, <code>status</code>, <code>cloud</code>, <code>keyspace</code>, <code>keyspaces</code>, <code>region</code>, <code>regions</code>. Notice that the output is raw. This command is expected to be used in scripts</p> <pre><code>astra db get demo --key id\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>dde308f5-a8b0-474d-afd6-81e5689e3e25\n</code></pre> <p>\u2705 3.5.c. To get database status in a human-readable form, use <code>status</code> command</p> <pre><code>astra db status demo\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>[ INFO ] - Database 'demo' has status 'ACTIVE'\n</code></pre>"},{"location":"pages/astra/astra-cli/#36-delete-database","title":"3.6. Delete Database","text":"<p>\u2705 3.6.a. To delete a db use the command <code>delete</code>.</p> <pre><code>astra db delete demo2\n</code></pre>"},{"location":"pages/astra/astra-cli/#37-working-with-keyspaces","title":"3.7. Working with keyspaces","text":"<p>A keyspace is created when you create the database. The default CLI behaviour is to provide the same values for keyspace and database names. You can also define your own keyspace name with the flag <code>-k</code>.</p> <p>\u2705 3.7.a. Create new keyspace </p> <ul> <li>To add a keyspace <code>ks2</code> to an existing database <code>demo</code> use the following. The option <code>--if-not-exist</code> is optional but could help you provide idempotent scripts.</li> </ul> <pre><code>astra db create-keyspace demo -k ks2 --if-not-exist\n</code></pre> <ul> <li>If the database is not found, you will get a warning message and a dedicated code returned. To see your new keyspace, you can display your database details.</li> </ul> <pre><code>astra db list-keyspaces demo\n</code></pre> <p>\u2705 3.7.b Get help </p> <pre><code>astra help db create-keyspace\n</code></pre>"},{"location":"pages/astra/astra-cli/#38-cqlsh","title":"3.8. Cqlsh","text":"<p>Cqlsh is a standalone shell to work with Apache Cassandra\u2122. It is compliant with Astra but requires a few extra steps of configuration. The purpose of the CLI is to integrate with <code>cqlsh</code> and do the integration for you.</p> <p>Astra CLI will download, install, setup and wrap <code>cqlsh</code> for you to interact with Astra.</p> <p>\u2705 3.8.a - Interactive </p> <p>If no options are provided,  you enter <code>cqlsh</code> interactive mode</p> <pre><code>astra db cqlsh demo\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>Cqlsh is starting please wait for connection establishment...\nConnected to cndb at 127.0.0.1:9042.\n[cqlsh 6.8.0 | Cassandra 4.0.0.6816 | CQL spec 3.4.5 | Native protocol v4]\nUse HELP for help.\ntoken@cqlsh&gt;\n</code></pre> <p>\u2705 3.8.b - Execute CQL </p> <p>To execute CQL Statements with <code>cqlsh</code> use the flag <code>-e</code>.</p> <pre><code>astra db cqlsh demo -e \"describe keyspaces;\"\n</code></pre> <p>\u2705 3.8.c - Execute CQL Files </p> <p>To execute CQL Files with <code>cqlsh</code> use the flag <code>-f</code>. You could also use the CQL syntax SOURCE.</p> <pre><code>astra db cqlsh demo -f sample.cql\n</code></pre>"},{"location":"pages/astra/astra-cli/#39-load-and-unload","title":"3.9. Load and Unload","text":"<p>\u2705 3.9.a - Setup </p> <p>DSBulk stands for DataStax Bulk Loader. It is a standalone program to load, unload, and count data in an efficient way with Apache Cassandra\u2122. It is compliant with DataStax Astra DB.</p> <p>Similar to <code>cqlsh</code> the CLI will download, install, setup and wrap the dsbulk command for you. All options are available. To give you an idea, let's take a simple example.</p> <ul> <li>Make sure we have a db <code>demo</code> with a keyspace <code>demo</code></li> </ul> <pre><code>astra db create demo\n</code></pre> <ul> <li>Looking at a dataset of cities in the world. cities.csv. We can show here the first lines of the  file.</li> </ul> <pre><code>id,name,state_id,state_code,state_name,country_id,country_code,country_name,latitude,longitude,wikiDataId\n52,Ashk\u0101sham,3901,BDS,Badakhshan,1,AF,Afghanistan,36.68333000,71.53333000,Q4805192\n68,Fayzabad,3901,BDS,Badakhshan,1,AF,Afghanistan,37.11664000,70.58002000,Q156558\n...\n</code></pre> <ul> <li>Let's create a table to store those values. Connect to CQLSH</li> </ul> <pre><code>astra db cqlsh demo -k demo\n</code></pre> <ul> <li>Create the table </li> </ul> <pre><code>CREATE TABLE cities_by_country (\ncountry_name text,\nname       text,\nid         int,\nstate_id   text,\nstate_code text,\nstate_name text,\ncountry_id text,\ncountry_code text,\nlatitude double,\nlongitude double,\nwikiDataId text,\nPRIMARY KEY ((country_name), name)\n);\ndescribe table cities_by_country;\nquit\n</code></pre> <p>\u2705 3.9.b - Load Data </p> <pre><code>astra db load demo \\\n  -url https://raw.githubusercontent.com/awesome-astra/docs/main/docs/assets/cities.csv \\\n  -k demo \\\n  -t cities_by_country \\\n  --schema.allowMissingFields true\n</code></pre> <p>The first time the line <code>DSBulk is starting please wait</code> can take a few seconds to appear. The reason is that the CLI is downloading <code>dsbulk</code> if it was not downloaded before.</p> \ud83d\udda5\ufe0f Sample output <pre><code>DSBulk is starting please wait ...\nUsername and password provided but auth provider not specified, inferring PlainTextAuthProvider\nA cloud secure connect bundle was provided: ignoring all explicit contact points.\nA cloud secure connect bundle was provided and selected operation performs writes: changing default consistency level to LOCAL_QUORUM.\nOperation directory: /Users/cedricklunven/Downloads/logs/LOAD_20220823-182343-074618\nSetting executor.maxPerSecond not set when connecting to DataStax Astra: applying a limit of 9,000 ops/second based on the number of coordinators (3).\nIf your Astra database has higher limits, please define executor.maxPerSecond explicitly.\n  total | failed | rows/s |  p50ms |  p99ms | p999ms | batches\n148,266 |      0 |  8,361 | 663.86 | 767.56 | 817.89 |   30.91\nOperation LOAD_20220823-182343-074618 completed successfully in 17 seconds.\nLast processed positions can be found in positions.txt\n</code></pre> <p>\u2705 3.9.c - Count </p> <p>Check that the data has been imported with cqlsh.</p> <pre><code>astra db cqlsh demo -e \"select * from demo.cities_by_country LIMIT 20;\"\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>Cqlsh is starting please wait for connection establishment...\n\ncountry_name | name                | country_code | country_id | id   | latitude | longitude | state_code | state_id | state_name          | wikidataid\n--------------+---------------------+--------------+------------+------+----------+-----------+------------+----------+---------------------+------------\n  Bangladesh |             Azimpur |           BD |         19 | 8454 |  23.7298 |   90.3854 |         13 |      771 |      Dhaka District |       null\n  Bangladesh |           Badarganj |           BD |         19 | 8455 | 25.67419 |  89.05377 |         55 |      759 |    Rangpur District |       null\n  Bangladesh |            Bagerhat |           BD |         19 | 8456 |     22.4 |     89.75 |         27 |      811 |     Khulna District |       null\n  Bangladesh |           Bandarban |           BD |         19 | 8457 |       22 |  92.33333 |          B |      803 | Chittagong Division |       null\n  Bangladesh |          Baniachang |           BD |         19 | 8458 | 24.51863 |  91.35787 |         60 |      767 |     Sylhet District |       null\n  Bangladesh |             Barguna |           BD |         19 | 8459 | 22.13333 |  90.13333 |         06 |      818 |    Barisal District |       null\n  Bangladesh |             Barisal |           BD |         19 | 8460 |     22.8 |      90.5 |         06 |      818 |    Barisal District |       null\n  Bangladesh |                Bera |           BD |         19 | 8462 | 24.07821 |  89.63262 |         54 |      813 |   Rajshahi District |       null\n  Bangladesh |       Bhairab B\u0101z\u0101r |           BD |         19 | 8463 |  24.0524 |   90.9764 |         13 |      771 |      Dhaka District |       null\n  Bangladesh |           Bher\u0101m\u0101ra |           BD |         19 | 8464 | 24.02452 |  88.99234 |         27 |      811 |     Khulna District |       null\n  Bangladesh |               Bhola |           BD |         19 | 8465 | 22.36667 |  90.81667 |         06 |      818 |    Barisal District |       null\n  Bangladesh |           Bh\u0101nd\u0101ria |           BD |         19 | 8466 | 22.48898 |  90.06273 |         06 |      818 |    Barisal District |       null\n  Bangladesh | Bh\u0101tp\u0101ra Abhaynagar |           BD |         19 | 8467 | 23.01472 |  89.43936 |         27 |      811 |     Khulna District |       null\n  Bangladesh |           Bibir Hat |           BD |         19 | 8468 | 22.68347 |  91.79058 |          B |      803 | Chittagong Division |       null\n  Bangladesh |               Bogra |           BD |         19 | 8469 | 24.78333 |     89.35 |         54 |      813 |   Rajshahi District |       null\n  Bangladesh |        Brahmanbaria |           BD |         19 | 8470 | 23.98333 |  91.16667 |          B |      803 | Chittagong Division |       null\n  Bangladesh |         Burh\u0101nuddin |           BD |         19 | 8471 | 22.49518 |  90.72391 |         06 |      818 |    Barisal District |       null\n  Bangladesh |            B\u0101jitpur |           BD |         19 | 8472 | 24.21623 |  90.95002 |         13 |      771 |      Dhaka District |       null\n  Bangladesh |            Chandpur |           BD |         19 | 8474 |    23.25 |  90.83333 |          B |      803 | Chittagong Division |       null\n  Bangladesh |    Chapai Nababganj |           BD |         19 | 8475 | 24.68333 |     88.25 |         54 |      813 |   Rajshahi District |       null\n</code></pre> <ul> <li>Count with ds bulkd</li> </ul> <pre><code>astra db count demo -k demo -t cities_by_country\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>DSBulk is starting please wait ...\n[INFO ] - RUNNING: /Users/cedricklunven/.astra/dsbulk-1.9.1/bin/dsbulk count -k demo -t cities_by_country -u token -p AstraCS:gdZaqzmFZszaBTOlLgeecuPs:edd25600df1c01506f5388340f138f277cece2c93cb70f4b5fa386490daa5d44 -b /Users/cedricklunven/.astra/scb/scb_071d7059-d55b-4cdb-90c6-41c26da1a029_us-east-1.zip\nUsername and password provided but auth provider not specified, inferring PlainTextAuthProvider\nA cloud secure connect bundle was provided: ignoring all explicit contact points.\nOperation directory: /Users/cedricklunven/Downloads/logs/COUNT_20220823-182833-197954\n  total | failed | rows/s |  p50ms |  p99ms | p999ms\n134,574 |      0 | 43,307 | 315.71 | 457.18 | 457.18\n</code></pre> <p>\u2705 3.9.d - Unload Data </p> <pre><code>astra db unload demo -k demo -t cities_by_country -url /tmp/unload\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>DSBulk is starting please wait ...\nUsername and password provided but auth provider not specified, inferring PlainTextAuthProvider\nA cloud secure connect bundle was provided: ignoring all explicit contact points.\nOperation directory: /Users/cedricklunven/Downloads/logs/UNLOAD_20220823-183054-208353\n  total | failed | rows/s |  p50ms |    p99ms |   p999ms\n134,574 |      0 | 14,103 | 927.51 | 1,853.88 | 1,853.88\nOperation UNLOAD_20220823-183054-208353 completed successfully in 9 seconds.\n</code></pre>"},{"location":"pages/astra/astra-cli/#310-download-secure-bundle","title":"3.10. Download Secure bundle","text":"<p>\u2705 3.10.a - Default values </p> <p>Download the different secure bundles (one per region) with the pattern <code>scb_${dbid}-${dbregion}.zip</code> in a current folder.</p> <pre><code>mkdir db-demo\ncd db-demo\nastra db download-scb demo\nls\n</code></pre> <p>\u2705 3.10.b - Download in target folder </p> <p>Download the different secure bundles (one per region) with the pattern <code>scb_${dbid}-${dbregion}.zip</code> in the folder provided with option <code>-d</code> (<code>--output-director</code>).</p> <pre><code>astra db download-scb demo -d /tmp\n</code></pre> <p>\u2705 3.10.c - Download in target folder </p> <p>Provide the target filename with <code>-f</code> (<code>--output-file</code>). It will work only if you have a SINGLE REGION for your database (or you will have to use the flag <code>-d</code>)</p> <pre><code>astra db download-scb demo -f /tmp/demo.zip\n</code></pre>"},{"location":"pages/astra/astra-cli/#311-create-env-file","title":"3.11. Create <code>.env</code> file","text":"<p>To code your application against Astra, a set of metadata could be handy like the database name, database region, url of the APIs.... </p> <p>This command will create a file <code>.env</code> with a set of variables that are relevant to be defined as environment variables</p> <pre><code>astra db create-dotenv -f /tmp/.env\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>ASTRA_DB_APPLICATION_TOKEN=\"AstraCS:QeUmROP...\"\nASTRA_DB_GRAPHQL_URL=\"https://a6b5cb4c-3267-4414-8bba-6706086a943a-us-east-1.apps.astra.datastax.com/api/graphql/order_management_data\"\nASTRA_DB_GRAPHQL_URL_ADMIN=\"https://a6b5cb4c-3267-4414-8bba-6706086a943a-us-east-1.apps.astra.datastax.com/api/graphql-admin\"\nASTRA_DB_GRAPHQL_URL_PLAYGROUND=\"https://a6b5cb4c-3267-4414-8bba-6706086a943a-us-east-1.apps.astra.datastax.com/api/playground\"\nASTRA_DB_GRAPHQL_URL_SCHEMA=\"https://a6b5cb4c-3267-4414-8bba-6706086a943a-us-east-1.apps.astra.datastax.com/api/graphql-schema\"\nASTRA_DB_ID=\"a6b5cb4c-3267-4414-8bba-6706086a943a\"\nASTRA_DB_KEYSPACE=\"order_management_data\"\nASTRA_DB_REGION=\"us-east-1\"\nASTRA_DB_REST_URL=\"https://a6b5cb4c-3267-4414-8bba-6706086a943a-us-east-1.apps.astra.datastax.com/api/rest\"\nASTRA_DB_REST_URL_SWAGGER=\"https://a6b5cb4c-3267-4414-8bba-6706086a943a-us-east-1.apps.astra.datastax.com/api/rest/swagger-ui/\"\nASTRA_DB_SECURE_BUNDLE_PATH=\"/Users/cedricklunven/.astra/scb/scb_a6b5cb4c-3267-4414-8bba-6706086a943a_us-east-1.zip\"\nASTRA_DB_SECURE_BUNDLE_URL=\"https://datastax-cluster-config-prod.s3.us-east-2.amazonaws.com/a6b5cb4c-3267-4414-8bba-6706086....X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIA2AI.....\"\nASTRA_ORG_ID=\"f9460f14-9879-....\"\nASTRA_ORG_NAME=\"ced...\"\nASTRA_ORG_TOKEN=\"AstraCS:QeUmROPLeNbd...\"\n</code></pre>"},{"location":"pages/astra/astra-cli/#312-list-regions","title":"3.12. List Regions","text":"<p>For database creation or regions management, the region name is expected. Depending on the cloud provider needed or even the Astra service, the region names are not exactly the same.</p> <p>With Astra CLI, one can list every available regions per service.</p> <p>\u2705 3.12.a - List Serverless regions</p> <pre><code>astra db list-regions-serverless -c aws\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>+----------------+---------------------+-------------------------------+\n| Cloud Provider | Region              | Full Name                     |\n+----------------+---------------------+-------------------------------+\n| aws            | ap-east-1           | Asia Pacific (Hong Kong)      |\n| aws            | ap-south-1          | Asia Pacific (Mumbai)         |\n| aws            | ap-southeast-1      | Asia Pacific (Singapore)      |\n| aws            | ap-southeast-2      | Asia Pacific (Sydney)         |\n| aws            | eu-central-1        | Europe (Frankfurt)            |\n| aws            | eu-west-1           | Europe (Ireland)              |\n| aws            | sa-east-1           | South America (Sao Paulo)     |\n| aws            | us-east-1           | US East (N. Virginia)         |\n| aws            | us-east-2           | US East (Ohio)                |\n| aws            | us-west-2           | US West (Oregon)              |\n+----------------+---------------------+-------------------------------+\n</code></pre> <ul> <li><code>-c</code> or <code>--cloud</code> allows to select a cloud provider, the 3 accepted values will be <code>aws</code>, <code>gcp</code> and <code>azure</code></li> <li><code>-f</code> or <code>--filter</code> allows to look for either a location of region (eg. <code>-f France</code>, -f <code>us</code></li> <li><code>-o</code> or <code>--output</code> to change output from table (human) to csv or json</li> <li><code>-v</code> for verbose mode</li> <li><code>-t</code> to provide token of organization if not default selected</li> </ul> <p>\u2705 3.12.b - List Serverless regions</p> <pre><code>astra db list-regions-classic\n</code></pre>"},{"location":"pages/astra/astra-cli/#4-astra-streaming","title":"4. Astra STREAMING","text":""},{"location":"pages/astra/astra-cli/#41-list-tenants","title":"4.1. List tenants","text":"<p>\u2705 4.1.a - list</p> <p>To get the list of tenants in your organization, use the command <code>list</code> in the group <code>streaming</code>.</p> <pre><code>astra streaming list\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>+---------------------+-----------+----------------+----------------+\n| name                | cloud     | region         | Status         |\n+---------------------+-----------+----------------+----------------+\n| cedrick-20220910    | aws       | useast2        | active         |\n| trollsquad-2022     | aws       | useast2        | active         |\n+---------------------+-----------+----------------+----------------+\n</code></pre> <p>\u2705 4.1.b - Change output as <code>csv</code> amd <code>json</code></p> <pre><code>astra streaming list -o csv\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>name,cloud,region,Status\ncedrick-20220910,aws,useast2,active\ntrollsquad-2022,aws,useast2,active\n</code></pre> <pre><code>astra streaming list -o json\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>{\n\"code\" : 0,\n\"message\" : \"astra streaming list -o json\",\n\"data\" : [ {\n\"cloud\" : \"aws\",\n\"Status\" : \"active\",\n\"name\" : \"cedrick-20220910\",\n\"region\" : \"useast2\"\n}, {\n\"cloud\" : \"aws\",\n\"Status\" : \"active\",\n\"name\" : \"trollsquad-2022\",\n\"region\" : \"useast2\"\n} ]\n}\n</code></pre>"},{"location":"pages/astra/astra-cli/#42-create-tenant","title":"4.2. Create tenant","text":"<p>\u2705 4.2.a - Check tenant existence with <code>exist</code> </p> <p>The tenant name needs to be unique for the cluster (Cloud provider / region). It may be useful to check if the name is already in use by somebody else.</p> <pre><code>astra streaming exist new_tenant_from_cli\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>[ INFO ] - Tenant 'new_tenant_from_cli' does not exist.\n</code></pre> <p>\u2705 4.2.b - Create tenant </p> <p>To create a tenant with default cloud (<code>aws</code>), default region (<code>useast2</code>), plan (<code>free</code>) and namespace (<code>default</code>):</p> <pre><code>astra streaming create new_tenant_from_cli\n</code></pre> <p>To view all supported options, please use:</p> <pre><code>astra help streaming create\n</code></pre>"},{"location":"pages/astra/astra-cli/#43-get-tenant-details","title":"4.3. Get tenant details","text":"<p>\u2705 4.3.a - To get i nformation or details on an entity use the command <code>get</code>.</p> <pre><code>astra streaming get trollsquad-2022\n</code></pre> <p>The pulsar token is not displayed in this view as it is too long, but there are dedicated commands to display it.</p> \ud83d\udda5\ufe0f Sample output <pre><code>+------------------+-------------------------------------------------------------+\n| Attribute        | Value                                                       |\n+------------------+-------------------------------------------------------------+\n| Name             | trollsquad-2022                                             |\n| Status           | active                                                      |\n| Cloud Provider   | aws                                                         |\n| Cloud region     | useast2                                                     |\n| Cluster Name     | pulsar-aws-useast2                                          |\n| Pulsar Version   | 2.10                                                        |\n| Jvm Version      | JDK11                                                       |\n| Plan             | payg                                                        |\n| WebServiceUrl    | https://pulsar-aws-useast2.api.streaming.datastax.com       |\n| BrokerServiceUrl | pulsar+ssl://pulsar-aws-useast2.streaming.datastax.com:6651 |\n| WebSocketUrl     | wss://pulsar-aws-useast2.streaming.datastax.com:8001/ws/v2  |\n+------------------+-------------------------------------------------------------+\n</code></pre> <p>\u2705 4.3.b. To get a special property you can add the option <code>--key</code>. Multiple keys are available: <code>status</code>, <code>cloud</code>, <code>pulsar_token</code>. Notice that the output is raw. This command is expected to be used in scripts</p> <pre><code>astra streaming get trollsquad-2022 --key cloud\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>aws\n</code></pre> <p>\u2705 4.3.c. To get tenant pulsar-token please use <code>pulsar-token</code> command</p> <pre><code>astra streaming pulsar-token trollsquad-2022\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE2NjI5NzcyNzksImlzcyI6ImRhdGFzdGF4Iiwic3ViIjoiY2xpZW50O2Y5NDYwZjE0LTk4NzktNGViZS04M2YyLTQ4ZDNmM2RjZTEzYztkSEp2Ykd4emNYVmhaQzB5TURJeTsxOTZlYjg0YTMzIiwidG9rZW5pZCI6IjE5NmViODRhMzMifQ.rjJYDG_nJu0YpgATfjeKeUUAqwJGyVlvzpA5iP-d5-bReQf1FPaDlGxo40ADHHn2kx2NOdgMsm-Ys4K...\n</code></pre> <p>\u2705 4.3.d. To get tenant status in a human readble for use <code>status</code> command</p> <pre><code>astra streaming status trollsquad-2022\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>[ INFO ] - Tenant 'trollsquad-2022' has status 'active'\n</code></pre>"},{"location":"pages/astra/astra-cli/#44-delete-tenant","title":"4.4. Delete Tenant","text":"<p>\u2705 4.4.a. To delete a tenant simply use the command <code>delete</code></p> <pre><code>astra streaming delete trollsquad\n</code></pre>"},{"location":"pages/astra/astra-cli/#45-pulsar-shell","title":"4.5. Pulsar-Shell","text":"<p>Pulsar-Shell is a standalone shell to work with Apache Pulsar. It is compliant with Astra but requires a few extra steps of configuration. The purpose of the CLI is to integrate with <code>pulsar-shell</code> and do the integration and setup for you.</p> <p>Astra CLI will download, install, setup and wrap <code>pulsar-shell</code> for you to interact with Astra.</p> <p>\u2705 4.5.a - Interactive mode </p> <p>If no options are provided,  you enter <code>pulsar-shell</code> interactive mode</p> <pre><code>astra streaming pulsar-shell trollsquad-2022\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>/Users/cedricklunven/.astra/lunastreaming-shell-2.10.1.1/conf/client-aws-useast2-trollsquad-2022.conf\nPulsar-shell is starting please wait for connection establishment...\nUsing directory: /Users/cedricklunven/.pulsar-shell\nWelcome to Pulsar shell!\n    Service URL: pulsar+ssl://pulsar-aws-useast2.streaming.datastax.com:6651\n    Admin URL: https://pulsar-aws-useast2.api.streaming.datastax.com\n\nType help to get started or try the autocompletion (TAB button).\nType exit or quit to end the shell session.\n\ndefault(pulsar-aws-useast2.streaming.datastax.com)&gt;\n</code></pre> <p>You can quit with exit.</p> <p>\u2705 4.5.b - Execute Pulsar Shell command </p> <p>To execute command with <code>pushar-shell</code> use the flag <code>-e</code>.</p> <pre><code>astra streaming pulsar-shell trollsquad-2022 -e \"admin namespaces list trollsquad-2022\"\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>/Users/cedricklunven/.astra/lunastreaming-shell-2.10.1.1/conf/client-aws-useast2-trollsquad-2022.conf\nPulsar-shell is starting please wait for connection establishment...\nUsing directory: /Users/cedricklunven/.pulsar-shell\n[1/1] Executing admin namespaces list trollsquad-2022\n[1/1] \u2714 admin namespaces list trollsquad-2022\n</code></pre> <p>\u2705 4.5.c - Execute Pulsar Shell files </p> <p>To execute CQL Files with  <code>pushar-shell</code> use the flag <code>-e</code>.</p> <pre><code>astra streaming pulsar-shell trollsquad-2022 -f create_topics.txt\n</code></pre>"},{"location":"pages/astra/astra-cli/#46-pulsar-client-and-admin","title":"4.6. Pulsar-client and Admin","text":"<p>Pulsar client and admin are provided within pulsar-shell. This section simply provides some examples to write and read in a topic with a client.</p> <p>\u2705 4.6.a - Create a topic <code>demo</code>.</p> <ul> <li>First start the pulsar-shell on 2 different terminals</li> </ul> <pre><code>astra streaming pulsar-shell trollsquad-2022\n</code></pre> <ul> <li>Then on first terminal create a topic <code>demo</code> in the namespace <code>default</code></li> </ul> <pre><code>admin topics create persistent://trollsquad-2022/default/demo\n</code></pre> <ul> <li>You can now list the different topics in the namespace <code>default</code></li> </ul> <pre><code>admin topics list trollsquad-2022/default\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>persistent://trollsquad-2022/default/demo\n</code></pre> <ul> <li>Start a consumer on this topic</li> </ul> <pre><code>client consume persistent://trollsquad-2022/default/demo -s astra_cli_tuto -n 0\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>.. init ...\n83 - R:pulsar-aws-useast2.streaming.datastax.com/3.16.119.226:6651]] Connected to server\n2022-09-12T12:28:34,869+0200 [pulsar-client-io-1-1] INFO  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc5ce3ec4, L:/192.168.82.1:53683 - R:pulsar-aws-useast2.streaming.datastax.com/3.16.119.226:6651] Connected through proxy to target broker at 192.168.7.141:6650\n2022-09-12T12:28:35,460+0200 [pulsar-client-io-1-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://trollsquad-2022/default/demo][astra_cli_tuto] Subscribing to topic on cnx [id: 0xc5ce3ec4, L:/192.168.82.1:53683 - R:pulsar-aws-useast2.streaming.datastax.com/3.16.119.226:6651], consumerId 0\n2022-09-12T12:28:35,645+0200 [pulsar-client-io-1-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://trollsquad-2022/default/demo][astra_cli_tuto] Subscribed to topic on pulsar-aws-useast2.streaming.datastax.com/3.16.119.226:6651 -- consumer: 0\n</code></pre> <ul> <li>On the second terminal you can now start a producer</li> </ul> <pre><code>client produce persistent://trollsquad-2022/default/demo -m \"hello,world\" -n 20 \n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>2022-09-12T12:36:28,684+0200 [pulsar-client-io-14-1] INFO  org.apache.pulsar.client.impl.ClientCnx - [id: 0x682890b5, L:/192.168.1.106:53796 ! R:pulsar-aws-useast2.streaming.datastax.com/3.138.177.230:6651] Disconnected\n2022-09-12T12:36:30,756+0200 [main] INFO  org.apache.pulsar.client.cli.PulsarClientTool - 40 messages successfully produced\n\nAnd on the client side\nkey:[null], properties:[], content:world\n----- got message -----\nkey:[null], properties:[], content:hello\n</code></pre>"},{"location":"pages/astra/astra-cli/#47-list-regions","title":"4.7. List Regions","text":"<pre><code>astra streaming list-regions\n</code></pre> <ul> <li><code>-c</code> or <code>--cloud</code> allows to select a cloud provider, the 3 accepted values will be <code>aws</code>, <code>gcp</code> and <code>azure</code></li> <li><code>-f</code> or <code>--filter</code> allows to look for either a location of region (eg. <code>-f France</code>, -f <code>us</code></li> <li><code>-o</code> or <code>--output</code> to change output from table (human) to csv or json</li> <li><code>-v</code> for verbose mode</li> <li><code>-t</code> to provide token of organization if not default selected</li> </ul>"},{"location":"pages/astra/astra-cli/#48-create-env-file","title":"4.8. Create <code>.env</code> file","text":"<pre><code>astra streaming create-dot-env &lt;tenant&gt; [-d &lt;destination_folder&gt;]\n</code></pre>"},{"location":"pages/astra/astra-cli/#49-change-data-capture","title":"4.9. Change Data Capture","text":"<ul> <li>Create a DB</li> </ul> <pre><code>astra db create demo_cdc --if-not-exist\n</code></pre> <ul> <li>Create a tenant in same region</li> </ul> <pre><code>astra streaming create clun-demo-cdc --cloud gcp --region useast1 --if-not-exist\n</code></pre> <ul> <li>Creating a table for the test</li> </ul> <pre><code>astra db cqlsh demo_cdc -k demo_cdc\n</code></pre> <ul> <li>And insert</li> </ul> <pre><code>CREATE TABLE IF NOT EXISTS demo (foo text PRIMARY KEY, bar text);\nCREATE TABLE IF NOT EXISTS table2 (foo text PRIMARY KEY, bar text);\nINSERT INTO demo(foo,bar) VALUES('1','item1');\nINSERT INTO demo(foo,bar) VALUES('1','item2');\nquit;\n</code></pre> <ul> <li>Create CDC</li> </ul> <pre><code>astra db create-cdc demo_cdc \\\n  -k demo_cdc \\\n  --table demo \\ \n  --tenant clun-demo-cdc\n</code></pre> <ul> <li>List CDC (from Streaming)</li> </ul> <pre><code>astra streaming list-cdc clun-demo-cdc\n</code></pre> <ul> <li>List CDC (from DB)</li> </ul> <pre><code>astra db list-cdc demo_cdc\n</code></pre>"},{"location":"pages/astra/astra-cli/#5-user-and-roles","title":"5. User and Roles","text":""},{"location":"pages/astra/astra-cli/#51-list-users","title":"5.1. List users","text":"<pre><code>astra user list\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>+--------------------------------------+-----------------------------+---------------------+\n| User Id                              | User Email                  | Status              |\n+--------------------------------------+-----------------------------+---------------------+\n| b665658a-ae6a-4f30-a740-2342a7fb469c | cedrick.lunven@datastax.com | active              |\n+--------------------------------------+-----------------------------+---------------------+\n</code></pre>"},{"location":"pages/astra/astra-cli/#52-invite-user","title":"5.2. Invite User","text":"<pre><code>astra user invite cedrick.lunven@gmail.com\n</code></pre> <p>Check the list of users and notice the new user invited.</p> <pre><code>astra user list\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>+--------------------------------------+-----------------------------+---------------------+\n| User Id                              | User Email                  | Status              |\n+--------------------------------------+-----------------------------+---------------------+\n| 825bd3d3-82ae-404b-9aad-bbb4c53da315 | cedrick.lunven@gmail.com    | invited             |\n| b665658a-ae6a-4f30-a740-2342a7fb469c | cedrick.lunven@datastax.com | active              |\n+--------------------------------------+-----------------------------+---------------------+\n</code></pre>"},{"location":"pages/astra/astra-cli/#53-revoke-user","title":"5.3. Revoke User","text":"<pre><code>astra user delete cedrick.lunven@gmail.com\n</code></pre> \ud83d\udda5\ufe0f Sample output <pre><code>+--------------------------------------+-----------------------------+---------------------+\n| User Id                              | User Email                  | Status              |\n+--------------------------------------+-----------------------------+---------------------+\n| b665658a-ae6a-4f30-a740-2342a7fb469c | cedrick.lunven@datastax.com | active              |\n+--------------------------------------+-----------------------------+---------------------+\n</code></pre>"},{"location":"pages/astra/astra-cli/#54-list-roles","title":"5.4. List roles","text":"<pre><code>astra role list\n</code></pre>"},{"location":"pages/astra/astra-cli/#55-get-role-infos","title":"5.5. Get role infos","text":"<pre><code>astra role get \"Database Administrator\"\n</code></pre>"},{"location":"pages/astra/astra-httpie/","title":"\u2023 HTTPie","text":"<p>We've created a tool to let you make Stargate calls from the command line.</p> <p>The quick start is in this repository.</p> <p>If you want to use it on your own system, follow these steps:</p>"},{"location":"pages/astra/astra-httpie/#install-the-python-library-and-httpie","title":"Install the python library and httpie","text":"<pre><code>pip3 install httpie-astra\n</code></pre>"},{"location":"pages/astra/astra-httpie/#setup-your-astra-account","title":"Setup your Astra account","text":"<p>To use the Astra CLI you need to create a DataStax Astra account. You also need to create a token with the <code>Organization Administration</code> role.</p>"},{"location":"pages/astra/astra-httpie/#install-astra-cli","title":"Install Astra CLI","text":"<p>To get everything working, you need to have the Astra CLi installed and setup.</p>"},{"location":"pages/astra/astra-httpie/#initialize-your-setup","title":"Initialize your setup","text":"<p>We need to initialize the configuration file at <code>~/.astrarc</code>. To to so run the following command. You will be asked to provide your token (AstraCS:...). It will be saved and reused for your commands in the future.</p> <pre><code>astra setup\n</code></pre>"},{"location":"pages/astra/astra-httpie/#create-your-environment-files","title":"Create your environment files","text":"<p>In order to use httpie, you need to initialize a configuration file.</p> <p>As an example, for the example in the Katapod example, you could use \"stargate\" as the database and \"workshop\" for the keyspace - but you can use any combination you like for general exploration.</p> <pre><code>astra db create &lt;database&gt;\nastra db create-dotenv -k &lt;keyspace&gt; &lt;database&gt;\necho \"[stargate]\" &gt;&gt; ~/.astrarc\ncat .env &gt;&gt; ~/.astrarc\n</code></pre> <p>From here, you can call any Stargate API endpoint.  Check the example repository to see how this works.</p>"},{"location":"pages/astra/astra-httpie/#configuration-file","title":"Configuration file.","text":"<p>You can create a configuration file in ~/.config/httpie/config.json.  Adding this configuration file allows you to use a shorter command to call the endpoints.</p> <p>~/.config/httpie/config.json <pre><code>{\n    \"default_options\": [\n      \"--auth-type=astra\",\n      \"--auth=stargate:\"\n    ]\n}\n</code></pre></p> <p>This means that instead of using this command:</p> <pre><code>http --auth-type astra -a stargate: :/rest/v1/keyspaces\n</code></pre> <p>You can use this command:</p> <pre><code>http :/rest/v1/keyspaces\n</code></pre>"},{"location":"pages/astra/astra-httpie/#example-rest-calls","title":"Example REST calls","text":"<p>Create a table <pre><code>http POST :/rest/v2/schemas/keyspaces/workshop/tables json:='{\n  \"name\": \"cavemen\",\n  \"ifNotExists\": false,\n  \"columnDefinitions\": [\n    {\n      \"name\": \"firstname\",\n      \"typeDefinition\": \"text\",\n      \"static\": false\n    },\n    {\n      \"name\": \"lastname\",\n      \"typeDefinition\": \"text\",\n      \"static\": false\n    },\n        {\n          \"name\": \"occupation\",\n          \"typeDefinition\": \"text\"\n        }\n  ],\n  \"primaryKey\": {\n    \"partitionKey\": [\n      \"lastname\"\n    ],\n    \"clusteringKey\": [\n      \"firstname\"\n    ]\n  }\n}'\n</code></pre></p> <p>List tables in your keyspace <pre><code>http :/rest/v2/schemas/keyspaces/workshop/tables\n</code></pre></p> <p>Add a row <pre><code>http POST :/rest/v2/keyspaces/workshop/cavemen json:='\n{\n            \"firstname\" : \"Fred\",\n            \"lastname\": \"Flintstone\"\n}'\n</code></pre></p> <p>Update a row <pre><code>http PUT :/rest/v2/keyspaces/workshop/cavemen/Flintstone/Fred json:='\n{ \"occupation\": \"Quarry Screamer\"}'\n</code></pre></p> <p>Delete a row <pre><code>http DELETE :/rest/v2/keyspaces/workshop/cavemen/Flintstone/Fred\n</code></pre></p> <p>For more examples, check out the katapod exercises.</p>"},{"location":"pages/astra/cdc-for-astra/","title":"\u2023 CDC for Astra Streaming","text":"\ud83d\udcd6 Reference Documentation and resources <ol> <li>\ud83d\udcd6  Astra Docs - Reference documentation <li>\ud83c\udfa5 Youtube Video - Astra Streaming demo <li>\ud83c\udfa5 Pulsar Documentation - Getting Started <li>\ud83c\udfa5 Apache Pulsar Documentation"},{"location":"pages/astra/cdc-for-astra/#cdc-for-astra-db","title":"CDC for Astra DB","text":"<p>CDC for Astra DB automatically captures changes in real time, de-duplicates the changes, and streams the clean set of changed data into Astra Streaming where it can be processed by client applications or sent to downstream systems.</p> <p>Astra Streaming processes data changes via a Pulsar topic. By design, the Change Data Capture (CDC) component is simple, with a 1:1 correspondence between the table and a single Pulsar topic.</p> <p>This doc will show you how to create a CDC connector for your Astra DB deployment and send change data to an Elasticsearch sink.</p>"},{"location":"pages/astra/cdc-for-astra/#creating-a-tenant-and-topic","title":"Creating a tenant and topic","text":"<ol> <li>In astra.datastax.com, select Create Streaming.</li> <li> <p>Enter the name for your new streaming tenant and select a provider. </p> </li> <li> <p>Select Create Tenant.</p> </li> </ol> <p>Use the default persistent and non-partitioned topic settings.</p> <p>Note</p> <p>Astra Streaming CDC can only be used in a region that supports both Astra Streaming and AstraDB. See Regions for more information.</p>"},{"location":"pages/astra/cdc-for-astra/#creating-a-table","title":"Creating a table","text":"<ol> <li> <p>In your Astra Database, create a table with a primary key column: <pre><code>CREATE TABLE IF NOT EXISTS &lt;keyspacename&gt;.tbl1 (key text PRIMARY KEY, c1 text);\n</code></pre></p> </li> <li> <p>Confirm you created your table: <pre><code>select * from &lt;mykeyspace&gt;.tbl1;\n</code></pre></p> </li> </ol> <p>Results:</p> <p></p>"},{"location":"pages/astra/cdc-for-astra/#connecting-to-cdc-for-astra-db","title":"Connecting to CDC for Astra DB","text":"<ol> <li>Select the CDC tab in your database dashboard.</li> <li>Select Enable CDC.</li> <li> <p>Complete the fields to connect CDC. </p> </li> <li> <p>Select Enable CDC. Once created, your CDC connector will appear: </p> </li> <li> <p>Enabling CDC creates a new <code>astracdc</code> namespace with two new topics, <code>data-</code> and <code>log-</code>. The <code>log-</code> topic consumes schema changes, processes them, and then writes clean data to the <code>data-</code> topic. The <code>log-</code> topic is for CDC functionality and should not be used. The <code>data-</code> topic can be used to consume CDC data in Astra Streaming. </p> </li> </ol>"},{"location":"pages/astra/cdc-for-astra/#connecting-elasticsearch-sink","title":"Connecting Elasticsearch sink","text":"<p>After creating your CDC connector, connect an Elasticsearch sink to it. DataStax recommends using the default Astra Streaming settings.</p> <ol> <li> <p>Select Add Elastic Search Sink from the database CDC console to enforce the default settings.  </p> </li> <li> <p>Use your Elasticsearch deployment to complete the fields. To find your Elasticsearch URL, navigate to your deployment within the Elastic Common Schema (ECS).</p> </li> <li> <p>Copy the Elasticsearch endpoint to the Elastic Search URL field.  </p> </li> <li> <p>Complete the remaining fields. Most values will auto-populate. These values are recommended:</p> <ul> <li><code>ignoreKey</code> as <code>false</code></li> <li><code>nullValueAction</code> as <code>DELETE</code></li> <li><code>enabled</code> as <code>true</code> </li> </ul> </li> <li> <p>When the fields are completed, select Create. If creation is successful, <code>&lt;sink-name&gt; created successfully</code> appears at the top of the screen. You can confirm your new sink was created in the Sinks tab. </p> </li> </ol>"},{"location":"pages/astra/cdc-for-astra/#sending-messages","title":"Sending messages","text":"<p>Let's process some changes with CDC.</p> <ol> <li>Go to the CQL console.</li> <li> <p>Modify the table you created.  <pre><code>INSERT INTO &lt;keyspacename&gt;.tbl1 (key,c1) VALUES ('32a','bob3123');\nINSERT INTO &lt;keyspacename&gt;.tbl1 (key,c1) VALUES ('32b','bob3123b');\n</code></pre></p> </li> <li> <p>Confirm the changes you've made: <pre><code>select * from &lt;keyspacename&gt;.tbl1;\n</code></pre> Results:</p> </li> </ol> <p></p>"},{"location":"pages/astra/cdc-for-astra/#confirming-ecs-is-receiving-data","title":"Confirming ECS is receiving data","text":"<p>To confirm ECS is receiving your CDC changes, use a <code>curl</code> request to your ECS deployment.</p> <ol> <li> <p>Get your index name from your ECS sink tab: </p> </li> <li> <p>Issue your <code>curl</code> request with your Elastic <code>username</code>, <code>password</code>, and <code>index name</code>: <pre><code>curl  -u &lt;username&gt;:&lt;password&gt;  \\\n   -XGET \"https://asdev.es.westus2.azure.elastic-cloud.com:9243/&lt;index_name&gt;.tbl1/_search?pretty=true\"  \\\n   -H 'Content-Type: application/json'\n</code></pre></p> </li> </ol> <p>Note</p> <p>If you have a trial account, the username is <code>elastic</code>.</p> <p>You will receive a JSON response with your changes to the index, which confirms Astra Streaming is sending your CDC changes to your ECS sink.</p> <pre><code>{\n    \"_index\" : \"index.tbl1\",\n    \"_type\" : \"_doc\",\n    \"_id\" : \"32a\",\n    \"_score\" : 1.0,\n    \"_source\" : {\n        \"c1\" : \"bob3123\"\n    }\n}\n\n{\n    \"_index\" : \"index.tbl1\",\n    \"_type\" : \"_doc\",\n    \"_id\" : \"32b\",\n    \"_score\" : 1.0,\n    \"_source\" : {\n        \"c1\" : \"bob3123b\"\n    }\n}\n</code></pre>"},{"location":"pages/astra/cqlproxy/","title":"\u2023 CQL Proxy","text":"<p>This page is a copy</p> <p>This page is a copy of CQL PROXY Reference Documentation. if you encounter some discrepancies please open a JIRA in our repository.</p>"},{"location":"pages/astra/cqlproxy/#what-is-cql-proxy","title":"What is <code>cql-proxy</code>?","text":"<p><code>cql-proxy</code> is designed to forward your application's CQL traffic to an appropriate database service. It listens on a local address and securely forwards that traffic.</p> <p>Note: <code>cql-proxy</code> was made as a genrally available release on <code>2/16/2022</code>. See this blog for additional details.</p> <p>Please give it a try and let us know what you think!</p>"},{"location":"pages/astra/cqlproxy/#when-to-use-cql-proxy","title":"When to use <code>cql-proxy</code>","text":"<p>The <code>cql-proxy</code> sidecar enables unsupported CQL drivers to work with DataStax Astra. These drivers include both legacy DataStax drivers and community-maintained CQL drivers, such as the gocql driver and the rust-driver.</p> <p><code>cql-proxy</code> also enables applications that are currently using Apache Cassandra or DataStax Enterprise (DSE) to use Astra without requiring any code changes. Your application just needs to be configured to use the proxy.</p> <p>If you're building a new application using DataStax drivers, <code>cql-proxy</code> is not required, as the drivers can communicate directly with Astra. DataStax drivers have excellent support for Astra out-of-the-box, and are well-documented in the driver-guide guide.</p>"},{"location":"pages/astra/cqlproxy/#configuration","title":"Configuration","text":"<p>Use the <code>-h</code> or <code>--help</code> flag to display a listing all flags and their corresponding descriptions and environment variables (shown below as items starting with <code>$</code>):</p> <pre><code>$ ./cql-proxy -h\nUsage: cql-proxy\n\nFlags:\n  -h, --help                                              Show context-sensitive help.\n  -b, --astra-bundle=STRING                               Path to secure connect bundle for an Astra database. Requires '--username' and '--password'. Ignored if using the token or contact points option\n                                                          ($ASTRA_BUNDLE).\n  -t, --astra-token=STRING                                Token used to authenticate to an Astra database. Requires '--astra-database-id'. Ignored if using the bundle path or contact points option\n                                                          ($ASTRA_TOKEN).\n  -i, --astra-database-id=STRING                          Database ID of the Astra database. Requires '--astra-token' ($ASTRA_DATABASE_ID)\n--astra-api-url=\"https://api.astra.datastax.com\"    URL for the Astra API ($ASTRA_API_URL)\n-c, --contact-points=CONTACT-POINTS,...                 Contact points for cluster. Ignored if using the bundle path or token option ($CONTACT_POINTS).\n  -u, --username=STRING                                   Username to use for authentication ($USERNAME)\n-p, --password=STRING                                   Password to use for authentication ($PASSWORD)\n-r, --port=9042                                         Default port to use when connecting to cluster ($PORT)\n-n, --protocol-version=\"v4\"                             Initial protocol version to use when connecting to the backend cluster (default: v4, options: v3, v4, v5, DSEv1, DSEv2) ($PROTOCOL_VERSION)\n-m, --max-protocol-version=\"v4\"                         Max protocol version supported by the backend cluster (default: v4, options: v3, v4, v5, DSEv1, DSEv2) ($MAX_PROTOCOL_VERSION)\n-a, --bind=\":9042\"                                      Address to use to bind server ($BIND)\n-f, --config=CONFIG                                     YAML configuration file ($CONFIG_FILE)\n--debug                                             Show debug logging ($DEBUG)\n--health-check                                      Enable liveness and readiness checks ($HEALTH_CHECK)\n--http-bind=\":8000\"                                 Address to use to bind HTTP server used for health checks ($HTTP_BIND)\n--heartbeat-interval=30s                            Interval between performing heartbeats to the cluster ($HEARTBEAT_INTERVAL)\n--idle-timeout=60s                                  Duration between successful heartbeats before a connection to the cluster is considered unresponsive and closed ($IDLE_TIMEOUT)\n--readiness-timeout=30s                             Duration the proxy is unable to connect to the backend cluster before it is considered not ready ($READINESS_TIMEOUT)\n--num-conns=1                                       Number of connection to create to each node of the backend cluster ($NUM_CONNS)\n--rpc-address=STRING                                Address to advertise in the 'system.local' table for 'rpc_address'. It must be set if configuring peer proxies ($RPC_ADDRESS)\n--data-center=STRING                                Data center to use in system tables ($DATA_CENTER)\n--tokens=TOKENS,...                                 Tokens to use in the system tables. It's not recommended ($TOKENS)\n</code></pre> <p>To pass configuration to <code>cql-proxy</code>, either command-line flags, environment variables, or a configuration file can be used. Using the <code>docker</code> method as an example, the following samples show how the token and database ID are defined with each method.</p>"},{"location":"pages/astra/cqlproxy/#using-flags","title":"Using flags","text":"<pre><code>docker run -p 9042:9042 \\\n--rm datastax/cql-proxy:v0.1.2 \\\n--astra-token &lt;astra-token&gt; --astra-database-id &lt;astra-datbase-id&gt;\n</code></pre>"},{"location":"pages/astra/cqlproxy/#using-environment-variables","title":"Using environment variables","text":"<pre><code>docker run -p 9042:9042  \\\n--rm datastax/cql-proxy:v0.1.2 \\\n-e ASTRA_TOKEN=&lt;astra-token&gt; -e ASTRA_DATABASE_ID=&lt;astra-datbase-id&gt;\n</code></pre>"},{"location":"pages/astra/cqlproxy/#using-a-configuration-file","title":"Using a configuration file","text":"<p>Proxy settings can also be passed using a configuration file with the <code>--config /path/to/proxy.yaml</code> flag. This can be mixed and matched with command-line flags and environment variables. Here are some example configuration files:</p> <pre><code>contact-points:\n- 127.0.0.1\nusername: cassandra\npassword: cassandra\nport: 9042\nbind: 127.0.0.1:9042\n# ...\n</code></pre> <p>or with a Astra token:</p> <pre><code>astra-token: &lt;astra-token&gt;\nastra-database-id: &lt;astra-database-id&gt;\nbind: 127.0.0.1:9042\n# ...\n</code></pre> <p>All configuration keys match their command-line flag counterpart, e.g. <code>--astra-bundle</code> is <code>astra-bundle:</code>, <code>--contact-points</code> is <code>contact-points:</code> etc.</p>"},{"location":"pages/astra/cqlproxy/#setting-up-peer-proxies","title":"Setting up peer proxies","text":"<p>Multi-region failover with DC-aware load balancing policy is the most useful case for a multiple proxy setup.</p> <p>When configuring <code>peers:</code> it is required to set <code>--rpc-address</code> (or <code>rpc-address:</code> in the yaml) for each proxy and it must match is corresponding <code>peers:</code> entry. Also, <code>peers:</code> is only available in the configuration file and cannot be set using a command-line flag.</p>"},{"location":"pages/astra/cqlproxy/#multi-region-setup","title":"Multi-region setup","text":"<p>Here's an example of configuring multi-region failover with two proxies. A proxy is started for each region of the cluster connecting to it using that region's bundle. They all share a common configuration file that contains the full list of proxies.</p> <p>Note: Only bundles are supported for multi-region setups.</p> <pre><code>cql-proxy --astra-bundle astra-region1-bundle.zip --username token --passowrd &lt;astra-token&gt; \\\n--bind 127.0.0.1:9042 --rpc-address 127.0.0.1 --data-center dc-1 --config proxy.yaml\n</code></pre> <pre><code>cql-proxy ---astra-bundle astra-region2-bundle.zip --username token --passowrd &lt;astra-token&gt; \\\n--bind 127.0.0.2:9042 --rpc-address 127.0.0.2 --data-center dc-2 --config proxy.yaml\n</code></pre> <p>The peers settings are configured using a yaml file. It's a good idea to explicitly provide the <code>--data-center</code> flag, otherwise; these values are pulled from the backend cluster and would need to be pulled from the <code>system.local</code> and <code>system.peers</code> table to properly setup the peers <code>data-center:</code> values. Here's an example <code>proxy.yaml</code>:</p> <pre><code>peers:\n- rpc-address: 127.0.0.1\ndata-center: dc-1\n- rpc-address: 127.0.0.2\ndata-center: dc-2\n</code></pre> <p>Note: It's okay for the <code>peers:</code> to contain entries for the current proxy itself because they'll just be omitted.</p>"},{"location":"pages/astra/cqlproxy/#getting-started","title":"Getting started","text":"<p>There are three methods for using <code>cql-proxy</code>:</p> <ul> <li>Locally build and run <code>cql-proxy</code></li> <li>Run a docker image that has <code>cql-proxy</code> installed</li> <li>Install locally on a Mac with Homebrew</li> <li>Use a Kubernetes container to run <code>cql-proxy</code></li> </ul>"},{"location":"pages/astra/cqlproxy/#locally-build-and-run","title":"Locally build and run","text":"<ol> <li>Build <code>cql-proxy</code>.</li> </ol> <pre><code>go build\n</code></pre> <ol> <li> <p>Run with your desired database.</p> </li> <li> <p>DataStax Astra cluster:</p> <pre><code>./cql-proxy --astra-token &lt;astra-token&gt; --astra-database-id &lt;astra-database-id&gt;\n</code></pre> <p>The <code>&lt;astra-token&gt;</code> can be generated using these instructions. The proxy also supports using the Astra Secure Connect Bundle along with a client ID and secret generated using these instructions:</p> <pre><code>./cql-proxy --astra-bundle &lt;your-secure-connect-zip&gt; \\\n--username &lt;astra-client-id&gt; --password &lt;astra-client-secret&gt;\n</code></pre> </li> <li> <p>Apache Cassandra cluster:</p> <pre><code>./cql-proxy --contact-points &lt;cluster node IPs or DNS names&gt; [--username &lt;username&gt;] [--password &lt;password&gt;]\n</code></pre> </li> </ol>"},{"location":"pages/astra/cqlproxy/#run-a-cql-proxy-docker-image","title":"Run a <code>cql-proxy</code> docker image","text":"<ol> <li> <p>Run with your desired database.</p> <ul> <li>DataStax Astra cluster:</li> </ul> <pre><code>docker run -p 9042:9042 \\\ndatastax/cql-proxy:v0.1.2 \\\n--astra-token &lt;astra-token&gt; --astra-database-id &lt;astra-database-id&gt;\n</code></pre> <p>The <code>&lt;astra-token&gt;</code> can be generated using these instructions. The proxy also supports using the Astra Secure Connect Bundle, but it requires mounting the bundle to a volume in the container:</p> <pre><code>docker run -v &lt;your-secure-connect-bundle.zip&gt;:/tmp/scb.zip -p 9042:9042 \\\n--rm datastax/cql-proxy:v0.1.2 \\\n--astra-bundle /tmp/scb.zip --username &lt;astra-client-id&gt; --password &lt;astra-client-secret&gt;\n</code></pre> <ul> <li> <p>Apache Cassandra cluster:</p> <pre><code>docker run -p 9042:9042 \\\ndatastax/cql-proxy:v0.1.2 \\\n--contact-points &lt;cluster node IPs or DNS names&gt; [--username &lt;username&gt;] [--password &lt;password&gt;]\n</code></pre> </li> </ul> <p>If you wish to have the docker image removed after you are done with it, add <code>--rm</code> before the image name <code>datastax/cql-proxy:v0.1.2</code>.</p> </li> </ol>"},{"location":"pages/astra/cqlproxy/#homebrew-on-a-mac","title":"Homebrew on a Mac","text":"<p>Install with one simple command: <pre><code>brew install cql-proxy\n</code></pre></p>"},{"location":"pages/astra/cqlproxy/#use-kubernetes","title":"Use Kubernetes","text":"<p>Using Kubernetes with <code>cql-proxy</code> requires a number of steps:</p> <ol> <li> <p>Generate a token following the Astra instructions. This step will display your Client ID, Client Secret, and Token; make sure you download the information for the next steps. Store the secure bundle in <code>/tmp/scb.zip</code> to match the example below.</p> </li> <li> <p>Create <code>cql-proxy.yaml</code>. You'll need to add three sets of information: arguments, volume mounts, and volumes.</p> </li> <li> <p>Argument: Modify the local bundle location, username and password, using the client ID and client secret obtained in the last step to the container argument.</p> </li> </ol> <pre><code>command: [\"./cql-proxy\"]\nargs: [\"--astra-bundle=/tmp/scb.zip\",\"--username=Client ID\",\"--password=Client Secret\"]\n</code></pre> <ul> <li> <p>Volume mounts: Modify <code>/tmp/</code> as a volume mount as required.</p> <p>volumeMounts:     - name: my-cm-vol     mountPath: /tmp/</p> </li> <li> <p>Volume: Modify the <code>configMap</code> filename as required. In this example, it is named <code>cql-proxy-configmap</code>. Use the same name for the <code>volumes</code> that you used for the <code>volumeMounts</code>.</p> <p>volumes:     - name: my-cm-vol       configMap:         name: cql-proxy-configmap</p> </li> <li> <p>Create a configmap. Use the same secure bundle that was specified in the <code>cql-proxy.yaml</code>.</p> </li> </ul> <pre><code>kubectl create configmap cql-proxy-configmap --from-file /tmp/scb.zip\n</code></pre> <ol> <li>Check the configmap that was created.</li> </ol> <pre><code>kubectl describe configmap config\n\nName:         config\n  Namespace:    default\n  Labels:       &lt;none&gt;\n  Annotations:  &lt;none&gt;\n\nData\n====\nBinaryData\n====\nscb.zip: 12311 bytes\n</code></pre> <ol> <li>Create a Kubernetes deployment with the YAML file you created:</li> </ol> <pre><code>kubectl create -f cql-proxy.yaml\n</code></pre> <ol> <li>Check the logs:    <pre><code>kubectl logs &lt;deployment-name&gt;\n</code></pre></li> </ol>"},{"location":"pages/astra/cqlproxy/#known-issues","title":"Known issues","text":""},{"location":"pages/astra/cqlproxy/#token-aware-load-balancing","title":"Token-aware load balancing","text":"<p>Drivers that use token-aware load balancing may print a warning or may not work when using cql-proxy. Because cql-proxy abstracts the backend cluster as a single endpoint this doesn't always work well with token-aware drivers that expect there to be at least \"replication factor\" number of nodes in the cluster. Many drivers print a warning (which can be ignored) and fallback to something like round-robin, but other drivers might fail with an error. For the drivers that fail with an error it is required that they disable token-aware or configure the round-robin load balancing policy.</p>"},{"location":"pages/astra/create-account/","title":"\u2023 Create Account","text":""},{"location":"pages/astra/create-account/#a-overview","title":"A - Overview","text":"<p>ASTRA DB is the simplest way to run Cassandra with zero operations. No credit card required and $25.00 USD credit every month (roughly 20M reads/writes, 80GB storage monthly) which is sufficient to run small production workloads.</p> <p>https://astra.datastax.com is the URL create an account and get started with the solution.</p> <p> \u00a0Sign Up to Astra </p>"},{"location":"pages/astra/create-account/#b-sign-up","title":"B - Sign Up","text":"<p>You can use your <code>Github</code>, <code>Google</code> accounts or register with an <code>email</code>.</p>"},{"location":"pages/astra/create-account/#1-sign-in-with-github","title":"1. Sign In with Github","text":"Click the <code>[Sign In with Github]</code> button 1\ufe0f\u20e3 Click <code>Continue</code> on the OAuth claims delegation <p>The OAuth2 delegation screen from github is asking for permissions.</p> <p></p> 2\ufe0f\u20e3 You are redirected to the homepage <p></p>"},{"location":"pages/astra/create-account/#2-sign-in-with-google","title":"2. Sign In with Google","text":"1\ufe0f\u20e3 Click the <code>[Sign In with Google]</code> button 2\ufe0f\u20e3 You are redirected to the homepage"},{"location":"pages/astra/create-account/#3-sign-up","title":"3. Sign Up","text":"1\ufe0f\u20e3 Click the <code>Sign up</code> on the bottom of the page 2\ufe0f\u20e3 Provide your information and validate the captcha 3\ufe0f\u20e3 Accept terms and policies <p>Astra is now looking for you to validate your email adress</p> <p></p> 4\ufe0f\u20e3 Open the mail in your inbox and validate with the <code>Verify my email</code> link <p></p> <ul> <li>Astra will show a validation message, select Click Here to proceed.</li> </ul> <p></p> <ul> <li>Select back to application </li> </ul> 5\ufe0f\u20e3 You are redirected to the homepage <p></p>"},{"location":"pages/astra/create-account/#c-account-and-organization","title":"C - Account and Organization","text":""},{"location":"pages/astra/create-account/#1-overview","title":"1. Overview","text":"<p>When you create an account your personal Organization is created, this is your tenant:</p> <ul> <li>The name of the organization is your email address, (1) in the picture below</li> <li>The unique identifier (GUID) is present in the URL on the dashboard. (2) in the picture below</li> </ul> <p></p>"},{"location":"pages/astra/create-account/#2-organization-objects","title":"2. Organization Objects","text":"<p><code>Databases</code>, <code>Tenants</code> and <code>Security Tokens</code> objects are created within the organization, as shown on the Organization Dashboard.</p> <pre><code>  graph TD\n    User(User) --&gt;|n..m| ORG(Organization)\n    ORG(Organization) --&gt; User(User) \n    ORG(Organization) --&gt;|0..n| DB(Databases)\n    ORG(Organization) --&gt;|0..n| ST(Streaming Tenants)\n    ORG(Organization) --&gt;|0..n| ROLE(Roles)\n    ORG(Organization) --&gt;|0..n| TOK(Security Tokens)\n    TOK(Security Tokens) --&gt;|1..1| ROLE\n    DB(Databases) --&gt;|1..n| KEY(Keyspaces)\n    KEY(Keyspaces) --&gt;|0..n| TABLE(Tables)\n    ST(Streaming Tenants) --&gt;|1..n| NAMESPACES(Namespaces)\n    NAMESPACES(Namespaces) --&gt;|0..n| TOPICS(Topics)</code></pre> <p></p>"},{"location":"pages/astra/create-account/#3-multiple-organizations","title":"3. Multiple Organizations","text":"<p>You can create multiple organizations through the <code>Manage Organizations</code> menu option and invite other users to join as well. It is useful when the same database could be accessed by multiple users with different emails.</p> <p></p> <p>As a consequence a user can be part of multiple organizations; the personal organization created during registration, new user-defined organizations, and shared organizations.</p> <pre><code>  graph TD\n    USER(User) --&gt;|1..n| PORG(Personal Organization - registration)\n    USER --&gt;|0..n| CORG(Organizations he created)\n    USER --&gt;|0...n| IORG(Organizations he was invited to)</code></pre>"},{"location":"pages/astra/create-instance/","title":"\u2023 Create Database","text":"\ud83d\udcd6 Reference Documentation and resources <ol> <li>\ud83d\udcd6  Astra Docs - The Astra database creation procedure <li>\ud83c\udfa5 Youtube Video - Walk through instance creation"},{"location":"pages/astra/create-instance/#a-overview","title":"A - Overview","text":"<p><code>ASTRA DB</code> is the simplest way to run Cassandra with zero operations - just push the button and get your cluster. No credit card required and $25.00 USD credit every month (roughly 20M reads/writes, 80GB storage monthly) which is sufficient to run small production workloads.</p>"},{"location":"pages/astra/create-instance/#b-prerequisites","title":"B - Prerequisites","text":"<ul> <li>You should have an Astra account. If you don't have one yet, keep reading and we'll show you how to create it.</li> </ul>"},{"location":"pages/astra/create-instance/#c-procedure","title":"C - Procedure","text":"Not a fan of user interfaces ? <p>The procedure on this page describes how to create a database  through the user interface. If you have the CLI installed you can also go with a <code>astra db create my_db</code>. More information on the CLI page</p> <p>\u2705 Step 1: Click the <code>Create Account</code> button to login or register.</p> <p>You can use your <code>Github</code>, <code>Google</code> accounts or register with an <code>email</code>. Make sure to chose a password with a minimum of 8 characters, containing upper and lowercase letters, and at least one number and special character.</p> <p>If you already have an Astra account, you can skip this step. Locate the \"Create Database\" button, as shown in the next step, and read on.</p> <p></p> <p>\u2705 Step 2: Complete the creation form</p> <p>If you are creating a new account, you will be brought to the DB-creation form directly.</p> <p>Otherwise, get to the databases dashboard (by clicking on <code>Databases</code> in the left-hand navigation bar, expanding it if necessary), and click the <code>[Create Database]</code> button on the right.</p> <p><p></p> </p> <p>Take a moment to fill the form:</p> <ul> <li>\u2139\ufe0f Fields Description</li> </ul> Field Description Vector Database vs Serverless Database In june 2023, Cassandra introduced the support of vector search to enable Generative AI use cases. You might consume your credit faster with a vector database. database name It does not need to be unique, is not used to initialize a connection, and is only a label (keep it between 2 and 50 characters). It is recommended to have a database for each of your applications. The free tier is limited to 5 databases. keyspace It is a logical grouping of your tables (keep it between 2 and 48 characters). Prefer lower case or <code>snake_case</code>, and avoid spaces. Cloud Provider Choose whatever you like. Click a cloud provider logo, pick an Area in the list and finally pick a region. We recommend choosing a region that is closest to you to reduce latency. In free tier, there is very little difference. <p>If all fields are filled properly, clicking the \"Create Database\" button will start the process. It should take a couple of minutes for your database to become <code>Active</code>.</p> <p>In the meantime, you will be brought to the \"Connect\" page of the database, that is being provisioned in the meantime. You can track the progression by looking at the status label at the top, next to the database's name.</p> <p>\u2705 Step 3: Obtain a Database Token</p> <p>While the database is created, you can generate and download a Token to later access it from your applications. The automatically-generated token has a \"reasonable\" set of permissions (limited to this database), but you can also generate a custom token to better suit your needs.</p> <p></p> <p>To generate the token click on the \"Generate Token\" button. The new token can (and should!) be copied elsewhere with the clipboard-icon button and/or downloaded in JSON format for safe storage - it will not be shown anymore once you leave the dialog.</p> <p></p> <p>\u2705 Step 4: Ready</p> <p>In a couple of minutes, the database will switch to <code>Active</code>. You can now interact with it, for example by downloading a Secure Connect Bundle) and start running applications that access it.</p>"},{"location":"pages/astra/create-token/","title":"\u2023 Create Token","text":""},{"location":"pages/astra/create-token/#a-overview","title":"A - Overview","text":"<p> A page also exists in the Astra Reference Documentation</p> <p>As stated in the Create Account page, the security token is associated to one and  only one organization and only one role. There are a set of predefined roles within an organization which are associated  with some default permissions. The full list of permissions and roles is available in the  Astra Documentation.</p> <p>Default roles available for a token:</p> <pre><code>Administrator User\nOrganization Administrator\nBilling Administrator\nDatabase Administrator\nUI View Only\nAdministrator Service Account\nRead/Write Service Account\nRead Only Service Account\nRead/Write User\nRead Only User\nAPI Administrator User\nAPI Read Only Service Account\nAPI Read/Write User\nAPI Administrator Service Account\nAPI Read/Write Service Account\nAPI Read Only User\n</code></pre> <p>Permissions for a role (here Organization Administrator got access to everything)</p> <p><p></p> </p> <p>Custom Roles</p> <p>It is possible to manually create custom roles and tune the corresponding permissions in a fine-grained fashion (<code>Settings / Role Management</code>), to later create tokens based on them. For example, each time a database is created, it comes with an autogenerated brand-new token, backed by an ad-hoc custom role essentially scoped to that database only.</p> <p><p></p> </p>"},{"location":"pages/astra/create-token/#b-prerequisites","title":"B - Prerequisites","text":"<p>To create a new token:</p> <ul> <li>You should have an Astra account</li> </ul>"},{"location":"pages/astra/create-token/#c-procedure","title":"C - Procedure","text":"<p>Note that a token, albeit with a fixed set of permissions, is generated automatically for you as a database is created. In many cases, however, you need to manually issue tokens, and here is explained how to do that.</p> <p>1\ufe0f\u20e3 First go to the Organization settings panel in one of the following ways:</p> Settings page <p>On the bottom-right corner of the Astra UI, in the navigation bar, click on \"Settings\" next to the cog icon. (The navigation bar might be collapsed to the left). Then, select the \"Token management\" entry in the Settings menu.</p> <p></p> From a database <p>Click on the \"...\" next to a database in the main DB dashboard, then select \"Generate a Token\".</p> <p></p> From the Connect tab <p>On the Connect tab of your database, click on the \"create a custom token\" link in the Quickstart section.</p> <p></p> <p>2\ufe0f\u20e3 Pick the desired role for the token in the drop-down list and click \"Generate\".</p> <p><p></p> </p> <p>3\ufe0f\u20e3 A new token is generated for you. Make sure to copy/download the values before leaving the page, since the secrets will not be shown anymore. You can copy the individual secrets with the button next to the text fields, or directly download the whole token as a file and store it safely.</p> <p><p></p> </p> Anatomy of a Token <p>The Token is in fact three separate strings: a Client ID, a Client Secret and the token proper. You will need some of these strings to access the database, depending on the type of access you plan. Although the Client ID, strictly speaking, is not a secret, you should regard this whole object as a secret and make sure not to share it inadvertently (e.g. committing it to a Git repository) as it grants access to your databases.</p> <p></p> <p>4\ufe0f\u20e3 The token will not expire, unless you decide to revoke (i.e. delete) it, for example in case it is compromised. To do so, in the \"Token Management\" page, click on the \"...\" menu next to the token you want to delete.</p> <p><p></p> </p> <p>Reference Documentation and resources</p> <p><ol> <li> Astra Docs - The Astra token creation procedure <li> Youtube Video - Walk through token creation <li> Youtube Video - More about token and roles in Astra"},{"location":"pages/astra/create-topic/","title":"\u2023 Create Topic","text":"\ud83d\udcd6 Reference Documentation and resources <ol> <li>\ud83d\udcd6  Astra Docs - Reference documentation <li>\ud83c\udfa5 Youtube Video - Astra Streaming demo <li>\ud83c\udfa5 Pulsar Documentation - Getting Started <ul> <li>Apache Pulsar documentation</li> </ul>"},{"location":"pages/astra/create-topic/#a-overview","title":"A - Overview","text":"<p><code>ASTRA STREAMING</code> is the simplest way to use the Apache Pulsar messaging/streaming service with zero operations - just push the button and get your messages flowing. No credit card required, $25.00 USD credit every month, and all of thethe strength and features of Apache Pulsar managed for you in the cloud.</p> <p>This page explains how to create a new tenant in Astra Streaming, a new namespace in the tenant (if desired) and a new topic in the namespace. Also instructions are given to retrieve the connection parameters to later connect to the topic and start messaging from your application.</p>"},{"location":"pages/astra/create-topic/#b-prerequisites","title":"B - Prerequisites","text":"<ul> <li>You should have an Astra account.</li> <li>Have a <code>tenant_name</code>, optionally a <code>namespace</code> (if not using \"default\"), and a <code>topic_name</code> ready to create the topic.</li> </ul>"},{"location":"pages/astra/create-topic/#c-procedure","title":"C - Procedure","text":"<p>Make sure you are logged in to your Astra account before proceeding.</p> <p>\u2705 Step 1: Create a tenant</p> <p>Go to your Astra console, click the \"Create Stream\" button next to the Streaming section.</p> <p>Set up a new Tenant (remember Pulsar has a multi-tenant architecture): you have to find a globally unique name for it. Pick the provider and region (try to have it close to you for reduced latency) and finally hit \"Create Tenant\".</p> <p>You'll shortly see the dashboard for your newly-created Tenant.</p> <p>\u2705 Step 2: Create a namespace</p> <p>A <code>default</code> namespace is created for you with the tenant and you can use it as is. However, you may want to create a separate namespace to host your topic(s).</p> <p>Go to the \"Namespaces\" tab of your Tenant dashboard and click on the \"Create namespace\" button on the right. Choose a name and hit \"Create\". You should see it listed among the available namespaces in a moment.</p> <p>\u2705 Step 3: Create a topic</p> <p>Switch to the \"Topics\" tab and click the \"Add Topic\" button next to the namespace that you want to use.</p> <p>Choose a topic name, review and/or modify the topic settings (such as <code>persistent=yes, partitioned=no</code>), and click \"Save\".</p> <p>It takes no more than a couple of minutes to create your new topic. It will then be ready to receive and dispatch messages.</p> <p>\ud83d\udc41\ufe0f Walkthrough for topic creation</p> <p></p> <p>\u2705 Step 4: retrieve the Broker URL</p> <p>All that is left is to make sure you have the connection parameters needed to reach the topic programmatically. If you click the \"Connect\" tab you will see a list of \"Tenant Details\", along with links to look at code examples in various languages.</p> <p>There are several ways to connect to the topic. If you plan to use the Pulsar drivers from your application, the important bits are the \"Broker Service URL\" and the \"Streaming Token\" secret.</p> <p>The \"Broker Service URL\" is shown right in the \"Connect\" tab and looks like <code>pulsar+ssl://pulsar-[...].streaming.datastax.com:6651</code>. You can click on the clipboard icon to copy it.</p> <p>\u2705 Step 5: Manage secrets and retrieve the Streaming Token</p> <p>You will also need a Token, a long secret string providing authentication info when the driver will connect to the topic. The token must be treated as a secret, which means do not post it publicly and do not check it in to version control.</p> <p>Note: Streaming Tokens are completely separate from Astra DB Tokens.</p> <p>Navigate to the \"Token Manager\" by clicking on the link in the \"Tenant Details\" list: there you will be able to create, copy and revoke streaming tokens for your tenant.</p> <p>Note that a default token has already been created for you, so you don't necessarily need to create a new token. Click on the clipboard icon to copy it.</p> <p>The token is a long random-looking string, such as <code>eyJhbGci [...] cpNpX_qN68Q</code> (about 500 chars long).</p> <p>\ud83d\udc41\ufe0f Screenshot for the connection parameters</p> <p></p>"},{"location":"pages/astra/download-scb/","title":"\u2023 Secure Connect Bundle","text":"\ud83d\udcd6 Reference Documentation and resources <ol> <li>\ud83d\udcd6  Astra Docs - Download Cloud Secure Bundle <li>\ud83c\udfa5 Youtube Video - Walk through secure"},{"location":"pages/astra/download-scb/#a-overview","title":"A - Overview","text":"<p>To initialize a secured 2-way TLS connection between clients and Astra x509 certificates are needed. The strong authentication is key for maximum security and still benefits from robust driver features (health-check, load-balancing, fail-over). Under the hood the protocol SNI over TCP is used to contact each node independently.</p> <p>The configuration and required certificates are provided to the user through a zip file called the secure connect bundle which can be downloaded for each DATABASE REGION. This means that a database deployed across multiple regions will have one secure connect bundle per region. (1 region = 1 underlying Apache Cassandra\u2122 datacenter)</p> <p></p>"},{"location":"pages/astra/download-scb/#b-prerequisites","title":"B - Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> </ul>"},{"location":"pages/astra/download-scb/#c-procedure","title":"C - Procedure","text":"Not a fan of user interfaces ? <p>The procedure on this page describes how to download the secure connect bundle  through the user interface. If you have the CLI installed you can also go with a <code>astra db download-scb my_db</code>. More information on the CLI page</p> <p>\u2705 Step 1 : Go to your database's Connect Quick Start</p> <p>Once you sign in and land on your Astra Home, reach the Quick Start page for the database whose Secure Connect Bundle you want to obtain. You can do so either through the database list in the left-panel navigation bar, or from the Databases overview in the main panel:</p> From the navigation bar <p>The right-hand navigation bar lists your most commonly-used databases under the \"Databases\" heading. Click on the desired database.</p> <p></p> <p>(Note: the side navigation bar might be collapsed. Click on the \"DS\" logo at the top to expand it.)</p> <p>The main panel will show the database-specific dashboard. Locate the \"Connect\" button on the top right and click on it.</p> <p></p> From the overall database dashboard <p>Alternatively, click on the \"Databases\" entry in the left-hand navigation bar to get to the main databases dashboard.</p> <p></p> <p>(Note: the side navigation bar might be collapsed. Click on the \"DS\" logo at the top to expand it.)</p> <p>Locate the desired database in the list in the main panel and click on the corresponding \"Connect\" button in the table.</p> <p></p> <p>\u2705 Step 2 : Download the bundle ZIP</p> <p>The \"Quick Start\" section features a \"Get Bundle\" button. Click on it to bring up the download-bundle dialog.</p> <p></p> <p>Your database might be multi-region (remember there is a separate bundle for each DB region). In the dialog, choose the desired region for which you need the bundle: a download URL is now generated for you. You now have several options to download the file:</p> <ol> <li>get the file directly with the \"Download Secure Bundle\" button;</li> <li>copy the generated URL to the bundle, by clicking on the \"clipboard\" icon, and use it wherever you want (within a few minutes, before the link expires);</li> <li>directly copy a ready-made <code>cURL</code> command to paste in a console and have the downloaded bundle zipfile there;</li> <li>similar to the previous case, but using the <code>wget</code> console utility.</li> </ol> <p></p>"},{"location":"pages/astra/download-scb/#remarks","title":"Remarks","text":"<ul> <li> <p>If you download the file directly, be aware that most browsers will give you the option to open the zip file directly. Do not do that, save it locally instead: the bundle zipfile has to be passed   to the drivers as is!</p> </li> <li> <p>The link to the bundle zipfile will expire a few minutes after it is generated. If you wait too long,   you might end up with a faulty bundle. As a check, make sure the zipfile you downloaded is around 12-13 KB in size.</p> </li> </ul>"},{"location":"pages/astra/faq/","title":"\u2023 FAQ","text":""},{"location":"pages/astra/faq/#questions-list","title":"Questions List","text":"<ul> <li>Where should I find a database identifier ?</li> <li>Where should I find a database region name ?</li> <li>How do I create a keyspace or a namespace ?</li> <li>How to open the Web CQL Console?</li> </ul>"},{"location":"pages/astra/faq/#where-should-i-find-a-database-identifier","title":"Where should I find a database identifier ?","text":"<p>The database <code>id</code> is a unique identifier (<code>GUID</code>) for your database. You can find it on the main dashboard of Astra DB, reachable by clicking \"Databases\" on the left-side navigation bar. Copy it to your clipboard by clicking on the small \ud83d\udccb icon.</p> <p></p> <p>(Note: the side navigation bar might be collapsed. Click on the \"DS\" logo at the top to expand it.)</p> <p>Remember that, unlike the database <code>id</code>, the database name is not necessarily unique in an organization. That is, with reference to the above image, one could have more than one <code>multics</code> databases, each with a different <code>id</code>.</p>"},{"location":"pages/astra/faq/#where-should-i-find-a-database-region-name","title":"Where should I find a database region name ?","text":"<p>A database can span one or more regions. Each region will have a \"Datacenter ID\" and a \"Region\". The \"Region\" is the one used in the API endpoints.</p> <p>Reach the dashboard specific to the database by either:</p> <ul> <li>clicking on the DB name in the left navigation bar,</li> <li>clicking on the DB name in the overall \"Databases\" list (reachable by clicking \"Databases\" in the navigation bar).</li> </ul> <p>The \"Overview\" page for the database lists all regions in a table. In the example below, you see two regions, identified by <code>eu-central-1</code> and <code>us-east-1</code>.</p> <p></p> <p>Should you need the ID of a certain Datacenter, simply click on the clipboard icon next to the ID to copy it.</p>"},{"location":"pages/astra/faq/#how-do-i-create-a-namespace-or-a-keyspace","title":"How do I create a namespace or a keyspace ?","text":"<p>In Astra DB, \"namespace\" and \"keyspace\" mean exactly the same thing, i.e. mainly a logical grouping of tables. There are two ways to create them.</p>"},{"location":"pages/astra/faq/#at-db-creation-time","title":"At DB-creation time","text":"<p>When creating a new database, you automatically create a keyspace in it:</p> <p></p>"},{"location":"pages/astra/faq/#add-a-keyspace-to-an-existing-database","title":"Add a keyspace to an existing database","text":"<p>From the database's dashboard, find the \"Add Keyspace\" button in the \"Keyspaces\" section of the dashboard and click on it.</p> <p></p> <p>You can access your database dashboard by clicking its name either in the navigation bar on the left or on the overall \"Databases\" main panel.</p> <p>Keep in mind that in order to add a keyspace, the database must be in \"Active\" state - resume it first if necessary.</p> <p></p> <p>The database will switch to <code>Maintenance</code> mode for a few seconds, but no fear: all running applications are still able to access other keyspaces in the database.</p>"},{"location":"pages/astra/faq/#how-to-open-the-web-cql-console","title":"How to open the Web CQL Console?","text":"<p>To access the CQL Console for a certain database, go to the dashboard for the database (clicking on the DB name on the left-hand navigation bar) and simply select the \"CQL Console\" tab in the main panel.</p> <p>An in-browser Web-based console to exchange CQL commands with the database will be available in few seconds, already connected to your database.</p> <p></p> <p>Note: if your database spans multiple regions, your will have the possibility to choose a region for the connection. Each choice of a region will completely reset the console.</p>"},{"location":"pages/astra/multi-regions/","title":"\u2023 Multi Regions","text":"\ud83d\udcd6 Reference Documentation and resources <ol> <li>\ud83d\udcd6  Astra Docs - Reference documentation <li>\ud83c\udfa5 Youtube Video - Walk through instance creation"},{"location":"pages/astra/multi-regions/#a-overview","title":"A - Overview","text":"<p><code>AstraDB</code> allows you to replicate data across multiple regions to maintain data availability for multi-region application architectures. Configuring multiple regions can also satisfy data locality requirements and save money.</p> <p></p>"},{"location":"pages/astra/multi-regions/#eventual-consistency","title":"\ud83d\udd04 Eventual Consistency","text":"<p>Apache Cassandra\u00ae and DataStax Astra DB follow the eventual consistency model. As a result, data written to one datacenter/region may not be immediately accessible in other datacenters/regions in the same database cluster. It normally only takes a few minutes to fully replicate the data. However, it could take longer, and possibly span one or more days. There are several contributing factors to the latter scenario; such as the workload volume, the number of regions, the process that runs data repair operations, and network resources.</p>"},{"location":"pages/astra/multi-regions/#data-sovereignty","title":"\u2696\ufe0f Data sovereignty","text":"<p>Astra DB serverless replicates all data in the database to all of a database\u2019s regions. By contrast, multiple keyspaces in Apache Cassandra\u00ae and DataStax Enterprise (DSE) allow a database to replicate some tables to a subset of regions. To achieve the same behavior as Cassandra or DSE, create a separate Astra DB instance that adheres to the necessary region restrictions. The database client will need to add a separate connection for the additional database and send queries to the appropriate connection depending on the table being queried.</p>"},{"location":"pages/astra/multi-regions/#limitations","title":"\u26a0\ufe0f Limitations","text":"<ul> <li>Lightweight transactions only work for a single-region datacenter.</li> <li>If your original region is disconnected, schema changes are suspended and repairs do not run. If any regions are disconnected, the writes to those regions will not be forwarded.</li> <li>While adding a new region, you cannot drop a table or keyspace and you cannot truncate a table.</li> <li>If any region is not online, you cannot truncate a table.</li> </ul>"},{"location":"pages/astra/multi-regions/#b-prerequisites","title":"B - Prerequisites","text":"<ul> <li> <p>You should have an Astra account</p> </li> <li> <p>You should have a CREDIT CARD in the system AND/OR have MORE THAN 25$ in Astra Credits.</p> </li> </ul> <p></p>"},{"location":"pages/astra/multi-regions/#c-create-a-new-region","title":"C - Create a new Region","text":"<p>\u2705 Step 1: Click the <code>Add Region</code> Button</p> <ul> <li>Select the database to show the Dashboard and select <code>Add Region.</code></li> </ul> <p></p> <p>\u2705 Step 2: Select your region</p> <ul> <li>Select your desired region from the dropdown menu.</li> </ul> <p></p> <ul> <li>Your selected region and the costs appear below the dropdown menu. You can add only a single region at a time.</li> </ul> <p></p> <p>\u2705 Step 3: Validate your region</p> <ul> <li> <p>Select Add Region to add the region to your database.</p> </li> <li> <p>The database with switch to Maintenance status. Do not worry, the existing regions will remain active and available for operations. There is no downtime.</p> </li> </ul> <p></p> <p>After you add the new region, your new region will show up in the list of regions on your database Dashboard.</p> <p></p> <p>After the initialization, you will get:</p> <p></p>"},{"location":"pages/astra/multi-regions/#d-delete-a-new-region","title":"D - Delete a new Region","text":"<p>\u2705 Step 1: Select Region to delete</p> <ul> <li>From your database Dashboard, select the overflow menu for the database that you want to delete and select Delete. You will notice that you CANNOT delete the original main region.</li> </ul> <p></p> <p>\u2705 Step 2: Validate your action</p> <ul> <li>Removing a region is not reversible so proceed with caution. A pop-up will ask you to validate this operation by entering the <code>delete</code> word.</li> </ul> <p></p> <ul> <li>The database will switch to Maintenance mode. After a few seconds, you will see the status of the deleted regions change from <code>Active</code> to <code>Offline</code>.</li> </ul> <p></p> <ul> <li>Finally the region will not be visible in the Regions list.</li> </ul> <p></p>"},{"location":"pages/astra/resume-db/","title":"\u2023 Resume a database","text":""},{"location":"pages/astra/resume-db/#a-overview","title":"A - Overview","text":"<p>In the free tier (serverless), after <code>48 hours</code> of inactivity, your database will be hibernated and the status will change to Hibernated. From there it needs to be resumed, there are multiple ways to do it.</p>"},{"location":"pages/astra/resume-db/#b-prerequisites","title":"B - Prerequisites","text":"<ul> <li>You should have an Astra account</li> </ul>"},{"location":"pages/astra/resume-db/#c-procedure","title":"C - Procedure","text":"<p>\u2705 Option 1: Resume with button in the User interface</p> <ul> <li>Access the database by clicking its name in the menu on the left.</li> </ul> <p></p> <ul> <li>For Vector Enabled Database the user interface is a little different but the process is the same.</li> </ul> <p></p> <ul> <li>Once the database is selected, on any tab you will get the <code>Resume Database</code> button available at top.</li> </ul> <p></p> <p>\u2705 Option 2: Resume with the CLI</p> <p>Assuming you have the Astra CLi installed and setup.</p> <pre><code>astra db resume &lt;my_db&gt;\n</code></pre> <p>\u2705 Option 3: Resume with a first request to the database</p> <p>Invoking and Stargate endpoints associated with your database will also trigger resuming. You would have to replace the <code>dbId</code>, <code>dbRegion</code> and <code>token</code> below with values for your environment.</p> <pre><code>curl --location \\\n--request GET 'https://{dbId}-{dbRegion}.apps.astra.datastax.com/api/rest/v2/schemas/keyspaces/' \\\n--header 'X-Cassandra-Token: {token}'\n</code></pre> <p>You will get a <code>503</code> error with the following payload.</p> <pre><code>{\n\"message\": \"Resuming your database, please try again shortly.\"\n}\n</code></pre> <ul> <li>In the user interface the status changes to <code>resuming...</code></li> </ul> <p></p> <ul> <li>After a few seconds the database will be active.</li> </ul> <p></p>"},{"location":"pages/data/","title":"Home","text":""},{"location":"pages/data/#load-and-export","title":"\ud83d\udce5 Load and Export","text":"<p>In this section are listed third party tools that will help you import or export data from your databases. They are designed for bulk operations.</p> <p> </p>"},{"location":"pages/data/#browse","title":"\ud83d\udd0d Browse","text":"<p>In this section are listed third party tools that will help you browse your data. You will find listed keyspaces, tables and will be able to edit the values.</p> <p> </p>"},{"location":"pages/data/#data-modelling","title":"\ud83d\udccb Data Modelling","text":""},{"location":"pages/data/explore/awsglue/","title":"AWS Glue","text":"\ud83d\udcd6 Reference Documentation and Resources <ol> <li>AWS Glue Documentation </li> </ol>"},{"location":"pages/data/explore/awsglue/#overview","title":"Overview","text":"<p>AWS Glue is a serverless data integration service that makes it easy for analytics users to discover, prepare, move, and integrate data from multiple sources. You can use it for analytics, machine learning, and application development. It also includes additional productivity and dataOps tooling for authoring, running jobs, and implementing business workflows.</p> <p>With AWS Glue, you can discover and connect to more than 70 diverse data sources and manage your data in a centralized data catalog. You can visually create, run, and monitor extract, transform, and load (ETL) pipelines to load data into your data lakes. Also, you can immediately search and query cataloged data using Amazon Athena, Amazon EMR, and Amazon Redshift Spectrum.</p> <ul> <li>\u2139\ufe0f What is Glue?</li> </ul> <p>AWS Glue uses the Astra JDBC Driver to connect to Cassandra as the storage backend. The Java driver itself supports connections to Astra DB natively.</p>"},{"location":"pages/data/explore/awsglue/#prerequisites","title":"Prerequisites","text":"<p>This tutorial will take you through the process of connecting your Astra database to Glue.  This process is somewhat extensive, so please take care to read all of the instructions carefully.</p> <ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database <ul> <li>Type: serverless</li> <li>Database: astraglue_db</li> <li>keyspace: astraglue_ks</li> </ul> </li> <li>You should Have an Astra Token</li> <li>You need an AWS account with permissions for Glue, S3, IAM, and the AWS Secrets Manager"},{"location":"pages/data/explore/awsglue/#step-1-setup","title":"Step 1 - Setup","text":""},{"location":"pages/data/explore/awsglue/#step-11-put-data-in-your-database","title":"\u2705 Step 1.1:  Put data in your database","text":"<ol> <li>From the Astra homepage, select your astraglue_db database from the list on the left hand side.</li> <li>Click the Load Data button at the top of the page.</li> <li>Download this sample CSV file.</li> <li>Drag the CSV file onto the file drop area and then click the Next button.</li> <li>Scroll to the bottom and choose a partition key (country_name).</li> <li>For \"Target Keyspace\" use the astraglue_ks you created as a prerequisite.</li> <li>Click \"Finish\".</li> <li>Move on to the next steps, the upload should be done before you need the data.</li> </ol>"},{"location":"pages/data/explore/awsglue/#step-12-create-role-in-iam","title":"\u2705 Step 1.2:  Create Role in IAM","text":"<ol> <li>Open the AWS Identity and Access Management console.</li> <li> <p>In the left hand column, select Select Roles.</p> <p> Show me! </p> </li> <li> <p>On the right hand side, click the Create Role button.</p> </li> <li> <p>For your Trusted entity type, choose  AWS service.  In the \"Use cases for other AWS Services\" type and select \"Glue\".</p> </li> <li> <p>Select Next.</p> </li> <li> <p>On the Add permissions page you will need to search for a few different permissions.  You should select the following:</p> <ul> <li>AmazonS3FullAccess</li> <li>AWSGlueServiceRole</li> <li>AWSGlueConsoleFullAccess</li> <li>SecretsManagerReadWrite  Show me! </li> </ul> </li> <li> <p>On the Name, review, and create page, choose a name for your role (For example purposes I will use AstraGlueRole), then scroll to the bottom of the page and click the blue Create role button.</p> </li> </ol>"},{"location":"pages/data/explore/awsglue/#step-13-setup-jdbc-driver-in-s3","title":"\u2705 Step 1.3:  Setup JDBC Driver in S3","text":"<ol> <li>Download Astra JDBC connector jar from Github.</li> <li>Open the S3 Console.</li> <li>Click the Create bucket button on the right hand side.</li> <li>Choose a bucket name - this must be unique across all accounts so you will need to pick something unique to you.</li> <li>Choose which type of permission model to use.  I chose here to use ACLs as they are easier for managing access to buckets and their contents.      Show me! </li> <li>Scroll to the bottom and click the orange Create bucket button.</li> <li>Open the bucket by clicking its name on the bucket listings.</li> <li>Click the Upload button and follow the steps to upload the driver you just downloaded.</li> <li>Once that's done, from the Objects page for your bucket in S3, click on the driver.</li> <li>From here you can copy the S3 URI, which you will need later, or you can get it when it's needed.</li> </ol>"},{"location":"pages/data/explore/awsglue/#step-14-secrets-manager","title":"\u2705 Step 1.4:  Secrets Manager","text":"<p>This assumes that you have gone through the process of getting an Astra token for your database.</p> <ol> <li>Open the AWS Secrets Manager console.</li> <li>Click the orange Store a new secret button on the right hand side.</li> <li>Choose Other type of secret.</li> <li>Add key/value pairs for user and password:<ul> <li>user: <code>token</code></li> <li>password: Your AstraCS token here  Show me! </li> </ul> </li> <li>Click the orange Next button at the bottom of the page.</li> <li>Choose a secret name like \"AstraGlueCreds\" and click Next.</li> <li>On the \"Configure Rotation\" screen just click Next.</li> <li>Review the entries then click Store.</li> </ol> <p>Ok, that was a lot of steps, great job getting things set up. Feel free to take a moment before moving on to the next section.</p>"},{"location":"pages/data/explore/awsglue/#step-2-glue-connector","title":"Step 2 - Glue Connector","text":"<p>Now that all of the pieces have been put in place, it's time to create the connector and connection from Astra to Glue.</p>"},{"location":"pages/data/explore/awsglue/#step-21-create-glue-connector","title":"\u2705 Step 2.1:  Create Glue Connector","text":"<ol> <li>Open the AWS Glue Studio console.</li> <li>In the left hand column, click on Data connections.</li> <li>Click on Create custom connector in the \"Custom connectors\" section.</li> <li>Paste the S3 URI for your JDBC driver under Connector S3 URL.  If you need to retrieve this URI you can find it by browsing from the S3 Console.</li> <li>Choose a Name for your connector.</li> <li>Select JDBC as the connector type.</li> <li>For the Class name enter com.datastax.astra.jdbc.AstraJdbcDriver.</li> <li>The JDBC URL base is composed of the following pieces<ul> <li>jdbc:astra:///?user=token&amp;password= <li>If you followed the instructions on naming your db and keyspace it will be:<ul> <li>jdbc:astra://astraglue_db/astraglue_ks?user=token&amp;password=AstraCS:YourTokenHere</li> </ul> </li> <li>(yes, this is the same username/password you used in the secrets manager, just go with it)</li> <li>For the URL parameter delimiter enter '&amp;'.      Show me! </li> <li>Click Create connector.</li>"},{"location":"pages/data/explore/awsglue/#step-22-create-connection","title":"\u2705 Step 2.2:  Create Connection","text":"<p>From the Connectors page (Data connections in the left panel):</p> <ol> <li>Click on your connector name in the central Connectors section.</li> <li>Click the Create connection button.</li> <li>Enter a Name for your connection.</li> <li>Under Connection credential type select \"default\".</li> <li>Under AWS Secret - optional choose the secret you created during setup.</li> <li>Click the Create connection button at the bottom of the page.</li> </ol>"},{"location":"pages/data/explore/awsglue/#step-3-job-setup","title":"Step 3 - Job setup","text":""},{"location":"pages/data/explore/awsglue/#31-job-details","title":"3.1 - Job details","text":"<p>From the Connectors page (Data connections in the left panel):</p> <ol> <li>Click on your connection name in the central Connections section.</li> <li>Click the orange Create job button.</li> <li>Enter a name for the job at the top of the console</li> <li>Click on the node with your connection name in the visual editor.</li> <li>Under Table name enter \"demographics\".</li> <li>Click the Data preview tab on the right hand side of the page.</li> <li>Click Start data preview session to start the data transfer.</li> <li>Wait for it to complete.  You should see the data from the original Astra database here.  This indicates that the extraction of the data from Astra has successfully completed.</li> <li>Click the Output schema tab on the right side of the page.  Choose Use datapreview schema.</li> </ol>"},{"location":"pages/data/explore/awsglue/#33-transform","title":"3.3 - Transform","text":"<ol> <li>Click the ApplyMapping node in the visual editor.</li> <li>Under Transform you will see the fields from the Connection node.</li> <li>Output schema shows the schema it will send forward.</li> <li>Data preview shows the data.</li> </ol> <p>Click Save at the upper right of the page to save your work for later.</p>"},{"location":"pages/data/explore/awsglue/#step-4-load-into-glue-tables","title":"Step 4 - Load into Glue Tables","text":"<p>At this point your data has been loaded into the system and you can use any load node going forward; if you wish to load your data into a Glue database and table, move on to the next step</p>"},{"location":"pages/data/explore/awsglue/#41-create-buckets","title":"4.1 - Create buckets","text":"<p>A Glue database requires a separate S3 bucket for storing your data.</p> <ol> <li>Open the S3 Console.</li> <li>Create a new empty bucket for Glue to use (see the steps above during setup for details).  Name it something memorable for you, like <code>astradatabase</code>.</li> </ol>"},{"location":"pages/data/explore/awsglue/#42-create-database-and-table","title":"4.2 - Create database and table","text":"<ol> <li>In the AWS Glue console, choose Data Catalog/Databases from the left column.</li> <li>Click the orange Add database button at the top right</li> <li>Name your database and click the orange Create database button at the bottom.</li> <li>Click on Tables in the left hand panel.</li> <li>Click Add table.<ul> <li>Name your table whatever you like.</li> <li>Choose the database you just created.</li> <li>Data store is S3:<ul> <li>Browse and select the S3 bucket you created, with a slash at the end (you may need to click outside the box for it to accept your entry). The prefix is not needed for this entry.</li> </ul> </li> <li>Data format is 'CSV' with Comma(,) as the delimiter.</li> <li>Click Next.</li> </ul> </li> <li>Next is Choose or define schema.<ul> <li>Download the Schema</li> <li>Click Edit schema as JSON.</li> <li>Click Choose file and pick the schema.txt file you downloaded.</li> <li>Click Save then Next.</li> <li>Review the entry and then click Create.</li> </ul> </li> </ol>"},{"location":"pages/data/explore/awsglue/#43-add-glue-database-target","title":"4.3 - Add Glue Database target","text":"<ol> <li>Open your job from the list of ETL entries(ETL Jobs on the Glue navbar).</li> <li>Select the ApplyMapping node.</li> <li>Click the big plus circle to add a new node to the flow.</li> <li>Click Data then collapse Sources and expand Targets.</li> <li>Choose the Glue Data Catalog target.</li> <li>For the Glue Data Catalog configure it in the right panel:<ul> <li>Make sure the parent node is ApplyMapping.</li> <li>Choose your database and table.</li> <li>Save your job and click Run.</li> </ul> </li> </ol>"},{"location":"pages/data/explore/awsglue/#44-view-table-requires-athena","title":"4.4 - View table (requires Athena)","text":"<ol> <li>Click on Tables in the left hand column, under Data Catalog/Databases.</li> <li>Click Table data for the table you created/populated.</li> <li>Acknowledge the charges for Athena.</li> <li>You will be taken to the Athena console.</li> <li>If the Run button is not active:<ul> <li>Go to the Settings tab in the editor.</li> <li>Click the Manage button.</li> <li>Click Browse S3 next to \"Location of query result\", locate the bucket you created earlier to store the Glue data, and click \"Choose\" to confirm.</li> <li>Click Save to leave the settings management and go back to the Editor tab of Athena.</li> </ul> </li> <li>Check out the resulting data by clicking the Run button.</li> </ol>"},{"location":"pages/data/explore/cqlsh/","title":"\u2023 Cqlsh","text":"\ud83d\udcd6 Reference Documentations and resources <ol> <li>\ud83d\udcd6  Astra Docs - Reference documentation <li>\ud83d\udcd6  Cql Tool Docs - Reference Documentation"},{"location":"pages/data/explore/cqlsh/#a-overview","title":"A - Overview","text":"<p>CqlSH is a command-line interface for interacting with Cassandra using CQL (the Cassandra Query Language). It is shipped with every Cassandra package, and can be found in the bin/ directory alongside the cassandra executable. cqlsh is implemented with the Python native protocol driver, and connects to the single specified node.</p> <p>You can setup the software by providing options in the command line and/OR provide the settings in a file called <code>cqlshrc</code> located in <code>~/.cassandra</code></p> <pre><code>&gt; cqlsh --help\nUsage: cqlsh [options] [host [port]]\n\nCQL Shell\n\nOptions:\n  --version             show program's version number and exit\n  -h, --help            show this help message and exit\n  -C, --color           Always use color output\n  --no-color            Never use color output\n  --browser=BROWSER     The browser to use to display CQL help, where BROWSER\n                        can be:\n                        - one of the supported browsers in\n                        https://docs.python.org/2/library/webbrowser.html.\n                        - browser path followed by %s, example: /usr/bin\n                        /google-chrome-stable %s\n  --ssl                 Use SSL\n  -u USERNAME, --username=USERNAME\n                        Authenticate as user.\n  -p PASSWORD, --password=PASSWORD\n                        Authenticate using password.\n  -k KEYSPACE, --keyspace=KEYSPACE\n                        Authenticate to the given keyspace.\n  -b SECURE_CONNECT_BUNDLE, --secure-connect-bundle=SECURE_CONNECT_BUNDLE\n                        Connect using secure connect bundle. If this option is\n                        specified host, port settings are ignored\n  -f FILE, --file=FILE  Execute commands from FILE, then exit\n  --debug               Show additional debugging information\n  --coverage            Collect coverage data\n  --encoding=ENCODING   Specify a non-default encoding for output. (Default:\n                        utf-8)\n  --cqlshrc=CQLSHRC     Specify an alternative cqlshrc file location.\n  --cqlversion=CQLVERSION\n                        Specify a particular CQL version, by default the\n                        highest version supported by the server will be used.\n                        Examples: \"3.0.3\", \"3.1.0\"\n  --protocol-version=PROTOCOL_VERSION\n                        Specify a specific protocol version; otherwise the\n                        client will default and downgrade as necessary.\n                        Mutually exclusive with --dse-protocol-version.\n  -e EXECUTE, --execute=EXECUTE\n                        Execute the statement and quit.\n  --connect-timeout=CONNECT_TIMEOUT\n                        Specify the connection timeout in seconds (default: 5\n                        seconds).\n  --request-timeout=REQUEST_TIMEOUT\n                        Specify the default request timeout in seconds\n                        (default: 10 seconds).\n  --consistency-level=CONSISTENCY_LEVEL\n                        Specify the initial consistency level.\n  --serial-consistency-level=SERIAL_CONSISTENCY_LEVEL\n                        Specify the initial serial consistency level.\n  -t, --tty             Force tty mode (command prompt).\n  --no-file-io          Disable cqlsh commands that perform file I/O.\n  --disable-history     Disable saving of history\n\nConnects to 127.0.0.1:9042 by default. These defaults can be changed by\nsetting $CQLSH_HOST and/or $CQLSH_PORT. When a host (and optional port number)\nare given on the command line, they take precedence over any defaults.\n</code></pre>"},{"location":"pages/data/explore/cqlsh/#b-prerequisites","title":"B - Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should download the Cqlsh Version for Astra DB</li> <li>You should NOT use a WINDOWS machine, as of today <code>cqlsh</code> is not supported on Windows</li> </ul>"},{"location":"pages/data/explore/cqlsh/#c-installation","title":"C - Installation","text":"<p>\u2705 Step 1: Download and extract the archive</p> <ul> <li>To download the archive you can go on the download page, check the box and download the file:</li> </ul> <p></p> <ul> <li>You can also use the command line:</li> </ul> <pre><code>wget https://downloads.datastax.com/enterprise/cqlsh-astra.tar.gz \\\n          &amp;&amp; tar xvzf cqlsh-astra.tar.gz \\\n          &amp;&amp; rm -f cqlsh-astra.tar.gz\n</code></pre> <ul> <li>The archive should look like:</li> </ul> <p></p> <p>\u2705 Step 2: Start <code>cqlsh</code> providing parameters in the command line:</p> <ul> <li>From the directory where you extracted the CQLSH tarball, run the <code>cqlsh</code> script from the command line:</li> </ul> <pre><code>$ cd /cqlsh-astra/bin\n\n$ ./cqlsh -u ${CLIENT_ID} -p ${CLIENT_SECRET} -b ${PATH_TO_SECURE_BUNDLE.zip}\n</code></pre> <ul> <li><code>-u</code> (username) - Client ID as provided in the token generation page</li> <li><code>-p</code> (password) - Client secret as provided in the token generation page</li> <li><code>-b</code> (bundle) - location of the secure connect bundle that you downloaded for your database.</li> </ul> <p>\u2705 Step 3: Start <code>Cqlsh</code> providing parameters in <code>cqlshrc</code></p> <p>Configure the cqlshrc file If you do not want to pass the secure connect bundle on the command line every time, set up the location in your <code>cqlshrc</code> file in <code>~/.cassandra</code></p> <pre><code>[authentication]\nusername = ${CLIENT_ID}\npassword = ${CLIENT_SECRET}\n\n[connection]\nsecure_connect_bundle = ${PATH_TO_SECURE_BUNDLE.zip}\n</code></pre>"},{"location":"pages/data/explore/cqlsh/#d-tips-and-tricks","title":"D - Tips and tricks","text":"<ul> <li> <p>If is a good idea to add <code>cqlsh</code> in your path to be able to use from everywhere</p> </li> <li> <p>If you want to work with multiple DB use some alias with the parameters</p> </li> </ul> <pre><code>alias cqlsh_db1='cqlsh -u user -p password -b secure-connect-db1.zip'\nalias cqlsh_db2='cqlsh --cqlshrc_db2'\n</code></pre>"},{"location":"pages/data/explore/datagrip/","title":"DataGrip","text":""},{"location":"pages/data/explore/datagrip/#overview","title":"Overview","text":"<ul> <li>\u2139\ufe0f Astra Docs - Reference documentation</li> <li>\u2139\ufe0f Instructions on Sebastian Estevez's blog post</li> <li>\u2139\ufe0f Datagrip reference documentation</li> </ul> <p>DataGrip is a database management environment for developers. It is designed to query, create, and manage databases. Databases can work locally, on a server, or in the cloud. Supports MySQL, PostgreSQL, Microsoft SQL Server, Oracle, and more. If you have a JDBC driver, add it to DataGrip, connect to your DBMS, and start working.</p>"},{"location":"pages/data/explore/datagrip/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create an Astra Database</li> <li>Create an Astra Token</li> <li>Download your secure connect bundle ZIP</li> <li>Download and install DataGrip</li> </ul>"},{"location":"pages/data/explore/datagrip/#astra-community-jdbc-drivers","title":"Astra Community JDBC Drivers","text":""},{"location":"pages/data/explore/datagrip/#1-download-jdbc-driver","title":"1. Download JDBC Driver","text":"<p>Download latest archive astra-jdbc-driver-x.y.jar  from Github Release page</p>"},{"location":"pages/data/explore/datagrip/#2-configure-the-connection","title":"2. Configure the Connection","text":"<ol> <li>Select <code>Drivers</code> Tab </li> <li>Click the plus <code>+</code> symbol to create a new User Driver</li> <li>Populate the name as you like, in the screenshot we picked <code>Astra JDBC Driver</code></li> <li>Add the shaded jar by clicking the plus <code>+</code> symbol in the <code>Driver Files</code> panel.</li> <li>For the <code>Class</code> field, select the following (the list has been build by scanner the library we just imported)</li> </ol> <pre><code>com.datastax.astra.jdbc.AstraJdbcDriver\n</code></pre>"},{"location":"pages/data/explore/datagrip/#3-create-the-datasource","title":"3. Create the DataSource","text":"<ul> <li>1. Select <code>Data Source</code> tabs.Using the <code>+</code> add a new Data source pick the driver we just created from the list</li> <li>2. Define a name for your datasource on the screenshot we picked <code>Astra JDBC DataSource</code></li> <li>3. Validate that you are using the driver define above, for us <code>Astra JDBC Driver</code> </li> <li>4. Provide User Name. It can be the string <code>token</code> or the value of a <code>clientId</code>.</li> <li>5. Provide the password. It can be the value for your starting with AstraCS:...  or the value of a <code>clientSecret</code>.</li> <li>6. Provide the URL as a single line</li> </ul> <pre><code>jdbc:astra://&lt;db&gt;/&lt;keyspace&gt;?region=&lt;region&gt;\n</code></pre> Field Required? Description db YES It is your database identifier it can be a name (then it must be unique) or a database identifier (UUID) keyspace YES It is the keyspace you want to use. region NO Only useful if the database lives on multiple regions <ul> <li> <p>7. Test the connection you should get the following screen, apply and save.</p> </li> <li> <p>8. Validate with <code>Apply</code></p> </li> </ul>"},{"location":"pages/data/explore/datagrip/#4-use-datasource","title":"4. Use DataSource","text":"<ol> <li>Select the keyspace you want to use</li> </ol> <ol> <li>Enjoy your working environment</li> </ol>"},{"location":"pages/data/explore/datagrip/#using-jdbc-simba-drivers","title":"Using JDBC Simba Drivers","text":"<p>Tips for <code>SecureConnectionBundlePath</code></p> <p>You need to be a registered customer to use those drivers. If not consider the ING Driver alternative. </p>"},{"location":"pages/data/explore/datagrip/#1-download-jdbc-driver_1","title":"1. Download JDBC Driver","text":"<p>Download the JDBC driver from the Datastax customer support website </p> <ol> <li>Authenticate to Datastax Customer Portal.</li> <li>Select Simba JDBC Driver for Apache Cassandra.</li> <li>Select JDBC 4.2.</li> <li>Read the license terms and accept it (click the checkbox).</li> <li>Hit the blue Download button.</li> <li>Once the download completes, unzip the downloaded file.</li> </ol>"},{"location":"pages/data/explore/datagrip/#2-download-settingszip","title":"2.  Download <code>Settings.zip</code>","text":"<ul> <li>Download the settings.zip locally</li> </ul> <p>Think about backing up your <code>settings.xml</code></p> <p>If you are already a DataGrip user, back up your existing settings because downloading <code>settings.zip</code> might override your existing settings.</p>"},{"location":"pages/data/explore/datagrip/#3-import-settingszip-file","title":"3.  Import settings.zip file","text":"<ul> <li> <p>Selecting <code>File</code> \u2192 <code>Manage IDE Settings</code> \u2192 <code>Import Settings</code> in DataGrip.</p> </li> <li> <p>From the directory menu, select the <code>settings.zip</code> file from the directory where it is stored.</p> </li> <li> <p>Select Import and Restart.</p> </li> </ul> <p>You will see a new database connection type called Astra: Simba Cassandra JDBC 4.2 driver shown.</p> <p></p> <ul> <li>Go to the Driver <code>Advanced Settings</code> TAB to confirm the VM home path is set to <code>Default</code>.</li> </ul> <p></p>"},{"location":"pages/data/explore/datagrip/#4-establish-the-connection","title":"4.  Establish the connection","text":"<ul> <li> <p>The credentials are provided in the URL so for authentication field you can pick <code>No auth</code> in the select drop down.</p> </li> <li> <p>When you create your connection, the URL will look like this (on a single line): </p> </li> </ul> <pre><code>jdbc:cassandra://;AuthMech=2;\nUID=token;\nPWD=&lt;AstraCS:... your application token&gt;;\nSecureConnectionBundlePath=&lt;PATH TO YOUR SECURE CONNECT BUNDLE&gt;;\nTunableConsistency=6\n</code></pre> <p>Tips for <code>SecureConnectionBundlePath</code></p> <ul> <li> <p>You should use <code>/</code> as a path separator even on Windows.</p> </li> <li> <p>The use of quotes for the path is not supported, please try to provide a path with no spaces.</p> </li> </ul> <p></p> <p>URL in the screenshot shows the format described in the previous sentence.</p> <ul> <li>AuthMech: Specifies whether the driver connects to a Cassandra or Astra DB database and whether the driver authenticates the connection.</li> <li>ApplicationToken: Generated from Astra DB console.</li> <li>SecureConnectionBundlePath: Path to where your downloaded Secure Connect Bundle is located.</li> <li>TunableConsistency: Specifies Cassandra replica or the number of Cassandra replicas that must process a query for the query to be considered successful.</li> </ul>"},{"location":"pages/data/explore/dbeaver/","title":"DBeaver","text":"<ul> <li>This article includes information that was originally written by Erick Ramirez on DataStax Community</li> </ul>"},{"location":"pages/data/explore/dbeaver/#overview","title":"Overview","text":"<p>DBeaver is a universal database management tool for everyone who needs to work with data in a professional way. With DBeaver you are able to manipulate with your data like in a regular spreadsheet, create analytical reports based on records from different data storages, export information in an appropriate format.</p> <ul> <li>\u2139\ufe0f Introduction to DBeaver</li> <li>\ud83d\udce5 DBeaver Download Link</li> </ul>"},{"location":"pages/data/explore/dbeaver/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle</li> </ul> <p>This article assumes you have installed DBeaver Community Edition on your laptop or PC. It was written for version 21.2.0 on MacOS but it should also work for the Windows version.</p>"},{"location":"pages/data/explore/dbeaver/#astra-community-jdbc-drivers","title":"Astra Community JDBC Drivers","text":""},{"location":"pages/data/explore/dbeaver/#1-download-jdbc-driver","title":"1. Download JDBC Driver","text":"<p>Download latest archive astra-jdbc-driver-x.y.jar  from Github Release page</p>"},{"location":"pages/data/explore/dbeaver/#2-configure-the-connection","title":"2. Configure the Connection","text":"<p>The following screenshot are done with Dbeaver lite on MACOS</p> <p>Go the DBeaver download page and download the version associated with your laptop.</p> <ul> <li> <p>Open DBeaver application</p> </li> <li> <p>In the menu go <code>Database &gt; Driver manager</code></p> </li> <li> <p>The Panel Click <code>new</code> without selecting anything</p> </li> </ul> <p></p> <ul> <li>(1) Choose the TAB <code>libraries</code> </li> <li>(2) Select <code>Add File</code>, look for the jar we just downloaded.</li> </ul> <p></p> <ul> <li>(1) Go back to the tab <code>Settings</code> and enter the following values:</li> <li>(2) Driver Name: <code>Astra JDBC Driver</code> and driver Type: <code>Generic</code></li> <li>(3) Class Name: <code>com.datastax.astra.jdbc.AstraJdbcDriver</code></li> <li>(4) Username is <code>token</code></li> <li>(5) Save your modifications with <code>[OK]</code></li> </ul> <p></p> <ul> <li>You have new entry in the list of drivers called <code>Astra JDBC Driver</code></li> </ul> <p></p>"},{"location":"pages/data/explore/dbeaver/#3-create-the-datasource","title":"3. Create the DataSource","text":"<ul> <li>In the menu now select <code>Database &gt; New Database Collection</code></li> </ul> <ul> <li>Pick the driver <code>Astra JDBC Driver</code> and click <code>Next</code></li> </ul> <ul> <li> <p>Populate the URL as follow: <code>jdbc:astra://&lt;db&gt;/&lt;keyspace&gt;?region=&lt;region&gt;</code></p> </li> <li> <p>Populate your token value (it should start by <code>AstraCS:...</code>)</p> </li> </ul> <p></p> <ul> <li>Click on <code>Test Connection</code> and validate with <code>Finish</code></li> </ul> <p></p>"},{"location":"pages/data/explore/dbeaver/#4-use-datasource","title":"4. Use DataSource","text":"<ul> <li>On the left panel you can now see your data</li> </ul>"},{"location":"pages/data/explore/dbeaver/#using-jdbc-simba-drivers","title":"Using JDBC Simba Drivers","text":""},{"location":"pages/data/explore/dbeaver/#1-download-jdbc-driver_1","title":"1. Download JDBC Driver","text":"<p>Download the JDBC driver from the DataStax website:</p> <ol> <li>Go to https://downloads.datastax.com/#odbc-jdbc-drivers.</li> <li>Select Simba JDBC Driver for Apache Cassandra.</li> <li>Select JDBC 4.2.</li> <li>Read the license terms and accept it (click the checkbox).</li> <li>Hit the blue Download button.</li> <li>Once the download completes, unzip the downloaded file.</li> </ol>"},{"location":"pages/data/explore/dbeaver/#2-import-driver","title":"2.  Import Driver","text":"<ol> <li>Go to the Driver Manager.</li> <li>Click the New button.</li> <li>In the Libraries tab, click the Add File button.</li> <li>Locate the directory where you unzipped the driver download and add the <code>CassandraJDBC42.jar</code> file.</li> <li>Click the Find Class button which should identify the driver class as <code>com.simba.cassandra.jdbc42.Driver</code>.</li> <li> <p>In the Settings tab, set the following:</p> </li> <li> <p>Driver Name: <code>Astra DB</code></p> </li> <li>Driver Type: <code>Generic</code></li> <li> <p>Class Name: <code>com.simba.cassandra.jdbc42.Driver</code> </p> </li> <li> <p>Click the OK button to save the driver</p> </li> </ol> <p>At this point, you should see Astra DB as one of the drivers on the list:</p> <p> </p>"},{"location":"pages/data/explore/dbeaver/#3-create-new-connection","title":"3.  Create New Connection","text":"<p>Connect to your Astra DB in DBeaver:</p> <ol> <li>Open the New Database Connection dialog box.</li> <li>Select Astra DB from the list of drivers.</li> <li>In the Main tab, set the JDBC URL to:    <code>jdbc:cassandra://;AuthMech=2;TunableConsistency=6;SecureConnectionBundlePath=/path/to/secure-connect-dbeaver.zip</code> Note That you will need to specify the full path to your secure bundle.</li> <li>In the Username field, enter the string <code>token</code></li> <li> <p>In the Password field, paste the value of the token you created in the Prerequisites section above. The token looks like <code>AstraCS:AbC...XYz:123...edf0</code>.    </p> </li> <li> <p>Click on the Connection details button</p> </li> <li>In Connection name field, give your DB connection a name: </li> <li>Click the Finish button</li> <li>Click on the Test Connection button to confirm that the driver configuration is working:</li> </ol> <p> </p>"},{"location":"pages/data/explore/dbeaver/#4-final-test","title":"4.  Final Test","text":"<p>Connect to your Astra DB. If the connection was successful, you should be able to explore the keyspaces and tables in your DB on the left-hand side of the UI.</p> <p>Here's an example output:</p> <p></p>"},{"location":"pages/data/explore/dbschema/","title":"DbSchema","text":"\ud83d\udcd6 Reference Documentations and resources <ol> <li>\ud83d\udcd6 Astra Docs - Reference documentation <li>DBSchema Tutorials </li> </li> </ol>"},{"location":"pages/data/explore/dbschema/#overview","title":"Overview","text":"<p>DbSchema is a universal database designer for out-of-the-box schema management and documentation, sharing the schema in the team, and deploying on different databases. Visual tools can help developers, database administrators, and decision-makers to query, explore and manage the data.</p> <ul> <li>\u2139\ufe0f Introduction to DBSchema</li> <li>\ud83d\udce5 DBSchema Installation</li> </ul> <p>DBSchema uses the Astra JDBC Driver to connect to Cassandra as the storage backend. The Java driver itself supports connections to Astra DB natively.</p>"},{"location":"pages/data/explore/dbschema/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure bundle</li> </ul> <p>This article assumes you have installed the latest version of DBSchema on your laptop or PC.</p>"},{"location":"pages/data/explore/dbschema/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"pages/data/explore/dbschema/#step-1-jdbc-driver","title":"\u2705 Step 1:  JDBC Driver","text":"<p>Download Astra JDBC connector jar  from Github</p>"},{"location":"pages/data/explore/dbschema/#step-2-establish-the-connection","title":"\u2705 Step 2:  Establish the Connection","text":"<ol> <li>Open DB Schema</li> <li>Select Connect to the Database</li> <li> <p>Select Start </p> </li> <li> <p>In the Choose your database menu, select Cassandra.</p> </li> <li> <p>Select Next. </p> </li> <li> <p>Select JDBC Driver edit option.  This is the button on the right hand side of the JDBC driver line, with the key icon. </p> </li> <li> <p>In the JDBC Driver Manager, select New.</p> </li> <li> <p>In the Add RDBMS window, enter Astra and select OK </p> </li> <li> <p>Select OK in the confirmation message. </p> </li> <li> <p>Upload the Astra JDBC Driver.</p> </li> <li>Select Open</li> <li>Once you upload the Astra JDBC Driver, you will see Astra in the Choose your Database window. Select Next.</li> </ol> <p></p> <ol> <li> <p>In the connection window, select the JDBC Driver \"astra-jdbc-connector-5.0.jar com.datastax.astra.jdbc.AstraJdbcDriver.  Under JDBC URL select \"Edit Manually\".</p> </li> <li> <p>In the Astra Connection Dialog, add JDBC URL as     <pre><code>jdbc:astra://&lt;database_name&gt;/&lt;keyspace_name&gt;?token=&lt;application_token&gt;\n</code></pre>     with the following variables:</p> <ul> <li>database_name: The name or ID for the database you want to connect to</li> <li>keyspace_name: The keyspace you want to use</li> <li>application_token: Generated from Astra DB console. See Manage application tokens.</li> </ul> </li> <li> <p>Select Connect </p> </li> <li> <p>In the Select Schemas/Catalogs, select the keyspace to which you want to connect.</p> </li> <li>Select OK. </li> </ol>"},{"location":"pages/data/explore/dbschema/#step-3-final-test","title":"\u2705 Step 3:  Final Test","text":"<p>Now that your connection is working, you can create tables, introspect your keyspaces, view your data in the DBSchema GUI, and more.</p> <p>To learn more about DBSchema, see Quick start with DBSchema</p>"},{"location":"pages/data/explore/mindsdb/","title":"MindsDB","text":"<ul> <li>This article was originally written by Steven Matison on DataStax JIRA</li> </ul>"},{"location":"pages/data/explore/mindsdb/#overview","title":"Overview","text":"<p>This page will go into details about what I had to do to build the project, modify the <code>cassandra.py</code> and <code>scylla_ds.py</code>, and get mindsdb GUI connected to Astra.</p> <ul> <li>Source Repo: GitHub - mindsdb/mindsdb</li> <li>My Fork: ds-steven-matison/mindsdb</li> <li>Docs: https://docs.mindsdb.com/</li> </ul>"},{"location":"pages/data/explore/mindsdb/#process","title":"Process","text":""},{"location":"pages/data/explore/mindsdb/#files-changed","title":"Files Changed","text":"<pre><code>/root/mindsdb/lib64/python3.6/site-packages/mindsdb_datasources/datasources/scylla_ds.py\n</code></pre> <ul> <li>Path here is correct. \u201clib64\u201d not part of repo, so I put scylla_ds.py in repo so you can see source code here: github diff</li> </ul> <pre><code>/root/mindsdb/lib64/python3.6/site-packages/mindsdb_datasources/cassandra.py\n</code></pre> <ul> <li>Changes diff in repo here: github diff</li> </ul>"},{"location":"pages/data/explore/mindsdb/#git-status","title":"Git Status","text":"<pre><code>Untracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\nLICENSE.txt\n    MindsDB.egg-info/\n    bin/\n    config.json\n    include/\n    lib/\n    lib64\n    mindsdb/integrations/cassandra/cassandra.py.bk\n    mindsdb/integrations/cassandra/tmp.py\n    pip-selfcheck.json\n    share/\n</code></pre>"},{"location":"pages/data/explore/mindsdb/#terminal-history","title":"Terminal History","text":"<pre><code>  503  python -v\n  504  python -version\n  505  ls\n  506  python3 -v\n  507  python3 -m venv mindsdb\n  508  source mindsdb/bin/activate\n  509  pip3 install mindsdb\n  510  pip3 install Cython\n  511  pip3 install mindsdb\n  512  pip3 install sentencepiece\n  513  pip3 freeze\n  514  pip install --upgrade pip\n  515  pip3 install sentencepiece\n  516  pip3 install mindsdb\n  517  pip3 freeze\n  518  python3 -m mindsdb\n  533  pip3 install mindsdb\n  548  pip3 install cassandra-driver\n  549  python3 -c 'import cassandra; print (cassandra.__version__)'\n570  python3 mindsdb_cassandra.py\n  571  nano mindsdb_cassandra.py\n  572  python3 mindsdb_cassandra.py\n  573  python3 -m mindsdb --api=mysql --config=config.json\n  574  pip3 uninstall mindsdb\n  577  git clone https://github.com/ds-steven-matison/mindsdb.git\n  578  cd mindsdb/mindsdb/integrations/cassandra/\n  579  ls\n  580  nano cassandra.py\n  581  cp cassandra.py cassandra.py.bk\n  582  nano cassandra.py\n  588  pip3 install -r requirements.txt\n  592  pip3 install cassandra-driver\n  636  cp secure-connect-mindsdb.zip /tmp\n  637  chmod 755 /tmp/secure-connect-mindsdb.zip\n  654  pip3 freeze\n  658  pip3 install cassandra-driver\n  659  python3 -c 'import cassandra; print (cassandra.__version__)'\n662  nano /root/mindsdb/lib64/python3.6/site-packages/mindsdb_datasources/cassandra.py\n  669  nano ./mindsdb/lib/python3.6/site-packages/cassandra/cluster.py\n  670  nano /root/mindsdb/lib64/python3.6/site-packages/mindsdb_datasources/datasources/scylla_ds.py\n  672  python3 -m venv mindsdb\n  673  source mindsdb/bin/activate\n  674  cd mindsdb &amp;&amp; pip3 install -r requirements.txt\n  675  pip install --upgrade pip\n  676  pip3 install --upgrade pip\n  677  pip3 install -r requirements.txt\n  678  python setup.py develop\n  683  mkdir /storage\n  695  install mindsdb_native[cassandra]\n696  pip3 install mindsdb-sdk\n  721  nano config.json\n  722  python3 -m mindsdb --config=config.json\n</code></pre>"},{"location":"pages/data/explore/netflix-data-explorer/","title":"Netflix Data Explorer","text":"\ud83d\udcd6 Reference Documentations and resources <ol> <li>\ud83d\udcd6 Netlix Blog - Introduction of the tool by Netflix <li>Github Repository - Core project  <li>Github Repository - Fork for Astra  </li> </li> </li> </ol>"},{"location":"pages/data/explore/netflix-data-explorer/#overview","title":"Overview","text":"<p>The Data Explorer by netflix is a web-based tools that will help you navigate and edit your data. It supports both Cassandra and Dynomite but here we will focus on Astra. There a few killer features</p>"},{"location":"pages/data/explore/netflix-data-explorer/#multi-cluster-access","title":"Multi Cluster Access","text":"<p>Multi-cluster access provides easy access to all of the clusters in your environment. The cluster selector in the top nav allows you to switch to any of your discovered clusters quickly.</p> <p> </p>"},{"location":"pages/data/explore/netflix-data-explorer/#explore-your-data","title":"Explore your data","text":"<p>The Explore view provides a simple way to explore your data quickly. You can query by partition and clustering keys, insert and edit records, and easily export the results or download them as CQL statements.</p> <p> </p>"},{"location":"pages/data/explore/netflix-data-explorer/#schema-designer","title":"Schema Designer","text":"<p>Creating a new Keyspace and Table by hand can be error-prone</p> <p>Our schema designer UI streamlines creating a new Table with improved validation and enforcement of best practices.</p> <p> </p>"},{"location":"pages/data/explore/netflix-data-explorer/#query-ide","title":"Query IDE","text":"<p>The Query Mode provides a powerful IDE-like experience for writing free-form CQL queries.</p> <p></p>"},{"location":"pages/data/explore/netflix-data-explorer/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle</li> </ul>"},{"location":"pages/data/explore/netflix-data-explorer/#procedure","title":"Procedure","text":""},{"location":"pages/data/explore/netflix-data-explorer/#1-run-locally","title":"1 Run Locally","text":"<p>Prerequisites: You need <code>node</code>, <code>npm</code> and <code>yarn</code></p> <ul> <li>Install Yarn on MAC</li> </ul> <pre><code>brew install yarn\n</code></pre> <ul> <li>Clone the repository</li> </ul> <pre><code>git clone https://github.com/DataStax-Examples/nf-data-explorer.git\ncd nf-data-explorer\n</code></pre> <ul> <li>Install the dependencies (expect a 2min build it will download quite some packages)</li> </ul> <pre><code>yarn &amp;&amp; yarn build\n</code></pre> <ul> <li>Start the applications</li> </ul> <pre><code>yarn start\n</code></pre> <ul> <li>Import your secure connect bundle (see Prerequisite section above)</li> </ul> <p></p>"},{"location":"pages/data/explore/netflix-data-explorer/#2-execute-with-gitpod","title":"2 Execute with Gitpod","text":"<ul> <li>Click the button</li> </ul> <ul> <li>Open the application</li> </ul> <ul> <li>Import your secure connect bundle (see Prerequisite section above)</li> </ul>"},{"location":"pages/data/explore/presto/","title":"Presto","text":""},{"location":"pages/data/explore/presto/#overview","title":"Overview","text":"<p>Presto is a distributed SQL query engine for big data analytics. Presto can query data from over 30 different data sources, including Cassandra, MongoDB, MySQL, PostgresSQL, and Redis. Common Presto use cases include:</p> <ul> <li>interactive data analytics,</li> <li>SQL-based analytics over object storage systems,</li> <li>data access and analytics across multiple data sources with query federation,</li> <li>batch ETL processing across disparate systems.</li> </ul> <p>In this tutorial, we show how to use Presto to explore and query data in Astra DB with SQL. The overall architecture of this solution is depicted below. Presto CLI Client sends SQL queries to Presto Server. Presto Server retrieves data from Astra DB via CQL Proxy, computes the query results and returns them to the client.</p> <p></p>"},{"location":"pages/data/explore/presto/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create an Astra Database</li> <li>Create an Astra Token</li> </ul>"},{"location":"pages/data/explore/presto/#setup-astra-db","title":"Setup Astra DB","text":"<p>\u2705 1. Sign in</p> <p>Connect to your Astra account and create a new Astra database or select an existing one. Add a new keyspace with name <code>banking_db</code> or use an existing one.</p> <p>\u2705 2. Create the following tables using the CQL Console</p> <pre><code>USE banking_db;\n</code></pre> <pre><code>CREATE TABLE customer (\nid UUID,\nname TEXT,\nemail TEXT,\nPRIMARY KEY (id)\n);\nCREATE TABLE accounts_by_customer (\ncustomer_id UUID,\naccount_number TEXT,\naccount_type TEXT,\naccount_balance DECIMAL,\ncustomer_name TEXT STATIC,\nPRIMARY KEY ((customer_id), account_number)\n);\n</code></pre> <p>\u2705 3. Insert the rows using the CQL Console</p> <pre><code>INSERT INTO customer (id,name,email) VALUES (8d6c1271-16b6-479d-8ea9-546c37381ab3,'Alice','alice@example.org');\nINSERT INTO customer (id,name,email) VALUES (0e5d9e8c-2e3b-4576-8515-58b491cb859e,'Bob','bob@example.org');\nINSERT INTO accounts_by_customer (customer_id,account_number,account_type,account_balance,customer_name)\nVALUES (8d6c1271-16b6-479d-8ea9-546c37381ab3,'A-101','Checking',100.01,'Alice');\nINSERT INTO accounts_by_customer (customer_id,account_number,account_type,account_balance,customer_name)\nVALUES (8d6c1271-16b6-479d-8ea9-546c37381ab3,'A-102','Savings',200.02,'Alice');\nINSERT INTO accounts_by_customer (customer_id,account_number,account_type,account_balance,customer_name)\nVALUES (0e5d9e8c-2e3b-4576-8515-58b491cb859e,'B-101','Checking',300.03,'Bob');\nINSERT INTO accounts_by_customer (customer_id,account_number,account_type,account_balance,customer_name)\nVALUES (0e5d9e8c-2e3b-4576-8515-58b491cb859e,'B-102','Savings',400.04,'Bob');\n</code></pre>"},{"location":"pages/data/explore/presto/#deploy-cql-proxy","title":"Deploy CQL Proxy","text":"<p>\u2705 4. Installation</p> <p>Follow the instructions to deploy a CQL Proxy as close to a Presto Server as possible, preferrably deploying both components on the same server. The simplest way to start <code>cql-proxy</code> is to use an <code>&lt;astra-token&gt;</code> and <code>&lt;astra-database-id&gt;</code>:</p> <pre><code>./cql-proxy \\\n--astra-token &lt;astra-token&gt; \\\n--astra-database-id &lt;astra-database-id&gt;\n</code></pre> <p>An example command with a sample, invalid token and database id:</p> <pre><code>./cql-proxy \\\n--astra-token AstraCS:NoBhcuwCrIhZxqzjEMCSuGos:8a85142b47a588472a1f3b1314e2141f098785895411dee9db11f2a7ade457ce \\\n--astra-database-id e5e4e925-289a-8231-83fd-25918093257b\n</code></pre>"},{"location":"pages/data/explore/presto/#setup-presto-server","title":"Setup Presto Server","text":"<p>\u2705 5. Presto intallation</p> <p>Follow the instructions to download, install and configure a Presto Server or use an existing deployment. The minimal configuration requirements for a local single-machine deployment are:</p> <ul> <li>Node properties in file <code>etc/node.properties</code></li> </ul> <pre><code>node.environment=production\nnode.id=ffffffff-ffff-ffff-ffff-ffffffffffff\nnode.data-dir=/var/presto/data\n</code></pre> <ul> <li>JVM config in file <code>etc/jvm.config</code></li> </ul> <pre><code>-server\n-Xmx16G\n-XX:+UseG1GC\n-XX:G1HeapRegionSize=32M\n-XX:+UseGCOverheadLimit\n-XX:+ExplicitGCInvokesConcurrent\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:+ExitOnOutOfMemoryError\n-Djdk.attach.allowAttachSelf=true\n</code></pre> <ul> <li>Config properties in file <code>etc/config.properties</code></li> </ul> <pre><code>coordinator=true\nnode-scheduler.include-coordinator=true\nhttp-server.http.port=8080\nquery.max-memory=5GB\nquery.max-memory-per-node=1GB\nquery.max-total-memory-per-node=2GB\ndiscovery-server.enabled=true\ndiscovery.uri=http://localhost:8080\n</code></pre> <ul> <li>Catalog properties in file <code>etc/catalog/cassandra.properties</code></li> </ul> <pre><code>connector.name=cassandra\ncassandra.contact-points=localhost\ncassandra.consistency-level=QUORUM\n</code></pre> <p>The above configuration uses the Cassandra connector to interact with <code>cql-proxy</code>.</p> <p>\u2705 6. Start the Presto Server:</p> <pre><code>bin/launcher run\n</code></pre> <p>Wait for message <code>======== SERVER STARTED ========</code> to confirm a successful start.</p>"},{"location":"pages/data/explore/presto/#sql-queries-with-presto-client","title":"SQL Queries with Presto Client","text":"<p>In this section you will execute SQL Queries against Astra DB using Presto CLI Client.</p> <p>\u2705 7. Install Presto Client</p> <p>Follow the instructions to download and install a CLI Presto Client.</p> <p>\u2705 8. Start the CLI Presto Client:</p> <pre><code>./presto --server http://localhost:8080 --catalog cassandra\n</code></pre> <p>The <code>server</code> option specifies the HTTP(S) address and port of the Presto coordinator, and the <code>catalog</code> option sets the default catalog.</p> <p>\u2705 9. Execute the SQL query to find the total number of customers:</p> <pre><code>SELECT COUNT(*) AS customer_count\nFROM banking_db.customer;\n</code></pre> <p>Output:</p> <pre><code> customer_count\n----------------\n              2\n(1 row)\n</code></pre> <p>\u2705 10. Execute the SQL query to find emails of customers with account balances of <code>300.00</code> or higher:</p> <pre><code>SELECT DISTINCT email AS customer_email\nFROM banking_db.customer\nINNER JOIN banking_db.accounts_by_customer\nON (id = customer_id)\nWHERE account_balance &gt;= 300.00;\n</code></pre> <p>Output:</p> <pre><code> customer_email\n-----------------\n bob@example.org\n(1 row)\n</code></pre> <p>\u2705 11. Execute the SQL query to find customers and sums of their account balances:</p> <pre><code>SELECT id AS customer_id,\nname AS customer_name,\nemail AS customer_email,\nSUM ( CAST (\nCOALESCE(account_balance,0) AS DECIMAL(12,2)\n) ) AS customer_funds\nFROM banking_db.customer\nLEFT OUTER JOIN banking_db.accounts_by_customer\nON (id = customer_id)\nGROUP BY id, name, email;\n</code></pre> <p>Output:</p> <pre><code>             customer_id              | customer_name |  customer_email   | customer_funds\n--------------------------------------+---------------+-------------------+----------------\n 8d6c1271-16b6-479d-8ea9-546c37381ab3 | Alice         | alice@example.org | 300.03\n 0e5d9e8c-2e3b-4576-8515-58b491cb859e | Bob           | bob@example.org   | 700.07\n(2 rows)\n</code></pre>"},{"location":"pages/data/explore/tableau/","title":"Tableau","text":""},{"location":"pages/data/explore/tableau/#overview","title":"Overview","text":"<p>Tableau is a visual analytics platform for modern business intelligence. Tableau can be used to retrieve, explore, analyze and visualize data stored in Astra DB. The Tableau Platform features several products, inculding:</p> <ul> <li>Tableau Desktop,</li> <li>Tableau Prep,</li> <li>Tableau Cloud.</li> </ul> <p>In this tutorial, we show how to use Tableau Desktop to connect and query data in Astra DB. We use Simba JDBC Driver for Apache Cassandra\u00ae to connect Tableau Desktop and Astra DB .</p>"},{"location":"pages/data/explore/tableau/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create an Astra Database</li> <li>Create an Astra Token</li> <li>Download Secure Connect Bundle</li> </ul>"},{"location":"pages/data/explore/tableau/#setup-astra-db","title":"Setup Astra DB","text":"<p>\u2705 1. Sign in</p> <p>Connect to your Astra account and create a new Astra database or select an existing one. Add a new keyspace with name <code>banking_db</code> or use an existing one.</p> <p>\u2705 2. Create the following tables using the CQL Console</p> <pre><code>USE banking_db;\n</code></pre> <pre><code>CREATE TABLE customer (\nid UUID,\nname TEXT,\nemail TEXT,\nPRIMARY KEY (id)\n);\nCREATE TABLE accounts_by_customer (\ncustomer_id UUID,\naccount_number TEXT,\naccount_type TEXT,\naccount_balance DECIMAL,\ncustomer_name TEXT STATIC,\nPRIMARY KEY ((customer_id), account_number)\n);\n</code></pre> <p>\u2705 3. Insert the rows using the CQL Console</p> <pre><code>INSERT INTO customer (id,name,email) VALUES (8d6c1271-16b6-479d-8ea9-546c37381ab3,'Alice','alice@example.org');\nINSERT INTO customer (id,name,email) VALUES (0e5d9e8c-2e3b-4576-8515-58b491cb859e,'Bob','bob@example.org');\nINSERT INTO accounts_by_customer (customer_id,account_number,account_type,account_balance,customer_name)\nVALUES (8d6c1271-16b6-479d-8ea9-546c37381ab3,'A-101','Checking',100.01,'Alice');\nINSERT INTO accounts_by_customer (customer_id,account_number,account_type,account_balance,customer_name)\nVALUES (8d6c1271-16b6-479d-8ea9-546c37381ab3,'A-102','Savings',200.02,'Alice');\nINSERT INTO accounts_by_customer (customer_id,account_number,account_type,account_balance,customer_name)\nVALUES (0e5d9e8c-2e3b-4576-8515-58b491cb859e,'B-101','Checking',300.03,'Bob');\nINSERT INTO accounts_by_customer (customer_id,account_number,account_type,account_balance,customer_name)\nVALUES (0e5d9e8c-2e3b-4576-8515-58b491cb859e,'B-102','Savings',400.04,'Bob');\n</code></pre>"},{"location":"pages/data/explore/tableau/#setup-tableau-desktop","title":"Setup Tableau Desktop","text":"<p>\u2705 4. Install Tableau Desktop</p> <p>Use an existing deployment of Tableau Desktop or follow the instructions to download, install and register a new instance of Tableau Desktop.</p>"},{"location":"pages/data/explore/tableau/#install-jdbc-driver-for-apache-cassandra","title":"Install JDBC Driver for Apache Cassandra","text":"<p>\u2705 5. Download JDBC Driver</p> <ol> <li> <p>Download Astra JDBC connector jar  from Github</p> </li> <li> <p>Move the resulting <code>.jar</code> file to:</p> <ul> <li><code>/Users/[user]/Library/Tableau/Drivers</code> on macOS</li> <li><code>C:\\Program Files\\Tableau\\Drivers</code> on Windows</li> </ul> </li> </ol>"},{"location":"pages/data/explore/tableau/#connect-to-astra-db-from-tableau-desktop","title":"Connect to Astra DB from Tableau Desktop","text":"<p>\u2705 7. Restart Tableau Desktop</p> <p>Start or restart Tableau Desktop for the JDBC Driver installation to take effect.</p> <p>\u2705 8. Setup a connection to Astra DB</p>"},{"location":"pages/data/explore/tableau/#connect-with-the-jdbc-driver","title":"Connect with the JDBC Driver","text":"<ul> <li>Select Other Databases (JDBC) under Connect</li> <li> <p>Fill out the dialog box with the connection information:</p> <ul> <li> <p>URL = <code>jdbc:astra://&lt;db&gt;/&lt;keyspace&gt;?region=&lt;region&gt;</code> Fields are as follows:</p> <ul> <li>db (required) Your database identifier.  It can be a name (then it must be unique) or a database identifier (UUID)</li> <li>keyspace (required) The keyspace you want to use.</li> <li>region (optional) Only useful if the database lives in multiple regions</li> </ul> </li> <li> <p>Dialect = <code>SQL92</code></p> </li> <li>Username = <code>&lt;Client ID&gt;</code>, where a client id value is generated with your application token.</li> <li> <p>Password = <code>&lt;Client Secret&gt;</code>, where a client secret is generated with your application token. </p> </li> <li> <p>Click the Sign In button to establish a connection.</p> </li> </ul> </li> </ul> <p>\u2705 9. Create a data source from the banking database</p> <ul> <li>Select <code>cassandra</code> under Database.</li> <li>Select <code>banking_db</code> under Schema.</li> <li>Drag and drop tables <code>customer</code> and <code>accounts_by_customer</code> into the main area and establish the relationship between the tables.</li> </ul> <p></p> <p>\u2705 10. Create a new sheet with simple visualization</p> <p>Add up all account balances per customer and visualize the results: </p> <ul> <li>Click Sheet 1 at the bottom left corner.</li> <li>Drag and drop Name to Columns.</li> <li>Drag and drop Account Balance to Rows.</li> <li>Customize coloring and formatting settings as needed. </li> </ul> <p></p>"},{"location":"pages/data/explore/tableplus/","title":"TablePlus","text":"<ul> <li>This article includes information that was originally written by Erick Ramirez on DataStax Community</li> </ul>"},{"location":"pages/data/explore/tableplus/#overview","title":"Overview","text":"<p>TablePlus is a modern, native tool with elegant UI that allows you to simultaneously manage multiple databases such as MySQL, PostgreSQL, SQLite, Microsoft SQL Server and more.</p> <ul> <li>\u2139\ufe0f Introduction to TablePlus</li> <li>\ud83d\udce5 TablePlus Download Link</li> </ul>"},{"location":"pages/data/explore/tableplus/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure bundle</li> </ul> <p>This article assumes you have a running installation of Tableplus on your laptop or PC. It was written for the MacOS version but it should also work for the Windows version.</p>"},{"location":"pages/data/explore/tableplus/#installation-and-setup","title":"Installation and Setup","text":"<p>Note</p> <p>For simplicity, the secure connect bundle has been placed in <code>/path/to/scb</code></p>"},{"location":"pages/data/explore/tableplus/#step-1-db-information","title":"\u2705 Step 1: DB Information","text":"<p>On your laptop or PC where Tableplus is installed, unpack your secure bundle. For example:</p> <pre><code>$ cd /path/to/scb\n$ unzip secure-connect-getvaxxed.zip\n</code></pre> <p>Here is an example file listing after unpacking the bundle:</p> <pre><code>/\n  path/\n    to/\n      scb/\n        ca.crt\n        cert\n        cert.pfx\n        config.json\n        cqlshrc\n        identity.jks\n        key\n        trustStore.jks\n</code></pre> <p>Obtain information about your database from the config.json file. Here is an example:</p> <pre><code>{\n\"host\": \"&lt;YOUR_ENDPOINT&gt;.db.astra.datastax.com\",\n  \"port\": 98765,\n  \"cql_port\": 34567,\n  \"keyspace\": \"&lt;KEYSPACE_NAME&gt;\",\n  \"localDC\": \"us-west-2\",\n  \"caCertLocation\": \"./ca.crt\",\n  \"keyLocation\": \"./key\",\n  \"certLocation\": \"./cert\",\n  ...\n}\n</code></pre> <p>We will use this information to configure Astra DB as the data source in Tableplus.</p>"},{"location":"pages/data/explore/tableplus/#step-2-new-connection","title":"\u2705 Step 2: New Connection","text":"<ol> <li> <p>In Tableplus, create a new connection and select Cassandra as the target database.</p> </li> <li> <p>In the Host and Port fields, use the <code>host</code> and <code>cql_port</code> values in the <code>config.json</code> above.</p> </li> <li> <p>In the User and Password fields, use the client ID and client secret from the token you created in the Prerequisites section of this article.</p> </li> <li> <p>In the Keyspace field, use the <code>keyspace</code> values in the <code>config.json</code> above.</p> </li> <li> <p>Choose <code>SSL VERIFY NONE</code> for the SSL mode.</p> </li> <li> <p>For SSL keys, select the secure bundle files:</p> </li> </ol> <p>Secure Bundle Files</p> <ul> <li><code>key</code> for Private Key (leave the password blank when prompted)</li> <li><code>cert</code> for Cert</li> <li><code>ca.crt</code> for Trusted Cert</li> </ul> <p>Here's an example of what the Cassandra Connection dialog box should look like:</p> <p> </p>"},{"location":"pages/data/explore/tableplus/#step-3-final-test","title":"\u2705 Step 3: Final Test","text":"<p>Connect to your Astra DB. If the connection was successful, you should be able to see all the tables on the left-hand side of the UI.</p> <p>Here's an example output:</p> <p></p>"},{"location":"pages/data/explore/trino/","title":"Trino","text":""},{"location":"pages/data/explore/trino/#overview","title":"Overview","text":"<p>Trino is a distributed SQL query engine for big data analytics. Trino can query data from over 30 different data sources, including Cassandra, MongoDB, MySQL, PostgresSQL, and Redis. Common Trino use cases include:</p> <ul> <li>interactive data analytics,</li> <li>SQL-based analytics over object storage systems,</li> <li>data access and analytics across multiple data sources with query federation,</li> <li>batch ETL processing across disparate systems.</li> </ul> <p>In this tutorial, we show how to use Trino to explore and query data in Astra DB with SQL. The overall architecture of this solution is depicted below. Trino CLI Client sends SQL queries to Trino Server. Trino Server retrieves data from Astra DB via CQL Proxy, computes the query results and returns them to the client.</p> <p></p>"},{"location":"pages/data/explore/trino/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create an Astra Database</li> <li>Create an Astra Token</li> </ul>"},{"location":"pages/data/explore/trino/#setup-astra-db","title":"Setup Astra DB","text":"<p>\u2705 1. Sign in</p> <p>Connect to your Astra account and create a new Astra database or select an existing one. Add a new keyspace with name <code>banking_db</code> or use an existing one.</p> <p>\u2705 2. Create the following tables using the CQL Console</p> <pre><code>USE banking_db;\n</code></pre> <pre><code>CREATE TABLE customer (\nid UUID,\nname TEXT,\nemail TEXT,\nPRIMARY KEY (id)\n);\nCREATE TABLE accounts_by_customer (\ncustomer_id UUID,\naccount_number TEXT,\naccount_type TEXT,\naccount_balance DECIMAL,\ncustomer_name TEXT STATIC,\nPRIMARY KEY ((customer_id), account_number)\n);\n</code></pre> <p>\u2705 3. Insert the rows using the CQL Console</p> <pre><code>INSERT INTO customer (id,name,email) VALUES (8d6c1271-16b6-479d-8ea9-546c37381ab3,'Alice','alice@example.org');\nINSERT INTO customer (id,name,email) VALUES (0e5d9e8c-2e3b-4576-8515-58b491cb859e,'Bob','bob@example.org');\nINSERT INTO accounts_by_customer (customer_id,account_number,account_type,account_balance,customer_name)\nVALUES (8d6c1271-16b6-479d-8ea9-546c37381ab3,'A-101','Checking',100.01,'Alice');\nINSERT INTO accounts_by_customer (customer_id,account_number,account_type,account_balance,customer_name)\nVALUES (8d6c1271-16b6-479d-8ea9-546c37381ab3,'A-102','Savings',200.02,'Alice');\nINSERT INTO accounts_by_customer (customer_id,account_number,account_type,account_balance,customer_name)\nVALUES (0e5d9e8c-2e3b-4576-8515-58b491cb859e,'B-101','Checking',300.03,'Bob');\nINSERT INTO accounts_by_customer (customer_id,account_number,account_type,account_balance,customer_name)\nVALUES (0e5d9e8c-2e3b-4576-8515-58b491cb859e,'B-102','Savings',400.04,'Bob');\n</code></pre>"},{"location":"pages/data/explore/trino/#deploy-cql-proxy","title":"Deploy CQL Proxy","text":"<p>\u2705 4. Installation</p> <p>Follow the instructions to deploy a CQL Proxy as close to a Trino Server as possible, preferrably deploying both components on the same server. The simplest way to start <code>cql-proxy</code> is to use an <code>&lt;astra-token&gt;</code> and <code>&lt;astra-database-id&gt;</code>:</p> <pre><code>./cql-proxy \\\n--astra-token &lt;astra-token&gt; \\\n--astra-database-id &lt;astra-database-id&gt;\n</code></pre> <p>An example command with a sample, invalid token and database id:</p> <pre><code>./cql-proxy \\\n--astra-token AstraCS:NoBhcuwCrIhZxqzjEMCSuGos:8a85142b47a588472a1f3b1314e2141f098785895411dee9db11f2a7ade457ce \\\n--astra-database-id e5e4e925-289a-8231-83fd-25918093257b\n</code></pre>"},{"location":"pages/data/explore/trino/#setup-trino-server","title":"Setup Trino Server","text":"<p>\u2705 5. Trino intallation</p> <p>Follow the instructions to download, install and configure a Trino Server or use an existing deployment. The minimal configuration requirements for a local single-machine deployment are:</p> <ul> <li>Node properties in file <code>etc/node.properties</code></li> </ul> <pre><code>node.environment=production\nnode.id=ffffffff-ffff-ffff-ffff-ffffffffffff\nnode.data-dir=/var/trino/data\n</code></pre> <ul> <li>JVM config in file <code>etc/jvm.config</code></li> </ul> <pre><code>-server\n-Xmx16G\n-XX:-UseBiasedLocking\n-XX:+UseG1GC\n-XX:G1HeapRegionSize=32M\n-XX:+ExplicitGCInvokesConcurrent\n-XX:+ExitOnOutOfMemoryError\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:-OmitStackTraceInFastThrow\n-XX:ReservedCodeCacheSize=512M\n-XX:PerMethodRecompilationCutoff=10000\n-XX:PerBytecodeRecompilationCutoff=10000\n-Djdk.attach.allowAttachSelf=true\n-Djdk.nio.maxCachedBufferSize=2000000\n</code></pre> <ul> <li>Config properties in file <code>etc/config.properties</code></li> </ul> <pre><code>coordinator=true\nnode-scheduler.include-coordinator=true\nhttp-server.http.port=8080\nquery.max-memory=5GB\nquery.max-memory-per-node=1GB\ndiscovery.uri=http://localhost:8080\n</code></pre> <ul> <li>Catalog properties in file <code>etc/catalog/cassandra.properties</code></li> </ul> <pre><code>connector.name=cassandra\ncassandra.contact-points=localhost\ncassandra.consistency-level=QUORUM\n</code></pre> <p>The above configuration uses the Cassandra connector to interact with <code>cql-proxy</code>.</p> <p>\u2705 6. Start the Trino Server:</p> <pre><code>bin/launcher run\n</code></pre> <p>Wait for message <code>======== SERVER STARTED ========</code> to confirm a successful start.</p>"},{"location":"pages/data/explore/trino/#sql-queries-with-trino-client","title":"SQL Queries with Trino Client","text":"<p>In this section you will execute SQL Queries against Astra DB using Trino CLI Client.</p> <p>\u2705 7. Install Trino Client</p> <p>Follow the instructions to download and install a CLI Trino Client.</p> <p>\u2705 8. Start the CLI Trino Client:</p> <pre><code>./trino --server http://localhost:8080 --catalog cassandra\n</code></pre> <p>The <code>server</code> option specifies the HTTP(S) address and port of the Trino coordinator, and the <code>catalog</code> option sets the default catalog.</p> <p>\u2705 9. Insert a new customer into table <code>customer</code>:</p> <pre><code>INSERT INTO banking_db.customer (id,name,email)\nVALUES (uuid(),'Luis','luis@example.org');\n</code></pre> <p>\u2705 10. Execute the SQL query to find the total number of customers:</p> <pre><code>SELECT COUNT(*) AS customer_count\nFROM banking_db.customer;\n</code></pre> <p>Output:</p> <pre><code> customer_count\n----------------\n              3\n(1 row)\n</code></pre> <p>\u2705 11. Execute the SQL query to find emails of customers with account balances of <code>300.00</code> or higher:</p> <pre><code>SELECT DISTINCT email AS customer_email\nFROM banking_db.customer\nINNER JOIN banking_db.accounts_by_customer\nON (id = customer_id)\nWHERE account_balance &gt;= 300.00;\n</code></pre> <p>Output:</p> <pre><code> customer_email\n-----------------\n bob@example.org\n(1 row)\n</code></pre> <p>\u2705 12. Execute the SQL query to find customers and sums of their account balances:</p> <pre><code>SELECT id AS customer_id,\nname AS customer_name,\nemail AS customer_email,\nSUM ( CAST (\nCOALESCE(account_balance,0) AS DECIMAL(12,2)\n) ) AS customer_funds\nFROM banking_db.customer\nLEFT OUTER JOIN banking_db.accounts_by_customer\nON (id = customer_id)\nGROUP BY id, name, email;\n</code></pre> <p>Output:</p> <pre><code>             customer_id              | customer_name |  customer_email   | customer_funds\n--------------------------------------+---------------+-------------------+----------------\n 0e5d9e8c-2e3b-4576-8515-58b491cb859e | Bob           | bob@example.org   |         700.07\n c628dca6-a8a6-4f37-ac29-44975af069fb | Luis          | luis@example.org  |           0.00\n 8d6c1271-16b6-479d-8ea9-546c37381ab3 | Alice         | alice@example.org |         300.03\n(3 rows)\n</code></pre>"},{"location":"pages/data/load/astra-data-loader/","title":"Astra Data Loader","text":"\ud83d\udcd6 Reference Documentations and Resources <ol> <li>\ud83d\udcd6  Data Loader - Astra Reference documentation <li>\ud83c\udfa5 Youtube Video - Walk through data loader usage </li> </li> </ol>"},{"location":"pages/data/load/astra-data-loader/#overview","title":"Overview","text":"<p>Astra DB conveniently has its own data loader built in to the user interface. Use this DataStax Astra DB Data Loader to load your own data into your database or try one of our sample datasets.</p>"},{"location":"pages/data/load/astra-data-loader/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> </ul>"},{"location":"pages/data/load/astra-data-loader/#procedure","title":"Procedure","text":""},{"location":"pages/data/load/astra-data-loader/#step-1-from-your-astra-db-dashboard-select-load-data-for-the-database-where-you-want-to-load-data","title":"\u2705 Step 1 : From your Astra DB Dashboard, select Load Data for the database where you want to load data.","text":"<p>The Astra DB Data Loader launches.</p> <p></p>"},{"location":"pages/data/load/astra-data-loader/#step-2-load-your-data-using-one-of-the-options","title":"\u2705 Step 2 : Load your data using one of the options:","text":""},{"location":"pages/data/load/astra-data-loader/#upload-your-dataset","title":"Upload your dataset","text":"<p>Drag and drop your own <code>.csv</code> file into the Astra DB Data Loader.</p> <p> <code>CSV</code> files must be less than <code>40 MB</code>. You will see a status bar to show how much data has uploaded. Ensure the column names in your .csv do not include spaces. Underscores are accepted. For example, <code>ShoeSize</code>, <code>ShirtColor</code>, <code>Shoe_Size</code>, and <code>Shirt_Color</code> are accepted column names.</p>"},{"location":"pages/data/load/astra-data-loader/#load-example-dataset","title":"Load example dataset","text":"<p>Select one of the two examples given to use as a sample dataset.</p>"},{"location":"pages/data/load/astra-data-loader/#load-dynamodb-from-s3","title":"Load DynamoDB from S3","text":"<ul> <li>First, export your DynamoDB data to S3 as described here. Then in AWS console, grant read access to the following ARN: <code>arn:aws:iam::445559476293:role/astra-loader</code> Your bucket policy should use:</li> </ul> <pre><code>{\n\"Statement\": [\n{\n\"Action\": [\"s3:ListBucket\", \"s3:GetBucketLocation\"],\n\"Principal\": { \"AWS\": \"arn:aws:iam::445559476293:role/astra-loader\" },\n\"Effect\": \"Allow\",\n\"Resource\": \"arn:aws:s3:::YOUR_BUCKET_NAME\"\n},\n{\n\"Effect\": \"Allow\",\n\"Principal\": { \"AWS\": \"arn:aws:iam::445559476293:role/astra-loader\" },\n\"Action\": [\"s3:GetObject\"],\n\"Resource\": \"arn:aws:s3:::YOUR_BUCKET_NAME/*\"\n}\n]\n}\n</code></pre> <p>This bucket policy allows Astra DB automation to pull data from your identified shared S3 bucket, and load the data into Astra DB. You can remove the permission after the data load finishes.</p> <p>In the Option 3 prompts, enter your S3 Bucket name, and enter the Key value. To find the Key, navigate in AWS console to the S3 subdirectory that contains your exported DynamoDB data. Look for the Key on its Properties tab. Here\u2019s a sample screen with the Key shown near the lower-left corner:</p> <p></p> <p>S3 Properties with Key value for exported DynamoDB data file. Once you configure your option, select Next.</p>"},{"location":"pages/data/load/astra-data-loader/#import-procedure","title":"Import Procedure","text":"<ul> <li>Give your table for this dataset a name. Your dataset will be included in the Data Preview and Types.</li> </ul> <ul> <li>Select the data type for each column.</li> </ul> <p> The Astra DB Data Loader automatically selects data types for your dataset. If needed, you can change this to your own selection.</p> <ul> <li>Select your partition key and clustering column for your data.</li> </ul> <p></p> <ul> <li> <p>Select Next.</p> </li> <li> <p>Select your database from the dropdown menu.</p> </li> <li> <p>Select your keyspace from the available keyspaces.</p> </li> </ul> <p></p> <ul> <li>Select Next.</li> </ul> <p>You will see a confirmation that your data is being imported. Within a few minutes, your dataset will begin uploading to your database.</p> <p>You will receive an email when the job has started and when the dataset has been loaded.</p>"},{"location":"pages/data/load/dsbulk/","title":"\u2023 DSBulk","text":"\ud83d\udcd6 Reference Documentations and resources <ol> <li>\ud83d\udcd6  DSBulks Docs - Reference documentation <li>\ud83d\udcd6  DataStax Docs - Reference Documentation"},{"location":"pages/data/load/dsbulk/#a-overview","title":"A - Overview","text":""},{"location":"pages/data/load/dsbulk/#what-is-dsbulk","title":"\ud83d\udcd8 What is DSBulk ?","text":"<p>The DataStax Bulk Loader tool (DSBulk) is a unified tool for loading into and unloading from Cassandra-compatible storage engines, such as OSS Apache Cassandra\u00ae, DataStax Astra and DataStax Enterprise (DSE).</p> <p>Out of the box, DSBulk provides the ability to:</p> <ul> <li>Load (import) large amounts of data into the database efficiently and reliably;</li> <li>Unload (export) large amounts of data from the database efficiently and reliably;</li> <li>Count elements in a database table: how many rows in total, how many rows per replica and per token range, and how many rows in the top N largest partitions.</li> </ul> <pre><code># Load data\ndsbulk load &lt;options&gt;\n\n# Unload data\ndsbulk unload &lt;options&gt;\n\n# Count rows\ndsbulk count &lt;options&gt;\n</code></pre> <p>Currently, CSV and Json formats are supported for both loading and unloading data.</p>"},{"location":"pages/data/load/dsbulk/#datastax-bulk-loader-with-astra","title":"\ud83d\udcd8 DataStax Bulk Loader with Astra","text":"<p>Use DataStax Bulk Loader <code>(dsbulk)</code> to load and unload data in CSV or JSON format with your DataStax Astra DB database efficiently and reliably.</p> <p>You can use <code>dsbulk</code> as a standalone tool to remotely connect to a cluster. The tool is not required to run locally on an instances, but can be used in this configuration.</p>"},{"location":"pages/data/load/dsbulk/#b-prerequisites","title":"B - Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure bundle</li> </ul> <p>This article was written for DataStax Bulk Loader version <code>1.9.1</code>.</p> <p>Starting with version <code>1.9</code>, <code>dsbulk</code> can detect and respect server-side rate limiting. This is very useful when working with Astra DB, which by default has some throughput guardrails in place.</p>"},{"location":"pages/data/load/dsbulk/#c-installation","title":"C - Installation","text":"<p>\u2705 Step 1 : Download the archive and unzip locally</p> <pre><code>curl -OL https://downloads.datastax.com/dsbulk/dsbulk-1.9.1.tar.gz \\\n&amp;&amp; tar xvzf dsbulk-1.9.1.tar.gz \\\n&amp;&amp; rm -f dsbulk-1.9.1.tar.gz\n</code></pre> <p>it will take a few seconds (file is about 30M)...</p> <pre><code>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n 49 30.0M   49 14.8M    0     0   343k      0  0:01:29  0:00:44  0:00:45  244k\n</code></pre>"},{"location":"pages/data/load/dsbulk/#d-usage","title":"D - Usage","text":""},{"location":"pages/data/load/dsbulk/#load-data","title":"\ud83d\udcd8 Load Data","text":"<ul> <li>Given a table</li> </ul> <pre><code>CREATE TABLE better_reads.book_by_id (\nid text PRIMARY KEY,\nauthor_id list&lt;text&gt;,\nauthor_names list&lt;text&gt;,\nbook_description text,\nbook_name text,\ncover_ids list&lt;text&gt;,\npublished_date date\n)\n</code></pre> <ul> <li>A sample CSV could be:</li> </ul> <pre><code>id|author_id|author_names|book_description|book_name|cover_ids|published_date\n1234|[\"id1\",\"id2\",\"id3\"]|[\"name1\",\"name2\",\"name3\"]|this is a dsecription|Book name|[\"cover1\",\"cover2\"]|2022-02-02\n</code></pre> <ul> <li>Loaded with the following command:</li> </ul> <pre><code>dsbulk load \\\n-url book_by_id.csv \\\n-c csv \\\n-delim '|' \\\n-k better_reads \\\n-t book_by_id \\\n--schema.allowMissingFields true \\\n-u clientId \\\n-p clientSecret \\\n-b secureBundle.zip\n</code></pre>"},{"location":"pages/data/load/dsbulk/#export-data","title":"\ud83d\udcd8 Export Data","text":"<ul> <li>Unloaded the same table with the following command:</li> </ul> <pre><code>dsbulk unload \\\n-k better_reads \\\n-t book_by_id \\\n-c csv \\\n-u clientId \\\n-p clientSecret \\\n-b secureBundle.zip \\\n&gt; book_by_id_export.csv\n</code></pre>"},{"location":"pages/data/load/dsbulk/#count-table-records","title":"\ud83d\udcd8 Count Table Records","text":"<ul> <li>Counted the rows in the table with the following command:</li> </ul> <pre><code>dsbulk count \\\n-k better_reads \\\n-t book_by_id \\\n-u clientId \\\n-p clientSecret \\\n-b secureBundle.zip\n</code></pre> <ul> <li>Produces the following output:</li> </ul> <pre><code>Operation directory: /local/dsbulk-1.9.1/logs/COUNT_20220223-213637-046128\n  total | failed | rows/s |  p50ms |  p99ms | p999ms\n143,475 |      0 | 87,509 | 155.34 | 511.71 | 511.71\nOperation COUNT_20220223-213637-046128 completed successfully in 1 second.\n143475\n</code></pre>"},{"location":"pages/data/load/nosqlbench/","title":"NoSQLBench","text":""},{"location":"pages/data/load/nosqlbench/#overview","title":"Overview","text":"<ul> <li>\u2139\ufe0f NoSQLBench documentation</li> <li>\u2139\ufe0f Astra Docs on NoSQLBench</li> </ul>"},{"location":"pages/data/load/nosqlbench/#what-is-nosqlbench","title":"What is NoSQLBench ?","text":"<p>NoSQLBench is a powerful, state-of-the-art tool for emulating real application workloads and direct them to actual target data stores for reliable, reproducible benchmarking and population of a database with sophisticated synthetic data.</p> <p>NoSQLBench is extremely customizable, yet comes with many pre-defined workloads, ready for several types of distributed, NoSQL data systems. One of the target databases is Cassandra/Astra DB, supported out-of-the-box by NoSQLBench and complemented by some ready-made realistic workloads for benchmarking.</p> <p>At the heart of NoSQLBench are a few principles:</p> <ul> <li>ease-of-use, meaning that one can start using it without learning all layers of customizability;</li> <li>modularity in the design: building a new driver is comparatively easy;</li> <li>workloads are reproducible down to the individual statement (no \"actual randomness\" involved);</li> <li>reliable performance timing, i.e. care is taken on the client side to avoid unexpected JVM pauses.</li> </ul>"},{"location":"pages/data/load/nosqlbench/#nosqlbench-and-astra-db","title":"NoSQLBench and Astra DB","text":"<p>NoSQLBench uses the (CQL-based) Cassandra Java Drivers, which means that it supports Astra DB natively with its drivers. One just has to provide access to an Astra DB instance, which is done via command-line parameters.</p> <p>Keep in mind that, while benchmark workloads for OSS Cassandra installation usually take care of creating the target keyspace, with Astra DB you should make sure the keyspace exists already (the keyspace name can be passed as a command-line parameter when launching NoSQLBench). This is often handled in a typical NoSQLBench workload by having two named scenario in the same yaml (e.g. <code>cassandra</code> and <code>astra</code>), differing in their \"schema\" part only (with/without creation of the keyspace).</p>"},{"location":"pages/data/load/nosqlbench/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure bundle</li> </ul> Minimal token permissions <p>While you can certainly use a standard \"Database Administrator\" token, you may want to use a least-privilege token to run your benchmarks. These are the specifications for a minimal Custom Role for this purpose:</p> <ul> <li>limited to just the target database and the target keyspace in it;</li> <li>Table permissions: Select Table, Describe Table, Alter Table, Create Table, Drop Table, Modify Table;</li> <li>API Access: Access CQL.</li> </ul> <p></p>"},{"location":"pages/data/load/nosqlbench/#installation","title":"Installation","text":"<p>The following installation instructions are taken from the official NoSQLBench documentation. Please refer to it for more details and updates.</p>"},{"location":"pages/data/load/nosqlbench/#step-1-download-the-binaries","title":"Step 1 : Download the binaries","text":"<p>Go to the project's \"tags\" listing on Github and download the latest version. The suggested option is to download the Linux binary (<code>nb5</code>), but as an alternative the <code>nb5.jar</code> version can also be used: here we assume the Linux binary is used -- please see the NoSQLBench documentation for more options and details about installation.</p>"},{"location":"pages/data/load/nosqlbench/#step-2-make-executable-and-put-in-search-path","title":"Step 2 : Make executable and put in search path","text":"<p>Once the file is downloaded, make it executable with <code>chmod +x nb5</code> and put it (or make a symlink) somewhere in your system's search path, such as <code>/home/${USER}/.local/bin/</code>.</p> <p>As a quick test, try the command <code>nb --version</code>.</p>"},{"location":"pages/data/load/nosqlbench/#usage","title":"Usage","text":""},{"location":"pages/data/load/nosqlbench/#command","title":"Command","text":"<p>If you already use NoSQLBench... then all you need to know is that invocations should include the following parameters to locate an Astra DB instance and authenticate to it:</p> <pre><code>nb \\\n[...]                                              \\\nusername=CLIENT_ID                                 \\\npassword=CLIENT_SECRET                             \\\nsecureconnectbundle=/PATH/TO/SECURE-CONNECT-DB.zip \\\nkeyspace=KEYSPACE_NAME                             \\\n[...]\n</code></pre> <p>In the above, you should pass your Client ID and Client Secret as found in the Astra DB Token, and the path to the Secure Bundle zipfile you obtained earlier (see Prerequisites). Please prepend <code>./</code> to the bundle path if it is a relative path.</p> <p>You may want to specify a keyspace, as seen in the sample command quoted here, because, as Astra DB does not support the <code>CREATE KEYSPACE</code> CQL command, it would be your responsibility to match the keyspace used in the benchmark with the name of an existing one. Please inspect the contents of your workload <code>yaml</code> file more closely for more details on the keyspace name used by default.</p>"},{"location":"pages/data/load/nosqlbench/#quick-start","title":"Quick-start","text":"<p>There is a comprehensive Getting Started page on NoSQLBench documentation, so here only a couple of sample full commands will be given. Please consult the full documentation for more options and configurations.</p> <p>Some of the ready-made workloads included with NoSQLBench are specific for benchmarking realistic usage patterns of Cassandra/Astra DB. A quick way to get started is to launch those workloads with the \"workload scenario\" syntax (where the 'scenario' specifies that you are targeting an Astra DB instance).</p> <p>\"cql-keyvalue\" workload</p> <p>The following will run the \"cql-keyvalue\" workload, specifically its \"astra\" scenario, on an Astra DB instance:</p> <pre><code>nb5 cql-keyvalue astra                               \\\nusername=CLIENT_ID                                 \\\npassword=CLIENT_SECRET                             \\\nsecureconnectbundle=/PATH/TO/SECURE-CONNECT-DB.zip \\\nkeyspace=KEYSPACE_NAME\n</code></pre> <p>Alternatively, if you are using the <code>jar</code> version of the tool,</p> <pre><code>java -jar nb5.jar cql-keyvalue astra                 \\\nusername=CLIENT_ID                                 \\\npassword=CLIENT_SECRET                             \\\nsecureconnectbundle=/PATH/TO/SECURE-CONNECT-DB.zip \\\nkeyspace=KEYSPACE_NAME\n</code></pre> <p>This workload emulates usage of Astra DB/Cassandra as a simple key-value store, and does so by alternating \"random\" reads and writes to a single table. A preliminar \"rampup\" phase is run, consisting only of writes, and then the \"main\" phase takes place (this structure is a rather universal features of these benchmarks).</p> <p>(Note: most likely you may want to add further options, such as <code>cyclerate</code>, <code>rampup-cycles</code>, <code>main-cycles</code> or <code>--progress console</code>. See the NoSQLBench docs and inspect the workload <code>yaml</code> for more).</p> <p>The above command, as things progress, will produce an output similar to:</p> <pre><code>cqlkeyvalue_astra_schema: 100.00%/Stopped (details: min=0 cycle=1 max=1) (last report)\ncqlkeyvalue_astra_rampup: 15.60%/Running (details: min=0 cycle=234 max=1500)\ncqlkeyvalue_astra_rampup: 32.40%/Running (details: min=0 cycle=486 max=1500)\n[...]\ncqlkeyvalue_astra_main: 96.67%/Running (details: min=0 cycle=1450 max=1500)\ncqlkeyvalue_astra_main: 100.00%/Running (details: min=0 cycle=1500 max=1500) (last report)\n</code></pre> <p>followed by a final summary - reflected also in files in the <code>logs/</code> directory, similar to:</p> <pre><code>-- Gauges ----------------------------------------------------------------------\ncqlkeyvalue_astra_main.cycles.config.burstrate\n             value = 55.00000000000001\n\n[...]\n\ncqlkeyvalue_astra_schema.tokenfiller\n             count = 57086\n         mean rate = 941.29 calls/second\n     1-minute rate = 940.67 calls/second\n     5-minute rate = 939.91 calls/second\n    15-minute rate = 939.71 calls/second\n</code></pre> <p>You may want to check that at this point a new table has been created, if it did not exist yet, in the keyspace.</p> <p>\"cql-iot\" workload</p> <p>Similar to the above for the \"cql-iot\" workload, aimed at emulating time-series-based reads and writes for a hypothetical IoT system:</p> <pre><code>nb5 cql-iot astra                                    \\\nusername=CLIENT_ID                                 \\\npassword=CLIENT_SECRET                             \\\nsecureconnectbundle=/PATH/TO/SECURE-CONNECT-DB.zip \\\nkeyspace=KEYSPACE_NAME\n</code></pre> <p>Other workloads</p> <p>You can inspect all available workloads with:</p> <pre><code>nb5 --list-scenarios\n</code></pre> <p>and look for <code>astra</code> in the output example scenario invocations there.</p> <p>Moreover, you can design your own workload.</p>"},{"location":"pages/develop/","title":"List","text":"<p>Dear Developers, so you have a running database ... now what ? In this page we provide you with the minimum amount of code needed to use Astra with the language or framework of your choice.</p>"},{"location":"pages/develop/#pick-a-language","title":"\ud83d\udee0\ufe0f Pick a language","text":"<p>Click the tile and learn how to interact with each language-specific interface exposed in Astra.</p> <p> </p> <p> </p>"},{"location":"pages/develop/#pick-a-framework","title":"\ud83d\udee0\ufe0f Pick a framework","text":""},{"location":"pages/develop/#learn-about-an-api","title":"\ud83d\udee0\ufe0f Learn about an API","text":"<p>Astra offers different Apis and interfaces. The choice of one against another will be driven by your use cases.</p> <p> </p>"},{"location":"pages/develop/api/document/","title":"\u2022 Document","text":""},{"location":"pages/develop/api/document/#1-overview","title":"1. Overview","text":"<p>The Document API is an HTTP REST API and part of the open source Stargate.io. The idea is to provide an abstraction on top of Apache Cassandra\u2122 to allow document-oriented access patterns.</p> <p></p> <ul> <li> <p>A <code>namespace</code> (replacement for keyspace) will hold multiple <code>collections</code> (not tables) to store <code>Documents</code></p> </li> <li> <p>You interact with the database through <code>JSON documents</code> and no validation (sometimes called <code>_schemaless_</code> but a better term would be validationless).</p> </li> <li> <p>Each documents has a unique identifier within the collection. Each insert is an upsert.</p> </li> <li> <p>You can query on any field (thanks to out of the box support for the secondary index <code>SAI</code>)</p> </li> </ul> <pre><code>  graph LR\n    DB(Database) --&gt;|1...n|NS(Namespaces)\n    NS --&gt;|1..n|COL(Collections)\n    COL --&gt;|1..n|DOC(Documents)\n    DOC --&gt;|1..49 Nested docs|DOC</code></pre> How is the data stored in Cassandra? <p>The JSON documents are stored using an internal data model. The table schema is generic as is each collection. The algorithm used to transform the document is called document shredding. The schema is optimized for searches but also to limit tombstones on edits and deletes.</p> <pre><code>create table &lt;collection_name&gt; (\nkey       text,\np0        text,\n...\np[N]       text,\nbool_value boolean,\ntxt_value  text,\ndbl_value  double,\nleaf       text\n)\n</code></pre> <p>A JSON like <code>{\"a\": { \"b\": 1 }, \"c\": 2}</code> will be stored like</p> key p0 p1 dbl*value {docid} <code>a</code> <code>b</code> <code>1</code> {docid} <code>c</code> _null* <code>2</code> <p>This also works with arrays <code>{\"a\": { \"b\": 1 }, \"c\": [{\"d\": 2}]}</code></p> key p0 p1 p2 dbl_value {docid} <code>a</code> <code>b</code> null <code>1</code> {docid} <code>c</code> <code>[0]</code> <code>d</code> <code>2</code> <p>Known Limitations</p> <ul> <li> <p>As of today there is no aggregation or sorting available in the Document Api.</p> </li> <li> <p>Queries are paged with a pagesize of <code>3</code> records by default and you can increase up to a maximum of <code>20</code> records. Otherwise, the payload would be too large.</p> </li> </ul>"},{"location":"pages/develop/api/document/#2-prerequesites","title":"2. Prerequesites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> </ul>"},{"location":"pages/develop/api/document/#3-browse-api-with-swagger","title":"3. Browse Api with Swagger","text":""},{"location":"pages/develop/api/document/#31-provide-database-details","title":"3.1 Provide Database Details","text":"Astra DB Setup  \u00a0Authentication token\u00a0*  \u00a0Database identifier\u00a0* (Where find it ?)  \u00a0Database Region\u00a0* (Where find it ?) Pick your region (GCP) asia-south1 (GCP) europe-west1 (GCP) europe-west2  (GCP) northamerica-northeast1 (GCP) southamerica-east1 (GCP) us-central1 (GCP) us-east1 (GCP) us-east4 (GCP) us-west1 (AWS) ap-southeast-1 (AWS) eu-central-1 (AWS) eu-west-1 (AWS) us-east-1 (AWS) us-east-2 (AWS) us-west-2 (Azure) northeurope (Azure) westeurope (Azure) eastus (Azure) eastus2 (Azure) southcentralus (Azure) westus2 (Azure) canadacentral (Azure) brazilsouth (Azure) centralindia (Azure) australiaeast"},{"location":"pages/develop/api/document/#32-use-swagger","title":"3.2 Use Swagger","text":"<p>The swagger client below will have fields pre-populated with your database details.</p>"},{"location":"pages/develop/api/document/#4-browse-api-with-postman","title":"4. Browse Api with Postman","text":""},{"location":"pages/develop/api/document/#41-installation","title":"4.1 Installation","text":"<ul> <li> <p>Install Postman to import the sample collections that we have provided.</p> </li> <li> <p>You can also import the collection in Hoppscotch.io not to install anything.</p> </li> </ul> <p> \u00a0Postman Collection </p>"},{"location":"pages/develop/api/document/#42-postman-setup","title":"4.2 Postman Setup","text":"<ul> <li>Import the configuration File <code>Astra_Document_Api_Configuration.json</code> in postman. In the menu locate <code>File &gt; Import</code> and drag the file in the box.</li> </ul> <p> \u00a0Postman Configuration </p> <p></p> <ul> <li>Edit the values for your db:</li> </ul> Parameter Name parameter value Description token <code>AstraCS:....</code> When you generate a new token it is the third field. Make sure you add enough privileges to use the APis, Database Administrator is a good choice to develop db <code>00000000-0000-0000-0000-00000000000</code> Unique identifier of your DB, you find on the main dashboard region <code>us-east1</code> region name, you find on the datanase dashboard namespace <code>demo</code> Namespaces are the same as keyspaces. They are created with the database or added from the database dashboard: How to create a keyspace] collection <code>person</code> Collection name (like table) to store one type of documents. <ul> <li>this is what it is looks like</li> </ul> <p></p> <ul> <li>Import the Document Api Collection <code>Astra_Document_Api.json</code> in postman. Same as before <code>File &gt; Menu</code></li> </ul> <p></p> <ul> <li>That's it! You have now access to a few dozens operations for <code>namespace</code>, <code>collections</code> and <code>documents</code></li> </ul> <p></p>"},{"location":"pages/develop/api/document/#43-working-with-the-postman-workspace","title":"4.3 Working with the Postman Workspace","text":"<p>Alternatively, you can access the collections for Document API in the Postman workspace below.</p> <p></p>"},{"location":"pages/develop/api/document/#5-api-sandbox-with-curl","title":"5. Api Sandbox with Curl","text":"<p>Provide the parameters asked at the beginning and see a first set of commands in action.</p>"},{"location":"pages/develop/api/document/#6-extra-resources","title":"6. Extra Resources","text":"<p>Reference Documentation</p> <p><ol> <li>Document API reference Blogpost <li>Design Improvements in 2021 <li>QuickStart"},{"location":"pages/develop/api/graphql/","title":"\u2022 graphQL","text":""},{"location":"pages/develop/api/graphql/#overview","title":"Overview","text":"<p>GraphQL is a query language for APIs and a runtime for fulfilling those queries with existing data. Stargate.io provides a graphQL interface which allows you to easily modify and query your table data using GraphQL types, mutations, and queries.</p> <p>Stargate GraphQL API supports two modes of interaction:</p> <ul> <li> <p>schema-first which allows you to create idiomatic GraphQL types, mutations, and queries in a manner familiar to GraphQL developers. The schema is deployed and can be updated by deploying a new schema without recreating the tables and columns directly.</p> </li> <li> <p>cql-first which translates CQL tables into GraphQL types, mutations, and queries. The GraphQL schema is automatically generated from the keyspace, tables, and columns defined, but no customization is allowed.</p> </li> </ul>"},{"location":"pages/develop/api/graphql/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> </ul>"},{"location":"pages/develop/api/graphql/#exploring-the-graphql-api-with-the-graphql-playground","title":"Exploring the GraphQL API with the GraphQL playground","text":"<p>A simple way to get started with GraphQL is to use the built-in GraphQL playground. The playground allows you to create new schema and interact with the GraphQL APIs. The server paths are structured to provide access to creating and querying your schemas, as well as querying and modifying your data. </p> <p>\u2705 Open the GraphQL Playground</p> <p>Open the playground from the Connect tab in the APIs section.</p> <p></p> <p>Remember to add your token to the HTTP HEADERS at the bottom of the screen.</p> <p></p> <p>\u2705 Creating a keyspace :</p> <p>Before you can start using the GraphQL API, you must first create a keyspace and at least one table in your database. If you are connecting to a database with an existing schema, you can skip this step.</p> <p>For this example, we will use a keyspace called <code>library</code>:</p> <p></p> <p>\u2705 Creating a Table :</p> <p>There are three Stargate GraphQL API endpoints, one for creating schema in cql-first, one for deploying a schema in the schema-first, and the third for querying or mutating a keyspace.</p> <p>Schema</p> <p> <code>https://$ASTRA_CLUSTER_ID-$ASTRA_REGION.apps.astra.datastax.com:8080/api/graphql-schema</code></p> <p>Admin</p> <p> <code>https://$ASTRA_CLUSTER_ID-$ASTRA_REGION.apps.astra.datastax.com:8080/api/graphql-admin</code></p> <p>Querying</p> <p> <code>https://$ASTRA_CLUSTER_ID-$ASTRA_REGION.apps.astra.datastax.com:8080/api/graphql/{keyspace}</code></p> <ul> <li>In the <code>graphql-schema</code> endpoint, use this query to create a new table</li> </ul> <pre><code>mutation {\n  books: createTable(\n    keyspaceName:\"library\",\n    tableName:\"books\",\n    partitionKeys: [ # The keys required to access your data\n      { name: \"title\", type: {basic: TEXT} }\n    ]\n    values: [ # The values associated with the keys\n      { name: \"author\", type: {basic: TEXT} }\n    ]\n  )\n  authors: createTable(\n    keyspaceName:\"library\",\n    tableName:\"authors\",\n    partitionKeys: [\n      { name: \"name\", type: {basic: TEXT} }\n    ]\n    clusteringKeys: [ # Secondary key used to access values within the partition\n      { name: \"title\", type: {basic: TEXT}, order: \"ASC\" }\n    ]\n  )\n}\n</code></pre> <p>You should see the following confirmation once the command executes.</p> <p></p> <p>\u2705 Inserting Data :</p> <p>Any of the created APIs can be used to interact with the GraphQL data, to write or read data.</p> <p>First, let\u2019s navigate to your new keyspace <code>library</code> inside the playground. Switch to <code>graphql</code> tab and pick the url <code>/graphql/library</code>.</p> <ul> <li>Use this query</li> </ul> <pre><code>mutation insert2Books {\n  moby: insertbooks(value: {title:\"Moby Dick\", author:\"Herman Melville\"}) {\n    value {\n      title\n    }\n  }\n  catch22: insertbooks(value: {title:\"Catch-22\", author:\"Joseph Heller\"}) {\n    value {\n      title\n    }\n  }\n}\n</code></pre> <ul> <li>Don't forget to update the header again with your token details</li> </ul> <pre><code>{\n  \"x-cassandra-token\":\"your token\"\n}\n</code></pre> <ul> <li>You should see that two books have been added to the table.</li> </ul> <p></p> <p>\u2705 Querying Data :</p> <p>To query the data, switch to the <code>graphql/library</code> endpoint and execute the following</p> <pre><code>query oneBook {\n    books (value: {title:\"Moby Dick\"}) {\n      values {\n        title\n        author\n      }\n    }\n}\n</code></pre> <p>The query results will look like the following </p> <p></p>"},{"location":"pages/develop/api/graphql/#using-postman-with-graphql","title":"Using Postman with GraphQL","text":"<p>Postman is a widely-used collaboration platform for API development and testing. Using this third-party tool, you can easily test APIs with environments generated for your test platforms and imported testing collections of API queries.</p> <p>A Postman collection is available for Astra using the GraphQL API. </p> <p></p>"},{"location":"pages/develop/api/graphql/#extra-resources","title":"Extra Resources","text":"<ul> <li>Developing with GraphQL</li> <li>Introduction to GraphQL Workshop</li> </ul>"},{"location":"pages/develop/api/grpc/","title":"\u2022 gRPC","text":""},{"location":"pages/develop/api/grpc/#1-what-is-grpc","title":"1. What is gRPC ?","text":""},{"location":"pages/develop/api/grpc/#11-overview","title":"1.1 Overview","text":"<p>gRPC is a modern open source high performance Remote Procedure Call (RPC) framework that can run in any environment. It can efficiently connect services in and across data centers with pluggable support for load balancing, tracing, health checking and authentication. It is also applicable in last mile of distributed computing to connect devices, mobile applications and browsers to backend services.</p> <p>One of the primary benefits of using gRPC is for documentation; you can use your service configuration and API interface definition files to generate reference documentation for your API.</p>"},{"location":"pages/develop/api/grpc/#12-what-you-need-to-know","title":"1.2 What you need to know","text":"<ul> <li> <p>gRPC underlying protocol is HTTP/2. It always blocking, asynchronous and reactive communications.</p> </li> <li> <p>Payloads are serialized in binary format call protocol buffers.</p> </li> <li> <p>Associated with protocol buffers the interfaces are define with <code>.proto</code> definitions files. From those definitions both server and clients are generated (stubbs).</p> </li> </ul> <p>Apache Cassandra is a NoSQL Distributed database built for performance. This fits very well use cases where this technology shines : when the performance requirements are demanding.</p>"},{"location":"pages/develop/api/grpc/#2-how-is-it-exposed-in-astra","title":"2. How is it exposed in Astra ?","text":"<p>The stargate team considers that gRPC could become the future of drivers for Apache Cassandra as describe in the following blogpost.</p> <p>As a consequence a grpc API layer is available within Stargate. Stargate is deployed within Astra and this is how Astra can provides a gRPC Api.</p> <p></p>"},{"location":"pages/develop/api/grpc/#3-getting-started","title":"3. Getting Started","text":""},{"location":"pages/develop/api/grpc/#31-prerequisites","title":"3.1 Prerequisites","text":"<ul> <li>Create an Astra Database</li> <li>Create an Astra Token</li> </ul>"},{"location":"pages/develop/api/grpc/#32-implementation","title":"3.2 Implementation","text":"<ul> <li> <p>Download to the <code>.proto</code> files. For Stargate they can be found here</p> </li> <li> <p>Generate stubs base on the proto files. In the case of Stargate gRPC apis datastax has  generated those stubs for a couple of languages already</p> <ul> <li>Java grpcClient</li> <li>Rust grpcClient</li> <li>Go grpcClient</li> <li>Node grpcClient</li> </ul> </li> <li> <p>To use those a complete documentation can be found on Stargate.io.</p> </li> <li> <p>A SDK has been implemented on top of those client to propose fluent Apis. Here are the different links</p> <ul> <li>Java SDK</li> <li>Python SDK</li> <li>JavaScript SDK</li> <li>Go SDK</li> </ul> </li> </ul>"},{"location":"pages/develop/api/grpc/#33-sample-codes","title":"3.3 Sample codes","text":"<p>To illustrate the usage of the grpc againt astra with and without the SDK look at the following code</p>"},{"location":"pages/develop/api/grpc/#code-with-grpc-client","title":"Code with gRPC client","text":"<pre><code>// Initialize Astra Client with token and database identifiers\ntry(AstraClient astraClient = AstraClient.builder()\n.withDatabaseId(ASTRA_DB_ID)\n.withDatabaseRegion(ASTRA_DB_REGION)\n.withToken(ASTRA_DB_TOKEN)\n.enableGrpc()\n.build()) {\n// Accessin the gRPC API\nApiGrpcClient cloudNativeClient = astraClient.apiStargateGrpc();\n// Reuse cql query\nString cqlQuery = \"SELECT data_center from system.local\";\n// Executing Query\nResultSetGrpc rs = cloudNativeClient.execute(cqlQuery);\n// Accessing reulst\nString datacenterName = rs.one().getString(\"data_center\");\nSystem.out.println(\"You are connected to '%s'\".formatted(datacenterName));\n// Validating the test\nAssertions.assertNotNull(datacenterName);\n}\n</code></pre>"},{"location":"pages/develop/api/grpc/#code-with-grpc-sdk","title":"Code with gRPC SDK","text":"<pre><code>// Open Grpc communicatino \nManagedChannel channel = ManagedChannelBuilder\n.forAddress(ASTRA_DB_ID + \"-\" + ASTRA_DB_REGION + \".apps.astra.datastax.com\", 443)\n.useTransportSecurity()\n.build();\n// use Grpc Stub generated from .proto as a client\nStargateGrpc.StargateBlockingStub cloudNativeClient = StargateGrpc\n.newBlockingStub(channel)\n.withCallCredentials(new StargateBearerToken(ASTRA_DB_TOKEN))\n.withDeadlineAfter(5, TimeUnit.SECONDS);\n// create Query\nString cqlQuery = \"SELECT data_center from system.local\";\n// Execute the Query\nResponse res = cloudNativeClient.executeQuery(QueryOuterClass\n.Query.newBuilder().setCql(cqlQuery).build());\n// Accessing Row result\nQueryOuterClass.Row row = res.getResultSet().getRowsList().get(0);\n// Access the single value\nString datacenterName = row.getValues(0).getString();\nSystem.out.println(\"You are connected to '%s'\".formatted(datacenterName));\n// Validating the test\nAssertions.assertNotNull(datacenterName);\n</code></pre>"},{"location":"pages/develop/api/rest/","title":"\u2022 REST","text":""},{"location":"pages/develop/api/rest/#overview","title":"Overview","text":"<p>Stargate is a data gateway (Proxy) on top of Apache Cassandra which exposes new interfaces to simplify integration in your applications. It is a way to create stateless components and ease the integration through one of four different HTTP Apis (rest, doc, graphQL, gRPC). In this chapter we will cover integration with <code>REST Apis</code> also called <code>DATA</code> in the swagger specifications.</p> <p>To know more regarding this interface specially you can have a look to dedicated section of the wiki or reference Stargate Rest Api Quick Start Guide.</p> <p>\u26a0\ufe0f We recommend using version <code>V2</code> (with V2 in the URL) as it covers more features and V1 will be deprecated eventually.</p>"},{"location":"pages/develop/api/rest/#design","title":"Design","text":""},{"location":"pages/develop/api/rest/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> </ul>"},{"location":"pages/develop/api/rest/#operations","title":"Operations","text":"<ul> <li>List keyspaces</li> </ul> <pre><code>private static void listKeyspaces(CloseableHttpClient httpClient, String apiRestEndpoint)\nthrows Exception {\n// Build Request\nHttpGet listKeyspacesReq = new HttpGet(apiRestEndpoint + \"/v2/schemas/keyspaces\");\nlistKeyspacesReq.addHeader(\"X-Cassandra-Token\", ASTRA_TOKEN);\n// Execute Request\ntry(CloseableHttpResponse res = httpClient.execute(listKeyspacesReq)) {\nif (200 == res.getCode()) {\nlogger.info(\"[OK] Keyspaces list retrieved\");\nlogger.info(\"Returned message: {}\", EntityUtils.toString(res.getEntity()));\n}\n}\n}\n</code></pre> <ul> <li>Create a Table</li> </ul> <p>Query used is <code>createTableJson</code> here:</p> <pre><code>{\n\"name\": \"users\",\n\"columnDefinitions\": [\n{\n\"name\": \"firstname\",\n\"typeDefinition\": \"text\"\n},\n{\n\"name\": \"lastname\",\n\"typeDefinition\": \"text\"\n},\n{\n\"name\": \"email\",\n\"typeDefinition\": \"text\"\n},\n{\n\"name\": \"color\",\n\"typeDefinition\": \"text\"\n}\n],\n\"primaryKey\": {\n\"partitionKey\": [\"firstname\"],\n\"clusteringKey\": [\"lastname\"]\n},\n\"tableOptions\": {\n\"defaultTimeToLive\": 0,\n\"clusteringExpression\": [{ \"column\": \"lastname\", \"order\": \"ASC\" }]\n}\n}\n</code></pre> <p>Create Table code</p> <pre><code>private static void createTable(CloseableHttpClient httpClient, String apiRestEndpoint)\nthrows Exception {\nHttpPost createTableReq = new HttpPost(apiRestEndpoint\n+ \"/v2/schemas/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/tables\");\ncreateTableReq.addHeader(\"X-Cassandra-Token\", ASTRA_TOKEN);\nString createTableJson = \"{...JSON.....}\";\ncreateTableReq.setEntity(new StringEntity(createTableJson, ContentType.APPLICATION_JSON));\n// Execute Request\ntry(CloseableHttpResponse res = httpClient.execute(createTableReq)) {\nif (201 == res.getCode()) {\nlogger.info(\"[OK] Table Created (if needed)\");\nlogger.info(\"Returned message: {}\", EntityUtils.toString(res.getEntity()));\n}\n}\n}\n</code></pre> <ul> <li>Insert a Row</li> </ul> <p></p> <pre><code>private static void insertRow(CloseableHttpClient httpClient, String apiRestEndpoint)\nthrows Exception {\nHttpPost insertCedrick = new HttpPost(apiRestEndpoint + \"/v2/keyspaces/\"\n+ ASTRA_DB_KEYSPACE + \"/users\" );\ninsertCedrick.addHeader(\"X-Cassandra-Token\", ASTRA_TOKEN);\ninsertCedrick.setEntity(new StringEntity(\"{\"\n+ \" \\\"firstname\\\": \\\"Cedrick\\\",\"\n+ \" \\\"lastname\\\" : \\\"Lunven\\\",\"\n+ \" \\\"email\\\"    : \\\"c.lunven@gmail.com\\\",\"\n+ \" \\\"color\\\"    : \\\"blue\\\" }\", ContentType.APPLICATION_JSON));\n// Execute Request\ntry(CloseableHttpResponse res = httpClient.execute(insertCedrick)) {\nif (201 == res.getCode()) {\nlogger.info(\"[OK] Row inserted\");\nlogger.info(\"Returned message: {}\", EntityUtils.toString(res.getEntity()));\n}\n}\n}\n</code></pre> <ul> <li>Retrieve a row</li> </ul> <p></p> <pre><code>private static void retrieveRow(CloseableHttpClient httpClient, String apiRestEndpoint)\nthrows Exception {\n// Build Request\nHttpGet rowReq = new HttpGet(apiRestEndpoint + \"/v2/keyspaces/\"\n+ ASTRA_DB_KEYSPACE + \"/users/Cedrick/Lunven\" );\nrowReq.addHeader(\"X-Cassandra-Token\", ASTRA_TOKEN);\n// Execute Request\ntry(CloseableHttpResponse res = httpClient.execute(rowReq)) {\nif (200 == res.getCode()) {\nString payload =  EntityUtils.toString(res.getEntity());\nlogger.info(\"[OK] Row retrieved\");\nlogger.info(\"Row retrieved : {}\", payload);\n}\n}\n}\n</code></pre> <p></p>"},{"location":"pages/develop/api/rest/#using-postman-with-rest","title":"Using Postman with REST","text":"<p>Postman is a widely-used collaboration platform for API development and testing. Using this third-party tool, you can easily test APIs with environments generated for your test platforms and imported testing collections of API queries.</p> <p>A Postman collection is available for Astra using the REST API. </p> <p></p>"},{"location":"pages/develop/frameworks/django/","title":"\u2022 Django","text":"<p>Django web framework. Ridiculously fast, fully loaded, reassuringly secure, exceedingly scalable, incredibly versatile. </p> <p>With Django, you can take web applications from concept to launch in a matter of hours. Django takes care of much of the hassle of web development, so you can focus on writing your app without needing to reinvent the wheel. It\u2019s free and open source.</p> <p>For more information, visit Django's website at djangoproject.com/.</p>"},{"location":"pages/develop/frameworks/django/#overview","title":"Overview","text":"<p>This guide describes how you can use Astra DB for your Django applications in a manner that is as idiomatic as possible within the Django way of doing things. The practices outlined here, in most cases, even make it possible to migrate an existing Django application to using Astra DB with minimal changes.</p> <p>RDBMS-based applications and Astra DB</p> <p>For more complex applications that fully leverage the capabilities of a relational database, such as foreign keys, adjustments of the data model would be needed according to the fundamental approach to data modeling in Astra DB (i.e. in Cassandra).</p> <p>In this page we adopt and suggest usage of the <code>django-cassandra-engine</code> Python package, which essentially provides Django object models on a Cassandra backend. Notice, however, that the package is not as feature-rich as its RDBMS counterpart, which in certain cases might require you to do a bit more of manual plumbing.</p>"},{"location":"pages/develop/frameworks/django/#reference-application","title":"Reference application","text":"<p>This page comes with a fully-working sample application as a companion repository, ready to be cloned and launched provided you go through its setup steps. All you need is an Astra DB instance and a corresponding database token. Refer to the README on the repository for more details, including a full setup guide, or click the button below to get a copy of the application:</p> <p></p> <p>The reference application (\"partyfinder\") is a very simple vanilla Django website to browse, create and delete \"parties\" happening in given cities at specific dates. Additionally, to illustrate the use of advanced Cassandra-specific features (namely, LWTs), a sort of \"count-me-in\" feature is also implemented to keep a consistent count of who will be attending a given party.</p>"},{"location":"pages/develop/frameworks/django/#astra-db-usage-in-django","title":"Astra DB usage in Django","text":"<p>With the Cassandra package for Django, you can switch between databases mostly in a seamless way: development still follows the \"object-mapper\" philosophy of defining models for the entities in the database and, so to speak, let the django engine figure the rest out by itself.</p> <p>A difference is that, instead of the native <code>django.db.models.Model</code>, you have to subclass <code>django_cassandra_engine.models.DjangoCassandraModel</code>; correspondingly, to comply with the underlying CQL data types available for columns, the fields in a model are drawn from the <code>cassandra.cqlengine.columns</code> package. Moreover, when creating a model for Cassandra, special syntax make it possible to specify which part of the primary key is in the clustering columns. The following example comes from the reference application:</p> <pre><code>import uuid\nfrom django.utils import timezone\nfrom cassandra.cqlengine import columns\nfrom django_cassandra_engine.models import DjangoCassandraModel\n# A model for this app\nclass Party(DjangoCassandraModel):\ncity = columns.Text(\nprimary_key=True,\n)\nid = columns.UUID(\nprimary_key=True,\nclustering_order='asc', # (allowed: 'asc' , 'desc', lowercase)\ndefault=uuid.uuid4,\n)\nname = columns.Text()\npeople = columns.Integer(default=0)\ndate = columns.DateTime(default=timezone.now)\nclass Meta:\nget_pk_field='id'\n</code></pre>"},{"location":"pages/develop/frameworks/django/#pitfalls-of-using-models","title":"Pitfalls of using Models","text":"<p>With object mappers, and the available Cassandra models, you can handle most of an application's needs. Still, a word of caution about usage of models is in order.</p> <p>Models, if used casually, may encourage the wrong read pattern on a Cassandra table: models implement methods such as <code>.all()</code>, which in general map to the dreaded \"allow filtering\" clause in terms of queries to Cassandra, and are generally to be avoided in production. Another example is that the model's <code>.filter(...)</code> method might be given filtering conditions that do not map to the sensible query patterns the table is designed for, thereby hidering performance or resulting in query timeouts. In short: do not let the model fool you, you still have to play by Cassandra's rules.</p> <p>Inadvertently querying a table the wrong way</p> <p>The table for <code>Party</code> objects above ends up having <code>PRIMARY KEY (( city ), id)</code>. This means that to get a given party one should do something like\"</p> <pre><code>parties = Party.objects.filter(city=city, id=id)\n</code></pre> <p>It is worth noting that if you omit the city, the following line would raise no error:</p> <pre><code>parties = Party.objects.filter(id=id)\n</code></pre> <p>and (assuming global uniqueness of the IDs) would even appear to work as fine, but the underlying CQL query would be a performance killer on a production application.</p> <p>There is another reason to be wary of models: some of the advanced techniques to use Cassandra simply don't fit into the models philosophy. For these, you need to access the underlying <code>Session</code> object and directly run CQL code on it. Luckily, there is a way to do so, and it is exemplified in the sample application (keep reading to see how to do it).</p>"},{"location":"pages/develop/frameworks/django/#implications-of-cassandra-data-models","title":"Implications of Cassandra data models","text":"<p>The proper road to a successful Cassandra (or Astra DB)-backed application starts from designing the right data model. But it is also possible to migrate existing Django applications: in most cases, as observed above, a one-to-one reformulation of the models would do the trick.</p> <p>However, the no-relations, no-joins, no-foreign-keys nature of the NoSQL database at hand means that if the existing application makes use of these things, a bit of work is warranted to go back to data-modeling-related issues.</p> <p>In other words, if models in your pre-existing, relational-based application contain RDBMS-related specifications such as:</p> <pre><code>from django.db import models\nfrom whatever import AnotherModel\nclass MyEntity(models.Model):\nfkField = models.ForeignKey(AnotherModel, on_delete=models.CASCADE)\n# etc, etc ...\n</code></pre> <p>you have to consider more structural changes, such as moving the burden of joins or cascading deletes to the application itself or - even better - rethink your tables (and models) in a way that works without these costly operations. You can find good tutorials and hands-on learning resources on data modeling with Cassandra here and here.</p>"},{"location":"pages/develop/frameworks/django/#beyond-models","title":"Beyond models","text":"<p>In some cases the best approach is to bypass the \"model layer\" altogether and directly execute CQL code on your Session object, taking care of manually handling the results in case of \"read\" queries.</p> <p>For example, the Session is needed to use Batches, LWTs and work with TTL.</p> <p>In the application example we have a feature that allows users to increment/decrement a <code>people</code> field for a party. However, we don't want these operations to succeed if the number seen by users on their browsers does not match the stored value anymore (think race conditions and concurrent access to the application: we certainly don't want risking this counter to go below zero!).</p> <p>A possible (if perhaps not optimal, performance-wise) solution to this problem is offered by using Lightweight Transactions. Essentially we want to run the CQL equivalent of \"update column <code>people</code> of that row, but only if the current value is so-and-so. Report back whether the update succeeded.\" This is achieved, in the appropriate view function, by the following code, which retrieves the database session and runs \"raw CQL\" on it:</p> <pre><code>from django.db import connection\n# ... ...\ndef change_party_people(request, city, id, prev_value, delta):\ndelta_num = int(delta)\ncursor = connection.cursor()\nchange_applied = cursor.execute(\n'UPDATE party SET people = %s WHERE city=%s AND id=%s IF people = %s',\n(\ndelta_num + prev_value,\ncity,\nuuid.UUID(id),  # must respect Cassandra type system\nprev_value,\n),\n).one()['[applied]']\nif not change_applied:\nlwt_message = '?LWT_FAILED=1'\nelse:\nlwt_message = ''\n# etc, etc ...\n</code></pre>"},{"location":"pages/develop/frameworks/django/#configuration-and-db-access-in-django","title":"Configuration and DB access in Django","text":"<p>Now let's look at how to configure a Django application to use Astra DB and how the access parameters and secrets are passed to it.</p>"},{"location":"pages/develop/frameworks/django/#settingspy","title":"settings.py","text":"<p>The general project-level settings are given in <code>parties/parties/settings.py</code>. In that file, you should first add the <code>\"django_cassandra_engine\"</code> item to the <code>INSTALLED_APPS</code> so that it comes first in the list.</p> <p>Second, you should replace the definition of the storage engine (sqlite3 by default on newly-created applications). That is, replace the following</p> <pre><code>DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': BASE_DIR / 'db.sqlite3',\n    }\n}\n</code></pre> <p>with something like</p> <pre><code>DATABASES = {\n'default': {\n'ENGINE': 'django_cassandra_engine',\n'NAME': KEYSPACE_NAME\n'OPTIONS': {\n'connection': {\n'auth_provider': PlainTextAuthProvider(\nAUTH_USERNAME,\nAUTH_PASSWORD,\n),\n'cloud': {\n'secure_connect_bundle': SECURE_BUNDLE_PATH,\n},\n}\n}\n}\n}\n</code></pre> <p>Note that you should add the line <code>from cassandra.auth import PlainTextAuthProvider</code> earlier in the file.</p> <p>In the above database connection settings, there are four variables that should be set in a secure and portable manner (e.g. through use of a <code>.env</code> file as shown in the application example, or otherwise): they are</p> <ul> <li><code>KEYSPACE_NAME</code>, the name of the keyspace in your Astra DB instance. Note that you don't have to create the tables yourself. Tables are created based on model definitions when you issue Django's <code>sync_cassandra</code> command before running the application the first time (see instructions on the sample application's readme);</li> <li><code>AUTH_USERNAME</code> and <code>AUTH_PASSWORD</code>: these may be either the \"clientID/clientSecret\" pair from your database token, or alternatively the literal <code>\"token\"</code> and the token string starting with <code>AstraCS:...</code>.</li> <li><code>SECURE_BUNDLE_PATH</code>, the full path to the Secure connect bundle for your database. This can be downloaded manually or, as described in the application's readme, through use of Astra CLI along with the rest of the above setup.</li> </ul> <p>Third, you may consider adding the line <code>CASSANDRA_FALLBACK_ORDER_BY_PYTHON = True</code>. This means that, when a model's <code>order_by()</code> directive cannot be mapped to CQL according to the table's clustering, the model can fall back to in-code sorting. Although this may be non-optimal in general (especially for large result sets), it can still be a safe and useful choice if you know that the amount of data involved is small.</p>"},{"location":"pages/develop/frameworks/django/#dependencies-and-cassandra-drivers","title":"Dependencies and Cassandra drivers","text":"<p>Two dependencies are needed for a Django application backed by Astra DB:</p> <ul> <li><code>Django</code></li> <li><code>django-cassandra-engine</code></li> </ul> <p>(The other package found in the sample app's <code>requirements.txt</code>, <code>python-dotenv</code>, serves the purpose of reading secrets from a <code>.env</code> file in the Django app's <code>settings.py</code>.)</p> <p>It should be noted that current versions of the Cassandra engine for Django automatically installs ScyllaDB's version of the Cassandra drivers, i.e. package <code>scylla-driver</code>. These are a drop-in replacement for the package by DataStax (<code>cassandra-driver</code>), meaning that:</p> <ol> <li>both are imported with statements such as <code>from cassandra.cluster import Cluster</code> and the like;</li> <li>it is unwise to install both at once as that would introduce namespace collisions.</li> </ol> <p>If you prefer to work with the driver package by DataStax, the application would work just fine indeed: to do so, one can simply uninstall the drivers by Scylla (<code>pip uninstall scylla-driver</code>), and then install the desired drivers (<code>pip install cassandra-driver</code>). Not even a line of code should then be changed.</p> <p>Note: at the time of writing (January 2023), the differences between the two drivers are little and mostly confined to additional support for Scylla-specific database architecture. As such, there would be no implications on the functionality, nor the performance, of applications based on Astra DB.</p>"},{"location":"pages/develop/frameworks/django/#caveats-and-troubleshooting","title":"Caveats and Troubleshooting","text":"<p>In this section we collect a handy list of warnings and things to keep in mind when using Astra DB with Django, whether by migration or when designing an app from scratch.</p> <ul> <li> <p>In Cassandra models, there is no <code>max_length</code> parameter for text fields, corresponding to the absence of such a property for the CQL <code>TEXT</code> data type.</p> </li> <li> <p>Likewise, you should not add the <code>editable=False</code> parameter for primary-key columns when defining models.</p> </li> <li> <p>For a model class subclassing <code>DjangoCassandraModel</code> with a multi-column primary key (regardless of the partition/clustering distinction) one must provide a <code>get_pk_field</code> attribute through a <code>Meta</code> class: in this way the Django engine would be able to resolve queries such as <code>&lt;Model class&gt;.objects.get(pk=...)</code>. You can see an example of this in the model quoted earlier. Failure to comply with this requirement would make the application fail to start with an informative error. If you are using the model in a sensible way (from a Cassandra perspective), you can pay little attention to this since you should not, as a matter of fact, be triggering such a query anywhere in your code, implicitly or explicitly.</p> </li> <li> <p>The <code>django-cassandra-engine</code> package does support most of the features of its native, RDBMS counterpart; however, in some cases, a little more manual plumbing might be in order. In particular, the native models support fields of type <code>FileField</code>, which pairs with the form field of the same name  and handles upload of files by storing the actual file content on disk and a path to it on DB. The Cassandra engine has no such facility, requiring you to manually handle what happens once the endpoint has received file uploads via a form POST (you can still use the form field, though). A similar consideration holds for the more specific <code>ImageField</code> model field.</p> </li> <li> <p>Once the application is ready and the DB has been synchronized with it (using <code>./manage.py sync_cassandra</code> or equivalent command), you will still see warnings about a number of \"unapplied migrations\". You can ignore these warnings (incidentally, the <code>migrate</code> command is not even supported by the Cassandra engine, being supplanted by <code>sync_cassandra</code>).</p> </li> <li> <p>If you change the model and try to run the application, or forget to run the <code>sync_cassandra</code> management operation altogether, changes are you will see the application crash with no messages or with just a unhelpful <code>Segmentation fault (core dumped)</code> message. In this case, please make sure that (1) your database is not in \"Hibernated\" state, (2) you have launched a sync operation after all changes to any model.</p> </li> <li> <p>If you use a model's <code>filter(...)</code> method but with a filtering condition (a <code>WHERE</code> clause) that is not a good match to the structure of your database table, the application will most likely function, but possibly exhibit bad performance. It is your responsibility to make sure that usage of models does not sweep violations of data modeling best practices under the rug.</p> </li> <li> <p>As remarked above, if you request objects to be sorted in a way that is not compliant with the structure of your table, you can still enable a fallback behaviour whereby the rows are sorted post-retrieval in Python code (you do this through <code>CASSANDRA_FALLBACK_ORDER_BY_PYTHON</code> in <code>settings.py</code>, but you should do this only if there are few rows involved). Don't be alarmed if you still see something like the following in the application logs (the warning would be a true exception if you hadn't enabled the fallback):</p> </li> </ul> <pre><code>UserWarning: .order_by() with column \"-date\" failed!\nFalling back to ordering in python.\nException was:\nCan't order on 'date', can only order on (clustered) primary keys\n</code></pre>"},{"location":"pages/develop/frameworks/django/#references","title":"References","text":"<ul> <li>Django homepage, djangoproject.com;</li> <li><code>django-cassandra-engine</code> documentation, r4fek.github.io/django-cassandra-engine;</li> <li>The sample application referenced throughout this page, \"partyfinder\";</li> <li>Another Django application using Cassandra from DataStax' Sample App Gallery: a simple standard blog engine.</li> </ul>"},{"location":"pages/develop/frameworks/fastapi/","title":"\u2022 FastAPI","text":"<p>FastAPI framework, high performance, easy to learn, fast to code, ready for production. FastAPI is a modern, fast web framework for building APIs with Python 3.6+ based on standard Python type hints. FastAPI strives to minimize boilerplate and maximize performance.</p> <p>To get more information regarding the framework visit the reference website fastapi.tiangolo.com.</p>"},{"location":"pages/develop/frameworks/fastapi/#1-overview","title":"1. Overview","text":"<p>This guide, and the accompanying sample code, highlight the practices and the patterns to best integrate FastAPI with Astra DB to use the latter as backing storage.</p> <p>Two important choices are made in the following:</p> <ul> <li>Astra DB is accessed with the Python driver;</li> <li>no Object Mappers are used, just plain simple CQL statements.</li> </ul>"},{"location":"pages/develop/frameworks/fastapi/#2-fastapi-and-astra-db","title":"2. FastAPI and Astra DB","text":"<p>The goal is to provide access to one or more tables stored in Astra DB to the FastAPI endpoint functions, so that the API can write data to them and read from them. This should be done keeping in mind the best practices for using the Cassandra drivers, and in as flexible and concise way as possible.</p>"},{"location":"pages/develop/frameworks/fastapi/#session","title":"Session","text":"<p>Virtually every endpoint needs access to the database. On the other hand, the driver's <code>cassandra.cluster.Session</code> is a stateful, resource-intensive object that should be created once and re-used throughout the life cycle of the Python process.</p> <p>For this reason (file <code>storage/db_connect.py</code> in the sample app) there is a <code>get_session()</code> function that keeps a globally-cached session object and returns it any time it is called, in a singleton fashion. On its first invocation, of course, the session is created in the idiomatic way, looking for the necessary connection parameters from a <code>.env</code> file. This file contains secrets, so it should never be checked in to version control.</p> <p>Note. If the application runs on regular Cassandra (as opposed to Astra DB), this is the only part of the code that would change: the parameters for instantiating the <code>Cluster</code> and the <code>Session</code> would differ slightly.</p> <p>Since it is a good practice to explicitly free resources once we're done, in this module there's also a shutdown hook that takes care of cleanup by closing the session and shutting down the <code>Cluster</code> object. (which for a FastAPI application that runs indefinitely is a bit of a moot point, but still illustrates the point).</p>"},{"location":"pages/develop/frameworks/fastapi/#endpoint-dependencies","title":"Endpoint dependencies","text":"<p>Next comes the task of making the session object available to the function endpoints: these will need to retrieve rows and/or write them, after all.</p> <p>Taking advantage of FastAPI's advanced dependency injection facilities, one can add a <code>Depends</code> parameter to the endpoint functions, which will be automatically resolved when the function gets executed. This makes the session available to the function:</p> <pre><code>@app.get('/animal/{genus}')\nasync def get_animals(genus, session=Depends(g_get_session)):\n# etc, etc ...\n</code></pre> <p>The argument of <code>Depends</code> is a function itself, more precisely an async generator, which must <code>yield</code> the session object. For this reason there is a thin wrapper function that, in practice, promotes the ordinary function <code>get_session</code> to a generator with the desired signature:</p> <pre><code>async def g_get_session():\nyield get_session()\n</code></pre> <p>At this point, FastAPI takes care of the wiring. What is still missing is the business logic itself, i.e. what happens within the endpoint functions.</p>"},{"location":"pages/develop/frameworks/fastapi/#prepared-statements","title":"Prepared statements","text":"<p>It is a good practice to keep the code in the endpoint function short and not to embed much logic into it, except for the handling of the request-response cycle itself.</p> <p>For this reason, each endpoint function in turn invokes a function in the <code>storage/db_io.py</code> module, which is where the actual database operations are executed.</p> <p>In this module another important observation is in order: since it is expected that the API endpoints will be called many times, the corrsponding CQL statements are made into \"prepared statements\" once and then re-used over and over.</p> <p>To achieve that, the <code>db_io.py</code> module holds a cache of prepared statements, one per different type of database query. This cache (<code>prepared_cache</code>) is filled on the first invocation of each endpoint, but after that there is a sizable gain in performance and reduction of overhead for all subsequent calls.</p>"},{"location":"pages/develop/frameworks/fastapi/#streaming-a-large-response-from-db","title":"Streaming a large response from DB","text":"<p>In some cases, an API endpoint may return a large response (such as a GET returning a long list of items). It might be unwieldy, and suboptimal, to retrieve the full list at API level and then prepare a whole response string to return to the caller.</p> <p>Ideally, one would like to start sending out the response as the data keeps coming in (to the API) from the database. This is exactly what <code>StreamingResponse</code> makes possible.</p> <p>The Cassandra driver handles pagination of large result sets transparently: regardless of the grouping of rows into pages, at the Python-code level all you see is a homogeneous iterable over all rows. This means that one can simply make the corresponding data-retrieval function a generator almost with no changes in the code.</p> <p>Things get slightly more tricky on the other side, that is, between the endpoint function and the caller. Fortunately, FastAPI offers the <code>StreamingResponse</code> construct that makes it possible to \"consume\" a generator and return its components as a \"Chunked\" type of response. The client will still receive a full response (and will be able to start processing it once it is there in full), but never throughout the live of the request will there be \"the full thing\" on the API side.</p> <p>But beware: in this case, the endpoint function will need to manually construct \"pieces of a syntactically valid JSON\". In the sample app, this is achieved by a <code>format_streaming_response</code> function which takes care of the opening/closing square brackets for a list and of the correct placement of the commas. In practice, this function makes a generator over homogeneous items into a generator returning something like (row-by-row; note the commas):</p> <pre><code>1.     [\n2.     {\"a\": 1, \"b\": 100}\n3.     ,{\"a\": 2, \"b\": 200}\n4.     ,{\"a\": 3, \"b\": 300}\n5.     ,{\"a\": 4, \"b\": 400}\n6.     ]\n</code></pre>"},{"location":"pages/develop/frameworks/fastapi/#3-reference-application","title":"3. Reference application","text":"<p>You can clone the reference application coming with this page and run it in minutes, provided you have an Astra DB instance (click here to create one).</p> <p>The setup instructions are outlined below: for more details, refer to the repo's README.</p>"},{"location":"pages/develop/frameworks/fastapi/#setup","title":"Setup","text":"<p>An Astra DB instance, with corresponding Token and Secure connect bundle, are required to run this app: make sure you have them at your disposal.</p> <p>Once you cloned the repository, create the <code>.env</code> file with the required secrets and (preferrably in a Python virtual environment) install all dependencies with <code>pip install -r requirements.txt</code>.</p> <p>In order to populate the database (table creation and insertion of sample rows), you should run once the script <code>python storage/db_initialize.py</code>. You are now ready to run the API.</p>"},{"location":"pages/develop/frameworks/fastapi/#run-sample-app","title":"Run sample app","text":"<p>Running the API is as simple as <pre><code>uvicorn api:app\n</code></pre></p> <p>You can now issue requests to it. Look in the repo's README for example requests, testing all provided endpoints, as <code>curl</code> commands (of course you can use any tool you like, such as Postman, to achieve the same effect).</p> <p></p>"},{"location":"pages/develop/frameworks/flask/","title":"\u2022 Flask","text":"<p>Flask - Web development, one drop at a time. Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. It began as a simple wrapper around Werkzeug and Jinja and has become one of the most popular Python web application frameworks.</p> <p>To get more information regarding the framework visit the reference website flask.palletsprojects.com.</p>"},{"location":"pages/develop/frameworks/flask/#1-overview","title":"1. Overview","text":"<p>This guide, and the accompanying sample code, highlight the practices and the patterns to best integrate Flask with Astra DB to use the latter as backing storage.</p> <p>Two important choices are made in the following:</p> <ul> <li>Astra DB is accessed with the Python driver;</li> <li>no Object Mappers are used, just plain simple CQL statements.</li> </ul>"},{"location":"pages/develop/frameworks/flask/#2-flask-and-astra-db","title":"2. Flask and Astra DB","text":"<p>The goal is to provide access to one or more tables stored in Astra DB to the Flask endpoint functions, so that the API can write data to them and read from them. This should be done keeping in mind the best practices for using the Cassandra drivers, and in as flexible and concise way as possible.</p>"},{"location":"pages/develop/frameworks/flask/#session","title":"Session","text":"<p>Virtually every endpoint needs access to the database. On the other hand, the driver's <code>cassandra.cluster.Session</code> is a stateful, resource-intensive object that should be created once and re-used throughout the life cycle of the Python process.</p> <p>For this reason (file <code>storage/db_connect.py</code> in the sample app) there is a <code>get_session()</code> function that keeps a globally-cached session object and returns it any time it is called, in a singleton fashion. On its first invocation, of course, the session is created in the idiomatic way, looking for the necessary connection parameters from a <code>.env</code> file. This file contains secrets, so it should never be checked in to version control.</p> <p>Note. If the application runs on regular Cassandra (as opposed to Astra DB), this is the only part of the code that would change: the parameters for instantiating the <code>Cluster</code> and the <code>Session</code> would differ slightly.</p> <p>Since it is a good practice to explicitly free resources once we're done, in this module there's also a shutdown hook that takes care of cleanup by closing the session and shutting down the <code>Cluster</code> object. (which for a Flask application that runs indefinitely is a bit of a moot point, but still illustrates the point).</p>"},{"location":"pages/develop/frameworks/flask/#endpoint-dependencies","title":"Endpoint dependencies","text":"<p>Next comes the task of making the session object available to the function endpoints: these will need to retrieve rows and/or write them, after all.</p> <p>Typically all API endpoints need to access the database: in this case, the easiest way is to use Flask's application-context <code>g</code> object and the <code>before_request</code> hook to make sure each request will find a reference to the session. Once the pre-request hook is set up with <pre><code>@app.before_request\ndef get_db_session():\ng.session = get_session()\n</code></pre> all endpoints will be able to read and use <code>g.session</code> as they need. Remember that <code>get_session()</code> does not create a new Cassandra <code>Session</code> object at each invocation!</p> <pre><code>@app.route('/animal/&lt;genus&gt;')\ndef get_animals(genus):\nanimals = retrieve_animals_by_genus(g.session, genus)\nreturn jsonify([animal.dict() for animal in animals])\n</code></pre>"},{"location":"pages/develop/frameworks/flask/#prepared-statements","title":"Prepared statements","text":"<p>It is a good practice to keep the code in the endpoint function short and not to embed much logic into it, except for the handling of the request-response cycle itself.</p> <p>For this reason, each endpoint function in turn invokes a function in the <code>storage/db_io.py</code> module, which is where the actual database operations are executed.</p> <p>In this module another important observation is in order: since it is expected that the API endpoints will be called many times, the corrsponding CQL statements are made into \"prepared statements\" once and then re-used over and over.</p> <p>To achieve that, the <code>db_io.py</code> module holds a cache of prepared statements, one per different type of database query. This cache (<code>prepared_cache</code>) is filled on the first invocation of each endpoint, but after that there is a sizable gain in performance and reduction of overhead for all subsequent calls.</p>"},{"location":"pages/develop/frameworks/flask/#streaming-a-large-response-from-db","title":"Streaming a large response from DB","text":"<p>In some cases, an API endpoint may return a large response (such as a GET returning a long list of items). It might be unwieldy, and suboptimal, to retrieve the full list at API level and then prepare a whole response string to return to the caller.</p> <p>Ideally, one would like to start sending out the response as the data keeps coming in (to the API) from the database. This is exactly what Flask's \"streaming responses\" make possible.</p> <p>The Cassandra driver handles pagination of large result sets transparently: regardless of the grouping of rows into pages, at the Python-code level all you see is a homogeneous iterable over all rows. This means that one can simply make the corresponding data-retrieval function a generator almost with no changes in the code.</p> <p>Things get slightly more tricky on the other side, that is, between the endpoint function and the caller. Fortunately, Flask endpoint functions may return a <code>(generator, headers)</code> pair and will construct, from this, a \"Chunked\" response which will be sent to the caller piecewise, as the generator gets consumed. The client will still receive a full response (and will be able to start processing it once it is there in full), but never throughout the live of the request will there be \"the full thing\" on the API side.</p> <p>But beware: in this case, the endpoint function will need to manually construct \"pieces of a syntactically valid JSON\". In the sample app, this is achieved by a <code>format_streaming_response</code> function which takes care of the opening/closing square brackets for a list and of the correct placement of the commas. In practice, this function makes a generator over homogeneous items into a generator returning something like (row-by-row; note the commas):</p> <pre><code>1.     [\n2.     {\"a\": 1, \"b\": 100}\n3.     ,{\"a\": 2, \"b\": 200}\n4.     ,{\"a\": 3, \"b\": 300}\n5.     ,{\"a\": 4, \"b\": 400}\n6.     ]\n</code></pre>"},{"location":"pages/develop/frameworks/flask/#pydantic-usage","title":"Pydantic usage","text":"<p>The reference API uses Pydantic for validation and handling of request/response data types (this is not strictly necessary, but very handy). However, this requires some manual plumbing, as can be seen in the code in two ways:</p> <p>First, to validate/cast the POST request payload, a <code>animal = Animal(**request.get_json())</code> is wrapped in a try/except construct, in order to return a meaningful error (from Pydantic) and a status 422 (\"unprocessable entity\") if anything is off. Note: <code>request</code> is a Flask abstraction</p> <p>Moreover, when returning responses, one must explicitly <code>jsonify</code> not directly the Pydantic object, rather its <code>.dict()</code> representation. So, for example, <code>return jsonify(animal.dict())</code>. Note: <code>jsonify</code> is a Flask primitive, whole <code>.dict()</code> is a built-in method for Pydantic models.</p>"},{"location":"pages/develop/frameworks/flask/#3-reference-application","title":"3. Reference application","text":"<p>You can clone the reference application coming with this page and run it in minutes, provided you have an Astra DB instance (click here to create one).</p> <p>The setup instructions are outlined below: for more details, refer to the repo's README.</p>"},{"location":"pages/develop/frameworks/flask/#setup","title":"Setup","text":"<p>An Astra DB instance, with corresponding Token and Secure connect bundle, are required to run this app: make sure you have them at your disposal.</p> <p>Once you cloned the repository, create the <code>.env</code> file with the required secrets and (preferrably in a Python virtual environment) install all dependencies with <code>pip install -r requirements.txt</code>.</p> <p>In order to populate the database (table creation and insertion of sample rows), you should run once the script <code>python storage/db_initialize.py</code>. You are now ready to run the API.</p>"},{"location":"pages/develop/frameworks/flask/#run-sample-app","title":"Run sample app","text":"<p>Running the API is as simple as <pre><code>flask --app api run --reload\n</code></pre></p> <p>You can now issue requests to it. Look in the repo's README for example requests, testing all provided endpoints, as <code>curl</code> commands (of course you can use any tool you like, such as Postman, to achieve the same effect).</p> <p></p>"},{"location":"pages/develop/frameworks/lagom/","title":"Lagom","text":""},{"location":"pages/develop/frameworks/lagom/#overview","title":"Overview","text":""},{"location":"pages/develop/frameworks/lagom/#working-with-lagom","title":"Working with Lagom","text":"<p>Lagom is an open source framework for building out Reactive microservices.  Lagom essentially wires-up your services, freeing you from having to spend time writing lots of \"boiler-plate\" code.  To get more information regarding the framework visit its website @ lagomframework.com.</p>"},{"location":"pages/develop/frameworks/lagom/#prerequisites","title":"Prerequisites","text":""},{"location":"pages/develop/frameworks/lagom/#prerequisites-astra-db","title":"\ud83d\udce6. Prerequisites [ASTRA DB]","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> </ul>"},{"location":"pages/develop/frameworks/lagom/#prerequisites-development-environment","title":"\ud83d\udce6. Prerequisites [Development Environment]","text":"<ul> <li> <p>You should install the Java Development Kit (JDK), of at least version 8: Use the reference documentation to install a Java Development Kit.</p> </li> <li> <p>Java 8</p> </li> <li>Java 11</li> <li>Java 17</li> </ul> <p>Validate your installation with:</p> <pre><code>java --version\n</code></pre> <ul> <li>You should install Apache Maven: Use the reference documentation and validate your installation with:</li> </ul> <pre><code>mvn -version\n</code></pre>"},{"location":"pages/develop/frameworks/lagom/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"pages/develop/frameworks/lagom/#building-a-sample-lagom-project","title":"Building a sample Lagom project","text":"<p>Create a new Maven project in your IDE which uses the <code>maven-archetype-lagom-java</code> archetype.  You can also do this from the command line:</p> <pre><code>mvn archetype:generate -DarchetypeGroupId=com.lightbend.lagom \\\n-DarchetypeArtifactId=maven-archetype-lagom-java -DarchetypeVersion=1.2.0\n</code></pre> <p>This will create a sample Lagom project with two services: Hello and Stream.  The project can be built and run with this command:</p> <pre><code>mvn lagom:runAll\n</code></pre> <p>Note that by default, Lagom will start using embedded Cassandra as its data store, running on <code>localhost:4000</code>.  Port 4000 was chosen (instead of 9042) so as not to collide with another instance of Cassandra running locally.</p>"},{"location":"pages/develop/frameworks/lagom/#using-with-apache-cassandra","title":"Using with Apache Cassandra","text":"<p>To get Lagom to connect to Cassandra (for local development) there are two places which need changes: Maven's <code>pom.xml</code> and the services' <code>application.conf</code> files.</p> <p>Inside the <code>pom.xml</code> file, locate the <code>lagom-maven-plugin</code> and make the following adjustments: <pre><code>&lt;plugin&gt;\n&lt;groupId&gt;com.lightbend.lagom&lt;/groupId&gt;\n&lt;artifactId&gt;lagom-maven-plugin&lt;/artifactId&gt;\n&lt;version&gt;${lagom.version}&lt;/version&gt;\n&lt;configuration&gt;\n&lt;unmanagedServices&gt;\n&lt;cas_native&gt;localhost:9042&lt;/cas_native&gt;\n&lt;/unmanagedServices&gt;                    &lt;cassandraEnabled&gt;false&lt;/cassandraEnabled&gt;\n&lt;/configuration&gt;\n&lt;/plugin&gt;\n</code></pre></p> <p>That these settings perform two functions: - Disables Lagom's embedded Cassandra, causing it not to start. - Informs Lagom designate Cassandra as an \"unmanaged\" service, and provides it with the contact point for the cluster/instance.</p> <p>If your local Cassandra does not use SSL or authentication, then you are finished.  But if your local Cassandra does have security enabled, you'll want to make these changes to each of your services' <code>application.conf</code> files: <pre><code>your-service.cassandra {\n    authentication {\n        username = \"yourUserName\"\n        password = \"yourPassword\"\n    }\n    ssl {\n        truststore.path = \"/Users/youruser/cassandra/truststore.jks\"\n        truststore.password = \"yourTrustStorePassword\"\n        keystore.path = \"/Users/youruser/cassandra/keystore.jks\"\n        keystore.password = \"yourKeyStorePassword\"\n    }\n    keyspace = your_service\n}\n\ncassandra-journal {\n    keyspace = ${your-service.cassandra.keyspace}\n    authentication = ${your-service.cassandra.authentication}\n    ssl = ${your-service.cassandra.ssl}\n}\n\ncassandra-snapshot-store {\n    keyspace = ${your-service.cassandra.keyspace}\n    authentication = ${your-service.cassandra.authentication}\n    ssl = ${your-service.cassandra.ssl}\n}\n\nlagom.persistence.read-side.cassandra {\n    keyspace = ${your-service.cassandra.keyspace}\n    authentication = ${your-service.cassandra.authentication}\n    ssl = ${your-service.cassandra.ssl}\n}\n</code></pre></p> <p>These settings will allow Lagom to connect to your local Cassandra with authentication and client-to-node SSL.  If you're only using auth, simply remove the config lines containing <code>ssl</code>.</p>"},{"location":"pages/develop/frameworks/lagom/#using-with-datastax-astra-db","title":"Using with DataStax Astra DB","text":"<p>For connecting to DataStax Astra DB, it is similar.  You will need to set both authentication and SSL to connect with Astra DB, as well as a few additional properties.</p> <p>The <code>pom.xml</code> is largely the same, except that you'll need to add your Astra host name here.  Note that in \"production mode,\" you should not need to modify this file.  But if you're connecting to an Astra DB cluster in \"development mode,\" you'll still need to disable embedded Cassandra and designate the Cassandra service as \"unmanaged\" with a contact point: <pre><code>&lt;configuration&gt;\n&lt;unmanagedServices&gt;\n&lt;cas_native&gt;https://ASTRA_DB_ID-ASTRA_DB_REGION.db.astra.datastax.com:29042&lt;/cas_native&gt;\n&lt;/unmanagedServices&gt;                    &lt;cassandraEnabled&gt;false&lt;/cassandraEnabled&gt;\n&lt;/configuration&gt;\n</code></pre></p> <p>The <code>application.conf</code> service files will require similar modifications: <pre><code>stream.cassandra {\n    contact-points = [\"ASTRA_DB_ID-ASTRA_DB_REGION.db.astra.datastax.com:29042\"]\n    authentication {\n        username = \"token\"\n        password = \"AstraCS:yourAstraT0ken\"\n    }\n    ssl {\n        truststore.path = \"/Users/youruser/astradb/trustStore.jks\"\n        truststore.password = \"yourTrustStorePassword\"\n        keystore.path = \"/Users/youruser/stackoverflow/identity.jks\"\n        keystore.password = \"Tte3jRy07ocEf6Z8h\"\n    }\n    session-provider = akka.persistence.cassandra.ConfigSessionProvider\n    keyspace = \"stream\"\n}\n\ncassandra-journal {\n    contact-points = ${stream.cassandra.contact-points}\n    keyspace = ${stream.cassandra.keyspace}\n    authentication = ${stream.cassandra.authentication}\n    ssl = ${stream.cassandra.ssl}\n    session-provider = ${stream.cassandra.session-provider}\n    keyspace-autocreate = false\n    tables-autocreate = true\n}\n\ncassandra-snapshot-store {\n    contact-points = ${stream.cassandra.contact-points}\n    keyspace = ${stream.cassandra.keyspace}\n    authentication = ${stream.cassandra.authentication}\n    ssl = ${stream.cassandra.ssl}\n    session-provider = ${stream.cassandra.session-provider}\n    keyspace-autocreate = false\n    tables-autocreate = true\n}\n\nlagom.persistence.read-side.cassandra {\n    contact-points = ${stream.cassandra.contact-points}\n    keyspace = ${stream.cassandra.keyspace}\n    authentication = ${stream.cassandra.authentication}\n    ssl = ${stream.cassandra.ssl}\n    session-provider = ${stream.cassandra.session-provider}\n    keyspace-autocreate = false\n    tables-autocreate = true\n}\n</code></pre></p> <p>Note the options for <code>keyspace-autocreate</code> and <code>tables-autocreate</code> are shown set here.  By default, these are both set to <code>true</code>.  However, Astra DB only permits keyspace creation to happen via the Astra Dashboard.  This means that:</p> <ul> <li>Keyspaces must be created before connecting a Lagom microservice to Astra DB.</li> <li>Lagom's attempts to create keyspaces will fail (due to a permissions error).</li> <li>Table creation should function appropriately, assuming the required keyspaces already exist.</li> </ul>"},{"location":"pages/develop/frameworks/micronaut/","title":"Micronaut","text":"<p>This guide was built based on the Micronaut Cassandra Guide </p>"},{"location":"pages/develop/frameworks/micronaut/#overview","title":"Overview","text":"<p>Micronaut is a modern, JVM-based, full stack Java framework designed for building modular, easily testable JVM applications with support for Java, Kotlin, and Groovy. Micronaut is developed by the creators of the Grails framework and takes inspiration from lessons learnt over the years building real-world applications from monoliths to microservices using Spring, Spring Boot and Grails. For more information refer to the user guide</p> <p>The micronaut-cassandra module includes support for integrating Micronaut services with Cassandra.</p>"},{"location":"pages/develop/frameworks/micronaut/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle</li> <li>You should install `Java JDK 1.8+` and Apache Maven</li> </ul>"},{"location":"pages/develop/frameworks/micronaut/#configuration","title":"Configuration","text":"<p>You can find a working sample here</p>"},{"location":"pages/develop/frameworks/micronaut/#step-1-create-your-project","title":"\u2705 Step 1: Create your project","text":"<ul> <li>To create a micronaut project and CLI <code>mn</code> is provided. You can install it using <code>sdkman</code> as describe in the doc</li> </ul> <pre><code>#Download SDKMan\ncurl -s https://get.sdkman.io | bash\n\n#Setup SDKMan\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\n#Download Micronaut\nsdk install micronaut\n</code></pre> <ul> <li>To generate a new project use <code>mn create-app</code> adding the feature cassandra</li> </ul> <pre><code>mn create-app astra-todo-micronaut --features cassandra\n</code></pre> <ul> <li>Notice that in your <code>pom.xml</code> you now have the following</li> </ul> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.micronaut.cassandra&lt;/groupId&gt;\n&lt;artifactId&gt;micronaut-cassandra&lt;/artifactId&gt;\n&lt;scope&gt;compile&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"pages/develop/frameworks/micronaut/#step-2-setup-your-project","title":"\u2705 Step 2: Setup your project","text":"<p>All configuration of your project will be defined in <code>application.yaml</code> in <code>src/main/resources</code>. The module is clever enough to load all properties as if it was the driver configuration file.</p> <p>You can defined multiple profiles and each profile will be identified with key <code>cassandra.${profile_name}</code>. There is a <code>default</code> profile. In the following sample file we provide 2 profiles one for local and one for Astra. There is no extra code needed, simply configuration.</p> <pre><code>cassandra:\ndefault:\nbasic:\nsession-keyspace: micronaut\ncontact-points:\n- \"localhost:9042\"\nload-balancing-policy:\nlocal-datacenter: datacenter1\nastra:\nbasic:\nrequest:\ntimeout: 5 seconds\nconsistency: LOCAL_QUORUM\npage-size: 5000\nsession-keyspace: micronaut\ncloud:\nsecure-connect-bundle: /Users/cedricklunven/Downloads/secure-connect-workshops.zip\nadvanced:\nauth-provider:\nclass: PlainTextAuthProvider\nusername: token\npassword: \"AstraCS:blahblahblah\"\nconnection:\ninit-query-timeout: 10 seconds\nset-keyspace-timeout: 10 seconds\ncontrol-connection.timeout: 10 seconds\n</code></pre>"},{"location":"pages/develop/frameworks/micronaut/#step-3-application-startup","title":"\u2705 Step 3: Application Startup","text":"<p>At startup you may want create the different tables needed for you application. In Astra you can only create keyspaces from the devops API or the user interface..</p> <p>To enable content at startup simple implement <code>ApplicationEventListener&lt;ServiceReadyEvent&gt;</code> as shown below</p> <pre><code>@Singleton\npublic class TodoApplicationStartup  implements ApplicationEventListener&lt;ServiceReadyEvent&gt; {\n/** Logger for the class. */\nprivate static final Logger LOGGER = LoggerFactory.getLogger(TodoApplicationStartup.class);\n@Property(name = \"todo.cassandra.create_schema\", defaultValue=\"false\")\nprivate boolean createTable;\n@Inject\nprivate CqlSession cqlSession;\n/** {@inheritDoc} */\n@Override\npublic void onApplicationEvent(final ServiceReadyEvent event) {\nLOGGER.info(\"Startup Initialization\");\nif (createTable) {\nTodoServiceCassandraCql.createTableTodo(cqlSession);\nLOGGER.info(\"+ Table TodoItems created if needed.\");\n}\nLOGGER.info(\"[OK]\");\n}\n}\n</code></pre>"},{"location":"pages/develop/frameworks/micronaut/#step-4-use-cassandra","title":"\u2705 Step 4: Use Cassandra","text":"<p>To use Cassandra you will reuse the <code>CqlSession</code> from the DataStax drivers. You can simply inject it where you needed as shown in this sample code</p> <pre><code>@Validated\n@Controller(\"/api/v1\")\npublic class TodoRestController {\n/** Logger for our Client. */\nprivate static final Logger LOGGER = LoggerFactory.getLogger(TodoRestController.class);\n/** CqlSession initialized from application.yaml */\n@Inject\nprivate CqlSession cqlSession;\n</code></pre> <p>Happy coding.</p>"},{"location":"pages/develop/frameworks/quarkus/","title":"\u2022 Quarkus","text":"<p>Quarkus - Supersonic Subatomic Java is a modern Kubernetes-native Java framework tailored for GraalVM and HotSpot to create applications for a modern, cloud-native world. </p> <p>The goal is to make Java the leading platform in Kubernetes and serverless environments while offering developers a framework to address a wider range of distributed application architectures.</p> <p>Wth its reactive nature, it complements Apache Cassandra and together they are a great fit for responsive microservices.</p>"},{"location":"pages/develop/frameworks/quarkus/#1-overview","title":"1. Overview","text":"<p>quarkus.io is the central repoistory for everything Quarkus related.</p> <p>The Cassandra Quarkus extension renders cassandra a first class citizen on the platform.</p> <p>You can [use the Cassandra client guide for Quarkus] (https://quarkus.io/guides/cassandra) to get started on using Quarkus and Astra together.</p>"},{"location":"pages/develop/frameworks/quarkus/#2-create-a-data-model","title":"2. Create a Data Model","text":"<p>Create a Data model for the simple application using the following CQL statement in the Astra console.</p> <pre><code>CREATE TABLE k1.Fruit ( name text PRIMARY KEY, description text);\n</code></pre> <p>where <code>k1</code> is the keyspace.</p>"},{"location":"pages/develop/frameworks/quarkus/#3-try-it-out","title":"3. Try it out!","text":"<p>Start by cloning the repo.</p> <pre><code>git clone https://github.com/datastaxdevs/Cassandra-Quarkus-Demo\n</code></pre> <p>Version check.</p> <p>Check the Java version with the following command.</p> <pre><code>java -version\nopenjdk version \"11.0.14\" 2022-01-18\nOpenJDK Runtime Environment GraalVM CE 22.0.0.2 (build 11.0.14+9-jvmci-22.0-b05)\nOpenJDK 64-Bit Server VM GraalVM CE 22.0.0.2 (build 11.0.14+9-jvmci-22.0-b05, mixed mode, sharing)\n</code></pre> <p>Setup Astra credentials.</p> <p>Include the credentials from the Astra console in the file <code>src/main/resources/application.properties</code> as shown below by downloading and including the path of the security connect bundle and the username and secret for the database.</p> <pre><code>-#quarkus.cassandra.cloud.secure-connect-bundle=/path/to/secure-connect-bundle.zip\n+quarkus.cassandra.cloud.secure-connect-bundle=k1.zip\n\n # Authentication\n # See https://docs.datastax.com/en/developer/java-driver/latest/manual/core/authentication/\n-#quarkus.cassandra.auth.username=&lt;your username&gt;\n-#quarkus.cassandra.auth.password=&lt;your password&gt;\n+quarkus.cassandra.auth.username=user\n+quarkus.cassandra.auth.password=secret\n</code></pre> <p>Package the app as below.</p> <pre><code>mvn clean package\n</code></pre> <p>and run the application as below.</p> <pre><code>java -jar target/cassandra-quarkus-quickstart-1.0.1-runner.jar\n</code></pre> <p>Check if the REST endpoints are accessible as below.</p> <pre><code>curl http://localhost:8080/fruits\n</code></pre> <p>As we've no data (yet), we should get an empty list.</p> <p>Let's add an entry as indicated in the example.</p> <pre><code>curl --header \"Content-Type: application/json\" \\\n  --request POST \\          \n  --data '{\"name\":\"apple\",\"description\":\"red and tasty\"}' \\\n  http://localhost:8080/fruits\n</code></pre> <p>Let's check via the <code>curl</code> command as below</p> <pre><code>curl http://localhost:8080/fruits\n</code></pre> <p>and also in the Astra CQL console.</p> <pre><code>select * from k1.Fruit;\n\n name  | description\n-------+---------------\n apple | red and tasty\n\n(1 rows)\n</code></pre>"},{"location":"pages/develop/frameworks/quarkus/#4-next-steps-and-conclusions","title":"4. Next Steps and Conclusions","text":"<p>You can check out the object mapper details, metrics and health reports from the app as outlined in the user guide.</p> <p>You could even package it as a native app and realize the full power of the Quarkus platform.</p>"},{"location":"pages/develop/frameworks/quarkus/#5-more-resources","title":"5. More Resources!","text":"<ul> <li>A complete Todo Application with Quarkus</li> <li>[Workshop outlining a step-by-step approach to run the application] (https://github.com/datastaxdevs/workshop-intro-quarkus-cassandra)</li> <li>Recording of the workshop</li> </ul>"},{"location":"pages/develop/frameworks/spring/","title":"Spring","text":"<p>Spring makes programming Java quicker, easier, and safer for everybody. Spring\u2019s focus on speed, simplicity, and productivity has made it the world's most popular Java framework. To get more information regarding the framework visit the reference website Spring.io.</p> <p><code>Spring-Data</code> is the module use to interact with Databases whereas <code>Spring Boot</code> is the runtime for microservices. In this page we detail how to setup both modules to interact with Astra.</p>"},{"location":"pages/develop/frameworks/spring/#1-overview","title":"1. Overview","text":""},{"location":"pages/develop/frameworks/spring/#11-modules-dependencies","title":"1.1 Modules dependencies","text":"<p>Spring is an ecosystem with dozens of modules. The component used to connect a Spring application to Astra (Cassandra) is Spring Data and especially Spring Data Cassandra. It relies on the DataStax native java cassandra drivers and only provides an abstraction with Spring concepts (templates, repository, Entities...)</p> <p>The stateful object <code>CqlSession</code> is instantiated and injected in spring <code>CassandraTemplate</code> (aka <code>CassandraOperations</code>). From there, it is used either directly or injected in different <code>CassandraRepository</code> (specialization of Spring Data <code>CrudRepository</code> for Apache Cassandra\u2122).</p> <p>The configuration of <code>spring-data-cassandra</code> in <code>Spring-Boot</code> applications is simplified with the usage of starters. One is associated to the standard web stack and called <code>spring-boot-starter-data-cassandra</code> and the other is named <code>spring-boot-starter-data-cassandra-reactive</code> for the reactive stack.</p> <p></p>"},{"location":"pages/develop/frameworks/spring/#12-compatibility-matrix","title":"1.2 Compatibility Matrix","text":"<p>In January 2019, the native Cassandra Drivers got an important, not backward compatible, upgrade. To get informations regarding Apache Cassandra\u2122 support here is the Cassandra compatibility matrix.</p> <p>Spring Data copes with the new generation of drivers starting with Spring data 3.x. Support of Astra was introduced in 2020 for all native versions (4.x and 3.x). This leads to the following table for minimal library versions for Astra Support:</p> Drivers Release Drivers Version Spring-Data Spring Boot <code>Unified 4.x</code> <code>4.6.0</code> <code>3.0.0.RELEASE</code> <code>2.3.0.RELEASE</code> <code>OSS 3.x</code> <code>3.8.0</code> Setup below table <code>2.2.13.RELEASE</code> <code>DSE 2.x</code> <code>2.3.0</code> <code>3.0.0.RELEASE</code> <code>2.3.0.RELEASE</code> <code>DSE 1.x</code> <code>1.9.0</code> Setup below table <code>2.2</code> <ul> <li>Setup Spring Data 2.2.x (and before) to work with Astra</li> </ul> <p>As stated in the matrix, even the latest Spring Data <code>2.2.13.RELEASE</code> rely on <code>cassandra-driver</code> version <code>3.7.2</code> that where not yet compatible to Astra. To work with Astra you  have to override the <code>cassandra-drivers</code> version as below.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n&lt;artifactId&gt;spring-boot-starter-data-cassandra&lt;/artifactId&gt;\n&lt;version&gt;2.2.13.RELEASE&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;\n&lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;\n&lt;version&gt;3.11.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>You can find here a sample project that uses Spring Boot version as old as <code>1.5.4</code>.</p> <ul> <li>Setup Spring Data 2.2 (and before) to work with DataStax Enterprise (DSE)</li> </ul> <p>Before 4.x and the unified drivers you have to use <code>dse-java-driver-core</code> to have access to enterprise features but also the be elligible for the support. To enable it  you need to exclude <code>cassandra-driver-core</code> and import <code>dse-java-driver-core</code> as show below</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n&lt;artifactId&gt;spring-boot-starter-data-cassandra&lt;/artifactId&gt;\n&lt;version&gt;2.2.13.RELEASE&lt;/version&gt;\n&lt;exclusions&gt;\n&lt;exclusion&gt;\n&lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;\n&lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;\n&lt;/exclusion&gt;\n&lt;/exclusions&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.dse&lt;/groupId&gt;\n&lt;artifactId&gt;dse-java-driver-core&lt;/artifactId&gt;\n&lt;version&gt;1.9.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"pages/develop/frameworks/spring/#13-rules-and-pitfalls","title":"1.3 Rules and Pitfalls","text":"<ul> <li>Define your own <code>CqlSession</code> bean (spring-data will find it !)</li> </ul> <p><code>Spring Data Cassandra</code> starters provide some dedicated keys in the configuration file <code>application.yaml</code> (<code>spring.data.cassandra.*</code>) but you do not get the complete list of options of the drivers. In the same way some super classes like <code>AbstractCassandraConfiguration</code> are provided where you can specify a few configuration properties but a limited set of keys are available.</p> <ul> <li>Do not use <code>findAll()</code></li> </ul> <p>It can be tempting to use this method to test new repositories as no parameter is required - but this is dangerous. The default paging mechanism is skipped and this method will retrieve every single record of the table. As such, it would perform a full scan of the cluster (pick data for each node) that (1) would be slow and (2) could lead to <code>OutOfMemoryException</code> as Cassandra Tables are expected to store billions of records.</p> <ul> <li>Do not use <code>@AllowFiltering</code></li> </ul> <p>This annotation (some for associated CQL Statement) is limited for the use cases where (1) you provide the partition key AND (2) you know your partition size is fairly small. In 99% of the cases the need of this annotation (or <code>ALLOW FILTERING</code> in the <code>CQL</code>) is a sign of a wrong data model: your primary key is invalid and you need another table to store the same data (or eventually to create a secondary index).</p> <ul> <li>Do not rely (only) on Spring Data to create your schema</li> </ul> <p>SDC provide a configuration key <code>spring.data.cassandra.schema-action: CREATE_IF_NOT_EXISTS</code> that proposes to create the Cassandra Tables based on your annotated beans. It is NOT a good idea. Indeed, it could lead to wrong data model (cf next point) but also it does not give access to fine grained properties like <code>COMPACTION</code> and <code>TTL</code> that might be different in development and production. Let a <code>Cassandra Administrator</code> reviews your DDL scripts and updates them for production.</p> <ul> <li>Data Model First, Entities second</li> </ul> <p>With the <code>JPA</code> (entity, repository) methodology, you are tempting to reuse the same entities and repositories to perform multiple queries against the same table. Most new requests will be not valid as you will not request using the primary key. You can be tempting to create a secondary index or use allow filtering; WRONG !. The good practice is to CREATE ANOTHER TABLE, ANOTHER ENTITY and ANOTHER REPOSITORY - and even if data stored is the same. With Cassandra 1 query = 1 table (mostly).</p> <ul> <li><code>CassandraRepository</code> probably cannot implement it all</li> </ul> <p>With real-life applications you might probably need to go back to the <code>CqlSession</code> and execute custom fine-grained queries (<code>Batches</code>, <code>TTL</code>, <code>LWT</code>...). The interfaces and <code>CassandraRepostiory</code> would not be enough. The class <code>SimpleCassandraRepository</code> is an abstract class (not interface0 you can inherit from that give you access to the <code>CqlSession</code> and execute your queries as you like, it is a good trade off.</p>"},{"location":"pages/develop/frameworks/spring/#2-astra-spring-boot-starter","title":"2. Astra Spring Boot Starter","text":""},{"location":"pages/develop/frameworks/spring/#21-introduction","title":"2.1 Introduction","text":"<p>The Astra Spring Boot Starter will configure both Astra SDK and Spring Data Cassandra to work with AstraDB. Configuration keys are provided in <code>application.yaml</code> like any spring applications with a dedicated prefix <code>astra.*</code>. The starter will initialize any beans you would need (<code>AstraClient</code>, <code>CqlSession</code>, <code>StargateClient</code>) to use every interfaces exposes by Astra. Not all are activated by default though, you want to initialize only what you need.</p> <p></p>"},{"location":"pages/develop/frameworks/spring/#22-project-setup","title":"2.2 Project Setup","text":""},{"location":"pages/develop/frameworks/spring/#prerequisites-astra","title":"Prerequisites [ASTRA]","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> </ul>"},{"location":"pages/develop/frameworks/spring/#prerequisites-development-environment","title":"Prerequisites [Development Environment]","text":"<ul> <li>You should install Java Development Kit (JDK) 8: Use the reference documentation to install a Java Development Kit, Validate your installation with</li> </ul> <pre><code>java --version\n</code></pre> <ul> <li>You should install Apache Maven: Use the reference documentation and validate your installation with</li> </ul> <pre><code>mvn -version\n</code></pre>"},{"location":"pages/develop/frameworks/spring/#setup-project","title":"Setup Project","text":"<ul> <li>Create your project with Spring Initializr. Dependencies needed are <code>web</code> and <code>data-cassandra</code> but we did the work for you if you click the template link</li> </ul> Property Value Property Value groupId <code>com.datastax.tutorial</code> package <code>com.datastax.tutorial</code> artifactId <code>sdk-quickstart-spring</code> description Sample Spring App name <code>sdk-quickstart-spring</code> dependencies <code>Spring Web</code> and <code>Spring Data for Cassandra</code> packaging <code>JAR</code> Java Version <code>8</code> or <code>11</code> <ul> <li> <p>Import the application in your favorite IDE but do not start the application immediately.</p> </li> <li> <p>Add the latest version of starter as a dependency in <code>pom.xml</code>  of <code>astra-spring-boot-starter</code> in the project.</p> </li> </ul> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.astra&lt;/groupId&gt;\n&lt;artifactId&gt;astra-spring-boot-starter&lt;/artifactId&gt;\n&lt;version&gt;0.3.4&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"pages/develop/frameworks/spring/#23-code-and-configuration","title":"2.3 Code and Configuration","text":"<ul> <li>Change the main class with the following code, we are leveraging on the unique <code>AstraClient</code> to interact with multiple interfaces.</li> </ul> <pre><code>@RestController\n@SpringBootApplication\npublic class SdkQuickstartSpringApplication {\npublic static void main(String[] args) {\nSpringApplication.run(SdkQuickstartSpringApplication.class, args);\n}\n// Provided by the Starter\n@Autowired\nprivate AstraClient astraClient;\n// Spring Data using the CqlSession initialized by the starter\n@Autowired\nprivate CassandraTemplate cassandraTemplate;\n@GetMapping(\"/api/devops/organizationid\")\npublic String showOrganizationId() {\nreturn astraClient.apiDevopsOrganizations().organizationId();\n}\n@GetMapping(\"/api/spring-data/datacenter\")\npublic String showDatacenterNameWithSpringData() {\nreturn cassandraTemplate.getCqlOperations()\n.queryForObject(\"SELECT data_center FROM system.local\", String.class);\n}\n@GetMapping(\"/api/cql/datacenter\")\npublic String showDatacenterNameWithSpringData() {\nreturn astraClient.cqlSession()\n.execute(\"SELECT data_center FROM system.local\")\n.one().getString(\"data_center\");\n}\n}\n</code></pre> <p>Rename <code>src/main/resources/application.properties</code> to <code>src/main/resources/application.yaml</code>. This step eases the configuration with hierarchical keys. Populate <code>application.yaml</code> with the following content and replace the values with expected values (how to retrieve the values are explained in the Quickstart Astra</p> <pre><code>astra:\n# Allow usage of devops and Stargate apis\napi:\napplication-token: &lt;your_token&gt;\ndatabase-id: &lt;your_database_id&gt;\ndatabase-region: &lt;your_database_region&gt;\n# Connectivity to Cassandra\ncql:\nenabled: true\ndownload-scb:\nenabled: true\ndriver-config:\nbasic:\nsession-keyspace: &lt;your_keyspace&gt;\n</code></pre> <ul> <li>Start the application</li> </ul> <pre><code>mvn clean install spring-boot:run\n</code></pre> <ul> <li>Access the resources we created</li> <li>Get your Organization ID: <code>http://localhost:8080/api/devops/organizationid</code></li> <li>Get your Datacenter Name (Spring-data): <code>http://localhost:8080/api/spring-data/datacenter</code></li> <li>Get your Datacenter Name (cql): <code>http://localhost:8080/api/cql/datacenter</code></li> </ul> <p></p>"},{"location":"pages/develop/frameworks/spring/#3-spring-data-cassandra","title":"3. Spring Data Cassandra","text":""},{"location":"pages/develop/frameworks/spring/#31-project-setup","title":"3.1 Project Setup","text":""},{"location":"pages/develop/frameworks/spring/#prerequisites-astra_1","title":"Prerequisites [ASTRA]","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Have downloaded your Cloud Secure Bundle</li> </ul>"},{"location":"pages/develop/frameworks/spring/#prerequisites-development-environment_1","title":"Prerequisites [Development Environment]","text":"<ul> <li>You should install Java Development Kit (JDK) 8: Use the reference documentation to install a Java Development Kit, Validate your installation with</li> </ul> <pre><code>java --version\n</code></pre> <ul> <li>You should install Apache Maven: Use the reference documentation and validate your installation with</li> </ul> <pre><code>mvn -version\n</code></pre>"},{"location":"pages/develop/frameworks/spring/#setup-project_1","title":"Setup Project","text":"<ul> <li>Create a Spring Boot application from the initializer and add the <code>spring-boot-starter-data-cassandra</code></li> </ul> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n&lt;artifactId&gt;spring-boot-starter-data-cassandra&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"pages/develop/frameworks/spring/#32-code-and-configuration","title":"3.2 Code and Configuration","text":"<ul> <li>Setup the configuration file <code>application.yaml</code></li> </ul> <pre><code>spring.data.cassandra:\nkeyspace-name: myks\nusername: myClientId\npassword: myClientSecret\nschema-action: CREATE_IF_NOT_EXISTS # for dev purpose\nrequest:\ntimeout: 10s\nconnection:\nconnect-timeout: 10s\ninit-query-timeout: 10s\ndatastax.astra:\n# You must download it before\nsecure-connect-bundle: /tmp/secure-connect-bundle.zip\n</code></pre> <ul> <li>Create a dedicated configuration bean to parse <code>datastax.astra</code></li> </ul> <pre><code>@ConfigurationProperties(prefix = \"datastax.astra\")\npublic class DataStaxAstraProperties {\nprivate File secureConnectBundle;\n// Getter and Setter omitted\n}\n</code></pre> <ul> <li>Define a bean of <code>CqlSessionBuilderCustomizer</code> to add this <code>CloudSecureBundle</code></li> </ul> <pre><code>@SpringBootApplication\n@EnableConfigurationProperties(DataStaxAstraProperties.class)\npublic class SpringDataCassandraApplication {\npublic static void main(String[] args) {\nSpringApplication.run(SpringDataCassandraApplication.class, args);\n}\n@Bean\npublic CqlSessionBuilderCustomizer sessionBuilderCustomizer(DataStaxAstraProperties astraProperties) {\nPath bundle = astraProperties.getSecureConnectBundle().toPath();\nreturn builder -&gt; builder.withCloudSecureConnectBundle(bundle);\n}\n}\n</code></pre>"},{"location":"pages/develop/languages/csharp/","title":"\u2022 CSharp","text":""},{"location":"pages/develop/languages/csharp/#1-overview","title":"1. Overview","text":"<p>Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces. There are different frameworks and tools to connect to Astra depending on the Api interface you choose.</p> <p>Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here.</p>"},{"location":"pages/develop/languages/csharp/#2-interfaces-list","title":"2. Interfaces List","text":"Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming"},{"location":"pages/develop/languages/csharp/#3-cql","title":"3. CQL","text":""},{"location":"pages/develop/languages/csharp/#31-cassandra-drivers","title":"3.1 Cassandra Drivers","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/csharp/#32-astra-sdk","title":"3.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/csharp/#4-stargate-rest-api","title":"4. Stargate REST Api","text":""},{"location":"pages/develop/languages/csharp/#41-axios","title":"4.1 Axios","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/csharp/#42-astra-sdk","title":"4.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/csharp/#5-stargate-document-api","title":"5. Stargate Document Api","text":""},{"location":"pages/develop/languages/csharp/#51-axios","title":"5.1 Axios","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/csharp/#52-astra-sdk","title":"5.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/csharp/#6-stargate-graphql","title":"6 Stargate GraphQL","text":""},{"location":"pages/develop/languages/csharp/#61-cql-first","title":"6.1 CQL First","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/csharp/#62-graphql-first","title":"6.2 GraphQL First","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/csharp/#7-stargate-grpc","title":"7. Stargate gRPC","text":""},{"location":"pages/develop/languages/csharp/#71-stargate-client","title":"7.1 Stargate Client","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/csharp/#72-astra-sdk","title":"7.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/csharp/#8-pulsar-client","title":"8. Pulsar Client","text":""},{"location":"pages/develop/languages/csharp/#81-pulsar-client","title":"8.1 Pulsar Client","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/csharp/#82-astra-sdk","title":"8.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/csharp/#9-pulsar-admin","title":"9. Pulsar Admin","text":""},{"location":"pages/develop/languages/csharp/#10-devops-api-database","title":"10 Devops API Database","text":""},{"location":"pages/develop/languages/csharp/#11-devops-api-organization","title":"11 Devops API Organization","text":""},{"location":"pages/develop/languages/csharp/#12-devops-api-streaming","title":"12 Devops API Streaming","text":""},{"location":"pages/develop/languages/go/","title":"\u2022 GoLang","text":""},{"location":"pages/develop/languages/go/#1-overview","title":"1. Overview","text":"<p>Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces. There are different frameworks and tools to connect to Astra depending on the Api interface you choose.</p> <p>Pick the interface below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. </p> <p>If you have issues or requests about these code samples, please open a ticket under Awesome-Astra.</p> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <ul> <li>You should have an Astra account</li> <li>You should Have an Astra Token with \"Database Administrator\" permissions</li> <li>You should Install the Astra CLI</li> </ul> <p>You will need to have a recent (1.17+) version of Go.  Visit the official download page, and select the appropriate version for your machine architecture.  To verify that Go is installed, run the following command:</p> <pre><code>go version\n</code></pre> <p>You want to have a go version of at least 1.17.</p> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <p>To get started you need to Install the Astra CLI. Create a directory you want to use and change into that directory. </p> <p>Using the token you created with the \"Database Administrator\" permission, use the CLI to setup your environment.</p> <pre><code>astra setup\n</code></pre> <p>Create a database and keyspace to work with.</p> <pre><code>astra db create workshops -k library --if-not-exist\n</code></pre>"},{"location":"pages/develop/languages/go/#2-interfaces-list","title":"2. Interfaces List","text":"<p>"},{"location":"pages/develop/languages/go/#3-cql","title":"3. CQL","text":""},{"location":"pages/develop/languages/go/#31-the-gocql-astra-driver","title":"3.1 The gocql-astra driver","text":"<p>\u2139\ufe0f Overview</p> <p>These instructions are aimed at helping people connect to Astra DB programmatically using the custom Astra Gocql driver.  </p> <p>Basic driver instructions Basic instructions can be found at the home page for gocql-astra. You can use these instructions, or scroll down to find some ready-made code for you to use.</p> README from gocql-astra <p>This provides a custom <code>gocql.HostDialer</code> that can be used to allow gocql to connect to DataStax Astra. The goal is to provide native support for gocql on Astra.</p> <p>This library relies on the following features of gocql:</p> <ul> <li>The ability to customize connection features via the HostDialer interface</li> <li>Querying system.peers if system.peers_v2 should be used but isn't available </li> </ul> <p>You must use a version of gocql which supports both of these features.  Both features have been merged into master as of version 1.2.1 so any release &gt;= 1.2.1 should work.</p> How to use gocql-astra <p>Using an Astra bundle:</p> <pre><code>cluster, err := gocqlastra.NewClusterFromBundle(\"/path/to/your/bundle.zip\", \"&lt;username&gt;\", \"&lt;password&gt;\", 10 * time.Second)\nif err != nil {\npanic(\"unable to load the bundle\")\n}\nsession, err := gocql.NewSession(*cluster)\n// ...\n</code></pre> <p>Using an Astra token:</p> <pre><code>cluster, err = gocqlastra.NewClusterFromURL(gocqlastra.AstraAPIURL, \"&lt;astra-database-id&gt;\", \"&lt;astra-token&gt;\", 10 * time.Second)\nif err != nil {\npanic(\"unable to load the bundle\")\n}\nsession, err := gocql.NewSession(*cluster)\n// ...\n</code></pre> <p>Environment variable version</p> <p>To use this library with environment variables, you can follow these steps.</p> <p>\ud83d\udda5\ufe0f Sample Code</p> <p>The best way to use this sample is to clone it from the repository.</p> <p>Follow the prerequisites above. <p>Clone the  repository and change into the 'gocql-astra' directory in that repository.</p> <pre><code>git clone https://github.com/awesome-astra/go-sample-code\ncd go-sample-code/gocql-astra/envvars\n</code></pre> <p>Create .env with astra CLI</p> <pre><code>astra db create-dotenv workshops -k gotest \n</code></pre> <p>Build the environment variable example.</p> <pre><code>go build envvars.go\n</code></pre> <p>Run the code in your environment.</p> <pre><code>./envvars\n</code></pre> Envvars example<pre><code>    package main\nimport (\n\"fmt\"\n\"log\"\n\"os\"\n\"time\"\ngocqlastra \"github.com/datastax/gocql-astra\"\n\"github.com/gocql/gocql\"\n)\nfunc main() {\nvar err error\nvar cluster *gocql.ClusterConfig\nif len(os.Getenv(\"ASTRA_DB_APPLICATION_TOKEN\")) &gt; 0 {\nif len(os.Getenv(\"ASTRA_DB_ID\")) == 0 {\npanic(\"database ID is required when using a token\")\n}\n}\nfmt.Println(\"Creating the cluster now\")\nfmt.Println(os.Getenv(\"ASTRA_DB_ID\"))\nfmt.Print(os.Getenv(\"ASTRA_DB_APPLICATION_TOKEN\"))\nfmt.Println(os.Getenv(\"ASTRA_DB_REGION\"))\nfmt.Println(cluster)\ncluster, err = gocqlastra.NewClusterFromURL(\"https://api.astra.datastax.com\", os.Getenv(\"ASTRA_DB_ID\"), os.Getenv(\"ASTRA_DB_APPLICATION_TOKEN\"), 10*time.Second)\nfmt.Println(cluster)\nif err != nil {\nfmt.Errorf(\"unable to load cluster %s from astra: %v\", os.Getenv(\"ASTRA_DB_APPLICATION_TOKEN\"), err)\n}\nstart := time.Now()\nsession, err := gocql.NewSession(*cluster)\nelapsed := time.Now().Sub(start)\nif err != nil {\nlog.Fatalf(\"unable to connect session: %v\", err)\n}\nfmt.Println(\"Making the query now\")\niter := session.Query(\"SELECT release_version FROM system.local\").Iter()\nvar version string\nfor iter.Scan(&amp;version) {\nfmt.Println(version)\n}\nif err = iter.Close(); err != nil {\nlog.Printf(\"error running query: %v\", err)\n}\nfmt.Printf(\"Connection process took %s\\n\", elapsed)\n}\n</code></pre> Sample code details <p>If you want to use this sample without cloning the repository, you will need to have the following:</p> <ul> <li>github.com/datastax/astra-client-go/v2 v2.2.9</li> <li>github.com/datastax/cql-proxy v0.1.3</li> </ul> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n    \"os\"\n    \"time\"\n\n    gocqlastra \"github.com/datastax/gocql-astra\"\n    \"github.com/gocql/gocql\"\n    \"github.com/joho/godotenv\"\n)\n\nfunc main() {\n\n    var err error\n\n    err = godotenv.Load()\n\n    var cluster *gocql.ClusterConfig\n    if len(os.Getenv(\"ASTRA_DB_SECURE_BUNDLE_PATH\")) &gt; 0 {\n        cluster, err = gocqlastra.NewClusterFromBundle(os.Getenv(\"ASTRA_DB_SECURE_BUNDLE_PATH\"), \"token\", os.Getenv(\"ASTRA_DB_APPLICATION_TOKEN\"), 10*time.Second)\n        if err != nil {\n            err = fmt.Errorf(\"unable to open bundle %s from file: %v\", os.Getenv(\"ASTRA_DB_SECURE_BUNDLE_PATH\"), err)\n            panic(err)\n        }\n    } else if len(os.Getenv(\"ASTRA_DB_APPLICATION_TOKEN\")) &gt; 0 {\n        if len(os.Getenv(\"ASTRA_DB_ID\")) == 0 {\n            panic(\"database ID is required when using a token\")\n        }\n        cluster, err = gocqlastra.NewClusterFromURL(\"https://api.astra.datastax.com\", os.Getenv(\"ASTRA_DB_ID\"), os.Getenv(\"ASTRA_DB_APPLICATION_TOKEN\"), 10*time.Second)\n        fmt.Println(cluster)\n        if err != nil {\n            fmt.Errorf(\"unable to load cluster %s from astra: %v\", os.Getenv(\"ASTRA_DB_APPLICATION_TOKEN\"), err)\n        }\n    } else {\n        fmt.Errorf(\"must provide either bundle path or token\")\n    }\n\n    start := time.Now()\n    session, err := gocql.NewSession(*cluster)\n    elapsed := time.Now().Sub(start)\n    if err != nil {\n        log.Fatalf(\"unable to connect session: %v\", err)\n    }\n\n    fmt.Println(\"Making the query now\")\n\n    iter := session.Query(\"SELECT release_version FROM system.local\").Iter()\n\n    var version string\n    for iter.Scan(&amp;version) {\n        fmt.Println(version)\n    }\n\n    if err = iter.Close(); err != nil {\n        log.Printf(\"error running query: %v\", err)\n    }\n\n    fmt.Printf(\"Connection process took %s\\n\", elapsed)\n}\n</code></pre> <p>To get this to work, you will need to pull the dependencies:</p> <pre><code>go mod init gocql\ngo mod tidy\ngo build envvars.go\n./envvars\n</code></pre>"},{"location":"pages/develop/languages/go/#issues","title":"Issues","text":"<ul> <li>Need to verify that topology/status events correctly update the driver when using Astra.</li> <li>This seems to work correctly and was tested by removing Astra coordinators</li> <li>There is a bit of weirdness around contact points. I'm just using a place holder <code>\"0.0.0.0\"</code> (some valid IP address)  then the <code>HostDialer</code> provides a host ID from the metadata service when the host ID in the <code>HostInfo</code> is empty.</li> </ul>"},{"location":"pages/develop/languages/go/#32-other-astra-cql-interfaces","title":"3.2 Other Astra CQL Interfaces","text":"<ul> <li>cql-proxy This proxy sidecar is not Go-specific, but it works with the existing Go drivers to provide an interface into Astra.</li> </ul>"},{"location":"pages/develop/languages/go/#4-the-stargate-api-signing-library","title":"4. The Stargate API Signing Library","text":""},{"location":"pages/develop/languages/go/#41-using-the-stargate-api-signing-library","title":"4.1 Using the Stargate API Signing Library","text":"<p>\u2139\ufe0f Overview</p> <p>These instructions are aimed at helping people connect to Astra DB programmatically using Stargate's API interface</p> <p>\ud83d\udda5\ufe0f Sample Code</p> <p>To use the signing library, you simply include it in your code and then create a client for making calls.</p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n    \"os\"\n    \"time\"\n\n    gocqlastra \"github.com/datastax/gocql-astra\"\n    \"github.com/gocql/gocql\"\n)\n\nfunc main() {\n\n    var err error\n\n    var cluster *gocql.ClusterConfig\n\n    if len(os.Getenv(\"ASTRA_DB_APPLICATION_TOKEN\")) &gt; 0 {\n        if len(os.Getenv(\"ASTRA_DB_ID\")) == 0 {\n            panic(\"database ID is required when using a token\")\n        }\n    }\n\n    fmt.Println(\"Creating the cluster now\")\n    fmt.Println(os.Getenv(\"ASTRA_DB_ID\"))\n    fmt.Print(os.Getenv(\"ASTRA_DB_APPLICATION_TOKEN\"))\n    fmt.Println(os.Getenv(\"ASTRA_DB_REGION\"))\n    fmt.Println(cluster)\n\n    cluster, err = gocqlastra.NewClusterFromURL(\"https://api.astra.datastax.com\", os.Getenv(\"ASTRA_DB_ID\"), os.Getenv(\"ASTRA_DB_APPLICATION_TOKEN\"), 10*time.Second)\n    fmt.Println(cluster)\n\n    if err != nil {\n        fmt.Errorf(\"unable to load cluster %s from astra: %v\", os.Getenv(\"ASTRA_DB_APPLICATION_TOKEN\"), err)\n    }\n\n    start := time.Now()\n    session, err := gocql.NewSession(*cluster)\n    elapsed := time.Now().Sub(start)\n\n    if err != nil {\n        log.Fatalf(\"unable to connect session: %v\", err)\n    }\n\n    fmt.Println(\"Making the query now\")\n\n    iter := session.Query(\"SELECT release_version FROM system.local\").Iter()\n\n    var version string\n\n    for iter.Scan(&amp;version) {\n        fmt.Println(version)\n    }\n\n    if err = iter.Close(); err != nil {\n        log.Printf(\"error running query: %v\", err)\n    }\n\n    fmt.Printf(\"Connection process took %s\\n\", elapsed)\n\n}\n</code></pre>"},{"location":"pages/develop/languages/go/#5-stargate-rest-api","title":"5. Stargate REST API","text":"<p>While the basic code is shown above, you can interact with a more extensive REST example by following these instructions.</p> <p>Follow the prerequisites above.</p> <p>Clone the  repository and change into the 'gocql-astra' directory in that repository.</p> <pre><code>git clone https://github.com/awesome-astra/go-sample-code\ncd go-sample-code/gocql-astra/astra_stargate_rest\n</code></pre> <p>Create .env with astra CLI</p> <pre><code>astra db create-dotenv workshops -k gotest \n</code></pre> <p>Run the code in your environment.</p> <pre><code>go build astra_stargate_rest_example.go\n./astra_stargate_rest_example\n</code></pre>"},{"location":"pages/develop/languages/go/#6-stargate-graphql-api","title":"6. Stargate GraphQL API","text":"<p>While the basic code is shown above, you can interact with a more extensive REST example by following these instructions.</p> <p>Follow the prerequisites above.</p> <p>Clone the repository into your directory, then change into the astra_stargate_rest directory.</p> <pre><code>git clone https://github.com/awesome-astra/go-sample-code\ncd astra_stargate_graphql\n</code></pre> <p>Create .env with astra CLI</p> <pre><code>astra db create-dotenv workshops -k library \n</code></pre> <p>Run the code in your environment.</p> <pre><code>go build astra_stargate_graphql.go\n./astra_stargate_graphql\n</code></pre>"},{"location":"pages/develop/languages/go/#7-stargate-document-api","title":"7. Stargate Document API","text":"<p>While the basic code is shown above, you can interact with a more extensive REST example by following these instructions.</p> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <p>To get started you need to Install the Astra CLI. Create a directory you want to use and change into that directory. </p> <p>Follow the prerequisites above.</p> <p>Clone the repository into your directory, then change into the astra_stargate_rest directory.</p> <pre><code>git clone https://github.com/awesome-astra/go-sample-code\ncd astra_stargate_document\n</code></pre> <p>Create .env with astra CLI</p> <pre><code>astra db create-dotenv workshops -k library \n</code></pre> <p>Run the code in your environment.</p> <pre><code>go build astra_stargate_document.go\n./astra_stargate_document\n</code></pre>"},{"location":"pages/develop/languages/go/#8-cql-api-grpc","title":"8. CQL API GRPC","text":""},{"location":"pages/develop/languages/go/#81-the-gocql-grpc-cassandra-astra-driver","title":"8.1 The Gocql GRPC Cassandra Astra Driver","text":"<p>\u2139\ufe0f Overview</p> <p>These instructions are aimed at helping people connect to Astra DB programmatically using the Astra specific Golang driver  </p> <p>Follow the prerequisites above.</p> <p>\ud83d\udda5\ufe0f Sample Code</p> <p>Clone the repository into your directory, then change into the astra-gprc directory.</p> <p>Create .env with astra CLI</p> <pre><code>astra db create-dotenv workshops -k library \n</code></pre> <pre><code>git clone https://github.com/awesome-astra/go-sample-code\ncd astra-grpc\n</code></pre> <p>Run the code in your environment.</p> <pre><code>go build AstraGRPCQuickStart.go\n./AstraGRPCQuickStart\n</code></pre>"},{"location":"pages/develop/languages/java/","title":"\u2022 Java","text":"<p>Astra offers different Apis. Select the API you want to use below to get documentation.</p> <p> </p>"},{"location":"pages/develop/languages/java/#1-pre-requisites","title":"1. Pre-requisites","text":"Setup your <code>JAVA</code> Development environment <ul> <li> Install Java Development Kit (JDK) 8+</li> </ul> <p>Use java reference documentation targetting your operating system to install a Java Development Kit. You can then validate your installation with the following command.</p> <pre><code>java --version\n</code></pre> <ul> <li> Install Apache Maven (3.8+)</li> </ul> <p>Samples and tutorials have been designed with <code>Apache Maven</code>. Use the reference documentation top install maven validate your installation with </p> <pre><code>mvn -version\n</code></pre> Setup Datastax <code>Astra DB</code> <ul> <li> Create your DataStax Astra account: </li> </ul> <p>Sign Up</p> <ul> <li> Create an Astra Token</li> </ul> <p>An astra token acts as your credentials, it holds the different permissions. The scope of a token is the whole organization (tenant) but permissions can be edited to limit usage to a single database.</p> <p>To create a token, please follow this guide</p> <p>The Token is in fact three separate strings: a <code>Client ID</code>, a <code>Client Secret</code> and the <code>token</code> proper. You will need some of these strings to access the database, depending on the type of access you plan. Although the Client ID, strictly speaking, is not a secret, you should regard this whole object as a secret and make sure not to share it inadvertently (e.g. committing it to a Git repository) as it grants access to your databases.</p> <pre><code>{\n\"ClientId\": \"ROkiiDZdvPOvHRSgoZtyAapp\",\n\"ClientSecret\": \"fakedfaked\",\n\"Token\":\"AstraCS:fake\"\n}\n</code></pre> <p>It is handy to have your token declare as an environment variable (replace with proper value):</p> <pre><code>export ASTRA_TOKEN=\"AstraCS:replace_me\"\n</code></pre> <ul> <li> Create a Database and a keyspace</li> </ul> <p>With your account you can run multiple databases, a Databases is an Apache Cassandra cluster. It can live in one or multiple regions (dc). In each Database you can have multiple keyspaces. In the page we will use the database name <code>db_demo</code> and the keyspace <code>keyspace_demo</code>.</p> <p>You can create the DB using the user interface and here is a tutorial. You can also use Astra command line interface. To install and setup the CLI run the following:</p> <pre><code>curl -Ls \"https://dtsx.io/get-astra-cli\" | bash\nsource ~/.astra/cli/astra-init.sh\nastra setup --token ${ASTRA_TOKEN}\n</code></pre> <p>To create DB and keyspace with the CLI:</p> <pre><code>astra db create db_demo -k keyspace_demo --if-not-exists\n</code></pre> <ul> <li> Download the Secure Connect Bundle for current database</li> </ul> <p>A Secure Connect Bundle contains the certificates and endpoints informations to open a mTLS connection. Often mentionned as <code>scb</code> its scope is a database AND a region. If your database is deployed on multiple regions you will have to download the bundle for each one and initiate the connection accordingly. Instructions to download Secure Connect Bundle are here</p> <p></p> <p>You can download the secure connect bundle from the user interface and here is a tutorial. You can also use Astra command line interface.</p> <pre><code>astra db download-scb db_demo -f /tmp/secure-connect-bundle-db-demo.zip\n</code></pre>"},{"location":"pages/develop/languages/java/#2-cassandra-drivers","title":"2. Cassandra Drivers","text":""},{"location":"pages/develop/languages/java/#21-drivers-4x","title":"2.1 Drivers 4.x","text":"<p>The official documentation for Cassandra drivers is available on datastax documentation portal</p> <p><code>4.x</code> is the recommended version of the cassandra drivers.</p>"},{"location":"pages/develop/languages/java/#quickstart","title":"Quickstart","text":"Project Setup <ul> <li> <p>Any version <code>4.x</code> should be compatible with Astra.</p> </li> <li> <p> Update your <code>pom.xml</code> file with the latest version of the 4.x libraries </p> </li> </ul> <pre><code>&lt;!-- (REQUIRED) --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.oss&lt;/groupId&gt;\n&lt;artifactId&gt;java-driver-core&lt;/artifactId&gt;\n&lt;version&gt;${latest4x}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;!-- OPTIONAL --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.oss&lt;/groupId&gt;\n&lt;artifactId&gt;java-driver-query-builder&lt;/artifactId&gt;\n&lt;version&gt;${latest4x}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.oss&lt;/groupId&gt;\n&lt;artifactId&gt;java-driver-mapper-runtime&lt;/artifactId&gt;\n&lt;version&gt;${latest4x}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> Sample Code <ul> <li> Create an <code>AstraDriver4x.java</code> class with the following code</li> </ul> AstraDriver4x.java<pre><code>package com.datastax.astra;\nimport java.nio.file.Paths;\nimport com.datastax.oss.driver.api.core.CqlSession;\npublic class AstraDriver4x {\npublic static void main(String[] args) {\ntry (CqlSession cqlSession = CqlSession.builder()\n.withCloudSecureConnectBundle(Paths.get(\"/tmp/secure-connect-bundle-db-demo.\"))\n.withAuthCredentials(\"client_id\",\"client_secret\")\n.withKeyspace(\"keyspace_demo\")\n.build()) {\nSystem.out.println(\"Connected to \" + cqlSession.getKeyspace().get());\n}\n}\n}\n</code></pre> <p> \u00a0Download The project   </p> What you need to know <p>\ud83d\udce6 About Secure Connect Bundle</p> <ul> <li>The path to the secure connect bundle for your Astra database is specified with <code>withCloudSecureConnectBundle()</code>, it accepts <code>String</code>, <code>File</code> and <code>URL</code>. </li> <li>An SSL connection will be established automatically. Manual SSL configuration is not allowed, any settings in the driver configuration (<code>advanced.ssl-engine-factory</code>) will be ignored.</li> <li>The secure connect bundle contains all of the necessary contact information. Specifying contact points manually is not allowed, and will result in an error</li> </ul> <p>\u2699\ufe0f About Parameters</p> <ul> <li> <p>The authentication credentials must be specified separately with <code>withAuthCredentials()</code>, and match the username and password that were configured when creating the Astra database.</p> </li> <li> <p>Another pair is accepted for the credentials: <code>token</code> for the username and the value of the token starting by <code>AstraCS:...</code> is accepted</p> </li> </ul> <pre><code>// Initialization with a token and not pair clientId/slientSecret\nCqlSession.builder().withAuthCredentials(\"token\",\"AstraCS:....\")\n</code></pre> <ul> <li> <p>The keyspace is here required and provided with <code>.withKeyspace()</code></p> </li> <li> <p>if the driver configuration does not specify an explicit consistency level, it will default to <code>LOCAL_QUORUM</code> (instead of LOCAL_ONE when connecting to a normal Cassandra database).</p> </li> <li> <p>Extra configuration can be provided in <code>application.conf</code> file. </p> </li> </ul> <p>\ud83d\udd0c About <code>CqlSession</code></p> <ul> <li> <p>All operations of the drivers can be execute from this object.</p> </li> <li> <p>It a stateful, <code>autocloseable</code>, object, and must be a singleton in your application.</p> </li> </ul> <p>File-based configuration</p> <p>Alternatively, or complementary the connection information can be specified in the driver\u2019s configuration file (<code>application.conf</code>). Merge the following options with any content already present. All keys available in the file are available in reference.conf</p> Recommended <code>application.conf</code> <pre><code>datastax-java-driver {\n  basic {\n    request {\n        timeout     = 10 seconds\n      consistency = LOCAL_QUORUM\n    }\n    # change this to match the target keyspace\n    session-keyspace = keyspace_name\n    cloud {\n      secure-connect-bundle = /path/to/secure-connect-database_name.zip\n    }\n  }\n  advanced {\n    auth-provider {\n      class = PlainTextAuthProvider\n      username = user_name \n      password = password\n    }\n    connection {\n      init-query-timeout = 10 seconds\n      set-keyspace-timeout = 10 seconds\n      pool {\n        local-size = 1\n      }\n    }\n    reconnection-policy {\n      class = ExponentialReconnectionPolicy\n      base-delay = 5 seconds\n      max-delay = 60 seconds\n    }\n    control-connection.timeout = 10 seconds\n  }\n}\n</code></pre> <p>With the file in the classpath, the previous code is updated as the following:</p> <pre><code>import java.nio.file.Paths;\nimport com.datastax.oss.driver.api.core.CqlSession;\npublic class AstraDriver4x {\npublic static void main(String[] args) {\ntry (CqlSession cqlSession = CqlSession.builder().build()) {\nSystem.out.println(\"Hello keyspace {} !\" + cqlSession.getKeyspace().get());\n}\n}\n}\n</code></pre> What you need to know <ul> <li>The configuration file <code>application.conf</code> is automatically loaded when present on the classpath. It can be used in any java-based application with no difference (spring, quarkus...)</li> <li><code>dc-failover</code> is NOT available as a different secure connect bundles are required for different regions (1 region = 1 dc in Astra)</li> </ul>"},{"location":"pages/develop/languages/java/#sample-code-library","title":"Sample Code Library","text":"Classname Description ShowMetaData4x Connect to Astra and show keyspaces and metadata from the CqlSession CreateSchema4x Create schema with different <code>table</code> and <code>type</code> (UDT) if they do not exist in keyspace DropSchema4x Remove assets of the schema,<code>table</code> and <code>type</code> (UDT) if they exist in target keyspace ConfigurationFile4x Setup the driver to use customize configuration file and not default <code>application.conf</code> ProgrammaticConfiguration Setup the driver in a programmatic way and not reading <code>application.conf</code> Getting Started First touch with executing queries Simple4x Read, update, insert, delete operations using <code>QueryBuilder</code> Paging4x Illustrating FetchSize and how to retrieve page by page Batches4x Group statements within batches ListSetMapUdt4x Advanced types insertions with <code>list</code>, <code>set</code>, <code>map</code> but also <code>User Defined Type</code> Json4x Work with columns or full record with <code>JSON</code> Async4x Sample operations as Simple in <code>Asynchronous</code> way ObjectMapping4x Map table record to Java POJO at driver level Counter4x Working with <code>counters</code> increment/decrement Lwt4x Working for Lightweight transactions read-before-write BlobAndCodec4x Working with <code>BLOB</code> and binary data but also how to create your own <code>CustomCodec</code> CloudAstra4x Working with <code>BLOB</code> and binary data but also how to create your own <code>CustomCodec</code> Reactive4x Working with the Reactive API introduce in driver 4.x"},{"location":"pages/develop/languages/java/#sample-projects-gallery","title":"Sample Projects Gallery","text":"Classname Description Spring PetClinic in Reactive Implementation of the <code>PetClinic</code> spring application using the reactive part of the drivers. Other frameworks used are <code>spring-data-cassandra</code> and <code>spring-boot</code> Quarkus Todo application Leveraging Quarkus extension to build a quarkus application Better Reads A clone of Good reads using Spring Boot and Spring Data Cassandra E-Commerce A full fledge e-commerce portal with catalog, shopping cart, payment and order processing with Spring Boot Build Microservices Microservices with Spring Devoxx 2022 3h of deep dive on how to build java applications with Spring, Quarkus and Micronaut Java Native Build todo application in Java Native with Spring, Quarkus and Micronaut Stargate TV Show Reproduce the wheel for Stargate TV Show with destinations saved in Astra Spring Data Cassandra Deep dive with Spring data cassandra"},{"location":"pages/develop/languages/java/#22-drivers-3x","title":"2.2 Drivers 3.x","text":"<p>The official documentation for the drivers can be found here</p> <p>Version <code>3.x</code> is still maintained but not recommended version. It will not get evolutions in the future\"</p>"},{"location":"pages/develop/languages/java/#quickstart_1","title":"QuickStart","text":"Project Setup <ul> <li> <p>Version 3.8+ or more is required to connect to Astra.</p> </li> <li> <p> Update your <code>pom.xml</code> file with the latest version of the 3.x libraries: </p> </li> </ul> <pre><code>&lt;!-- Mandatory --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;\n&lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;\n&lt;version&gt;${latest3x}&lt;/version&gt; &lt;/dependency&gt;\n&lt;!-- Optional, Used for object mapping--&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;\n&lt;artifactId&gt;cassandra-driver-mapping&lt;/artifactId&gt;\n&lt;version&gt;${latest3x}&lt;/version&gt; &lt;/dependency&gt;\n&lt;!-- Optional, Used for conversion ad-hoc--&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;\n&lt;artifactId&gt;cassandra-driver-extra&lt;/artifactId&gt;\n&lt;version&gt;${latest3x}&lt;/version&gt; &lt;/dependency&gt;\n</code></pre> Sample Code <ul> <li> Create class <code>AstraDriver3x.java</code> as followed:</li> </ul> AstraDriver3x.java<pre><code>package com.datastax.astra;\nimport java.io.File;\nimport com.datastax.driver.core.Cluster;\nimport com.datastax.driver.core.Session;\npublic class AstraDriver3x {\npublic static void main(String[] args) {\ntry(Cluster cluster = Cluster.builder()\n.withCloudSecureConnectBundle(new File(\"/tmp/scb.zip\"))\n.withCredentials(\"client_id\", \"client_secret\")\n.build()) {\ntry(Session session = cluster.connect(\"demo_keyspace\")) {\nSystem.out.println(session.getLoggedKeyspace());\n}\n}\n}\n}\n</code></pre> <p>\u00a0Download Project</p> What you need to know <ul> <li>If you work with previous versions of the driver (lower than <code>3.8</code> ) the support of Astra is not Ad-hoc it is recommended to migrate. Yet it is possible to use the <code>SSL</code> options. Documentation and sample codes can be found here. </li> </ul>"},{"location":"pages/develop/languages/java/#sample-code-library_1","title":"Sample Code Library","text":"Classname Description GettingStarted3x First touch with executing queries Simple3x Read, update, insert, delete operations using <code>QueryBuilder</code> ShowMetaData3x Connect to cluster then show keyspaces and metadata CreateKeyspace3x Create the <code>killrvideo</code> keyspace using <code>SchemaBuilder</code> if not exist CreateSchema3x Create <code>table</code> and <code>type</code> in <code>killrvideo</code> keyspace if they don't exist DropKeyspace3x Drop the <code>killrvideo</code> keyspace if existis using  <code>SchemaBuilder</code> DropSchema3x Drop all  <code>table</code> and <code>type</code> in <code>killrvideo</code> keyspace if they exist Paging3x Illustrating FetchSize and how to retrieve page by page Batches3x Group statements within batches ListSetMapUdt3x Advanced types insertions with <code>list</code>, <code>set</code>, <code>map</code> but also <code>User Defined Type</code> Json3x Work with columns or full record with <code>JSON</code> Async3x Sample operations as Simple in <code>Asynchronous</code> way ObjectMapping3x Map table record to Java POJO at driver level Counter3x Working with <code>counters</code> increment/decrement Lwt3x Working for Lightweight transactions read-before-write BlobAndCodec3x Working with <code>BLOB</code> and binary data but also how to create your own <code>CustomCodec</code> CloudAstra3x Working with <code>BLOB</code> and binary data but also how to create your own <code>CustomCodec</code>"},{"location":"pages/develop/languages/java/#23-astra-sdk","title":"2.3 Astra SDK","text":"<p>The <code>Astra</code> Software Deployment Kit, or <code>SDK</code>, allows developers to connect to Astra with all the different interfaces available. In this section we will detailed how to setup this client library to use the cassandra drivers interface.</p>"},{"location":"pages/develop/languages/java/#quickstart_2","title":"Quickstart","text":"Project Setup <ul> <li> Update your <code>pom.xml</code> file with the latest version of the SDK </li> </ul> <pre><code>&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.astra&lt;/groupId&gt;\n&lt;artifactId&gt;astra-sdk&lt;/artifactId&gt;\n&lt;version&gt;${latestSDK}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> Sample Code <ul> <li> Create a class <code>AstraSdkDrivers.java</code> with the following code.</li> </ul> AstraSdkDrivers.java<pre><code>package com.datastax.astra;\nimport com.datastax.astra.sdk.AstraClient;\nimport com.datastax.oss.driver.api.core.CqlSession;\npublic class AstraSdkDrivers {\npublic static void main(String[] args) {\n// Connect\ntry (AstraClient astraClient = AstraClient.builder()\n.withClientId(\"client_id\")\n.withClientSecret(\"client_secret\")\n.withDatabaseId(\"database_id\")\n.withDatabaseRegion(\"database_region\")\n.withCqlKeyspace(\"demo\")\n.enableCql()\n.build()) {\ntry(CqlSession cqlSession = astraClient.cqlSession()) {\nSystem.out.println(\"+ Cql Version (cql)   : \" + cqlSession\n.execute(\"SELECT cql_version from system.local\")\n.one().getString(\"cql_version\"));\n}\n}\n}\n}\n</code></pre> <p> \u00a0Download Project   </p> What you need to know <p>\ud83d\udd11 About Credentials</p> <ul> <li>The pair <code>clientId</code>/ <code>clientSecret</code> hold your credentials. It can be replaced by the value of the token only.</li> </ul> <pre><code>AstraClient.builder().withToken(\"AstraCS:...\");\n</code></pre> <ul> <li>There is no need to download the cloud securebundle in advance as it will be downloaded for you in folder <code>~.astra/scb</code> by default. Stil, but you can also provide the file location with <code>.withCqlCloudSecureConnectBundle()</code>:</li> </ul> <pre><code>AstraClient.builder().withCqlCloudSecureConnectBundle(\"/tmp/scb.zio\");\n</code></pre> <ul> <li>Notice than <code>enableCQL()</code> must be explictely provided. The <code>sdk</code> will open only the asked interfaces in order to limit the resource consumption.</li> </ul> <p>\u2699\ufe0f About Database identifiers</p> <ul> <li><code>databaseId</code>/<code>databaseRegion</code> will be required to locate the proper endpoint. You can find them for a particular database with either the cli.</li> </ul> <pre><code>$astra db list\n\n+---------------------+--------------------------------------+---------------------+-----------+\n| Name                | id                                   | Default Region      | Status    |\n+---------------------+--------------------------------------+---------------------+-----------+\n| db_demo             | 3043a40f-39bf-464e-8337-dc283167b2c3 | us-east1            | ACTIVE    |\n+---------------------+--------------------------------------+---------------------+-----------+\n\n$astra db describe db_demo\n\n+------------------------+-----------------------------------------+\n| Attribute              | Value                                   |\n+------------------------+-----------------------------------------+\n| Name                   | db_demo                                 |\n| id                     | 3043a40f-39bf-464e-8337-dc283167b2c3    |\n| Status                 | ACTIVE                                  |\n| Default Cloud Provider | GCP                                     |\n| Default Region         | us-east1                                |\n| Default Keyspace       | keyspace_demo                           |\n| Creation Time          | 2023-04-17T09:03:14Z                    |\n| Keyspaces              | [0] demo                                |\n| Regions                | [0] us-east1                            |\n+------------------------+-----------------------------------------+\n</code></pre>"},{"location":"pages/develop/languages/java/#4-api-rest","title":"4. Api Rest","text":"<p>\u26a0\ufe0f We recommend to use version <code>V2</code> (with V2 in the URL) as it covers more features and the V1 would be deprecated sooner.</p> <p></p> <p>To know more regarding this interface specially you can have a look to dedicated section of the wiki or reference Stargate Rest Api Quick Start Guide.</p>"},{"location":"pages/develop/languages/java/#41-http-client","title":"4.1 <code>Http Client</code>","text":"<p>You need an <code>HTTP Client</code> to use the Rest API. There are a lot of clients in the Java languages like HttpURLConnection, HttpClient introduced in Java 11, Apache HTTPClient, OkHttpClient, Jetty HttpClient. A comparison is provided is this blogpost to make your choice. In this tutorial, we will use the <code>Apache HttpClient</code>, which is included in the SDK. You should adapt the code depending on the framework you have chosen.</p> Import dependencies in your <code>pom.xml</code> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.httpcomponents.client5&lt;/groupId&gt;\n&lt;artifactId&gt;httpclient5&lt;/artifactId&gt;\n&lt;version&gt;5.1.3&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> Standalone Code <pre><code>import java.io.File;\npublic class AstraRestApiHttpClient {\nstatic final String ASTRA_TOKEN       = \"&lt;change_with_your_token&gt;\";\nstatic final String ASTRA_DB_ID       = \"&lt;change_with_your_database_identifier&gt;\";\nstatic final String ASTRA_DB_REGION   = \"&lt;change_with_your_database_region&gt;\";\nstatic final String ASTRA_DB_KEYSPACE = \"&lt;change_with_your_keyspace&gt;\";\npublic static void main(String[] args) throws Exception {\nString apiRestEndpoint = new StringBuilder(\"https://\")\n.append(ASTRA_DB_ID).append(\"-\")\n.append(ASTRA_DB_REGION)\n.append(\".apps.astra.datastax.com/api/rest\")\n.toString();\nSystem.out.println(\"Rest Endpoint is \" + apiRestEndpoint);\ntry (CloseableHttpClient httpClient = HttpClients.createDefault()) {\n// Work with HTTP CLIENT\nlistKeyspaces(httpClient, apiRestEndpoint);\ncreateTable(httpClient, apiRestEndpoint);\n}\n}\n}\n</code></pre> <ul> <li>Operations</li> </ul> List Keyspaces <p></p> <ul> <li>Code</li> </ul> <pre><code>private static void listKeyspaces(CloseableHttpClient httpClient, String apiRestEndpoint)\nthrows Exception {\n// Build Request\nHttpGet listKeyspacesReq = new HttpGet(apiRestEndpoint + \"/v2/schemas/keyspaces\");\nlistKeyspacesReq.addHeader(\"X-Cassandra-Token\", ASTRA_TOKEN);\n// Execute Request\ntry(CloseableHttpResponse res = httpClient.execute(listKeyspacesReq)) {\nif (200 == res.getCode()) {\nlogger.info(\"[OK] Keyspaces list retrieved\");\nlogger.info(\"Returned message: {}\", EntityUtils.toString(res.getEntity()));\n}\n</code></pre> Creating a table <p></p> <ul> <li>Sample <code>JSON</code> payload <code>createTableJson</code>.</li> </ul> <pre><code>{\n\"name\": \"users\",\n\"columnDefinitions\": [\n{  \"name\": \"firstname\", \"typeDefinition\": \"text\" },\n{  \"name\": \"lastname\",  \"typeDefinition\": \"text\" },\n{  \"name\": \"email\",     \"typeDefinition\": \"text\" },\n{  \"name\": \"color\",     \"typeDefinition\": \"text\" }\n],\n\"primaryKey\": { \"partitionKey\": [\"firstname\"],\n\"clusteringKey\": [\"lastname\"]\n},\n\"tableOptions\": {\n\"defaultTimeToLive\": 0,\n\"clusteringExpression\": [{ \"column\": \"lastname\", \"order\": \"ASC\" }]\n}\n}\n</code></pre> <ul> <li>Creating the http request using that payload</li> </ul> <pre><code>private static void createTable(CloseableHttpClient httpClient, String apiRestEndpoint)\nthrows Exception {\nHttpPost createTableReq = new HttpPost(apiRestEndpoint\n+ \"/v2/schemas/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/tables\");\ncreateTableReq.addHeader(\"X-Cassandra-Token\", ASTRA_TOKEN);\nString createTableJson = \"{...JSON.....}\";\ncreateTableReq.setEntity(new StringEntity(createTableJson, ContentType.APPLICATION_JSON));\n// Execute Request\ntry(CloseableHttpResponse res = httpClient.execute(createTableReq)) {\nif (201 == res.getCode()) {\nlogger.info(\"[OK] Table Created (if needed)\");\nlogger.info(\"Returned message: {}\", EntityUtils.toString(res.getEntity()));\n}\n}\n}\n</code></pre> Insert a new Row <p></p> <pre><code>private static void insertRow(CloseableHttpClient httpClient, String apiRestEndpoint)\nthrows Exception {\nHttpPost insertCedrick = new HttpPost(apiRestEndpoint + \"/v2/keyspaces/\"\n+ ASTRA_DB_KEYSPACE + \"/users\" );\ninsertCedrick.addHeader(\"X-Cassandra-Token\", ASTRA_TOKEN);\ninsertCedrick.setEntity(new StringEntity(\"{\"\n+ \" \\\"firstname\\\": \\\"Cedrick\\\",\"\n+ \" \\\"lastname\\\" : \\\"Lunven\\\",\"\n+ \" \\\"email\\\"    : \\\"c.lunven@gmail.com\\\",\"\n+ \" \\\"color\\\"    : \\\"blue\\\" }\", ContentType.APPLICATION_JSON));\n// Execute Request\ntry(CloseableHttpResponse res = httpClient.execute(insertCedrick)) {\nif (201 == res.getCode()) {\nlogger.info(\"[OK] Row inserted\");\nlogger.info(\"Returned message: {}\", EntityUtils.toString(res.getEntity()));\n}\n}\n}\n</code></pre> Retrieve a row <p></p> <pre><code>private static void retrieveRow(CloseableHttpClient httpClient, String apiRestEndpoint)\nthrows Exception {\n// Build Request\nHttpGet rowReq = new HttpGet(apiRestEndpoint + \"/v2/keyspaces/\"\n+ ASTRA_DB_KEYSPACE + \"/users/Cedrick/Lunven\" );\nrowReq.addHeader(\"X-Cassandra-Token\", ASTRA_TOKEN);\n// Execute Request\ntry(CloseableHttpResponse res = httpClient.execute(rowReq)) {\nif (200 == res.getCode()) {\nString payload =  EntityUtils.toString(res.getEntity());\nlogger.info(\"[OK] Row retrieved\");\nlogger.info(\"Row retrieved : {}\", payload);\n}\n}\n}\n</code></pre> Resources <p> \u00a0Download REST HTTP CLIENT   </p> <ul> <li>To get the full fledged information regarding the SDK check the github repository</li> </ul>"},{"location":"pages/develop/languages/java/#42-java-sdk","title":"4.2 <code>Java SDK</code>","text":"<p>The <code>Astra SDK</code> sets up the connection to work with the AstraDB cloud-based service. You will work with the class <code>AstraClient</code>, Reference documentation.</p> Import dependencies in your <code>pom.xml</code> <ul> <li>Update your <code>pom.xml</code> file with the latest version of the SDK </li> </ul> <pre><code>&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.astra&lt;/groupId&gt;\n&lt;artifactId&gt;astra-sdk&lt;/artifactId&gt;\n&lt;version&gt;${latestSDK}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> Sample Code <ul> <li> Create a class <code>AstraSdkRestApi.java</code> with the following code.</li> </ul> AstraSdkDrivers.java<pre><code>package com.datastax.astra;\nimport com.datastax.astra.sdk.AstraClient;\nimport io.stargate.sdk.core.domain.RowResultPage;\nimport io.stargate.sdk.rest.StargateRestApiClient;\nimport io.stargate.sdk.rest.TableClient;\nimport io.stargate.sdk.rest.domain.CreateTable;\nimport io.stargate.sdk.rest.domain.SearchTableQuery;\nimport java.util.HashMap;\nimport java.util.Map;\npublic class AstraSdkRestApi {\npublic static void main(String[] args) {\n// Connect\ntry (AstraClient astraClient = AstraClient.builder()\n.withClientId(\"client_id\")\n.withClientSecret(\"client_secret\")\n.withDatabaseId(\"database_id\")\n.withDatabaseRegion(\"database_region\")\n.build()) {\nStargateRestApiClient restApiClient =\nastraClient.apiStargateData();\n// -- Create a table\nCreateTable createTable = CreateTable.builder()\n.name(\"my_table\")\n.addPartitionKey(\"foo\", \"text\")\n.addColumn(\"bar\", \"text\")\n.ifNotExist(true)\n.build();\nrestApiClient.keyspace(\"demo\")\n.createTable(\"my_table\", createTable);\n// we can now work on the table\nTableClient tableClient = restApiClient\n.keyspace(\"demo\")\n.table(\"my_table\");\n// -- Insert a row\nMap&lt;String, Object&gt; record = new HashMap&lt;&gt;();\nrecord.put(\"foo\", \"Hello\");\nrecord.put(\"bar\", \"World\");\ntableClient.upsert(record);\n// -- Retrieve rows\nSearchTableQuery query = SearchTableQuery.builder().\nselect(\"foo\", \"bar\")\n.where(\"foo\")\n.isEqualsTo(\"Hello\")\n.build();\nRowResultPage result = tableClient.search(query);\nSystem.out.println(result.getResults()\n.get(0).get(\"bar\"));\n}\n}\n}\n</code></pre> <p> \u00a0Download Project   </p>"},{"location":"pages/develop/languages/java/#5-api-document","title":"5. Api Document","text":"<p>The Document API is an HTTP REST API and part of the open source Stargate.io. The idea is to provide an abstraction on top of Apache Cassandra\u2122 to allow document-oriented access patterns. To get familiar with it you can access documentation and sandbox here</p>"},{"location":"pages/develop/languages/java/#51-http-client","title":"5.1 <code>Http Client</code>","text":"Import dependencies in your <code>pom.xml</code> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;org.apache.httpcomponents.client5&lt;/groupId&gt;\n&lt;artifactId&gt;httpclient5&lt;/artifactId&gt;\n&lt;version&gt;5.1.3&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> Standalone Code <pre><code>static final String ASTRA_TOKEN       = \"change_me\";\nstatic final String ASTRA_DB_ID       = \"change_me\";\nstatic final String ASTRA_DB_REGION   = \"change_me\";\nstatic final String ASTRA_DB_KEYSPACE = \"change_me\";\nstatic  Logger logger = LoggerFactory.getLogger(AstraDocApiHttpClient.class);\npublic static void main(String[] args) throws Exception {\ntry (CloseableHttpClient httpClient = HttpClients.createDefault()) {\n// Build Request\nString apiRestEndpoint = new StringBuilder(\"https://\")\n.append(ASTRA_DB_ID).append(\"-\")\n.append(ASTRA_DB_REGION)\n.append(\".apps.astra.datastax.com/api/rest\")\n.toString();\nHttpGet req = new HttpGet(apiRestEndpoint + \"/v2/schemas/namespaces\");\nreq.addHeader(\"X-Cassandra-Token\", ASTRA_TOKEN);\n// Execute Request\ntry(CloseableHttpResponse res = httpClient.execute(req)) {\nif (200 == res.getCode()) {\nlogger.info(\"[OK] Namespaces list retrieved\");\nlogger.info(\"Returned message: {}\", EntityUtils.toString(res.getEntity()));\n}\n}\n}\n}\n</code></pre> Resources <p> \u00a0Download SDK Sample   </p>"},{"location":"pages/develop/languages/java/#52-java-sdk","title":"5.2 <code>Java SDK</code>","text":"<p>The <code>Astra SDK</code> sets up the connection to work with the AstraDB cloud-based service. You will work with the class <code>AstraClient</code>, Reference documentation.</p> Import dependencies in your <code>pom.xml</code> <ul> <li>Update your <code>pom.xml</code> file with the latest version of the SDK </li> </ul> <pre><code>&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.astra&lt;/groupId&gt;\n&lt;artifactId&gt;astra-sdk&lt;/artifactId&gt;\n&lt;version&gt;${latestSDK}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> Sample Code <ul> <li> Create a class <code>AstraSdkDocApi.java</code> with the following code.</li> </ul> AstraSdkDrivers.java<pre><code>package com.datastax.astra;\nimport com.datastax.astra.sdk.AstraClient;\nimport io.stargate.sdk.doc.CollectionClient;\nimport io.stargate.sdk.doc.Document;\nimport io.stargate.sdk.doc.StargateDocumentApiClient;\nimport java.util.stream.Collectors;\npublic class AstraSdkDocApi {\n// Given a Java POJO\npublic static final class User {\nString email;\npublic String getEmail() { return email; }\npublic void setEmail(String email) { this.email = email; }\n}\npublic static void main(String[] args) {\n// Connect\ntry (AstraClient astraClient = AstraClient.builder()\n.withClientId(\"client_id\")\n.withClientSecret(\"client_secret\")\n.withDatabaseId(\"database_id\")\n.withDatabaseRegion(\"database_region\")\n.build()) {\nStargateDocumentApiClient apiDoc =\nastraClient.apiStargateDocument();\n// List namespaces\nSystem.out.println(\"- List namespaces  : \" + apiDoc\n.namespaceNames()\n.collect(Collectors.toList()));\n// List Collection in a Keyspace\nSystem.out.println(\"- List collections : \" + apiDoc\n.namespace(\"demo\")\n.collectionNames()\n.collect(Collectors.toList()));\n// Create collection\nCollectionClient userCollection = apiDoc\n.namespace(\"demo\")\n.collection(\"user\");\nif (!userCollection.exist()) userCollection.create();\n// Working with documents\nUser userA = new User();userA.setEmail(\"a@a.com\");\nUser userB = new User();userB.setEmail(\"b@b.com\");\n// Create a Document and let Astra create the ID\nString userADocumentId = userCollection.create(userA);\nSystem.out.println(\"Document Created: \" + userADocumentId);\n// Create a Document with a specific ID\nuserCollection.document(\"b@b.com\").upsert(userB);\n// List Documents\nSystem.out.println(\"Documents:\");\nfor(Document&lt;User&gt; doc : userCollection.findPage(User.class).getResults()) {\nSystem.out.println(\"\" +\n\"- id: \" + doc.getDocumentId() +\n\", email : \" + doc.getDocument().getEmail());\n}\n}\n}\n}\n</code></pre> <p> \u00a0Download Project   </p>"},{"location":"pages/develop/languages/java/#53-repository-pattern","title":"5.3 Repository Pattern","text":"<p>With modern java applications you want to interact with the DB using the repository pattern where most operations have been implemented for you <code>findAll(), findById()...</code>. Astra SDK provide this feature for the document API.</p> Sample Code <ul> <li> Create a class <code>AstraSdkDocRepository.java</code> with the following code.</li> </ul> AstraSdkDrivers.java<pre><code>package com.datastax.astra;\nimport com.datastax.astra.sdk.AstraClient;\nimport io.stargate.sdk.doc.StargateDocumentRepository;\npublic class AstraSdkDocRepository {\n// Given a Java POJO\npublic static final class User {\nString email;\npublic String getEmail() { return email; }\npublic void setEmail(String email) { this.email = email; }\n}\npublic static void main(String[] args) {\n// Connect\ntry (AstraClient astraClient = AstraClient.builder()\n.withClientId(\"client_id\")\n.withClientSecret(\"client_secret\")\n.withDatabaseId(\"database_id\")\n.withDatabaseRegion(\"database_region\")\n.withCqlKeyspace(\"demo\")\n.enableCql()\n.build()) {\n// Doc Repository\nStargateDocumentRepository&lt;User&gt; userRepo =\nnew StargateDocumentRepository&lt;User&gt;(\nastraClient.apiStargateDocument().namespace(\"demo\"),\nUser.class);\n// Working with documents\nUser userA = new User();\nuserA.setEmail(\"a@a.com\");\nUser userB = new User();\nuserB.setEmail(\"b@b.com\");\n// Create Documents\nuserRepo.insert(userA);\nuserRepo.insert(\"b@b.com\", userB);\n// List Documents\nuserRepo.findAll().forEach(doc -&gt; {\nSystem.out.println(\n\"id=\" + doc.getDocumentId()\n+ \", email= \" + doc.getDocument().getEmail());\n});\n}\n}\n}\n</code></pre> <p> \u00a0Download Project   </p>"},{"location":"pages/develop/languages/java/#6-api-graphql","title":"6 Api GraphQL","text":""},{"location":"pages/develop/languages/java/#61-astra-sdk","title":"6.1 <code>Astra SDK</code>","text":"<p>The <code>Astra SDK</code> sets up the connection to work with the AstraDB cloud-based service. You will work with the class <code>AstraClient</code>, Reference documentation.</p> Sample Code <ul> <li> Create a class <code>AstraSdkGraphQLApi.java</code> with the following code.</li> </ul> AstraSdkGraphQLApi.java<pre><code>package com.datastax.astra;\nimport com.datastax.astra.sdk.AstraClient;\nimport io.stargate.sdk.gql.StargateGraphQLApiClient;\npublic class AstraSdkGraphQLApi {\npublic static void main(String[] args) {\n// Connect\ntry (AstraClient astraClient = AstraClient.builder()\n.withClientId(\"client_id\")\n.withClientSecret(\"client_secret\")\n.withDatabaseId(\"database_id\")\n.withDatabaseRegion(\"database_region\")\n.build()) {\nStargateGraphQLApiClient graphClient = astraClient.apiStargateGraphQL();\ngraphClient.keyspaceDDL().keyspaces().forEach(k-&gt; System.out.println(k.getName()));\n}\n}\n}\n</code></pre> <p> \u00a0Download Project   </p>"},{"location":"pages/develop/languages/java/#7-api-grpc","title":"7. Api gRPC","text":""},{"location":"pages/develop/languages/java/#71-grpc-client","title":"7.1 Grpc Client","text":"Import dependencies in your <code>pom.xml</code> <ul> <li>Update your <code>pom.xml</code> file with the latest version of the SDK </li> </ul> <pre><code>&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.stargate.grpc&lt;/groupId&gt;\n&lt;artifactId&gt;grpc-proto&lt;/artifactId&gt;\n&lt;version&gt;${latest-grpc-stargate}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.grpc&lt;/groupId&gt;\n&lt;artifactId&gt;grpc-netty-shaded&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> Standalone Code <pre><code>  public class GrpcClient {\n// Define inputs\nstatic final String ASTRA_DB_TOKEN  = \"&lt;provide_a_clientSecret&gt;\";\nstatic final String ASTRA_DB_ID     = \"&lt;provide_your_database_id&gt;\";\nstatic final String ASTRA_DB_REGION = \"&lt;provide_your_database_region&gt;\";\n// Open Grpc communicatino \nManagedChannel channel = ManagedChannelBuilder\n.forAddress(ASTRA_DB_ID + \"-\" + ASTRA_DB_REGION + \".apps.astra.datastax.com\", 443)\n.useTransportSecurity()\n.build();\n// use Grpc Stub generated from .proto as a client\nStargateGrpc.StargateBlockingStub cloudNativeClient = StargateGrpc\n.newBlockingStub(channel)\n.withCallCredentials(new StargateBearerToken(ASTRA_DB_TOKEN))\n.withDeadlineAfter(5, TimeUnit.SECONDS);\n// create Query\nString cqlQuery = \"SELECT data_center from system.local\";\n// Execute the Query\nResponse res = cloudNativeClient.executeQuery(QueryOuterClass\n.Query.newBuilder().setCql(cqlQuery).build());\n// Accessing Row result\nQueryOuterClass.Row row = res.getResultSet().getRowsList().get(0);\n// Access the single value\nString datacenterName = row.getValues(0).getString();\nSystem.out.println(\"You are connected to '%s'\".formatted(datacenterName));\n</code></pre>"},{"location":"pages/develop/languages/java/#72-astra-sdk","title":"7.2 Astra SDK","text":"<p>The <code>Astra SDK</code> sets up the connection to work with the AstraDB cloud-based service. You will work with the class <code>AstraClient</code>, Reference documentation.</p> Import dependencies in your <code>pom.xml</code> <ul> <li>Update your <code>pom.xml</code> file with the latest version of the SDK </li> </ul> <pre><code>&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.astra&lt;/groupId&gt;\n&lt;artifactId&gt;astra-sdk&lt;/artifactId&gt;\n&lt;version&gt;${latestSDK}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> Sample Code <ul> <li> Create a class <code>AstraSdkGrpcApi.java</code> with the following code.</li> </ul> AstraSdkGrpcApi.java<pre><code>package com.datastax.astra;\nimport com.datastax.astra.sdk.AstraClient;\nimport io.stargate.sdk.grpc.StargateGrpcApiClient;\npublic class AstraSdkGrpcApi {\npublic static void main(String[] args) {\n// Connect\ntry (AstraClient astraClient = AstraClient.builder()\n.withClientId(\"client_id\")\n.withClientSecret(\"client_secret\")\n.withDatabaseId(\"database_id\")\n.withDatabaseRegion(\"database_region\")\n.build()) {\nStargateGrpcApiClient grpcClient = astraClient.apiStargateGrpc();\nSystem.out.println(\"+ Cql Version (grpc)  : \" + grpcClient\n.execute(\"SELECT cql_version from system.local\")\n.one().getString(\"cql_version\"));\n}\n}\n}\n</code></pre> <p> \u00a0Download Project    </p>"},{"location":"pages/develop/languages/javascript/","title":"\u2022 Javascript","text":""},{"location":"pages/develop/languages/javascript/#1-overview","title":"1. Overview","text":"<p>Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces. There are different frameworks and tools to connect to Astra depending on the Api interface you choose.</p> <p>Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started.  More information here.</p>"},{"location":"pages/develop/languages/javascript/#2-interfaces-list","title":"2. Interfaces List","text":"Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming"},{"location":"pages/develop/languages/javascript/#3-cql","title":"3. CQL","text":""},{"location":"pages/develop/languages/javascript/#31-cassandra-native-driver","title":"3.1 Cassandra Native Driver","text":"<p>\u2139\ufe0f Overview</p> <p>These instructions are aimed at helping people connect to Astra DB programmatically using the DataStax Node driver.</p> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure bundle</li> </ul> <p>Covered the basics and looking for more? We\u2019ve got docs to help you complete a variety of tasks. Here are some relevant topics for you:</p> <ul> <li>Node.js Driver Overview</li> <li>Migrating Node.js Driver</li> </ul> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <p>You need a current version of Node (16+) and NPM (9+)</p> <p>\ud83d\udce6 Setup Project</p> <pre><code>npm install cassandra-driver\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code Create a connect-database.js file in the main directory of your Node.js project:</p> <pre><code>mkdir nodejsProject\ncd nodejsProject\ntouch connect-database.js\n</code></pre> <p>Add the following connection code to the new file. Set username to your App Token\u2019s Client ID. Set password to your App Token\u2019s Client Secret. Set PATH/TO secure with the path to your secure connect bundle zip file.</p> <pre><code>const { Client } = require(\"cassandra-driver\");\nasync function run() {\n   const client = new Client({\n      cloud: {\n      secureConnectBundle: \"&lt;&lt;PATH/TO/&gt;&gt;secure-connect-stargate.zip\",\n      },\n      credentials: {\n      username: \"&lt;&lt;CLIENT ID&gt;&gt;\",\n      password: \"&lt;&lt;CLIENT SECRET&gt;&gt;\",\n      },\n   });\n\n   await client.connect();\n\n   // Execute a query\n   const rs = await client.execute(\"SELECT * FROM system.local\");\n   console.log(`Your cluster returned ${rs.rowLength} row(s)`);\n\n   await client.shutdown();\n}\n\n// Run the async function\nrun();\n</code></pre> <p>Ensure you set username to your App Token's Client ID, password to your App Token's Client Secret, and path/to/secure-connect-database_name.zip with the path to your SCB.  This code creates a Client instance to connect to your Astra DB, runs a CQL query, and prints the output to the console.</p> <p>Then, Save and close the connect-database.js file and run the connect-database.js example with the Node.js runtime.</p> <pre><code>node connect-database.js\n</code></pre>"},{"location":"pages/develop/languages/javascript/#32-cassandra-cloud-driver-grpc","title":"3.2 Cassandra Cloud Driver (GRPC)","text":"<p>\u2139\ufe0f Overview</p> <p>The cloud native (known as Google Remote Procedure Call or gRPC) client is well-supported across multiple languages. Using the gRPC client means you can easily query CQL from any source without the worry of driver installation or upgrades.</p> <p>Covered the basics and looking for more? We\u2019ve got docs to help you complete a variety of tasks. Here are some relevant topics for you:</p> <ul> <li>Node.js Driver Overview</li> <li>Node.js Querying -Processing a result set</li> <li>Node.js Developing</li> <li>Node full sample script</li> </ul> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <ul> <li>If you do not already have one, get an API Token and set the role to \u201cDatabase Administrator\u201d.</li> <li>Create a keyspace.</li> <li>Create a table in your keyspace(optional): REST</li> </ul> <p>\ud83d\udce6 Setup Project</p> <p>Install stargate-grpc-node-client using either npm or yarn:</p> <p>npm command</p> <pre><code>npm i @stargate-oss/stargate-grpc-node-client\n</code></pre> <p>Yarn command</p> <pre><code>yarn add @stargate-oss/stargate-grpc-node-client\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <p>This example assumes that you\u2019re running Stargate on Astra DB. For more information, please see the documentation. You\u2019ll need to download your token from the Astra DB dashboard and add the token to the connection portion of the script.</p> <p>The token to use in the Header of API calls is the same as your database's Application token. It starts with an AstraCS: prefix, followed by a generated alphanumeric string. You can generate this token in Astra DB console, via Organization Settings &gt; Token Management &gt; Select Role &gt; Generate Token. Copy the token value, and paste it into your API call to authenticate with Astra DB resources.</p> <pre><code>// Astra DB configuration\n// replace with values from the Astra DB dashboard\nconst astra_uri = \"{astra-base-url}-{astra-region}.apps.astra.datastax.com:443\";\nconst bearer_token = \"AstraCS:xxxxxxx\";\n\n// Set up the authentication\n// For Astra DB: Enter a bearer token for Astra, downloaded from the Astra DB dashboard\nconst bearerToken = new StargateBearerToken(bearer_token);\nconst credentials = grpc.credentials.combineChannelCredentials(\n  grpc.credentials.createSsl(), bearerToken);\n\n// Uncomment if you need to check the credentials\n//console.log(credentials);\n</code></pre> <p>For a connection to a remote Stargate instance like Astra automatically generate on every call to the client:</p> <pre><code>// Create the gRPC client\n// For Astra DB: passing the credentials created above\nconst stargateClient = new StargateClient(astra_uri, credentials);\n\nconsole.log(\"made client\");\n\n// Create a promisified version of the client, so we don't need to use callbacks\nconst promisifiedClient = promisifyStargateClient(stargateClient);\n\nconsole.log(\"promisified client\")\n</code></pre>"},{"location":"pages/develop/languages/javascript/#33-astra-sdk-astrajscollections-and-astrajsrest","title":"3.3 Astra SDK (@astrajs/collections and @astrajs/rest)","text":"<p>\u2139\ufe0f Overview</p> <p>The JavaScript SDK allows you to perform standard CRUD operations on your data using Javascript.</p> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li> <p>You should Download your Secure bundle</p> </li> <li> <p>In the command-line interface associated with your development environment, paste the following and replace  with your Application Token: <pre><code>export ASTRA_DB_ID=887ff1a8-f81a-4a7a-a11b-d5379998b36e\nexport ASTRA_DB_REGION=us-east1\nexport ASTRA_DB_APPLICATION_TOKEN=&lt;app_token&gt;\n</code></pre> <li>Use printenv to ensure the environment variables were exported correctly.</li> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>A current version of node (16+) and NPM (8+)\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>npm install @astrajs/rest\nnpm install @astrajs/collections\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <p>Sample code for Document API: <pre><code>const { createClient } = require(\"@astrajs/rest\");\n\n// create an Astra DB client\nconst astraClient = await createClient({\n  astraDatabaseId: process.env.ASTRA_DB_ID,\n  astraDatabaseRegion: process.env.ASTRA_DB_REGION,\n  applicationToken: process.env.ASTRA_DB_APPLICATION_TOKEN,\n});\n\nconst basePath = \"/api/rest/v2/namespaces/app/collections/users\";\n\n// get a single user by document id\nconst { data, status } = await astraClient.get(`${basePath}/cliff@wicklow.com`);\n\n// get a subdocument by path\nconst { data, status } = await astraClient.get(\n  `${basePath}/cliff@wicklow.com/blog/comments`\n);\n\n// search a collection of documents\nconst { data, status } = await astraClient.get(basePath, {\n  params: {\n    where: {\n      name: { $eq: \"Cliff\" },\n    },\n  },\n});\n\n// create a new user without a document id\nconst { data, status } = await astraClient.post(basePath, {\n  name: \"cliff\",\n});\n\n// create a new user with a document id\nconst { data, status } = await astraClient.put(\n  `${basePath}/cliff@wicklow.com`,\n  {\n    name: \"cliff\",\n  }\n);\n\n// create a user subdocument\nconst { data, status } = await astraClient.put(\n  `${basePath}/cliff@wicklow.com/blog`,\n  {\n    title: \"new blog\",\n  }\n);\n\n// partially update user\nconst { data, status } = await astraClient.patch(\n  `${basePath}/cliff@wicklow.com`,\n  {\n    name: \"cliff\",\n  }\n);\n\n// delete a user\nconst { data, status } = await astraClient.delete(\n  `${basePath}/cliff@wicklow.com`\n);\n</code></pre></p>"},{"location":"pages/develop/languages/javascript/#4-stargate-rest-api","title":"4. Stargate REST Api","text":""},{"location":"pages/develop/languages/javascript/#41-astra-sdk","title":"4.1 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <p>The JavaScript SDK allows you to perform standard CRUD operations on your data using Javascript.</p> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure bundle</li> </ul> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>A current version of node (16+) and NPM (8+)\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>npm install @astrajs/rest\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <p>Create a REST API client <pre><code>const { createClient}= require(\"@astrajs/rest\")\nconst chalk = require('chalk')\nlet astraRestClient = null;\n\nconst requestWithRetry = async (url, client) =&gt; {\n  const MAX_RETRIES = 20;\n  for (let i = 1; i &lt;= MAX_RETRIES; i++) {\n    try {\n      let response = await client.get(url);\n      return response\n    } catch(e) {\n      const timeout = 500 * i * 10;\n      console.log(chalk.blue('         ... waiting', timeout, 'ms'));\n      await wait(timeout);\n    }\n  }\n}\n\nfunction wait(timeout) {\n    return new Promise((resolve) =&gt; {\n        setTimeout(() =&gt; {\n            resolve();\n        }, timeout);\n    });\n}\n\nconst getAstraRestClient = async () =&gt; {\n  if (astraRestClient === null) {\n    astraRestClient = await createClient(\n      {\n        astraDatabaseId: process.env.ASTRA_DB_ID,\n        astraDatabaseRegion: process.env.ASTRA_DB_REGION,\n        applicationToken: process.env.ASTRA_DB_APPLICATION_TOKEN,\n        debug: true\n      },\n      30000\n    );\n  }\n  return astraRestClient;\n};\n\nconst getRestClient = async () =&gt; {\n  if (astraRestClient === null) {\n    const astraRestClient = await getAstraRestClient();\n    await wait(1000);\n    return astraRestClient;\n  };\n  return astraRestClient;\n}\n\nmodule.exports = { getRestClient, requestWithRetry, wait, astraRestClient };\n</code></pre></p> <p>Then use that within another application: <pre><code>const { getRestClient, requestWithRetry, wait } = require(\"./utils/astraRestClient\");\n\nexports.handler = async (event, context) =&gt; {\n  const client = await getClient();\n  let res;\n  try {\n    res = await client.get('/api/rest/v2/keyspaces/todos/rest?where=\\{\"key\":\\{\"$eq\":\\\"rest\"\\}\\}')\n    const formattedTodos = Object.keys(res.data).map((item) =&gt; res.data[item]);\n    return {\n      headers: '{Content-Type: application/json}',\n      statusCode: 200,\n      body: JSON.stringify(formattedTodos),\n      headers: {\n        'Content-Type': 'application/json'\n      },\n    };\n  } catch (e) {\n    return {\n      statusCode: 400,\n      body: JSON.stringify(e),\n    };\n  }\n};\n\nasync function getClient() {\n  let client = await getRestClient();\n  if (client === null) {\n    wait(1000)\n    return getClient()\n  }\n  return client\n}\n</code></pre></p> <p>Other rest command examples: <pre><code>let path = '/api/rest/v2/keyspaces/todos/rest/' + body.id;\n    body = {\"text\":body.text, \"completed\":body.completed}\n    const res = await todos.put(path, body);\n</code></pre></p> <pre><code>const todos = await getRestClient();\n  const body = JSON.parse(event.body);\n  event.body.key = \"todo\"\n\n  const res = await todos.post('/api/rest/v2/keyspaces/todos/rest', event.body);\n</code></pre>"},{"location":"pages/develop/languages/javascript/#5-stargate-document-api","title":"5. Stargate Document Api","text":""},{"location":"pages/develop/languages/javascript/#51-astra-sdk","title":"5.1 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <p>The JavaScript SDK allows you to perform standard CRUD operations on your data using Javascript.</p> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure bundle</li> </ul> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>A current version of node (16+) and NPM (8+)\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>npm install @astrajs/collection\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <p>Sample code for Document API: <pre><code>const { createClient } = require(\"@astrajs/rest\");\n\n// create an Astra DB client\nconst astraClient = await createClient({\n  astraDatabaseId: process.env.ASTRA_DB_ID,\n  astraDatabaseRegion: process.env.ASTRA_DB_REGION,\n  applicationToken: process.env.ASTRA_DB_APPLICATION_TOKEN,\n});\n\nconst basePath = \"/api/rest/v2/namespaces/app/collections/users\";\n\n// get a single user by document id\nconst { data, status } = await astraClient.get(`${basePath}/cliff@wicklow.com`);\n\n// get a subdocument by path\nconst { data, status } = await astraClient.get(\n  `${basePath}/cliff@wicklow.com/blog/comments`\n);\n\n// search a collection of documents\nconst { data, status } = await astraClient.get(basePath, {\n  params: {\n    where: {\n      name: { $eq: \"Cliff\" },\n    },\n  },\n});\n\n// create a new user without a document id\nconst { data, status } = await astraClient.post(basePath, {\n  name: \"cliff\",\n});\n\n// create a new user with a document id\nconst { data, status } = await astraClient.put(\n  `${basePath}/cliff@wicklow.com`,\n  {\n    name: \"cliff\",\n  }\n);\n\n// create a user subdocument\nconst { data, status } = await astraClient.put(\n  `${basePath}/cliff@wicklow.com/blog`,\n  {\n    title: \"new blog\",\n  }\n);\n\n// partially update user\nconst { data, status } = await astraClient.patch(\n  `${basePath}/cliff@wicklow.com`,\n  {\n    name: \"cliff\",\n  }\n);\n\n// delete a user\nconst { data, status } = await astraClient.delete(\n  `${basePath}/cliff@wicklow.com`\n);\n</code></pre></p>"},{"location":"pages/develop/languages/javascript/#6-stargate-graphql","title":"6 Stargate GraphQL","text":""},{"location":"pages/develop/languages/javascript/#61-cql-first","title":"6.1 CQL First","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/javascript/#62-graphql-first","title":"6.2 GraphQL First","text":"<p>\u2139\ufe0f Overview</p> <p>\u2139\ufe0f Overview</p> <p>The JavaScript SDK allows you to perform standard CRUD operations on your data using Javascript.</p> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure bundle</li> </ul> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>A current version of node (16+) and NPM (8+)\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>npm install @astrajs/rest\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>const { getRestClient, requestWithRetry, wait, astraRestClient } = require(\"./utils/astraRestClient\");\n\n  let query = `mutation updategraphql {\n    graphql: updategraphql(value: {\n      id: \"${body.id}\",\n      completed: ${body.completed},\n      text: \"${body.text}\",\n      key: \"graphql\"\n  }) {value { text } }}`\n  let res = await client.post('/api/graphql/todos',\n    {query: query})\n</code></pre> <pre><code>let query = `query GQTodos {\n    graphql (value: {key:\"graphql\"}) {\n      values {\n        id\n        text\n        completed\n        key\n      }\n    }}`\n\nres = await client.post('/api/graphql/todos', query={query})\n</code></pre>"},{"location":"pages/develop/languages/javascript/#7-stargate-grpc","title":"7. Stargate gRPC","text":""},{"location":"pages/develop/languages/javascript/#71-stargate-client","title":"7.1 Stargate Client","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/javascript/#72-astra-sdk","title":"7.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/javascript/#8-pulsar-client","title":"8. Pulsar Client","text":""},{"location":"pages/develop/languages/javascript/#81-pulsar-client","title":"8.1 Pulsar Client","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/javascript/#82-astra-sdk","title":"8.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/javascript/#9-pulsar-admin","title":"9. Pulsar Admin","text":""},{"location":"pages/develop/languages/javascript/#10-devops-api-database","title":"10 Devops API Database","text":""},{"location":"pages/develop/languages/javascript/#11-devops-api-organization","title":"11 Devops API Organization","text":""},{"location":"pages/develop/languages/javascript/#12-devops-api-streaming","title":"12 Devops API Streaming","text":""},{"location":"pages/develop/languages/python/","title":"\u2022 Python","text":""},{"location":"pages/develop/languages/python/#1-overview","title":"1. Overview","text":"<p>Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces. There are different frameworks and tools to connect to Astra depending on the Api interface you choose.</p> <p>Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here.</p>"},{"location":"pages/develop/languages/python/#2-interfaces-list","title":"2. Interfaces List","text":"Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming"},{"location":"pages/develop/languages/python/#3-cql","title":"3. CQL","text":""},{"location":"pages/develop/languages/python/#31-cassandra-drivers","title":"3.1 Cassandra Drivers","text":"<p>\u2139\ufe0f Overview</p> <p>These instructions are aimed at helping people connect to Astra DB programmatically using the DataStax Python driver.</p> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure bundle</li> </ul> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <p>You will need a recent version of Python 3.  Visit https://www.python.org/downloads/ for more information on downloads and installation instructions for your machine architecture.  To verify your Python install, run the following command:</p> <pre><code>python -V\n</code></pre> <p>With Python installed locally, you can now use Pip (Python's package manager) to install the DataStax Python driver.</p> <pre><code>pip install cassandra-driver\n</code></pre> <p>You can verify that the DataStax Python driver was installed successfully with this command:</p> <pre><code>python -c 'import cassandra; print (cassandra.__version__)'\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <p>Create a new file and/or directory for your Python program.</p> <pre><code>mkdir python_project\ncd python_project\ntouch testAstra.py\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <p>To connect to an Astra DB cluster, you will need a secure token generated specifically for use with your Astra DB cluster.</p> <pre><code>mkdir ~/mySecureBundleDir\ncd ~/mySecureBundleDir\nmv ~/Downloads/secure-connect-bundle.zip .\n</code></pre> <p>Open up your favorite editor or IDE, and add 3 imports:</p> <pre><code>from cassandra.cluster import Cluster\nfrom cassandra.auth import PlainTextAuthProvider\nimport sys\n</code></pre> <p>Next we will inject the connection parameters into the code.  This can be done either by reading them as environment variables or passing them as command line arguments.</p> <p>This example will be done using command line arguments:</p> <pre><code>clientID=sys.argv[1]\nsecret=sys.argv[2]\nsecureBundleLocation=sys.argv[3]\n</code></pre> <p>We'll also define the location of our secure connect bundle, and set that as a property in our <code>cloud_config</code>:</p> <pre><code>cloud_config= {\n'secure_connect_bundle': secureBundleLocation\n}\n</code></pre> <p>Next, we'll define our authenticator and pass our credentials to it.</p> <pre><code>auth_provider = PlainTextAuthProvider(clientID, secret)\n</code></pre> <p>With all of that defined, we can build a cluster object and a connection:</p> <pre><code>cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\nsession = cluster.connect()\n</code></pre> <p>With a connection made, we can run a simple query to return the name of the cluster from the <code>system.local</code> table:</p> <pre><code>row = session.execute(\"select cluster_name from system.local\").one()\nif row:\nprint(row[0])\nelse:\nprint(\"An error occurred.\")\n</code></pre> <p>Running this code with arguments in the proper order should yield output similar to this:</p> <pre><code>python testAstra.py token \"AstraCS:ASjPlHbTYourSecureTokenGoesHered3cdab53b\" /Users/aaronploetz/mySecureBundleDir/secure-connect-bundle.zip\n\ncndb\n</code></pre> <p>The complete code to this example can be found here.</p>"},{"location":"pages/develop/languages/python/#32-astra-sdk","title":"3.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/python/#4-stargate-rest-api","title":"4. Stargate REST Api","text":""},{"location":"pages/develop/languages/python/#41-axios","title":"4.1 Axios","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/python/#42-astra-sdk","title":"4.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/python/#5-stargate-document-api","title":"5. Stargate Document Api","text":""},{"location":"pages/develop/languages/python/#51-axios","title":"5.1 Axios","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/python/#52-astra-sdk","title":"5.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/python/#6-stargate-graphql","title":"6 Stargate GraphQL","text":""},{"location":"pages/develop/languages/python/#61-cql-first","title":"6.1 CQL First","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/python/#62-graphql-first","title":"6.2 GraphQL First","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/python/#7-stargate-grpc","title":"7. Stargate gRPC","text":""},{"location":"pages/develop/languages/python/#71-stargate-client","title":"7.1 Stargate Client","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/python/#72-astra-sdk","title":"7.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/python/#8-pulsar-client","title":"8. Pulsar Client","text":""},{"location":"pages/develop/languages/python/#81-pulsar-client","title":"8.1 Pulsar Client","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/python/#82-astra-sdk","title":"8.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/python/#9-pulsar-admin","title":"9. Pulsar Admin","text":""},{"location":"pages/develop/languages/python/#10-devops-api-database","title":"10 Devops API Database","text":""},{"location":"pages/develop/languages/python/#11-devops-api-organization","title":"11 Devops API Organization","text":""},{"location":"pages/develop/languages/python/#12-devops-api-streaming","title":"12 Devops API Streaming","text":""},{"location":"pages/develop/languages/rust/","title":"\u2022 Rust","text":""},{"location":"pages/develop/languages/rust/#1-overview","title":"1. Overview","text":"<p>Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces. There are different frameworks and tools to connect to Astra depending on the Api interface you choose.</p> <p>Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here.</p>"},{"location":"pages/develop/languages/rust/#2-interfaces-list","title":"2. Interfaces List","text":"Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming"},{"location":"pages/develop/languages/rust/#3-cql","title":"3. CQL","text":""},{"location":"pages/develop/languages/rust/#31-cassandra-drivers","title":"3.1 Cassandra Drivers","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/rust/#32-astra-sdk","title":"3.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/rust/#4-stargate-rest-api","title":"4. Stargate REST Api","text":""},{"location":"pages/develop/languages/rust/#41-axios","title":"4.1 Axios","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/rust/#42-astra-sdk","title":"4.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/rust/#5-stargate-document-api","title":"5. Stargate Document Api","text":""},{"location":"pages/develop/languages/rust/#51-axios","title":"5.1 Axios","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/rust/#52-astra-sdk","title":"5.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/rust/#6-stargate-graphql","title":"6 Stargate GraphQL","text":""},{"location":"pages/develop/languages/rust/#61-cql-first","title":"6.1 CQL First","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/rust/#62-graphql-first","title":"6.2 GraphQL First","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/rust/#7-stargate-grpc","title":"7. Stargate gRPC","text":""},{"location":"pages/develop/languages/rust/#71-stargate-client","title":"7.1 Stargate Client","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/rust/#72-astra-sdk","title":"7.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/rust/#8-pulsar-client","title":"8. Pulsar Client","text":""},{"location":"pages/develop/languages/rust/#81-pulsar-client","title":"8.1 Pulsar Client","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/rust/#82-astra-sdk","title":"8.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/rust/#9-pulsar-admin","title":"9. Pulsar Admin","text":""},{"location":"pages/develop/languages/rust/#10-devops-api-database","title":"10 Devops API Database","text":""},{"location":"pages/develop/languages/rust/#11-devops-api-organization","title":"11 Devops API Organization","text":""},{"location":"pages/develop/languages/rust/#12-devops-api-streaming","title":"12 Devops API Streaming","text":""},{"location":"pages/develop/languages/scala/","title":"\u2022 Scala","text":""},{"location":"pages/develop/languages/scala/#1-overview","title":"1. Overview","text":"<p>Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces. There are different frameworks and tools to connect to Astra depending on the Api interface you choose.</p> <p>Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here.</p>"},{"location":"pages/develop/languages/scala/#2-interfaces-list","title":"2. Interfaces List","text":"Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming"},{"location":"pages/develop/languages/scala/#3-cql","title":"3. CQL","text":""},{"location":"pages/develop/languages/scala/#31-cassandra-drivers","title":"3.1 Cassandra Drivers","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/scala/#32-astra-sdk","title":"3.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/scala/#4-stargate-rest-api","title":"4. Stargate REST Api","text":""},{"location":"pages/develop/languages/scala/#41-axios","title":"4.1 Axios","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/scala/#42-astra-sdk","title":"4.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/scala/#5-stargate-document-api","title":"5. Stargate Document Api","text":""},{"location":"pages/develop/languages/scala/#51-axios","title":"5.1 Axios","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/scala/#52-astra-sdk","title":"5.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/scala/#6-stargate-graphql","title":"6 Stargate GraphQL","text":""},{"location":"pages/develop/languages/scala/#61-cql-first","title":"6.1 CQL First","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/scala/#62-graphql-first","title":"6.2 GraphQL First","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/scala/#7-stargate-grpc","title":"7. Stargate gRPC","text":""},{"location":"pages/develop/languages/scala/#71-stargate-client","title":"7.1 Stargate Client","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/scala/#72-astra-sdk","title":"7.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/scala/#8-pulsar-client","title":"8. Pulsar Client","text":""},{"location":"pages/develop/languages/scala/#81-pulsar-client","title":"8.1 Pulsar Client","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/scala/#82-astra-sdk","title":"8.2 Astra SDK","text":"<p>\u2139\ufe0f Overview</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [ASTRA]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Prerequisites [Development Environment]</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udce6 Setup Project</p> <pre><code>TODO\n</code></pre> <p>\ud83d\udda5\ufe0f Sample Code</p> <pre><code>TODO\n</code></pre>"},{"location":"pages/develop/languages/scala/#9-pulsar-admin","title":"9. Pulsar Admin","text":""},{"location":"pages/develop/languages/scala/#10-devops-api-database","title":"10 Devops API Database","text":""},{"location":"pages/develop/languages/scala/#11-devops-api-organization","title":"11 Devops API Organization","text":""},{"location":"pages/develop/languages/scala/#12-devops-api-streaming","title":"12 Devops API Streaming","text":""},{"location":"pages/develop/platform/aws-gamesparks/","title":"AWS GameSparks","text":"<p>Notice</p> <p>This tutorial is currently in Beta. Feel free to contact us if you run into any issues or need additional help with completing the tutorial.</p>"},{"location":"pages/develop/platform/aws-gamesparks/#aws-gamesparks","title":"AWS GameSparks","text":""},{"location":"pages/develop/platform/aws-gamesparks/#overview","title":"Overview","text":"<p>In this tutorial, we will be creating a simple app that connects to Astra Block using AWS GameSparks, AWS Lambda, and Unity. </p> <p></p> <p>AWS GameSparks is a fully managed AWS service that provides a multi-service backend for game developers. GameSparks works together with AWS Lambda which connects directly to our Astra DB Database and Astra Block.</p>"},{"location":"pages/develop/platform/aws-gamesparks/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>Request for access to Astra Block.</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle</li> <li>Have Unity Hub installed.</li> <li>Have 2020.3 Unity version installed (This project uses 3.4.1).</li> </ul>"},{"location":"pages/develop/platform/aws-gamesparks/#step-1-aws-lambda","title":"Step 1: AWS Lambda","text":"<p>This section of the tutorial will be referencing a majority of the AWS Lambda tutorial with some minor adjustments. </p> <ol> <li> <p>First, choose one of the four options available: Python Driver, Python SDK, Java Driver, Java gRPC. This tutorial uses Python Driver. </p> </li> <li> <p>In Step 2 of the AWS Lambda integration tutorial, you are asked to create file <code>lambda_function.py</code> with the function source code. Use the following code for the <code>lambda_handler</code> function. <pre><code>def lambda_handler(event, context):\nnum1 = event['num1']\nnum2 = event['num2']\nrow = session.execute(\"select * from krypton_dev.sorted_nfts where block_number_hour=\" + str(num1) + \" and block_number=\" + str(num2) + \" limit 1;\").one()\nprint(type(row[5]))\nreturn {\n\"result\": row[5]\n}\n</code></pre> This queries the <code>sorted_nfts</code> table then returns the name of the NFT at the given <code>block_number</code> and <code>block_number_hour</code>.</p> </li> <li> <p>Create the deployment package and AWS Lambda function as stated in the tutorial. </p> </li> <li> <p>Test the function by navigating to the Test tab. Scroll down to the Event JSON section and update the values with the following values: <pre><code>{\n\"num1\": \"41447\",\n\"num2\": \"14920967\"\n}\n</code></pre></p> </li> <li>Click the Test button and observe the output. It should have returned the NFT name <code>Banksy Culture</code>.  </li> </ol>"},{"location":"pages/develop/platform/aws-gamesparks/#step-2-aws-gamesparks","title":"Step 2: AWS GameSparks","text":"<p>This section of the tutorial will focus on setting up AWS GameSparks as a backend and connect it to the AWS Lambda function that you just created.</p>"},{"location":"pages/develop/platform/aws-gamesparks/#create-a-game-backend","title":"Create a game backend","text":"<ol> <li> <p>Firstly, open the Amazon GameSparks Console and click on Create game to create a game and start developing your game backend: </p> </li> <li> <p>Set a name for your Game and click Create  You have now created your AWS GameSparks backend ready to be configured to your game and AWS Lambda. </p> </li> </ol>"},{"location":"pages/develop/platform/aws-gamesparks/#deployment","title":"Deployment","text":"<p>Prior to any major development, we are first going to deploy a fresh new snapshot of the game backend. A snapshot allows you to store data representing a certain point of progress in a game.  </p> <ol> <li> <p>From the Dev page, click Deploy as new snapshot.  </p> </li> <li> <p>Here, you can enter an optional description and click Save to continue.  </p> </li> <li> <p>In a few minutes, you should see that the snapshot and game backend has now been deployed. Note: Everytime a new feature is added to the game, a new snapshot will also have to be deployed.   </p> </li> </ol>"},{"location":"pages/develop/platform/aws-gamesparks/#give-your-gamesparks-backend-permission-to-access-lambda","title":"Give your GameSparks backend permission to access Lambda","text":"<p>Now that the game backend is deployed, we must give it permission to access the AWS Lambda function that we set up previously in Step 1. This will allow the two entities to communicate with each other, send/receive requests to the game backend, and more. Firstly, we will have to grant permissions by creating an IAM Policy. </p>"},{"location":"pages/develop/platform/aws-gamesparks/#create-policy","title":"Create Policy","text":"<ol> <li> <p>Go to the IAM Create policy page and click Import managed policy. This will allow you import a role with specific permissions to Lambda.  </p> </li> <li> <p>Filter the policies by searching for LambdaRole in the search bar. Choose AWSLambdaRole. Then select Import. </p> </li> <li> <p>AWSLambdaRole policy will allow you to invoke any Lambda function that we deploy. However, you also have the option to specify a specific function that you would like to invoke. Do this by going to the JSON tab, and navigating to where it says <code>\"Resource\"</code>. Replace the <code>\"*\"</code> with the ARN from your Lambda Function to invoke a specific function. For this tutorial, you can leave it as <code>\"*\"</code>.  </p> </li> <li> <p>Choose Next:Tags then Next:Review. </p> </li> <li> <p>In Review, give your policy a name such as <code>AllowLambdaInvokeAll</code> and finish by choosing Create Policy. </p> </li> </ol>"},{"location":"pages/develop/platform/aws-gamesparks/#attach-policy","title":"Attach policy","text":"<p>Next, we are going to the attach the policy we just created to our GameSparks backend IAM role.</p> <ol> <li> <p>Go back to your Amazon GameSparks Console, select your game, and go to the Dev section in your navigation bar. Under Dev, go to Configuration and choose \"View in IAM console\". </p> </li> <li> <p>The IAM console opens to the IAM role for your Dev stage. On the Permissions tab, choose Add permissions and Attach policies: </p> </li> <li> <p>Filter for the policy name you created, select it, and press Add permissions.  </p> </li> <li> <p>Great! You have now given your AWS GameSparks backend permission to call to your AWS Lambda Function.  </p> </li> </ol>"},{"location":"pages/develop/platform/aws-gamesparks/#step-3-connecting-your-gamesparks-backend-to-lambda","title":"Step 3: Connecting your GameSparks backend to Lambda","text":"How This Works? <p>To invoke Lambda functions from Amazon GameSparks you need to create a Message inside the game backend. There are 3 types of messages: Events, Requests, and Notifications. Once created, you can call a Lambda function from there. Learn more here.</p> <p>For this example, we will be using Request so that our front end client (Unity) can get data from our Amazon GameSparks backend. This request will internally call the Lambda function we previously set up and return a response. </p>"},{"location":"pages/develop/platform/aws-gamesparks/#create-getsortednft-request","title":"Create GetSortedNFT request","text":"<ol> <li> <p>Return to your GameSparks backend, and select Cloud code from the navigation bar. Once you are there, select Create Message.  </p> </li> <li> <p>Select Request and give it a name. For this example, we will name it GetSortedNFT based on our Lambda function. Then click, Create </p> </li> <li> <p>Here, you will configure the Request fields. Recall the AWS Lambda function that we created. There were 2 input fields that were needed to retrieve a response: <pre><code>{\n\"num1\": \"41447\",\n\"num2\": \"14920967\"\n}\n</code></pre> You will configure a Request field for each input value needed in the Lambda function and a Response field. We will call these fields <code>input1</code> and <code>input2</code>.   Make sure to also take note of the Shape so that it matches the type of your Response field.</p> </li> <li> <p>Update the Request handle code with the following: <pre><code>GameSparks().Logging().Debug(\"In lambdaAstraDBTest request handler\");\n\nconst response = GameSparks().Lambda(\"lambdaAstraDBTest\").Invoke(\n    {\n        \"num1\": message.input1,\n        \"num2\": message.input2\n    }\n);\n\nGameSparks().Logging().Debug(\"Result from Lambda is:\");\nGameSparks().Logging().Debug(JSON.stringify(response.Payload));\n\nreturn GameSparks().Messaging().Response({\"result\": JSON.stringify(response.Payload.result)});\n</code></pre> Then click Save.</p> </li> <li> <p>Test your Request in the Cloud Console by clicking Test.  </p> </li> <li> <p>Make sure your Player is connected and that you deploy a new snapshot by clicking the banner. You should always deploy a new snapshot between changes.</p> </li> <li> <p>Once Step 6 is done, select Populate example which will allow you to give input to the Request Body. Here, you can populate values <code>input1</code> and <code>input2</code> with the same values you gave when you set up your AWS Lambda function. <code>input1</code> being the <code>block_number_hour</code> and <code>input2</code> being the <code>block_number</code> from your <code>sorted_nfts</code> table. Remember to use valid block_number_hour and block_number values.</p> </li> </ol> <p><pre><code>{\n\"input1\": 41447,\n\"input2\": 14920967\n}\n</code></pre> 8. Finally, click Send message to send this request to the backend. You should see in the Log inspector your request being sent and received.  9. Congrats! You have now sent a request and received a response using AWS GameSparks and AWS Lambda. </p>"},{"location":"pages/develop/platform/aws-gamesparks/#step-4-connecting-unity-to-gamesparks-backend","title":"Step 4: Connecting Unity to GameSparks backend","text":"<p>Now you are ready to connect your backend to our Unity front end. First, download the Unity sample project we have prepared from our Awesome Astra repo. </p> <p>Note</p> <p>The sample project already includes the Amazon GameSparks SDK installed, or you can follow the instructions here.</p>"},{"location":"pages/develop/platform/aws-gamesparks/#setting-up","title":"Setting Up","text":"<ol> <li> <p>Open the project with Unity Hub </p> </li> <li> <p>On the Project tab go to Assets -&gt; Amazon -&gt; GameSparks and choose the Connection.asset </p> </li> <li> <p>In the Insepctor tab on the right, you will see the Amazon GameSparks connection settings. As you can see currently, Game Key is blank. You will obtain this from your GameSparks Console -&gt; Dev -&gt; Dev stage configuration -&gt; Under Key </p> </li> <li> <p>Copy this value and paste it into Game Key back in Unity. </p> </li> </ol> <p>Great! Setup is now complete! </p>"},{"location":"pages/develop/platform/aws-gamesparks/#running-the-game","title":"Running the game","text":"<ol> <li> <p>To run the game, go back to your Project tab -&gt; Assets -&gt; Scenes -&gt; Select the AstraBlockGameSparksDemo Scene. This is the scene that we are going to play. </p> </li> <li> <p>Click the \"Play\" button at the top of the screen. You should see a notification saying the scripts are rendering.  </p> </li> <li> <p>Once that is complete, you can type directly into the input box where it says Enter Block Number.... Once you click submit, you should see the Block Number populate, and shortly after, the NFT Title will return the name of the NFT at that given block number.  </p> </li> <li> <p>Try submitting different Block Numbers to see the different results you get back for your NFTs!</p> </li> </ol>"},{"location":"pages/develop/platform/aws-gamesparks/#finish","title":"Finish","text":"<p>Congratulations! You have completed the Astra Block with AWS GameSparks, AWS Lambda, and Unity tutorial! This is only the beginning as you can connect your AWS Lambda to any table given within Astra Block, query for different values, etc. </p>"},{"location":"pages/develop/platform/aws-lambda-function/","title":"AWS Lambda Functions","text":""},{"location":"pages/develop/platform/aws-lambda-function/#aws-lambda-functions","title":"AWS Lambda Functions","text":""},{"location":"pages/develop/platform/aws-lambda-function/#overview","title":"Overview","text":"<p>AWS Lambda is AWS' function-as-a-service offering that provides a serverless execution environment for your code. AWS Lambda functions are commonly used to:</p> <ul> <li>Extend Astra DB with additional data processing capabilities, such as aggregating, summarizing and validating data periodically;</li> <li>Connect Astra DB with other cloud services into data pipelines that move, process and analyze data.</li> </ul>"},{"location":"pages/develop/platform/aws-lambda-function/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle</li> <li>Optionally, if you are new to AWS Lambda, practice creating a simpler function first.</li> </ul>"},{"location":"pages/develop/platform/aws-lambda-function/#using-python-driver","title":"Using Python Driver","text":""},{"location":"pages/develop/platform/aws-lambda-function/#1-create-a-deployment-package","title":"\u2705 1.  Create a deployment package.","text":"<p>A deployment package is a <code>.zip</code> file with a function source code and dependencies. To access Astra DB from a function using Python Driver, we must add cassandra-driver, a Python client library for Apache Cassandra, DataStax Astra DB and DataStax Enterprise, as a dependency. In addition, as part of the deployment package, we need to include a secure connect bundle for a database in Astra DB that we want to query.</p> <ol> <li> <p>Open a command prompt and create a project directory: <pre><code>mkdir lambda-astra-db-project\ncd lambda-astra-db-project\n</code></pre></p> </li> <li> <p>Create file <code>lambda_function.py</code> with the function source code: <pre><code>from cassandra.cluster import Cluster\nfrom cassandra.auth import PlainTextAuthProvider\nimport os\nASTRA_DB_CLIENT_ID = os.environ.get('ASTRA_DB_CLIENT_ID')\nASTRA_DB_CLIENT_SECRET = os.environ.get('ASTRA_DB_CLIENT_SECRET')\ncloud_config= {\n'secure_connect_bundle': 'secure-connect-bundle-for-your-database.zip',\n'use_default_tempdir': True\n}\nauth_provider = PlainTextAuthProvider(ASTRA_DB_CLIENT_ID, ASTRA_DB_CLIENT_SECRET)\ncluster = Cluster(cloud=cloud_config, auth_provider=auth_provider, protocol_version=4)\nsession = cluster.connect()\ndef lambda_handler(event, context):\nrow = session.execute(\"SELECT cql_version FROM system.local WHERE key = 'local';\").one()\ncql_version = row[0]\nprint(cql_version) \nprint('Success')\nreturn cql_version\n</code></pre> You can learn more about the code above by reading the cassandra-driver documentation.</p> </li> <li> <p>Install the cassandra-driver library: <pre><code>pip install --target . cassandra-driver\n</code></pre></p> </li> <li> <p>Download the Secure Connect Bundle for your database and copy it into the project directory.</p> </li> <li> <p>Create a deployment package with <code>lambda_function.py</code>, <code>cassandra-driver</code>, and secure connect bundle: <pre><code>zip -r lambda-astra-db-deployment-package.zip .\n</code></pre></p> </li> </ol>"},{"location":"pages/develop/platform/aws-lambda-function/#2-create-a-function","title":"\u2705 2.  Create a function.","text":"<ol> <li> <p>Go to the Functions page of the Lambda console and click Create function. </p> </li> <li> <p>Choose Author from scratch.</p> </li> <li> <p>Under the Basic information section, specify preferred Function name, Runtime, and Architecture. </p> </li> <li> <p>Click Create function.</p> </li> <li> <p>Under the Code tab and the Code source section, select Upload from and upload the deployment package created in the previous steps.   Since the deployment package exceeds 3 MBs, the Console Editor may not be available to view the source code: </p> </li> <li> <p>Under the Configuration tab, select and create these Environment variables:</p> <ul> <li><code>ASTRA_DB_CLIENT_ID</code>: A Client ID is generated together with an application token (see the Prerequisites section above).</li> <li><code>ASTRA_DB_CLIENT_SECRET</code>: A Client secret is generated together with an application token (see the Prerequisites section above).  Note that, for better security, you can alternatively use the AWS Secret Manager service to store and manage client id and secret, and then retrieve them programmatically. </li> </ul> </li> <li> <p>Optionally, to optimize function performance, consider configuring reserved and provisioned concurrency under the Configuration tab.</p> </li> </ol>"},{"location":"pages/develop/platform/aws-lambda-function/#3-test-the-function","title":"\u2705 3.  Test the function.","text":"<p>Under the Test tab, click the Test button and observe the output.  Notice the CQL version output and return value of 3.4.5.</p>"},{"location":"pages/develop/platform/aws-lambda-function/#using-python-sdk","title":"Using Python SDK","text":""},{"location":"pages/develop/platform/aws-lambda-function/#1-create-a-deployment-package_1","title":"\u2705 1.  Create a deployment package.","text":"<p>A deployment package is a <code>.zip</code> file with a function source code and dependencies. To access Astra DB from a function using REST API, we must add AstraPy, a Pythonic SDK for DataStax Astra and Stargate, as a dependency.</p> <ol> <li> <p>Open a command prompt and create a project directory: <pre><code>mkdir lambda-astra-db-project\ncd lambda-astra-db-project\n</code></pre></p> </li> <li> <p>Create file <code>lambda_function.py</code> with the function source code: <pre><code>from astrapy.rest import create_client, http_methods\nimport os\nASTRA_DB_ID = os.environ.get('ASTRA_DB_ID')\nASTRA_DB_REGION = os.environ.get('ASTRA_DB_REGION')\nASTRA_DB_APPLICATION_TOKEN = os.environ.get('ASTRA_DB_APPLICATION_TOKEN')\nastra_http_client = create_client(astra_database_id=ASTRA_DB_ID,\nastra_database_region=ASTRA_DB_REGION,\nastra_application_token=ASTRA_DB_APPLICATION_TOKEN)\ndef lambda_handler(event, context):\nres = astra_http_client.request(\nmethod=http_methods.GET,\npath=f\"/api/rest/v2/keyspaces/system/local/local\"\n)\ncql_version = res[\"data\"][0]['cql_version']\nprint(cql_version) \nprint('Success')\nreturn cql_version\n</code></pre> You can learn more about the code above by reading the AstraPy documentation.</p> </li> <li> <p>Install the AstraPy library: <pre><code>pip install --target . astrapy\n</code></pre></p> </li> <li> <p>Create a deployment package with <code>lambda_function.py</code> and <code>astrapy</code>: <pre><code>zip -r lambda-astra-db-deployment-package.zip .\n</code></pre></p> </li> </ol>"},{"location":"pages/develop/platform/aws-lambda-function/#2-create-a-function_1","title":"\u2705 2.  Create a function.","text":"<ol> <li> <p>Go to the Functions page of the Lambda console and click Create function. </p> </li> <li> <p>Choose Author from scratch.</p> </li> <li> <p>Under the Basic information section, specify preferred Function name, Runtime, and Architecture. </p> </li> <li> <p>Click Create function.</p> </li> <li> <p>Under the Code tab and the Code source section, select Upload from and upload the deployment package created in the previous steps.  </p> </li> <li> <p>Verify that the uploaded function has the correct <code>lambda_function.py</code> and dependencies:  You can learn more about the code above by reading the AstraPy documentation.</p> </li> <li> <p>Click Deploy to deploy the function.</p> </li> <li> <p>Under the Configuration tab, select and create these Environment variables:</p> <ul> <li><code>ASTRA_DB_ID</code>: A Database ID value can be found on the Astra DB dashboard.</li> <li><code>ASTRA_DB_REGION</code>: A Region name can be found on the overview page for a specific Astra DB database.</li> <li><code>ASTRA_DB_APPLICATION_TOKEN</code>: An Application Token can be generated for a specific Astra DB database (see the Prerequisites section above).  Note that, for better security, you can alternatively use the AWS Secret Manager service to store and manage an application token as a secret. A secret can then be retrieved programmatically. </li> </ul> </li> <li> <p>Optionally, to optimize function performance, consider configuring reserved and provisioned concurrency under the Configuration tab.</p> </li> </ol>"},{"location":"pages/develop/platform/aws-lambda-function/#3-test-the-function_1","title":"\u2705 3.  Test the function.","text":"<p>Under the Test tab, click the Test button and observe the output.  Notice the CQL version output and return value of 3.4.5.</p>"},{"location":"pages/develop/platform/aws-lambda-function/#using-java-driver","title":"Using Java Driver","text":""},{"location":"pages/develop/platform/aws-lambda-function/#1-create-a-deployment-package_2","title":"\u2705 1.  Create a deployment package.","text":"<p>A deployment package is a <code>.zip</code> or <code>.jar</code> file archive with compiled function code and dependencies. In this tutorial, we use Apache Maven\u2122 to create, compile and package a function into a <code>.jar</code> file. We need to include the following pieces into a deployment package to access Astra DB from an AWS Lambda function: a) aws-lambda-java-core that defines necessary interfaces and classes to create functions; b) java-driver that enables connectivity to Apache Cassandra, DataStax Astra DB and DataStax Enterprise; and c) secure connect bundle for a database in Astra DB that we want to query.</p> <ol> <li> <p>Open a command prompt and create a new project using Apache Maven\u2122: <pre><code>mvn archetype:generate -DgroupId=com.example -DartifactId=AstraDBFunction -DinteractiveMode=false\n</code></pre></p> </li> <li> <p>Rename file <code>App.java</code> to <code>AstraDBFunction.java</code> and replace its content with the function source code: <pre><code>package com.example;\nimport com.amazonaws.services.lambda.runtime.Context;\nimport com.amazonaws.services.lambda.runtime.RequestHandler;\nimport com.amazonaws.services.lambda.runtime.LambdaLogger;\nimport com.datastax.oss.driver.api.core.CqlSession;\nimport com.datastax.oss.driver.api.core.cql.ResultSet;\nimport com.datastax.oss.driver.api.core.cql.Row;\nimport java.nio.file.Paths;\nimport java.util.Map;\npublic class AstraDBFunction implements RequestHandler&lt;Map&lt;String,String&gt;, String&gt;{\nprivate static final String ASTRA_DB_CLIENT_ID = System.getenv(\"ASTRA_DB_CLIENT_ID\");\nprivate static final String ASTRA_DB_CLIENT_SECRET = System.getenv(\"ASTRA_DB_CLIENT_SECRET\");\nprivate static CqlSession session = CqlSession.builder()\n.withCloudSecureConnectBundle(Paths.get(\"secure-connect-bundle-for-your-database.zip\"))\n.withAuthCredentials(ASTRA_DB_CLIENT_ID,ASTRA_DB_CLIENT_SECRET)\n.build();\npublic String handleRequest(Map&lt;String,String&gt; event, Context context)\n{\nLambdaLogger logger = context.getLogger();\nResultSet rs = session.execute(\"SELECT cql_version FROM system.local WHERE key = 'local';\");\nRow row = rs.one();\nString response = row.getString(\"cql_version\");\nlogger.log(response + \" Success \\n\"); return response;\n}  }\n</code></pre> You can learn more about the code above by reading the java-driver documentation.</p> </li> <li> <p>In the project directory, under <code>/src/main</code>, create directory <code>resources</code>.</p> </li> <li> <p>Download the Secure Connect Bundle for your database and copy it into the <code>resources</code> directory. The project directory structure should look like this: </p> </li> <li> <p>Add AWS Lambda and Java Driver dependencies to the <code>pom.xml</code> file: <pre><code>    &lt;dependency&gt;\n&lt;groupId&gt;com.amazonaws&lt;/groupId&gt;\n&lt;artifactId&gt;aws-lambda-java-core&lt;/artifactId&gt;\n&lt;!-- Use the latest version from https://central.sonatype.dev/artifact/com.amazonaws/aws-lambda-java-core/1.2.2/versions --&gt;\n&lt;version&gt;1.2.2&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.amazonaws&lt;/groupId&gt;\n&lt;artifactId&gt;aws-lambda-java-events&lt;/artifactId&gt;\n&lt;!-- Use the latest version from https://central.sonatype.dev/artifact/com.amazonaws/aws-lambda-java-events/3.11.0/versions --&gt;\n&lt;version&gt;3.11.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.amazonaws&lt;/groupId&gt;\n&lt;artifactId&gt;aws-lambda-java-log4j2&lt;/artifactId&gt;\n&lt;!-- Use the latest version from https://central.sonatype.dev/artifact/com.amazonaws/aws-lambda-java-log4j2/1.5.1/versions --&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.oss&lt;/groupId&gt;\n&lt;artifactId&gt;java-driver-core&lt;/artifactId&gt;\n&lt;!--Use the latest version from https://search.maven.org/artifact/com.datastax.oss/java-driver-core --&gt;\n&lt;version&gt;4.15.0&lt;/version&gt;\n&lt;/dependency&gt;  </code></pre></p> </li> <li> <p>Add or replace an existing <code>build</code> section in the <code>pom.xml</code> file with the following: <pre><code>  &lt;build&gt;\n&lt;plugins&gt;\n&lt;plugin&gt;\n&lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;\n&lt;version&gt;2.22.2&lt;/version&gt;\n&lt;/plugin&gt;\n&lt;plugin&gt;\n&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n&lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n&lt;version&gt;3.2.2&lt;/version&gt;\n&lt;configuration&gt;\n&lt;createDependencyReducedPom&gt;false&lt;/createDependencyReducedPom&gt;\n&lt;filters&gt;\n&lt;filter&gt;\n&lt;artifact&gt;*:*&lt;/artifact&gt;\n&lt;excludes&gt;\n&lt;exclude&gt;**/Log4j2Plugins.dat&lt;/exclude&gt;\n&lt;/excludes&gt;\n&lt;/filter&gt;\n&lt;/filters&gt;\n&lt;/configuration&gt;\n&lt;executions&gt;\n&lt;execution&gt;\n&lt;phase&gt;package&lt;/phase&gt;\n&lt;goals&gt;\n&lt;goal&gt;shade&lt;/goal&gt;\n&lt;/goals&gt;\n&lt;/execution&gt;\n&lt;/executions&gt;\n&lt;/plugin&gt;\n&lt;plugin&gt;\n&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n&lt;version&gt;3.8.1&lt;/version&gt;\n&lt;configuration&gt;\n&lt;source&gt;11&lt;/source&gt;\n&lt;target&gt;11&lt;/target&gt;\n&lt;/configuration&gt;\n&lt;/plugin&gt;\n&lt;/plugins&gt;\n&lt;/build&gt;\n</code></pre></p> </li> <li> <p>Run the Maven command in the project directory to compile code and create a <code>.jar</code> file: <pre><code> mvn clean compile package\n</code></pre> Find the deployment package file <code>AstraDBFunction-1.0-SNAPSHOT.jar</code> under the <code>target</code> directory: </p> </li> </ol>"},{"location":"pages/develop/platform/aws-lambda-function/#2-create-a-function_2","title":"\u2705 2.  Create a function.","text":"<ol> <li> <p>Go to the Functions page of the Lambda console and click Create function. </p> </li> <li> <p>Choose Author from scratch.</p> </li> <li> <p>Under the Basic information section, specify preferred Function name, Runtime, and Architecture. </p> </li> <li> <p>Click Create function.</p> </li> <li> <p>Under the Code tab and the Code source section, select Upload from and upload the deployment package created in the previous steps.   Since the deployment package exceeds 3 MBs, the Console Editor may not be available to view the source code: </p> </li> <li> <p>Under the Code tab, change Handler in section Runtime settings to <code>com.example.AstraDBFunction::handleRequest</code>: </p> </li> <li> <p>Under the Configuration tab, select and create these Environment variables:</p> <ul> <li><code>ASTRA_DB_CLIENT_ID</code>: A Client ID is generated together with an application token (see the Prerequisites section above).</li> <li><code>ASTRA_DB_CLIENT_SECRET</code>: A Client secret is generated together with an application token (see the Prerequisites section above).  Note that, for better security, you can alternatively use the AWS Secret Manager service to store and manage client id and secret, and then retrieve them programmatically. </li> </ul> </li> <li> <p>Optionally, to optimize function performance, consider configuring reserved and provisioned concurrency under the Configuration tab. For optimized cold start performance, use provisioned concurrency instead of Lambda SnapStart. In case of Lambda SnapStart, database session connections have to be re-initialized after a snapshot is restored, resulting in a substantial performance hit. </p> </li> </ol>"},{"location":"pages/develop/platform/aws-lambda-function/#3-test-the-function_2","title":"\u2705 3.  Test the function.","text":"<p>Under the Test tab, click the Test button and observe the output.  Notice the CQL version output and return value of 3.4.5.</p>"},{"location":"pages/develop/platform/aws-lambda-function/#using-java-grpc","title":"Using Java gRPC","text":""},{"location":"pages/develop/platform/aws-lambda-function/#1-create-a-deployment-package_3","title":"\u2705 1.  Create a deployment package.","text":"<p>A deployment package is a <code>.zip</code> or <code>.jar</code> file archive with compiled function code and dependencies. In this tutorial, we use Apache Maven\u2122 to create, compile and package a function into a <code>.jar</code> file. We need to include the following pieces into a deployment package to access Astra DB from an AWS Lambda function:   - aws-lambda-java-core that defines necessary interfaces and classes to create functions;   - Stargate that enables connectivity to Apache Cassandra, DataStax Astra DB and DataStax Enterprise; and   - gRPC that works as a high performance Remote Procedure Call (RPC) framework.</p> <ol> <li> <p>Open a command prompt and create a new project using Apache Maven\u2122: <pre><code>mvn archetype:generate -DgroupId=com.example -DartifactId=AstraDBFunction -DinteractiveMode=false\n</code></pre>  The project directory structure should look like this: </p> </li> <li> <p>Rename file <code>App.java</code> to <code>AstraDBFunction.java</code> and replace its content with the function source code: <pre><code>package com.example;\nimport com.amazonaws.services.lambda.runtime.Context;\nimport com.amazonaws.services.lambda.runtime.RequestHandler;\nimport com.amazonaws.services.lambda.runtime.LambdaLogger;\nimport java.util.Map;\nimport io.grpc.ManagedChannel;\nimport io.grpc.ManagedChannelBuilder;\nimport io.stargate.grpc.StargateBearerToken;\nimport io.stargate.proto.QueryOuterClass;\nimport io.stargate.proto.QueryOuterClass.Row;\nimport io.stargate.proto.StargateGrpc;\npublic class AstraDBFunction implements RequestHandler&lt;Map&lt;String,String&gt;, String&gt;{\nprivate static final String ASTRA_DB_TOKEN    = System.getenv(\"ASTRA_DB_APPLICATION_TOKEN\");\nprivate static final String ASTRA_DB_ID       = System.getenv(\"ASTRA_DB_ID\");\nprivate static final String ASTRA_DB_REGION   = System.getenv(\"ASTRA_DB_REGION\");\npublic static ManagedChannel channel = ManagedChannelBuilder\n.forAddress(ASTRA_DB_ID + \"-\" + ASTRA_DB_REGION + \".apps.astra.datastax.com\", 443)\n.useTransportSecurity()\n.build();\npublic static StargateGrpc.StargateBlockingStub blockingStub =\nStargateGrpc.newBlockingStub(channel).withCallCredentials(new StargateBearerToken(ASTRA_DB_TOKEN));\npublic String handleRequest(Map&lt;String,String&gt; event, Context context)\n{\nLambdaLogger logger = context.getLogger();\nQueryOuterClass.Response queryString = blockingStub.executeQuery(QueryOuterClass\n.Query.newBuilder()\n.setCql(\"SELECT cql_version FROM system.local WHERE key = 'local';\")\n.build());\nQueryOuterClass.ResultSet rs = queryString.getResultSet();\nString response = rs.getRows(0).getValues(0).getString();\nlogger.log(response + \" Success \\n\"); return response;\n}  }\n</code></pre> You can learn more about the code above by reading the Stargate documentation.</p> </li> <li> <p>Add AWS Lambda, Stargate and gRPC dependencies to the <code>pom.xml</code> file: <pre><code>    &lt;dependency&gt;\n&lt;groupId&gt;com.amazonaws&lt;/groupId&gt;\n&lt;artifactId&gt;aws-lambda-java-core&lt;/artifactId&gt;\n&lt;!-- Use the latest version from https://central.sonatype.dev/artifact/com.amazonaws/aws-lambda-java-core/1.2.2/versions --&gt;\n&lt;version&gt;1.2.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.amazonaws&lt;/groupId&gt;\n&lt;artifactId&gt;aws-lambda-java-events&lt;/artifactId&gt;\n&lt;!-- Use the latest version from https://central.sonatype.dev/artifact/com.amazonaws/aws-lambda-java-events/3.11.0/versions --&gt;\n&lt;version&gt;3.11.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.amazonaws&lt;/groupId&gt;\n&lt;artifactId&gt;aws-lambda-java-log4j2&lt;/artifactId&gt;\n&lt;!-- Use the latest version from https://central.sonatype.dev/artifact/com.amazonaws/aws-lambda-java-log4j2/1.5.1/versions --&gt;\n&lt;version&gt;1.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.stargate.grpc&lt;/groupId&gt;\n&lt;artifactId&gt;grpc-proto&lt;/artifactId&gt;\n&lt;!-- Use the latest version from https://central.sonatype.dev/artifact/io.stargate.grpc/grpc-proto/2.0.4/versions --&gt;\n&lt;version&gt;1.0.41&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.grpc&lt;/groupId&gt;\n&lt;artifactId&gt;grpc-netty-shaded&lt;/artifactId&gt;\n&lt;!-- Use the latest version from https://central.sonatype.dev/artifact/io.grpc/grpc-netty-shaded/1.51.1/versions --&gt;\n&lt;version&gt;1.41.0&lt;/version&gt;\n&lt;/dependency&gt;   </code></pre></p> </li> <li> <p>Add or replace an existing <code>build</code> section in the <code>pom.xml</code> file with the following: <pre><code>  &lt;build&gt;\n&lt;plugins&gt;\n&lt;plugin&gt;\n&lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;\n&lt;version&gt;2.22.2&lt;/version&gt;\n&lt;/plugin&gt;\n&lt;plugin&gt;\n&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n&lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n&lt;version&gt;3.2.2&lt;/version&gt;\n&lt;configuration&gt;\n&lt;createDependencyReducedPom&gt;false&lt;/createDependencyReducedPom&gt;\n&lt;/configuration&gt;\n&lt;executions&gt;\n&lt;execution&gt;\n&lt;phase&gt;package&lt;/phase&gt;\n&lt;goals&gt;\n&lt;goal&gt;shade&lt;/goal&gt;\n&lt;/goals&gt;\n&lt;/execution&gt;\n&lt;/executions&gt;\n&lt;/plugin&gt;\n&lt;plugin&gt;\n&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\n&lt;version&gt;3.8.1&lt;/version&gt;\n&lt;configuration&gt;\n&lt;source&gt;1.8&lt;/source&gt;\n&lt;target&gt;1.8&lt;/target&gt;\n&lt;/configuration&gt;\n&lt;/plugin&gt;\n&lt;/plugins&gt;\n&lt;/build&gt;\n</code></pre></p> </li> <li> <p>Run the Maven command in the project directory to compile code and create a <code>.jar</code> file: <pre><code> mvn clean compile package\n</code></pre> Find the deployment package file <code>AstraDBFunction-1.0-SNAPSHOT.jar</code> under the <code>target</code> directory: </p> </li> </ol>"},{"location":"pages/develop/platform/aws-lambda-function/#2-create-a-function_3","title":"\u2705 2.  Create a function.","text":"<ol> <li> <p>Go to the Functions page of the Lambda console and click Create function. </p> </li> <li> <p>Choose Author from scratch.</p> </li> <li> <p>Under the Basic information section, specify preferred Function name, Runtime, and Architecture. </p> </li> <li> <p>Click Create function.</p> </li> <li> <p>Under the Code tab and the Code source section, select Upload from and upload the deployment package created in the previous steps.   Since the deployment package exceeds 3 MBs, the Console Editor may not be available to view the source code: </p> </li> <li> <p>Under the Code tab, change Handler in section Runtime settings to <code>com.example.AstraDBFunction::handleRequest</code>: </p> </li> <li> <p>Under the Configuration tab, select and create these Environment variables:</p> <ul> <li><code>ASTRA_DB_ID</code>: A Database ID value can be found on the Astra DB dashboard.</li> <li><code>ASTRA_DB_REGION</code>: A Region name can be found on the overview page for a specific Astra DB database.</li> <li><code>ASTRA_DB_APPLICATION_TOKEN</code>: An Application Token can be generated for a specific Astra DB database (see the Prerequisites section above).  Note that, for better security, you can alternatively use the AWS Secret Manager service to store and manage an application token as a secret. A secret can then be retrieved programmatically. </li> </ul> </li> <li> <p>Optionally, to optimize function performance, consider configuring reserved and provisioned concurrency under the Configuration tab. For optimized cold start performance, prefer provisioned concurrency to Lambda SnapStart. For Lambda SnapStart to result in meaningful performance savings, a function needs to have substantial initiatization costs. </p> </li> </ol>"},{"location":"pages/develop/platform/aws-lambda-function/#3-test-the-function_3","title":"\u2705 3.  Test the function.","text":"<p>Under the Test tab, click the Test button and observe the output.  Notice the CQL version output and return value of 3.4.5.</p>"},{"location":"pages/develop/platform/azure-function/","title":"Azure Functions","text":""},{"location":"pages/develop/platform/azure-function/#azure-functions","title":"Azure Functions","text":""},{"location":"pages/develop/platform/azure-function/#overview","title":"Overview","text":"<p>Azure Functions is Microsoft Azure's function-as-a-service offering that provides a serverless execution environment for your code. Azure Functions are commonly used to:</p> <ul> <li>Extend Astra DB with additional data processing capabilities, such as aggregating, summarizing and validating data periodically;</li> <li>Connect Astra DB with other cloud services into data pipelines that move, process and analyze data.</li> </ul>"},{"location":"pages/develop/platform/azure-function/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle</li> </ul>"},{"location":"pages/develop/platform/azure-function/#using-python-driver","title":"Using Python Driver","text":""},{"location":"pages/develop/platform/azure-function/#1-create-a-function","title":"\u2705 1. Create a function.","text":"<ol> <li> <p>Follow the Quickstart to create a Python function in Azure from the command line using the <code>v1</code> Python programming model and Azure CLI. Complete all the steps to successfully deploy and test the function in Azure.</p> </li> <li> <p>Use Azure CLI to add these runtime environment variables to the application settings:</p> <ul> <li><code>ASTRA_DB_CLIENT_ID</code>: A Client ID is generated together with an application token (see the Prerequisites section above).</li> <li><code>ASTRA_DB_CLIENT_SECRET</code>: A Client secret is generated together with an application token (see the Prerequisites section above). <pre><code>az functionapp config appsettings set --name &lt;APP_NAME&gt; --resource-group &lt;RESOURCE_GROUP_NAME&gt; --settings \"ASTRA_DB_CLIENT_ID=Hdisr...\"\naz functionapp config appsettings set --name &lt;APP_NAME&gt; --resource-group &lt;RESOURCE_GROUP_NAME&gt; --settings \"ASTRA_DB_CLIENT_SECRET=UB3Tm8cR,Ic...\"\n</code></pre> Note that <code>&lt;APP_NAME&gt;</code> and <code>&lt;RESOURCE_GROUP_NAME&gt;</code> must be replaced with correct application and resurce group names used in the previous step. Similarly, <code>ASTRA_DB_CLIENT_ID</code> and <code>ASTRA_DB_CLIENT_SECRET</code> must be assigned correct values generated for your database.</li> </ul> </li> <li> <p>Copy the secure connect bundle file to the project directory. (See the Prerequisites section above if you need to download your secure connect bundle.)</p> </li> <li> <p>Add cassandra-driver, a Python client library for Apache Cassandra, DataStax Astra DB and DataStax Enterprise, to the <code>requirements.txt</code> file: </p> </li> <li> <p>Replace the <code>__init__.py</code> content with: <pre><code>from cassandra.cluster import Cluster\nfrom cassandra.auth import PlainTextAuthProvider\nimport logging\nimport azure.functions as func\nimport os\nASTRA_DB_CLIENT_ID = os.environ['ASTRA_DB_CLIENT_ID']\nASTRA_DB_CLIENT_SECRET = os.environ['ASTRA_DB_CLIENT_SECRET']\ncloud_config= {\n'secure_connect_bundle': 'secure-connect-bundle-for-your-database.zip',\n'use_default_tempdir': True\n}\nauth_provider = PlainTextAuthProvider(ASTRA_DB_CLIENT_ID, ASTRA_DB_CLIENT_SECRET)\ncluster = Cluster(cloud=cloud_config, auth_provider=auth_provider, protocol_version=4)\ndef main(req: func.HttpRequest) -&gt; func.HttpResponse:    \nsession = cluster.connect()\nrow = session.execute(\"SELECT cql_version FROM system.local WHERE key = 'local';\").one()\ncql_version = row[0]   \nlogging.info(f\"{cql_version} Success\")\nreturn func.HttpResponse(f\"{cql_version} Success\")\n</code></pre> You can learn more about the code above by reading the python-driver documentation. Note that <code>secure-connect-bundle-for-your-database.zip</code> must be replaced with a correct file name for your secure connect bundle.</p> </li> </ol>"},{"location":"pages/develop/platform/azure-function/#2-deploy-the-function","title":"\u2705 2. Deploy the function.","text":"<ol> <li> <p>Use Astra CLI to deploy the updated function: <pre><code>func azure functionapp publish &lt;APP_NAME&gt;\n</code></pre></p> </li> <li> <p>On the Microsoft Azure portal, find the newly deployed function: </p> </li> </ol>"},{"location":"pages/develop/platform/azure-function/#3-test-the-function","title":"\u2705 3. Test the function.","text":"<ol> <li> <p>Under Developer, select Code + Test and then Test/Run. Locate the Run button: </p> </li> <li> <p>Click Run and observe the output and logs:  Notice the CQL version output 3.4.5 and status code 200.</p> </li> </ol>"},{"location":"pages/develop/platform/azure-function/#using-python-sdk","title":"Using Python SDK","text":""},{"location":"pages/develop/platform/azure-function/#1-create-a-function_1","title":"\u2705 1. Create a function.","text":"<ol> <li> <p>Follow the Quickstart to create a Python function in Azure from the command line using the <code>v1</code> Python programming model and Azure CLI. Complete all the steps to successfully deploy and test the function in Azure.</p> </li> <li> <p>Use Azure CLI to add these runtime environment variables to the application settings:</p> <ul> <li><code>ASTRA_DB_ID</code>: A Database ID value can be found on the Astra DB dashboard.</li> <li><code>ASTRA_DB_REGION</code>: A Region name can be found on the overview page for a specific Astra DB database.</li> <li><code>ASTRA_DB_APPLICATION_TOKEN</code>: An Application Token can be generated for a specific Astra DB database (see the Prerequisites section above). <pre><code>az functionapp config appsettings set --name &lt;APP_NAME&gt; --resource-group &lt;RESOURCE_GROUP_NAME&gt; --settings \"ASTRA_DB_ID=0c2f6f34-41ea-...\"\naz functionapp config appsettings set --name &lt;APP_NAME&gt; --resource-group &lt;RESOURCE_GROUP_NAME&gt; --settings \"ASTRA_DB_REGION=us-east1\"\naz functionapp config appsettings set --name &lt;APP_NAME&gt; --resource-group &lt;RESOURCE_GROUP_NAME&gt; --settings \"ASTRA_DB_APPLICATION_TOKEN=AstraCS:avDWzU...\"\n</code></pre> Note that <code>&lt;APP_NAME&gt;</code> and <code>&lt;RESOURCE_GROUP_NAME&gt;</code> must be replaced with correct application and resurce group names used in the previous step. Similarly, <code>ASTRA_DB_ID</code>, <code>ASTRA_DB_REGION</code> and <code>ASTRA_DB_APPLICATION_TOKEN</code> must be assigned correct values associated with your database.</li> </ul> </li> <li> <p>Add AstraPy, a Pythonic SDK for DataStax Astra and Stargate, to the <code>requirements.txt</code> file: </p> </li> <li> <p>Replace the <code>__init__.py</code> content with: <pre><code>from astrapy.rest import create_client, http_methods\nimport logging\nimport azure.functions as func\nimport os\nASTRA_DB_ID = os.environ['ASTRA_DB_ID']\nASTRA_DB_REGION = os.environ['ASTRA_DB_REGION']\nASTRA_DB_APPLICATION_TOKEN = os.environ['ASTRA_DB_APPLICATION_TOKEN']\nastra_http_client = create_client(astra_database_id=ASTRA_DB_ID,\nastra_database_region=ASTRA_DB_REGION,\nastra_application_token=ASTRA_DB_APPLICATION_TOKEN)\ndef main(req: func.HttpRequest) -&gt; func.HttpResponse:    \nres = astra_http_client.request(\nmethod=http_methods.GET,\npath=f\"/api/rest/v2/keyspaces/system/local/local\"\n)\ncql_version = res[\"data\"][0]['cql_version']   \nlogging.info(f\"{cql_version} Success\")\nreturn func.HttpResponse(f\"{cql_version} Success\")\n</code></pre> You can learn more about the code above by reading the AstraPy documentation.</p> </li> </ol>"},{"location":"pages/develop/platform/azure-function/#2-deploy-the-function_1","title":"\u2705 2. Deploy the function.","text":"<ol> <li> <p>Use Astra CLI to deploy the updated function: <pre><code>func azure functionapp publish &lt;APP_NAME&gt;\n</code></pre></p> </li> <li> <p>On the Microsoft Azure portal, find the newly deployed function: </p> </li> </ol>"},{"location":"pages/develop/platform/azure-function/#3-test-the-function_1","title":"\u2705 3. Test the function.","text":"<ol> <li> <p>Under Developer, select Code + Test and then Test/Run. Locate the Run button: </p> </li> <li> <p>Click Run and observe the output and logs:  Notice the CQL version output 3.4.5 and status code 200.</p> </li> </ol>"},{"location":"pages/develop/platform/circleci/","title":"CircleCI","text":""},{"location":"pages/develop/platform/circleci/#overview","title":"Overview","text":"<p>CircleCI is a popular CICD (continuous integration continuous delivery) tool which integrates with your application's GitHub repository.  With CircleCI you can quickly spin-up resources to run your application's unit tests under production environment conditions.</p> <ul> <li>Reference Documentation</li> </ul>"},{"location":"pages/develop/platform/circleci/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an account with GitHub or another cloud-based code repository.</li> <li>You should have an account with CircleCI.  You can elect to sign-up with single-sign-on using GitHub (recommended).</li> <li>You should have created an Astra Database We will need the <code>database id</code> and <code>region</code> of your Astra DB instance.</li> <li>You should have an Astra Token for your application.</li> <li>This example will be for a Java / Spring Boot project, but the process here is not language dependent.</li> </ul>"},{"location":"pages/develop/platform/circleci/#steps","title":"Steps","text":""},{"location":"pages/develop/platform/circleci/#1-enable-your-project","title":"1 - Enable your project.","text":"<p>When you view your dashboard on CircleCI, click on \"Projects\" in the left nav.  This should display all of the repositories from your GitHub account.  Find the project that you want to enable with CircleCI, and click \"Set Up Project.\"</p> <p>You will then see a popup asking you to designate or create a new CircleCI <code>config.yml</code> file.  Choose the option to \"Take me to a config template that I can edit.\"</p> <p>Next, you will be prompted to pick the project's language and/or dependency manager.  This will help to generate the <code>config.yml</code> file.  For this example, I will scroll down and select Java (Maven).</p>"},{"location":"pages/develop/platform/circleci/#2-configure-your-project","title":"2 - Configure your project.","text":"<p>You should now be able to see the code to the <code>config.yml</code> file in the bottom-half of the screen. You can also get to the config editor point from the \"Projects\" screen, by clicking on the 3 dots to the right of a valid, CircleCI project repository, as shown below in figure 1.</p> <p></p> <p>Figure 1 - How to find a project's configuration file editor from the \"Projects\" screen.</p> <p>Here is a sample CircleCI <code>config.yml</code>.  This file can also be found in the following repository and branch: https://github.com/aar0np/live-coding-exercise/blob/circleci-project-setup/.circleci/config.yml</p>"},{"location":"pages/develop/platform/circleci/#sample-circleciconfigyml","title":"Sample <code>circleci/config.yml</code>","text":"<pre><code># version of CircleCI pipeline process engine.\nversion: 2.1\n\n# Define jobs\njobs:\n  build-and-test:\n    docker:\n      - image: cimg/openjdk:17.0.4\n    steps:\n      - checkout\n      - run:\n          name: Maven build\n          command: mvn -B -DskipTests clean package\n      - run:\n          name: Test\n          command: mvn test\n\n# Invoke jobs via workflows\nworkflows:\n  project-workflow:\n    jobs:\n      - build-and-test\n</code></pre> <p>In this case, we are using the Astra Spring Boot Starter Kit to connect to Astra DB.  Because of this, running a simple <code>mvn test</code> is enough for our project to open a test connection out to your Astra database.</p>"},{"location":"pages/develop/platform/circleci/#3-setting-astra-db-connection-properties","title":"3 - Setting Astra DB connection properties.","text":"<p>From the \"three dots\" menu on the project page that we used in Step 2, you can see that there is an option called \"Project Settings.\"  Click on this option, and then click \"Environment Variables\" on the left nav of the next screen.  Here you can create variables for credentials, tokens, and other sensitive properties for your project.</p> <p>Click the blue \"Add Environment Variable\" button to enter both its name and value.  For my application, I needed to add the following environment variables:</p> <ul> <li>ASTRA_DB_ID</li> <li>ASTRA_DB_APP_TOKEN</li> <li>ASTRA_DB_REGION</li> <li>ASTRA DB KEYSPACE</li> </ul> <p>A portion of the environment variable values will be obscured by Xs.  Note that you cannot edit or read existing environment variables.  For security reasons, they can only be deleted and re-created.</p>"},{"location":"pages/develop/platform/circleci/#4-running-your-cicd-pipeline","title":"4 - Running your CICD pipeline.","text":"<p>With the config.yml in place and environment variables defined, you should now be ready to run your pipeline.  Go back to the config editor, and verify that your <code>config.yml</code> shows \"green\" on the bottom.  If there is an error in the YAML, it will be in red.  From here, click on the name of the job defined in your config.  For my project, it will be the \"build-and-test\" option toward the bottom, as shown in figure 2.</p> <p></p> <p>Figure 2 - Configuration file editor screen.  Notice that my job name \"build-and-test\" is a clickable link at the bottom.</p> <p>Clicking on the job link will take you to the screen where you can run/rerun your pipeline.  Simply click on the \"Rerun\" button and select \"Rerun workflow from start\" option.  Assuming everything is properly configured and your Astra DB is not hibernated, the pipeline should complete successfully.  If it does not, click on the job link to see the individual steps, and open the failing step to troubleshoot the problem.</p>"},{"location":"pages/develop/platform/google-cloud-function/","title":"Google Cloud Functions","text":""},{"location":"pages/develop/platform/google-cloud-function/#google-cloud-functions","title":"Google Cloud Functions","text":""},{"location":"pages/develop/platform/google-cloud-function/#overview","title":"Overview","text":"<p>Cloud Functions is Google's function-as-a-service offering that provides a serverless execution environment for your code. Cloud Functions are commonly used to:</p> <ul> <li>Extend Astra DB with additional data processing capabilities, such as aggregating, summarizing and validating data periodically;</li> <li>Connect Astra DB with other cloud services into data pipelines that move, process and analyze data.</li> </ul>"},{"location":"pages/develop/platform/google-cloud-function/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle</li> <li>Optionally, if you are new to Cloud Functions, practice creating a simpler function first.</li> </ul>"},{"location":"pages/develop/platform/google-cloud-function/#using-python-driver","title":"Using Python Driver","text":""},{"location":"pages/develop/platform/google-cloud-function/#1-create-a-secret-with-the-secure-connect-bundle-file","title":"\u2705 1. Create a secret with the secure connect bundle file.","text":"<ol> <li> <p>Go to the Secret Manager page, select a project that has Secret Manager and Cloud Functions enabled, and click Create secret.</p> </li> <li> <p>Give a Name to the secret and upload the secure connect bundle file as a Secret value. (See the Prerequisites section above if you need to download your secure connect bundle.) Optionally, customize other secret management settings. </p> </li> <li> <p>Click Create secret.</p> </li> <li> <p>On the Secret Manager page, find the newly created secret. </p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#2-create-a-function","title":"\u2705 2. Create a function.","text":"<ol> <li>Go to the Functions Overview page, select the same project that has Secret Manager and Cloud Functions enabled, and click Create function.</li> <li>Under the Basics section, specify preferred Function name and Region.</li> <li> <p>Under the Trigger section, select HTTP, Allow unauthenticated invocations, and Require HTTPS. </p> </li> <li> <p>Click Save.</p> </li> <li> <p>Under the Runtime, build, connections and security settings section, customize additional settings and create these Runtime environment variables:</p> <ul> <li><code>ASTRA_DB_CLIENT_ID</code>: A Client ID is generated together with an application token (see the Prerequisites section above).</li> <li><code>ASTRA_DB_CLIENT_SECRET</code>: A Client secret is generated together with an application token (see the Prerequisites section above).  Note that, for better security, you can alternatively use the Secret Manager service to store and manage a client secret. A secret can then be similarly exposed as an environment variable. The settings can be found under the Runtime, build, connections and security settings section, the Security tab, and the Secrets field.</li> </ul> </li> <li> <p>Under the Runtime, build, connections and security settings section and the Security, click Reference a secret. Select the previously created Secret with the secure connect bundle file, Grant the service account access to the secret, if needed, use Mounted as volume in the Reference method field, and enter secrets in the Mount path field.  Notice the final Path that should be used to access the secure connect bundle in the function code.</p> </li> <li> <p>Click Done and Next.</p> </li> <li> <p>Select Python 3.7 or your preferred version in the Runtime field.</p> </li> <li> <p>Select Inline Editor in the Source code field.</p> </li> <li> <p>Enter query_astra_db in the Entry point field.</p> </li> <li> <p>Add cassandra-driver, a Python client library for Apache Cassandra, DataStax Astra DB and DataStax Enterprise, to the <code>requirements.txt</code> file. </p> </li> <li> <p>Replace the <code>main.py</code> content with: <pre><code>from cassandra.cluster import Cluster\nfrom cassandra.auth import PlainTextAuthProvider\nimport os\nASTRA_DB_CLIENT_ID = os.environ.get('ASTRA_DB_CLIENT_ID')\nASTRA_DB_CLIENT_SECRET = os.environ.get('ASTRA_DB_CLIENT_SECRET')\ncloud_config= {\n'secure_connect_bundle': '/secrets/secure-connect-secret',\n'use_default_tempdir': True\n}\nauth_provider = PlainTextAuthProvider(ASTRA_DB_CLIENT_ID, ASTRA_DB_CLIENT_SECRET)\ncluster = Cluster(cloud=cloud_config, auth_provider=auth_provider, protocol_version=4)\ndef query_astra_db(request):\nsession = cluster.connect()\nrow = session.execute(\"SELECT cql_version FROM system.local WHERE key = 'local';\").one()\nprint(row[0])\nprint ('Success')\n</code></pre>  You can learn more about the code above by reading the python-driver documentation.</p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#3-deploy-the-function","title":"\u2705 3. Deploy the function.","text":"<ol> <li> <p>Click Deploy.</p> </li> <li> <p>On the Cloud Functions Overview page, find the newly deployed function. </p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#4-test-the-function","title":"\u2705 4. Test the function.","text":"<ol> <li> <p>Under Actions, select Test function. </p> </li> <li> <p>On the testing page, click Test the function and observe the output.  Notice the CQL version output 3.4.5 and status code 200.</p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#5-view-logs","title":"\u2705 5. View logs.","text":"<p>You can further explore the log history by either clicking on the Logs tab or the View all logs link that opens Logs Explorer.  </p>"},{"location":"pages/develop/platform/google-cloud-function/#using-python-sdk","title":"Using Python SDK","text":""},{"location":"pages/develop/platform/google-cloud-function/#1-create-a-function","title":"\u2705 1. Create a function.","text":"<ol> <li> <p>Go to the Functions Overview page, select a project that has Cloud Functions enabled, and click Create function.</p> </li> <li> <p>Under the Basics section, specify preferred Function name and Region.</p> </li> <li> <p>Under the Trigger section, select HTTP, Allow unauthenticated invocations, and Require HTTPS. </p> </li> <li> <p>Click Save.</p> </li> <li> <p>Under the Runtime, build, connections and security settings section, customize additional settings and create these Runtime environment variables:</p> <ul> <li><code>ASTRA_DB_ID</code>: A Database ID value can be found on the Astra DB dashboard.</li> <li><code>ASTRA_DB_REGION</code>: A Region name can be found on the overview page for a specific Astra DB database.</li> <li><code>ASTRA_DB_APPLICATION_TOKEN</code>: An Application Token can be generated for a specific Astra DB database (see the Prerequisites section above).  Note that, for better security, you can alternatively use the Secret Manager service to store and manage an application token as a secret. A secret can then be similarly exposed as an environment variable. The settings can be found under the Runtime, build, connections and security settings section, the Security tab, and the Secrets field.</li> </ul> </li> <li> <p>Click Next.</p> </li> <li> <p>Select Python 3.7 or your preferred version in the Runtime field.</p> </li> <li> <p>Select Inline Editor in the Source code field.</p> </li> <li> <p>Enter query_astra_db in the Entry point field.</p> </li> <li> <p>Add AstraPy, a Pythonic SDK for DataStax Astra and Stargate, and its preferred version to the <code>requirements.txt</code> file. </p> </li> <li> <p>Replace the <code>main.py</code> content with: <pre><code>from astrapy.rest import create_client, http_methods\nimport os\nASTRA_DB_ID = os.environ.get('ASTRA_DB_ID')\nASTRA_DB_REGION = os.environ.get('ASTRA_DB_REGION')\nASTRA_DB_APPLICATION_TOKEN = os.environ.get('ASTRA_DB_APPLICATION_TOKEN')\nastra_http_client = create_client(astra_database_id=ASTRA_DB_ID,\nastra_database_region=ASTRA_DB_REGION,\nastra_application_token=ASTRA_DB_APPLICATION_TOKEN)\ndef query_astra_db(request):\n# Retrieve a row with primary key value 'local'\n# from table 'local' in keyspace 'system'\nres = astra_http_client.request(\nmethod=http_methods.GET,\npath=f\"/api/rest/v2/keyspaces/system/local/local\"\n)\n# Print the 'cql_version' field value of the row\nprint(res[\"data\"][0]['cql_version']) \nprint ('Success')\n</code></pre>  You can learn more about the code above by reading the AstraPy documentation.</p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#2-deploy-the-function","title":"\u2705 2. Deploy the function.","text":"<ol> <li> <p>Click Deploy.</p> </li> <li> <p>On the Cloud Functions Overview page, find the newly deployed function. </p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#3-test-the-function","title":"\u2705 3. Test the function.","text":"<ol> <li> <p>Under Actions, select Test function. </p> </li> <li> <p>On the testing page, click Test the function and observe the output.  Notice the CQL version output 3.4.5 and status code 200.</p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#4-view-logs","title":"\u2705 4. View logs.","text":"<p>You can further explore the log history by either clicking on the Logs tab or the View all logs link that opens Logs Explorer.  </p>"},{"location":"pages/develop/platform/google-cloud-function/#using-java-driver","title":"Using Java Driver","text":""},{"location":"pages/develop/platform/google-cloud-function/#1-create-a-secret-with-the-secure-connect-bundle-file_1","title":"\u2705 1. Create a secret with the secure connect bundle file.","text":"<ol> <li> <p>Go to the Secret Manager page, select a project that has Secret Manager and Cloud Functions enabled, and click Create secret.</p> </li> <li> <p>Give a Name to the secret and upload the secure connect bundle file as a Secret value. (See the Prerequisites section above if you need to download your secure connect bundle.) Optionally, customize other secret management settings. </p> </li> <li> <p>Click Create secret.</p> </li> <li> <p>On the Secret Manager page, find the newly created secret. </p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#2-create-a-function_1","title":"\u2705 2. Create a function.","text":"<ol> <li>Go to the Functions Overview page, select the same project that has Secret Manager and Cloud Functions enabled, and click Create function.</li> <li>Under the Basics section, specify preferred Function name and Region.</li> <li> <p>Under the Trigger section, select HTTP, Allow unauthenticated invocations, and Require HTTPS. </p> </li> <li> <p>Click Save.</p> </li> <li> <p>Under the Runtime, build, connections and security settings section, customize additional settings and create these Runtime environment variables:</p> <ul> <li><code>ASTRA_DB_CLIENT_ID</code>: A Client ID is generated together with an application token (see the Prerequisites section above).</li> <li><code>ASTRA_DB_CLIENT_SECRET</code>: A Client secret is generated together with an application token (see the Prerequisites section above).  Note that, for better security, you can alternatively use the Secret Manager service to store and manage a client secret. A secret can then be similarly exposed as an environment variable. The settings can be found under the Runtime, build, connections and security settings section, the Security tab, and the Secrets field.</li> </ul> </li> <li> <p>Under the Runtime, build, connections and security settings section and the Security, click Reference a secret. Select the previously created Secret with the secure connect bundle file, Grant the service account access to the secret, if needed, use Mounted as volume in the Reference method field, and enter secrets in the Mount path field.  Notice the final Path that should be used to access the secure connect bundle in the function code.</p> </li> <li> <p>Click Done and Next.</p> </li> <li> <p>Select Java 11 or your preferred version in the Runtime field.</p> </li> <li> <p>Select Inline Editor in the Source code field.</p> </li> <li> <p>Enter com.example.AstraDBFunction in the Entry point field.</p> </li> <li> <p>Add java-driver, a Java client library for Apache Cassandra, DataStax Astra DB and DataStax Enterprise, to the <code>pom.xml</code> file: <pre><code>    &lt;dependency&gt;\n&lt;groupId&gt;com.datastax.oss&lt;/groupId&gt;\n&lt;artifactId&gt;java-driver-core&lt;/artifactId&gt;\n&lt;!--Use the latest version from https://search.maven.org/artifact/com.datastax.oss/java-driver-core --&gt;\n&lt;version&gt;4.15.0&lt;/version&gt;\n&lt;/dependency&gt;  </code></pre> </p> </li> <li> <p>Rename the <code>Example.java</code> file to <code>AstraDBFunction.java</code> and replace its content with: <pre><code>package com.example;\nimport com.google.cloud.functions.HttpFunction;\nimport com.google.cloud.functions.HttpRequest;\nimport com.google.cloud.functions.HttpResponse;\nimport java.io.BufferedWriter;\nimport com.datastax.oss.driver.api.core.CqlSession;\nimport com.datastax.oss.driver.api.core.cql.ResultSet;\nimport com.datastax.oss.driver.api.core.cql.Row;\nimport java.nio.file.Paths;\npublic class AstraDBFunction implements HttpFunction {\npublic static final String ASTRA_DB_CLIENT_ID = System.getenv(\"ASTRA_DB_CLIENT_ID\");\npublic static final String ASTRA_DB_CLIENT_SECRET = System.getenv(\"ASTRA_DB_CLIENT_SECRET\");\npublic static CqlSession session = CqlSession.builder()\n.withCloudSecureConnectBundle(Paths.get(\"/secrets/secure-connect-secret\"))\n.withAuthCredentials(ASTRA_DB_CLIENT_ID,ASTRA_DB_CLIENT_SECRET)\n.build();\npublic void service(HttpRequest request, HttpResponse response) throws Exception {\nBufferedWriter writer = response.getWriter();\nResultSet rs = session.execute(\"SELECT cql_version FROM system.local WHERE key = 'local';\");\nRow row = rs.one();\nwriter.write( row.getString(\"cql_version\") );\nwriter.newLine();\nwriter.write(\"Success\");\n}\n}\n</code></pre>  You can learn more about the code above by reading the java-driver documentation.</p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#3-deploy-the-function_1","title":"\u2705 3. Deploy the function.","text":"<ol> <li> <p>Click Deploy.</p> </li> <li> <p>On the Cloud Functions Overview page, find the newly deployed function. </p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#4-test-the-function_1","title":"\u2705 4. Test the function.","text":"<ol> <li> <p>Under Actions, select Test function. </p> </li> <li> <p>On the testing page, click Test the function and observe the output.  Notice the CQL version output 3.4.5 and status code 200.</p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#5-view-logs_1","title":"\u2705 5. View logs.","text":"<p>You can further explore the log history by either clicking on the Logs tab or the View all logs link that opens Logs Explorer.  </p>"},{"location":"pages/develop/platform/google-cloud-function/#using-java-grpc","title":"Using Java gRPC","text":""},{"location":"pages/develop/platform/google-cloud-function/#1-create-a-function_1","title":"\u2705 1. Create a function.","text":"<ol> <li>Go to the Functions Overview page, select a project that has Cloud Functions enabled, and click Create function.</li> <li>Under the Basics section, specify preferred Function name and Region.</li> <li> <p>Under the Trigger section, select HTTP, Allow unauthenticated invocations, and Require HTTPS. </p> </li> <li> <p>Click Save.</p> </li> <li> <p>Under the Runtime, build, connections and security settings section, customize additional settings and create these Runtime environment variables:</p> <ul> <li><code>ASTRA_DB_ID</code>: A Database ID value can be found on the Astra DB dashboard.</li> <li><code>ASTRA_DB_REGION</code>: A Region name can be found on the overview page for a specific Astra DB database.</li> <li><code>ASTRA_DB_APPLICATION_TOKEN</code>: An Application Token can be generated for a specific Astra DB database (see the Prerequisites section above).  Note that, for better security, you can alternatively use the Secret Manager service to store and manage an application token as a secret. A secret can then be similarly exposed as an environment variable. The settings can be found under the Runtime, build, connections and security settings section, the Security tab, and the Secrets field.</li> </ul> </li> <li> <p>Click Next.</p> </li> <li> <p>Select Java 11 or your preferred version in the Runtime field.</p> </li> <li> <p>Select Inline Editor in the Source code field.</p> </li> <li> <p>Enter com.example.AstraDBFunction in the Entry point field.</p> </li> <li> <p>Add gRPC dependencies to the <code>pom.xml</code> file: <pre><code>    &lt;dependency&gt;\n&lt;groupId&gt;io.stargate.grpc&lt;/groupId&gt;\n&lt;artifactId&gt;grpc-proto&lt;/artifactId&gt;\n&lt;!-- Use the latest version from https://central.sonatype.dev/artifact/io.stargate.grpc/grpc-proto/2.0.4/versions --&gt;\n&lt;version&gt;1.0.41&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.grpc&lt;/groupId&gt;\n&lt;artifactId&gt;grpc-netty-shaded&lt;/artifactId&gt;\n&lt;!-- Use the latest version from https://central.sonatype.dev/artifact/io.grpc/grpc-netty-shaded/1.51.1/versions --&gt;\n&lt;version&gt;1.41.0&lt;/version&gt;\n&lt;/dependency&gt;   </code></pre> </p> </li> <li> <p>Rename the <code>Example.java</code> file to <code>AstraDBFunction.java</code> and replace its content with: <pre><code>package com.example;\nimport com.google.cloud.functions.HttpFunction;\nimport com.google.cloud.functions.HttpRequest;\nimport com.google.cloud.functions.HttpResponse;\nimport java.io.BufferedWriter;\nimport java.util.concurrent.TimeUnit;\nimport io.grpc.ManagedChannel;\nimport io.grpc.ManagedChannelBuilder;\nimport io.stargate.grpc.StargateBearerToken;\nimport io.stargate.proto.QueryOuterClass;\nimport io.stargate.proto.QueryOuterClass.Row;\nimport io.stargate.proto.StargateGrpc;\npublic class AstraDBFunction implements HttpFunction {\npublic static final String ASTRA_DB_TOKEN    = System.getenv(\"ASTRA_DB_APPLICATION_TOKEN\");\npublic static final String ASTRA_DB_ID       = System.getenv(\"ASTRA_DB_ID\");\npublic static final String ASTRA_DB_REGION   = System.getenv(\"ASTRA_DB_REGION\");\npublic static ManagedChannel channel = ManagedChannelBuilder\n.forAddress(ASTRA_DB_ID + \"-\" + ASTRA_DB_REGION + \".apps.astra.datastax.com\", 443)\n.useTransportSecurity()\n.build();\npublic static StargateGrpc.StargateBlockingStub blockingStub =\nStargateGrpc.newBlockingStub(channel).withCallCredentials(new StargateBearerToken(ASTRA_DB_TOKEN));\npublic void service(HttpRequest request, HttpResponse response) throws Exception {\nQueryOuterClass.Response queryString = blockingStub.executeQuery(QueryOuterClass\n.Query.newBuilder()\n.setCql(\"SELECT cql_version FROM system.local WHERE key = 'local';\")\n.build());\nQueryOuterClass.ResultSet rs = queryString.getResultSet();\nBufferedWriter writer = response.getWriter();\nwriter.write( rs.getRows(0).getValues(0).getString() );\nwriter.newLine();\nwriter.write(\"Success\");\n}\n}\n</code></pre>  You can learn more about the code above by reading the Stargate documentation.</p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#2-deploy-the-function_1","title":"\u2705 2. Deploy the function.","text":"<ol> <li> <p>Click Deploy.</p> </li> <li> <p>On the Cloud Functions Overview page, find the newly deployed function. </p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#3-test-the-function_1","title":"\u2705 3. Test the function.","text":"<ol> <li> <p>Under Actions, select Test function. </p> </li> <li> <p>On the testing page, click Test the function and observe the output.  Notice the CQL version output 3.4.5 and status code 200.</p> </li> </ol>"},{"location":"pages/develop/platform/google-cloud-function/#4-view-logs_1","title":"\u2705 4. View logs.","text":"<p>You can further explore the log history by either clicking on the Logs tab or the View all logs link that opens Logs Explorer.  </p>"},{"location":"pages/develop/sdk/astra-db-client-java/","title":"\u2022 Astra DB Client","text":""},{"location":"pages/develop/sdk/astra-db-client-java/#1-overview","title":"1. Overview","text":"<p>The Astra DB Client, as the name suggests, is a client library that interacts with the various APIs of the Astra DataStax Platform. It enables users to connect to, utilize, and administer the Astra Vector product. The library encompasses two distinct clients working in tandem:</p> <ul> <li> <p>AstraDBAmin: This class is initialized exclusively using an organization administrator token and enables the creation and deletion of databases via the DevOps API. It facilitates automation and administration within your organization's tenant.</p> </li> <li> <p>AstraDB: This is the primary endpoint, connecting exclusively to a single database to perform all operations for your applications. It requires initialization with a database administrator token and also necessitates the API endpoint of your database.</p> </li> <li> <p>AstraDBCollection: This client class facilitates all operations at the collection level, including find(), insert(), and delete(). It is instantiated through the AstraDB class and accommodates operations on both vector and non-vector collections.</p> </li> <li> <p>AstraDBRepository:  This class represents a specialized form of AstraDBCollection designed for use with Java beans (T). It embodies the repository pattern, streamlining the management and access of domain entities. Reference Architecture <p></p>"},{"location":"pages/develop/sdk/astra-db-client-java/#2-prerequisites","title":"2. Prerequisites","text":"Java and Apache Maven/Gradle Setup <ul> <li> Install Java Development Kit (JDK) 11++</li> </ul> <p>Use the java reference documentation  to install a Java Development Kit (JDK) tailored for your operating system. After installation, you can validate your setup with the following command:</p> <pre><code>java --version\n</code></pre> <ul> <li> Install Apache Maven (3.9+) or Gradle</li> </ul> <p>Samples and tutorials are designed to be used with <code>Apache Maven</code>. Follow the instructions in the reference documentation to install Maven. To validate your installation, use the following command:</p> <pre><code>mvn -version\n</code></pre> Astra Environment Setup <ul> <li> Create your DataStax Astra account:</li> </ul> <p>Sign Up to Datastax Astra</p> <ul> <li> Create an organization level Astra Token</li> </ul> <p>Once logged into the user interface, select settings from the left menu and then click on the tokens tab to create a new token.</p> <p></p> <p>You want to pick the following role:</p> Properties Values Token Role <code>Organization Administrator</code> <p>The Token contains properties <code>Client ID</code>, <code>Client Secret</code> and the <code>token</code>. You will only need the third (starting with <code>AstraCS:</code>)</p> <pre><code>{\n  \"ClientId\": \"ROkiiDZdvPOvHRSgoZtyAapp\",\n  \"ClientSecret\": \"fakedfaked\",\n  \"Token\":\"AstraCS:fake\" &lt;========== use this field\n}\n</code></pre> <p>To operate with <code>AstraDBAdmin</code>, this specific organization-level token is required. For tasks involving AstraDB at the database level, a database-level token suffices. The procedure for creating such a token is detailed in subsequent sections.</p>"},{"location":"pages/develop/sdk/astra-db-client-java/#3-getting-started","title":"3. Getting Started","text":""},{"location":"pages/develop/sdk/astra-db-client-java/#project-setup","title":"Project Setup","text":"Project Setup <ul> <li> If you are using <code>Maven</code> Update your <code>pom.xml</code> file with the latest version of the Vector SDK </li> </ul> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.astra&lt;/groupId&gt;\n&lt;artifactId&gt;astra-db-client&lt;/artifactId&gt;\n&lt;version&gt;${latest}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <ul> <li> If you are using gradle change the <code>build.dgradle</code> with</li> </ul> <pre><code>dependencies {\n    compile 'com.datastax.astra:astra-db-client-1.0'\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#quickstart","title":"Quickstart","text":"Getting your token and Api Endpoint <p><code>AstraDB</code> class is the entry point of the SDK. It enables interactions with one particular database within your Astra environment. The initialization can be achieved in multiple ways:</p> <ul> <li>Using a <code>token</code> along with the <code>api_endpoint</code>. Both are retrieved from the Astra user interface.</li> <li>Using a <code>token</code> with the database identifier and eventually the region.</li> </ul> <p>To establish this connection, you can generate a token via the user interface. This token will be assigned the <code>Database Administrator</code> permission level, which grants sufficient privileges for interacting with a specific database.</p> <p>The <code>api_endpoint</code> is obtained from the user interface. It adheres to the following pattern: <code>https://{database-identifier}-{database-region}.apps.astra.datastax.com.</code></p> <p></p> Quickstart.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport io.stargate.sdk.data.domain.query.Filter;\nimport io.stargate.sdk.data.domain.JsonDocument;\nimport io.stargate.sdk.data.domain.JsonDocumentResult;\nimport java.util.Map;\nimport java.util.stream.Stream;\npublic class QuickStart {\npublic static void main(String[] args) {\n// Initialize the client\nAstraDB myDb = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\n// Create a collection\nAstraDBCollection demoCollection = myDb.createCollection(\"demo\",14);\n// Insert vectors\ndemoCollection.insertOne(\nnew JsonDocument()\n.id(\"doc1\") // generated if not set\n.vector(new float[]{1f, 0f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f})\n.put(\"product_name\", \"HealthyFresh - Beef raw dog food\")\n.put(\"product_price\", 12.99));\ndemoCollection.insertOne(\nnew JsonDocument()\n.id(\"doc2\")\n.vector(new float[]{1f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f})\n.put(\"product_name\", \"HealthyFresh - Chicken raw dog food\")\n.put(\"product_price\", 9.99));\ndemoCollection.insertOne(\nnew JsonDocument()\n.id(\"doc3\")\n.vector(new float[]{1f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f})\n.data(Map.of(\"product_name\", \"HealthyFresh - Chicken raw dog food\")));\ndemoCollection.insertOne(\nnew JsonDocument()\n.id(\"doc4\")\n.vector(new float[]{1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f})\n.put(\"product_name\", \"HealthyFresh - Chicken raw dog food\")\n.put(\"product_price\", 9.99));\n// Perform a similarity search\nfloat[] embeddings = new float[] {1f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f};\nFilter metadataFilter = new Filter().where(\"product_price\").isEqualsTo(9.99);\nint maxRecord = 10;\nStream&lt;JsonDocumentResult&gt; resultsSet = demoCollection.findVector(embeddings, metadataFilter, maxRecord);\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#4-reference-guide","title":"4. Reference Guide","text":""},{"location":"pages/develop/sdk/astra-db-client-java/#connection","title":"Connection","text":"<p>Connect to AstraDB Vector by instantiating <code>AstraDB</code> class.</p> General Information <ul> <li>Connection is stateless and thread safe, we initialize an HTTP client.</li> <li>At initialization a check is performed to ensure enpoint and token are valid.</li> <li>If not provided default keyspace is <code>default_keyspace</code>.</li> <li>Database UUID and region are part of the endpoint URL.</li> </ul> <ul> <li> Signatures and JavaDoc</li> </ul> <pre><code>AstraDB(String token, String apiEndpoint);\nAstraDB(String token, String apiEndpoint, String keyspace);\nAstraDB(String token, UUID databaseId);\nAstraDB(String token, UUID databaseId, String keyspace);\nAstraDB(String token, UUID databaseId, String region, String keyspace);\nAstraDB(String token, UUID databaseId, String region, AstraEnvironment env, String keyspace);\n</code></pre> <ul> <li> Sample Code</li> </ul> Connection.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport java.util.UUID;\npublic class Connecting {\npublic static void main(String[] args) {\n// Default initialization\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\n// Initialize with a non-default keyspace\nAstraDB db1 = new AstraDB(\"TOKEN\", \"API_ENDPOINT\", \"&lt;keyspace&gt;\");\n// Initialize with an identifier instead of an endpoint\nUUID databaseUuid = UUID.fromString(\"&lt;database_id&gt;\");\nAstraDB db2 = new AstraDB(\"TOKEN\", databaseUuid);\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#working-with-collections","title":"Working with Collections","text":""},{"location":"pages/develop/sdk/astra-db-client-java/#overview","title":"Overview","text":"Overview <p>AstraDB is a vector database that manages multiple collections. Each collection (AstraDBCollection) is identified by a name and stores schema-less documents. It is capable of holding any JSON document, each uniquely identified by an _id. Additionally, a JSON document within AstraDB can contain a vector. It is important to note that all documents within the same collection should utilize vectors of the same type, characterized by consistent dimensions and metrics.</p> <p></p>"},{"location":"pages/develop/sdk/astra-db-client-java/#create-collection","title":"Create Collection","text":"<p>Create a collection in the current database.</p> General Information <ul> <li>A collection name is unique for a database</li> <li>A collection name should match <code>[A-Za-z_]</code></li> <li>Method <code>createCollection()</code> method returns an instance of <code>AstraDBCollection</code></li> <li>Collection is created only if it does not exist</li> <li>If collection exists, a check is performed for vector dimension and metric</li> <li>There are a maximum of 5 collections per database</li> <li>If not provided, default metric is <code>cosine</code></li> <li>Vector <code>dimension</code> and a <code>metric</code> are set at creation and cannot be changed later</li> <li>The <code>dimension</code> is the size of the vector</li> <li>The <code>metric</code> is the way the vector will be compared. It can be <code>cosine</code>, <code>euclidean</code> or <code>dot_product</code></li> </ul> <ul> <li> Signature and Javadoc \ud83d\udd17</li> </ul> <pre><code>AstraDBCollection createCollection(String name);\nAstraDBCollection createCollection(String name, int vectorDimension);\nAstraDBCollection createCollection(String name, int vectorDimension, SimilarityMetric metric);\nAstraDBCollection createCollection(CollectionDefinition def);\n</code></pre> <ul> <li> Sample Code</li> </ul> CreateCollection.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport io.stargate.sdk.data.domain.CollectionDefinition;\nimport io.stargate.sdk.data.domain.SimilarityMetric;\nimport io.stargate.sdk.data.exception.DataApiException;\npublic class CreateCollection {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\n// Create a non-vector collection\nAstraDBCollection collection1 = db.createCollection(\"collection_simple\");\n// Create a vector collection\nAstraDBCollection collection2 = db.createCollection(\n\"collection_vector1\",\n14,\nSimilarityMetric.cosine);\n// Create a vector collection with a builder\nAstraDBCollection collection3 = db.createCollection(CollectionDefinition\n.builder()\n.name(\"collection_vector2\")\n.vector(1536, SimilarityMetric.euclidean)\n.build());\n// Collection names should use snake case ([a-zA-Z][a-zA-Z0-9_]*)\ntry {\ndb.createCollection(\"invalid.name\");\n} catch(DataApiException e) {\n// invalid.name is not valid\n}\n}\n}\n</code></pre> <ul> <li> Data API</li> </ul> <p>Below is the associated REST API payload</p> <pre><code>{\n\"createCollection\": {\n\"name\": \"collection_vector\",\n\"options\": {\n\"vector\": {\n\"dimension\": 14,\n\"metric\": \"cosine\"\n}\n}\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#list-collections","title":"List Collections","text":"<p>List collections in the current database with their attributes. (similarity, dimension, indexing...)</p> General Information <ul> <li>A database can have up to 5 collections.</li> <li>A collection with a vector has a set of options like dimension, similarity and indexing.</li> </ul> <ul> <li> Signature and Javadoc \ud83d\udd17</li> </ul> <pre><code>Stream&lt;String&gt; findAllCollectionNames();\nStream&lt;CollectionDefinition&gt; findAllCollections();\n</code></pre> <ul> <li> Sample Code</li> </ul> FindAllCollections.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport io.stargate.sdk.data.domain.CollectionDefinition;\npublic class FindAllCollections {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\n// Get Collection Names\ndb.findAllCollectionsNames().forEach(System.out::println);\n// Iterate over all collections and print each vector definition\ndb.findAllCollections().forEach(col -&gt; {\nSystem.out.print(\"\\nname=\" + col.getName());\nif (col.getOptions() != null &amp;&amp; col.getOptions().getVector() != null) {\nCollectionDefinition.Options.Vector vector = col.getOptions().getVector();\nSystem.out.print(\", dim=\" + vector.getDimension());\nSystem.out.print(\", metric=\" + vector.getMetric());\n}\n});\n}\n}\n</code></pre> <ul> <li> Data API</li> </ul> <p>Below is the associated REST API payload</p> <pre><code>{\n\"findCollections\": {\n\"options\": {\n\"explain\": true\n}\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#find-collection","title":"Find Collection","text":"<p>Retrieve collection definition from its name.</p> General Information <ul> <li>name is the identifier of the collection.</li> </ul> <ul> <li> Signature and Javadoc \ud83d\udd17</li> </ul> <pre><code>Optional&lt;CollectionDefinition&gt; findCollectionByName(String name);\nboolean isCollectionExists(String name);\n</code></pre> <ul> <li> Sample Code</li> </ul> FindCollection.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport io.stargate.sdk.data.domain.CollectionDefinition;\nimport java.util.Optional;\npublic class FindCollection {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\n// Find a collection\nOptional&lt;CollectionDefinition&gt; collection = db.findCollectionByName(\"collection_vector1\");\n// Check if a collection exists\nboolean collectionExists = db.isCollectionExists(\"collection_vector2\");\n}\n}\n</code></pre> <ul> <li> Data API</li> </ul> <p>Below is the associated REST API payload.</p> <pre><code>{\n\"findCollections\": {\n\"options\": {\n\"explain\": true\n}\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#delete-collection","title":"Delete Collection","text":"<p>Delete a collection from its name</p> General Information <ul> <li>If the collection does not exist, the method will not return any error.</li> </ul> <ul> <li> Signature and Javadoc \ud83d\udd17</li> </ul> <pre><code>void deleteCollection(String name);\n</code></pre> <ul> <li> Sample Code</li> </ul> DeleteCollection.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\npublic class DeleteCollection {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\n// Delete an existing collection\ndb.deleteCollection(\"collection_vector2\");\n}\n}\n</code></pre> <ul> <li> Data API</li> </ul> <p>Below is the associated REST API payload.</p> <pre><code>{\n\"deleteCollection\": {\n\"name\": \"collection_vector2\"\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#working-with-documents","title":"Working with Documents","text":""},{"location":"pages/develop/sdk/astra-db-client-java/#insert-one","title":"Insert One","text":"<p>You can insert unitary record with the function <code>insertOne()</code>. Multiple signatures are available to insert a document.</p> General Informations <ul> <li>If not provided, the identifier is generated as a java UUID</li> <li>The method always return the document identifier.</li> <li>All attributes are optional (schemaless)</li> <li>You attribute names should match <code>[A-Za-z_]</code></li> <li>All Java simple standard types are supported</li> <li>Nested object are supported</li> <li>A field value should not exceed 5Kb</li> <li>Each attribute is indexed and searchable</li> <li>A vector cannot be filled only with 0s, it would lead to division by 0</li> </ul> <ul> <li> Signature</li> </ul> <pre><code>JsonDocumentMutationResult insertOne(JsonDocument doc);\nCompletableFuture&lt;JsonDocumentMutationResult&gt; insertOneASync(JsonDocument doc);\nDocumentMutationResult&lt;DOC&gt; insertOne(Document&lt;DOC&gt; document);\nCompletableFuture&lt;DocumentMutationResult&lt;DOC&gt;&gt; insertOneASync(Document&lt;DOC&gt; document);\n</code></pre> <ul> <li> Sample Code</li> </ul> InsertOne.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport io.stargate.sdk.data.domain.JsonDocumentMutationResult;\nimport io.stargate.sdk.data.domain.JsonDocument;\nimport java.util.Map;\npublic class InsertOne {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\n// Assumes a collection with a vector field of dimension 14\nAstraDBCollection collection = db.collection(\"collection_vector1\");\n// You must delete any existing rows with the same IDs as the\n// rows you want to insert\ncollection.deleteAll();\n// Insert rows defined by key/value\ncollection.insertOne(\nnew JsonDocument()\n.id(\"doc1\") // uuid is generated if not explicitely set\n.vector(new float[]{1f, 0f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f})\n.put(\"product_name\", \"HealthyFresh - Beef raw dog food\")\n.put(\"product_price\", 12.99));\n// Insert rows defined as a JSON String\ncollection.insertOne(\nnew JsonDocument()\n.data(\n\"{\" +\n\"\\\"_id\\\": \\\"doc2\\\", \" +\n\"\\\"$vector\\\": [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], \" +\n\"\\\"product_name\\\": \\\"HealthyFresh - Chicken raw dog food\\\", \" +\n\"\\\"product_price\\\": 9.99\" +\n\"}\"));\n// Insert rows defined as a Map Asynchronously\ncollection.insertOneASync(\nnew JsonDocument()\n.id(\"doc3\")\n.vector(new float[]{1f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f})\n.data(Map.of(\"product_name\", \"HealthyFresh - Chicken raw dog food\")));\n// If you do not provide an ID, they are generated automatically\nJsonDocumentMutationResult result = collection.insertOne(\nnew JsonDocument().put(\"demo\", 1));\nString generatedId = result.getDocument().getId();\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#upsert-one","title":"Upsert One","text":"General Informations <ul> <li><code>insert*</code> will give you an error when id that already exist in the collection is provided.</li> <li><code>upsert*</code> will update the document if it exists or insert it if it does not.</li> </ul> <ul> <li> Signatures</li> </ul> <pre><code>JsonDocumentMutationResult upsertOne(JsonDocument doc);\nCompletableFuture&lt;JsonDocumentMutationResult&gt;  upsertOneASync(JsonDocument doc);\nDocumentMutationResult&lt;DOC&gt;  upsertOne(Document&lt;DOC&gt; document);\nCompletableFuture&lt;DocumentMutationResult&lt;DOC&gt;&gt;  upsertOneASync(Document&lt;DOC&gt; document);\n</code></pre> <ul> <li> Sample Code</li> </ul> InsertOne.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport io.stargate.sdk.data.domain.JsonDocument;\nimport io.stargate.sdk.data.domain.JsonDocumentMutationResult;\nimport org.junit.jupiter.api.Assertions;\nimport static io.stargate.sdk.data.domain.DocumentMutationStatus.CREATED;\nimport static io.stargate.sdk.data.domain.DocumentMutationStatus.UNCHANGED;\nimport static io.stargate.sdk.data.domain.DocumentMutationStatus.UPDATED;\npublic class UpsertOne {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\n// Assumes a collection with a vector field of dimension 14\nAstraDBCollection collection = db.collection(\"collection_vector1\");\n// Insert rows defined by key/value\nJsonDocument doc1 = new JsonDocument()\n.id(\"doc1\") // uuid is generated if not explicitely set\n.put(\"product_name\", \"HealthyFresh - Beef raw dog food\")\n.put(\"product_price\", 12.99);\n// Create the document\nJsonDocumentMutationResult res1 = collection.upsertOne(doc1);\nAssertions.assertEquals(CREATED, res1.getStatus());\n// Nothing happened\nJsonDocumentMutationResult res2 = collection.upsertOne(doc1);\nAssertions.assertEquals(UNCHANGED, res1.getStatus());\n// Document is updated (async)\ndoc1.put(\"new_property\", \"value\");\ncollection.upsertOneASync(doc1).thenAccept(res -&gt;\nAssertions.assertEquals(UPDATED, res.getStatus()));\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#insert-many","title":"Insert Many","text":"General Informations <ul> <li>The underlying REST API is paged. The maximum page size is 20.</li> <li>To perform bulk loading, distribution of the workload is recommended</li> <li><code>insertMany**Chunked**</code> are a helper to distribute the workload</li> <li>If more than 20 documents are provided chunking is applied under the hood</li> </ul> <ul> <li> Signatures</li> </ul> <pre><code>// Use a json String\nList&lt;JsonDocumentMutationResult&gt; insertMany(String json);\nCompletableFuture&lt;List&lt;JsonDocumentMutationResult&gt;&gt; insertManyASync(String json);\n// Use an Array of JsonDocuments\nList&lt;JsonDocumentMutationResult&gt;\ninsertMany(JsonDocument... documents);\nCompletableFuture&lt;List&lt;JsonDocumentMutationResult&gt;&gt;\ninsertManyASync(JsonDocument... documents);\n// Use a list of JsonDocument\nList&lt;JsonDocumentMutationResult&gt; insertManyJsonDocuments(List&lt;JsonDocument&gt; documents);\nCompletableFuture&lt;List&lt;JsonDocumentMutationResult&gt;&gt; insertManyJsonDocumentsASync(List&lt;JsonDocument&gt; documents);\n// Use an Array of Document&lt;T&gt;\nList&lt;DocumentMutationResult&lt;DOC&gt;&gt; insertMany(Document&lt;DOC&gt;... documents);\nCompletableFuture&lt;List&lt;DocumentMutationResult&lt;DOC&gt;&gt;&gt;\ninsertManyASync(Document&lt;DOC&gt;... documents);\n// Use a list of Document&lt;T&gt;\nList&lt;DocumentMutationResult&lt;DOC&gt;&gt; insertMany(List&lt;Document&lt;DOC&gt;&gt; documents);\nCompletableFuture&lt;List&lt;DocumentMutationResult&lt;DOC&gt;&gt;&gt;\ninsertManyASync(List&lt;Document&lt;DOC&gt;&gt; documents);\n</code></pre> <ul> <li> Sample Code</li> </ul> InsertMany.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport io.stargate.sdk.data.domain.JsonDocumentMutationResult;\nimport io.stargate.sdk.data.domain.JsonDocument;\nimport java.util.List;\nimport java.util.stream.Collectors;\nimport java.util.stream.IntStream;\npublic class InsertMany {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBCollection collection = db.createCollection(\"collection_vector1\",14);\n// Insert documents into the collection (IDs are generated automatically)\nList&lt;JsonDocumentMutationResult&gt; identifiers = collection.insertManyJsonDocuments(List.of(\nnew JsonDocument()\n.vector(new float[]{1f, 0f, 1f, 1f, .5f, 1f, 0f, 0.3f, 0f, 0f, 0f, 0f, 0f, 0f})\n.put(\"product_name\", \"Yet another product\")\n.put(\"product_price\", 99.99),\nnew JsonDocument()\n.vector(new float[]{1f, 0f, 1f, 1f, .5f, 1f, 0f, 0.3f, 0f, 0f, 0f, 0f, 0f, 0f})\n.put(\"product_name\", \"product3\")\n.put(\"product_price\", 99.99)));\n// Insert large collection of documents\nList&lt;JsonDocument&gt; largeList = IntStream\n.rangeClosed(1, 1000)\n.mapToObj(id -&gt; new JsonDocument()\n.id(String.valueOf(id))\n.put(\"sampleKey\", id))\n.collect(Collectors.toList());\nint chunkSize   = 20;  // In between 1 and 20\nint threadCount = 10;  // How many chunks processed in parallel\nList&lt;JsonDocumentMutationResult&gt; result = collection\n.insertManyChunkedJsonDocuments(largeList, chunkSize, threadCount);\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#insert-many-chunked","title":"Insert Many Chunked","text":"<ul> <li> Signatures</li> </ul> <pre><code>// Insert a list of json documents\nList&lt;JsonDocumentMutationResult&gt; insertManyChunkedJsonDocuments(List&lt;JsonDocument&gt; documents, int chunkSize, int concurrency);\nCompletableFuture&lt;List&lt;JsonDocumentMutationResult&gt;&gt; insertManyChunkedJsonDocumentsAsync(List&lt;JsonDocument&gt; documents, int chunkSize, int concurrency);\n// Insert a list of documents\nList&lt;DocumentMutationResult&lt;DOC&gt;&gt; insertManyChunked(List&lt;Document&lt;DOC&gt;&gt; documents, int chunkSize, int concurrency);\nCompletableFuture&lt;List&lt;DocumentMutationResult&lt;DOC&gt;&gt;&gt; insertManyChunkedASync(List&lt;Document&lt;DOC&gt;&gt; documents, int chunkSize, int concurrency);\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#upsert-many","title":"Upsert Many","text":"<ul> <li> Signatures</li> </ul> <pre><code>// Use a json String\nList&lt;JsonDocumentMutationResult&gt;\nupsertMany(String json);\nCompletableFuture&lt;List&lt;JsonDocumentMutationResult&gt;&gt;\nupsertManyASync(String json);\n// Use a list of JsonDocument\nList&lt;JsonDocumentMutationResult&gt;\nupsertManyJsonDocuments(List&lt;JsonDocument&gt; documents);\nCompletableFuture&lt;List&lt;JsonDocumentMutationResult&gt;&gt;\nupsertManyJsonDocumentsASync(List&lt;JsonDocument&gt; documents);\n// Use a list of Document&lt;T&gt;\nList&lt;DocumentMutationResult&lt;DOC&gt;&gt;\nupsertMany(List&lt;Document&lt;DOC&gt;&gt; documents);\nCompletableFuture&lt;List&lt;DocumentMutationResult&lt;DOC&gt;&gt;&gt;\nupsertManyASync(List&lt;Document&lt;DOC&gt;&gt; documents);\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#find-by-id","title":"Find By Id","text":"<ul> <li> Signatures</li> </ul> <pre><code>Optional&lt;JsonDocumentResult&gt; findById(String id);\nOptional&lt;DocumentResult&lt;T&gt;&gt; findById(String id, Class&lt;T&gt; bean);\nOptional&lt;DocumentResult&lt;T&gt;&gt; findById(String id, DocumentResultMapper&lt;T&gt; mapper);\nboolean isDocumentExists(String id);\n</code></pre> <ul> <li> Sample Code</li> </ul> FindById.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport io.stargate.sdk.data.domain.JsonDocumentResult;\nimport io.stargate.sdk.data.domain.odm.DocumentResult;\nimport java.util.Optional;\npublic class FindById {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBCollection collection = db.collection(\"collection_vector1\");\n// Fetch a document by ID and return it as JSON\nOptional&lt;JsonDocumentResult&gt; res = collection.findById(\"doc1\");\nres.ifPresent(jsonResult -&gt; System.out.println(jsonResult.getSimilarity()));\n// Fetch a document by ID and map it to an object with ResultMapper\nOptional&lt;DocumentResult&lt;MyBean&gt;&gt; res2 = collection.findById(\"doc1\", record -&gt; {\nMyBean bean = new MyBean(\n(String) record.getData().get(\"product_name\"),\n(Double) record.getData().get(\"product_price\"));\nreturn new DocumentResult&lt;&gt;(record, bean);\n});\n// Fetch a document by ID and map it to a class\nOptional&lt;DocumentResult&lt;MyBean&gt;&gt; res3 = collection.findById(\"doc1\", MyBean.class);\n// Check if a document exists\nboolean exists = collection.isDocumentExists(\"doc1\");\n}\npublic static class MyBean {\n@JsonProperty(\"product_name\") String name;\n@JsonProperty(\"product_price\") Double price;\npublic MyBean(String name, Double price) {\nthis.name = name;\nthis.price = price;\n}\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#find-by-vector","title":"Find By Vector","text":"<ul> <li> Signatures</li> </ul> <pre><code>Optional&lt;JsonDocumentResult&gt; findOneByVector(float[] vector);\nOptional&lt;DocumentResult&lt;T&gt;&gt; findOneByVector(float[] vector, Class&lt;T&gt; bean);\nOptional&lt;DocumentResult&lt;T&gt;&gt; findOneByVector(float[] vector, DocumentResultMapper&lt;T&gt; mapper);\n</code></pre> <ul> <li> Sample Code</li> </ul> FindByVector.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport io.stargate.sdk.data.domain.odm.DocumentResult;\nimport java.util.Optional;\npublic class FindByVector {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBCollection collection = db.collection(\"collection_vector1\");\n// Fetch a row by vector and return JSON\ncollection\n.findOneByVector(new float[]{1f, 0f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f})\n.ifPresent(jsonResult -&gt; System.out.println(jsonResult.getSimilarity()));\n// Fetch a row by ID and map it to an object with ResultMapper\nOptional&lt;DocumentResult&lt;MyBean&gt;&gt; res2 = collection\n.findOneByVector(\nnew float[]{1f, 0f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f},\nrecord -&gt; {\nMyBean bean = new MyBean(\n(String)record.getData().get(\"product_name\"),\n(Double)record.getData().get(\"product_price\"));\nreturn new DocumentResult&lt;&gt;(record, bean);\n}\n);\n// Fetch a row by ID and map the result to a class\nOptional&lt;DocumentResult&lt;MyBean&gt;&gt; res3 = collection.findOneByVector(\nnew float[]{1f, 0f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f},\nMyBean.class);\n}\npublic static class MyBean {\n@JsonProperty(\"product_name\") String name;\n@JsonProperty(\"product_price\") Double price;\npublic MyBean(String name, Double price) {\nthis.name = name;\nthis.price = price;\n}\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#find-one","title":"Find One","text":"Introducing <code>SelectQuery</code> <p>Under the hood every search against the REST Api is done by providing 4 parameters:</p> <ul> <li><code>$filter</code>: which are your criteria (where clause)</li> <li><code>$projection</code>: which list the fields you want to retrieve (select)</li> <li><code>$sort</code>: which order the results in memory (order by) or the vector search (order by ANN)</li> <li><code>$options</code>: that will contains all information like paging, limit, etc.</li> </ul> <p>The <code>SelectQuery</code> class is a builder that will help you to build the query. It is a fluent API that will help you to build the query.</p> <p>As for <code>findById</code> and <code>findByVector</code> there are 3 methods available to retrieve a document. If the <code>SelectQuery</code> has multiple matches objects only the first will be returned. In doubt use <code>find()</code> or even better <code>findPage()</code> not to exhaust all the collection.</p> <pre><code>Optional&lt;JsonDocumentResult&gt; findOne(SelectQuery query);\nOptional&lt;DocumentResult&lt;DOC&gt;&gt; findOne(SelectQuery query, Class&lt;T&gt; clazz);\nOptional&lt;DocumentResult&lt;DOC&gt;&gt; findOne(SelectQuery query, ResultMapper&lt;T&gt; mapper);\n</code></pre> <p>Here is a sample class detailing the usage of the <code>findOne</code> method.</p> FindOne.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport io.stargate.sdk.data.domain.query.Filter;\nimport io.stargate.sdk.data.domain.query.SelectQuery;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\npublic class FindOne {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBCollection collection = db.createCollection(\"collection_vector1\", 14);\n// Retrieve the first document where product_price exists\nFilter filter = new Filter()\n.where(\"product_price\")\n.exists();\ncollection.findOne(SelectQuery.builder()\n.filter(filter).build())\n.ifPresent(System.out::println);\n// Retrieve the first document where product_price is 12.99\nFilter filter2 = new Filter()\n.where(\"product_price\")\n.isEqualsTo(12.99);\ncollection.findOne(SelectQuery.builder()\n.filter(filter2).build())\n.ifPresent(System.out::println);\n// Send the request as a JSON String\ncollection.findOne(\n\"{\" +\n\"\\\"filter\\\":{\" +\n\"\\\"product_price\\\":9.99,\" +\n\"\\\"product_name\\\":\\\"HealthyFresh - Chicken raw dog food\\\"}\" +\n\"}\")\n.ifPresent(System.out::println);\n// Only retrieve the product_name and product_price fields\ncollection.findOne(SelectQuery.builder()\n.select(\"product_name\", \"product_price\")\n.filter(filter2)\n.build())\n.ifPresent(System.out::println);\n// Perform a similarity search\ncollection.findOne(SelectQuery.builder()\n.filter(filter2)\n.orderByAnn(new float[]{1f, 0f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f})\n.build());\n// Perform a complex query with AND and OR\nSelectQuery sq2 = new SelectQuery();\nsq2.setFilter(new HashMap&lt;&gt;());\nMap&lt;String, List&lt;Map&lt;String, Object&gt;&gt;&gt; or1Criteria = new HashMap&lt;&gt;();\nor1Criteria.put(\"$or\", new ArrayList&lt;Map&lt;String, Object&gt;&gt;());\nor1Criteria.get(\"$or\").add(Map.of(\"product_price\", 9.99));\nor1Criteria.get(\"$or\").add(Map.of(\"product_name\", \"HealthyFresh - Beef raw dog food\"));\nMap&lt;String, List&lt;Map&lt;String, Object&gt;&gt;&gt; or2Criteria = new HashMap&lt;&gt;();\nor2Criteria.put(\"$or\", new ArrayList&lt;Map&lt;String, Object&gt;&gt;());\nor2Criteria.get(\"$or\").add(Map.of(\"product_price\", 12.99));\nor2Criteria.get(\"$or\").add(Map.of(\"product_name\", \"HealthyFresh - Beef raw dog food\"));\nList&lt;Map&lt;String, List&lt;Map&lt;String, Object&gt;&gt;&gt;&gt; andCriteria = new ArrayList&lt;&gt;();\nandCriteria.add(or1Criteria);\nandCriteria.add(or2Criteria);\nsq2.getFilter().put(\"$and\", andCriteria);\ncollection.findOne(sq2).ifPresent(System.out::println);\n// Perform a complex query with AND and OR as String\ncollection.findOne(\n\"{\\\"filter\\\":{\" +\n\"\\\"$and\\\":[\" +\n\"{\\\"$or\\\":[\" +\n\"  {\\\"product_price\\\":9.99},\" +\n\"  {\\\"product_name\\\":\\\"HealthyFresh - Beef raw dog food\\\"}\" +\n\"]\" +\n\"},\" +\n\"{\\\"$or\\\":[\" +\n\"  {\\\"product_price\\\":12.99},\" +\n\"  {\\\"product_name\\\":\\\"HealthyFresh - Beef raw dog food\\\"}\" +\n\"]\" +\n\"}\" +\n\"]\" +\n\"}}\")\n.ifPresent(System.out::println);\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#find-filters","title":"Find Filters","text":""},{"location":"pages/develop/sdk/astra-db-client-java/#find","title":"Find","text":"Reminders on <code>SelectQuery</code> <p>Under the hood every search against the REST Api is done by providing 4 parameters:</p> <ul> <li><code>$filter</code>: which are your criteria (where clause)</li> <li><code>$projection</code>: which list the fields you want to retrieve (select)</li> <li><code>$sort</code>: which order the results in memory (order by) or the vector search (order by ANN)</li> <li><code>$options</code>: that will contains all information like paging, limit, etc.</li> </ul> <p>The <code>SelectQuery</code> class is a builder that will help you to build the query. It is a fluent API that will help you to build the query.</p> <pre><code> SelectQuery.builder()\n.where(\"product_price\")\n.isEqualsTo(9.99)\n.build();\n</code></pre> <p></p> Important <p>With the Json API all queries are paged. The maximum page size is 20. The method findAll() and find() will fetch the pages one after the other until <code>pagingState</code> is null. Use those functions with caution. </p> <ul> <li> To retrieve every document of a collection use <code>findAll()</code></li> </ul> <pre><code>// Find All for VectorStore&lt;MyBean&gt;\nStream&lt;JsonResult&gt; all = col1.findAll();\n</code></pre> <ul> <li> Find with a <code>ResultQuery</code></li> </ul> <p>You can search on any field of the document. All fields are indexed. Using a <code>SelectQuery</code> populated through builder you can get some precise results.</p> <p><pre><code>Stream&lt;JsonResult&gt; all = col1.findAll(SelectQuery.builder()\n.where(\"product_price\")\n.isEqualsTo(9.99)\n.build());\n</code></pre> More examples in the following class:</p> Find.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport io.stargate.sdk.data.domain.query.Filter;\nimport io.stargate.sdk.data.domain.query.SelectQuery;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\npublic class Find {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBCollection collection = db.createCollection(\"collection_vector1\", 14);\n// Retrieve the first document with a product_price\nFilter filter = new Filter()\n.where(\"product_price\")\n.exists();\ncollection.find(\nSelectQuery.builder().filter(filter).build()\n).forEach(System.out::println);\n// Retrieve the first document where the product_price is 12.99\nFilter filter2 = new Filter()\n.where(\"product_price\")\n.isEqualsTo(12.99);\ncollection\n.find(SelectQuery.builder().filter(filter2).build())\n.forEach(System.out::println);\n// Only retrieve the product_name and product_price fields\ncollection.find(\nSelectQuery.builder()\n.select(\"product_name\", \"product_price\")\n.filter(filter2)\n.build())\n.forEach(System.out::println);\n// Order the results by similarity\ncollection.find(\nSelectQuery.builder()\n.filter(filter2)\n.orderByAnn(new float[]{1f, 0f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f})\n.build())\n.forEach(System.out::println);\n// Order the results by a specific field\nFilter filter3 = new Filter()\n.where(\"product_name\")\n.isEqualsTo(\"HealthyFresh - Chicken raw dog food\");\ncollection.find(\nSelectQuery.builder()\n.filter(filter3)\n.orderBy(\"product_price\", 1)\n.build())\n.forEach(System.out::println);\n// Complex query with AND and OR:\n//     (product_price == 9.99 OR product_name == \"HealthyFresh - Beef raw dog food\")\n// AND (product_price == 12.99 OR product_name == \"HealthyFresh - Beef raw dog food\")\nSelectQuery sq2 = new SelectQuery();\nsq2.setFilter(new HashMap&lt;&gt;());\nMap&lt;String, List&lt;Map&lt;String, Object&gt;&gt;&gt; or1Criteria = new HashMap&lt;&gt;();\nor1Criteria.put(\"$or\", new ArrayList&lt;Map&lt;String, Object&gt;&gt;());\nor1Criteria.get(\"$or\").add(Map.of(\"product_price\", 9.99));\nor1Criteria.get(\"$or\").add(Map.of(\"product_name\", \"HealthyFresh - Beef raw dog food\"));\nMap&lt;String, List&lt;Map&lt;String, Object&gt;&gt;&gt; or2Criteria = new HashMap&lt;&gt;();\nor2Criteria.put(\"$or\", new ArrayList&lt;Map&lt;String, Object&gt;&gt;());\nor2Criteria.get(\"$or\").add(Map.of(\"product_price\", 12.99));\nor2Criteria.get(\"$or\").add(Map.of(\"product_name\", \"HealthyFresh - Beef raw dog food\"));\nList&lt;Map&lt;String, List&lt;Map&lt;String, Object&gt;&gt;&gt;&gt; andCriteria = new ArrayList&lt;&gt;();\nandCriteria.add(or1Criteria);\nandCriteria.add(or2Criteria);\nsq2.getFilter().put(\"$and\", andCriteria);\ncollection.find(sq2).forEach(System.out::println);\n}\n}\n</code></pre> <ul> <li> To perform semantic search use <code>findVector()</code></li> </ul> FindOne.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport io.stargate.sdk.data.domain.JsonDocumentResult;\nimport io.stargate.sdk.data.domain.query.Filter;\nimport io.stargate.sdk.data.domain.query.SelectQuery;\nimport java.util.stream.Stream;\npublic class FindVector {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBCollection collection = db.createCollection(\"collection_vector1\", 14);\nfloat[] embeddings = new float[]{1f, 0f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f};\nFilter metadataFilter = new Filter().where(\"product_price\").isEqualsTo(9.99);\nint maxRecord = 10;\n// Retrieve all document with product price based on the ann search\ncollection.findVector(SelectQuery.builder()\n.filter(metadataFilter)\n.orderByAnn(embeddings)\n.withLimit(maxRecord)\n.build())\n.forEach(System.out::println);\n// Same using another signature\nStream&lt;JsonDocumentResult&gt; result = collection.findVector(embeddings, metadataFilter, maxRecord);\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#paging","title":"Paging","text":"<p>Every request is paged with the Json API and the maximum page size is 20. The methods return Page that contains the data but also a field called `pagingState <ul> <li> Find Page</li> </ul> <p>The signature are close to the <code>find()</code>. Reason is that <code>find()</code> is using findPage under the hood. The difference is that it will exhaust all the pages  and return a <code>Stream&lt;JsonResult&gt;</code>.</p> <pre><code>Page&lt;JsonResult&gt; jsonResult = findPage(SelectQuery query);\nPage&lt;Result&lt;T&gt;&gt; jsonResult2 = findPage(SelectQuery query, Class&lt;T&gt; clazz);\nPage&lt;Result&lt;T&gt;&gt; jsonResult3 = findPage(SelectQuery query, ResultMapper&lt;T&gt; clazz);\n</code></pre> FindPage.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport io.stargate.sdk.core.domain.Page;\nimport io.stargate.sdk.data.domain.JsonDocumentResult;\nimport io.stargate.sdk.data.domain.odm.DocumentResult;\nimport io.stargate.sdk.data.domain.query.Filter;\nimport io.stargate.sdk.data.domain.query.SelectQuery;\npublic class FindPage {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBCollection collection = db.createCollection(\"collection_vector1\", 14);\n// Retrieve page 1 of a search (up to 20 results)\nFilter filter = new Filter()\n.where(\"product_price\")\n.exists();\nPage&lt;JsonDocumentResult&gt; page1 = collection.findPage(\nSelectQuery.builder()\n.filter(filter)\n.build());\n// Retrieve page 2 of the same search (if there are more than 20 results)\nFilter filter2 = new Filter()\n.where(\"product_price\")\n.isEqualsTo(12.99);\npage1.getPageState().ifPresent(pageState -&gt; {\nPage&lt;JsonDocumentResult&gt; page2 = collection.findPage(\nSelectQuery.builder()\n.filter(filter2)\n.withPagingState(pageState)\n.build());\n});\n// You can map the output as Result&lt;T&gt; using either a Java pojo or mapper\nPage&lt;DocumentResult&lt;MyBean&gt;&gt; page = collection.findPage(\nSelectQuery.builder().filter(filter2).build(),\nMyBean.class);\n}\npublic static class MyBean {\n@JsonProperty(\"product_name\") String name;\n@JsonProperty(\"product_price\") Double price;\npublic MyBean(String name, Double price) {\nthis.name = name;\nthis.price = price;\n}\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#update-one","title":"Update One","text":"<p>Allow to update an existing document:</p> UpdateOne.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport io.stargate.sdk.data.domain.JsonDocument;\nimport io.stargate.sdk.data.domain.query.UpdateQuery;\nimport static io.stargate.sdk.http.domain.FilterOperator.EQUALS_TO;\npublic class UpdateOne {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBCollection collection = db.collection(\"collection_vector1\");\n// You must delete any existing rows with the same IDs as the\n// rows you want to insert\ncollection.deleteAll();\n// Upsert a document based on a query\ncollection.updateOne(UpdateQuery.builder()\n.updateSet(\"product_name\", 12.99)\n.where(\"product_name\", EQUALS_TO, \"HealthyFresh - Beef raw dog food\")\n.build());\n// Upsert a document by ID\ncollection.upsertOne(new JsonDocument()\n.id(\"id1\")\n.put(\"product_name\", 12.99));\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#update-many","title":"Update Many","text":"<p>Allow to update a set of document matching a request.</p> UpdateMany.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport io.stargate.sdk.data.domain.query.Filter;\nimport io.stargate.sdk.data.domain.query.UpdateQuery;\nimport io.stargate.sdk.http.domain.FilterOperator;\npublic class UpdateMany {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBCollection collection = db.collection(\"collection_vector1\");\n// Update multiple documents based on a query\ncollection.updateMany(UpdateQuery.builder()\n.updateSet(\"product_name\", 12.99)\n.filter(new Filter(\"product_name\", FilterOperator.EQUALS_TO, \"HealthyFresh - Beef raw dog food\"))\n.build());\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#delete-one","title":"Delete One","text":"<p>Use to delete an existing document.</p> DeleteOne.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport io.stargate.sdk.data.domain.query.DeleteQuery;\nimport io.stargate.sdk.data.domain.query.DeleteResult;\npublic class DeleteOne {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBCollection collection = db.createCollection(\"collection_vector1\", 14);\n// Delete items from an existing collection with a query\nDeleteResult deletedCount = collection\n.deleteOne(DeleteQuery.deleteById(\"id1\"));\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#delete-many","title":"Delete Many","text":"<p>Used to delete a set of document matching a request.</p> DeleteMany.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport io.stargate.sdk.data.domain.query.DeleteQuery;\nimport io.stargate.sdk.data.domain.query.DeleteResult;\nimport io.stargate.sdk.data.domain.query.Filter;\nimport io.stargate.sdk.http.domain.FilterOperator;\nimport static io.stargate.sdk.http.domain.FilterOperator.EQUALS_TO;\npublic class DeleteMany {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBCollection collection = db.createCollection(\"collection_vector1\", 14);\n// Build our query\nDeleteQuery deleteQuery = DeleteQuery.builder()\n.where(\"product_price\", EQUALS_TO, 9.99)\n.build();\n// Deleting only up to 20 record\nDeleteResult page = collection\n.deleteManyPaged(deleteQuery);\n// Deleting all documents matching query\nDeleteResult allDeleted = collection\n.deleteMany(deleteQuery);\n// Deleting all documents matching query in distributed way\nDeleteResult result = collection\n.deleteManyChunked(deleteQuery, 5);\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#clear","title":"Clear","text":"<p>Used to empty a collection</p> ClearCollection.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\npublic class ClearCollection {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBCollection collection = db.createCollection(\"collection_vector1\", 14);\n// Delete all rows from an existing collection\ncollection.deleteAll();\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#object-mapping","title":"Object Mapping","text":"Overview <p>Instead of interacting with the database with key/values you may want to associate an object to each record in the collection for this you can use <code>CollectionRepository</code>. If we reproduce the sample before</p> <p></p>"},{"location":"pages/develop/sdk/astra-db-client-java/#repository-pattern","title":"Repository Pattern","text":"<p>Instead of working with raw <code>JsonDocument</code> you can work with your own object. The object will be serialized to JSON and stored in the database. You do not want to provide a <code>ResultMapper</code> each time but rather use the repository pattern. We will follow the signature of the <code>CrudRepository</code> from Spring Data. </p> <pre><code>long count();\nvoid delete(T entity);\nvoid deleteAll();\nvoid deleteAll(Iterable&lt;? extends T&gt; entities);\nvoid deleteAllById(Iterable&lt;? extends ID&gt; ids);\nvoid deleteById(ID id);\nboolean existsById(ID id);\nIterable&lt;T&gt; findAll();\nIterable&lt;T&gt; findAllById(Iterable&lt;ID&gt; ids);\nOptional&lt;T&gt; findById(ID id);\n&lt;S extends T&gt; S  save(S entity);\nIterable&lt;S&gt; saveAll(Iterable&lt;S&gt; entities);\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#create-collection_1","title":"Create collection","text":"ObjectMappingCreateCollection.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBRepository;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport io.stargate.sdk.data.domain.CollectionDefinition;\nimport io.stargate.sdk.data.domain.SimilarityMetric;\npublic class ObjectMappingCreateCollection {\nstatic class Product {\n@JsonProperty(\"product_name\") private String name;\n@JsonProperty(\"product_price\") private Double price;\n}\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\n// Create a non-vector collection\nAstraDBRepository&lt;Product&gt; collection1 =\ndb.createCollection(\"collection_simple\", Product.class);\n// Create a vector collection with a builder\nAstraDBRepository&lt;Product&gt; collection2 =\ndb.createCollection(\nCollectionDefinition.builder()\n.name(\"collection_vector2\")\n.vector(1536, SimilarityMetric.euclidean)\n.build(),\nProduct.class);\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#insert-one_1","title":"Insert One","text":"ObjectMappingInsertOne.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBRepository;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport io.stargate.sdk.data.domain.odm.Document;\npublic class ObjectMappingInsertOne {\nstatic class Product {\n@JsonProperty(\"product_name\") private String name;\n@JsonProperty(\"product_price\") private Double price;\nProduct(String name, Double price) {\nthis.name = name;\nthis.price = price;\n}\n}\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBRepository&lt;Product&gt; productRepository =\ndb.createCollection(\"collection_vector1\", 14, Product.class);\n// Upsert document\nproductRepository.save(new Document&lt;Product&gt;()\n.id(\"product1\")\n.vector(new float[]{1f, 0f, 1f, 1f, .5f, 1f, 0f, 0.3f, 0f, 0f, 0f, 0f, 0f, 0f})\n.data(new Product(\"product1\", 9.99)));\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#insert-many_1","title":"Insert Many","text":"ObjectMappingInsertMany.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBRepository;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport io.stargate.sdk.data.domain.DocumentMutationResult;\nimport io.stargate.sdk.data.domain.odm.Document;\nimport java.util.List;\npublic class ObjectMappingInsertMany {\nstatic class Product {\n@JsonProperty(\"product_name\") private String name;\n@JsonProperty(\"product_price\") private Double price;\nProduct(String name, Double price) {\nthis.name = name;\nthis.price = price;\n}\n}\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBRepository&lt;Product&gt; productRepository =\ndb.createCollection(\"collection_vector1\", 14, Product.class);\n// Insert documents into the collection (IDs are generated automatically)\nList&lt;DocumentMutationResult&lt;Product&gt;&gt; identifiers = productRepository.saveAll(\nList.of(\nnew Document&lt;Product&gt;()\n.vector(new float[]{1f, 0f, 1f, 1f, .5f, 1f, 0f, 0.3f, 0f, 0f, 0f, 0f, 0f, 0f})\n.data(new Product(\"product1\", 9.99)),\nnew Document&lt;Product&gt;()\n.vector(new float[]{1f, 0f, 1f, 1f, .5f, 1f, 0f, 0.3f, 0f, 0f, 0f, 0f, 0f, 0f})\n.data(new Product(\"product2\", 12.99))));\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#find-one_1","title":"Find One","text":"<ul> <li> To get a single document use <code>findById()</code> or <code>findByVector()</code></li> </ul> ObjectMappingFindOne.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBRepository;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport io.stargate.sdk.data.domain.odm.DocumentResult;\nimport java.util.Optional;\npublic class ObjectMappingFindOne {\nstatic class Product {\n@JsonProperty(\"product_name\") private String name;\n@JsonProperty(\"product_price\") private Double price;\n}\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBRepository&lt;Product&gt; productRepository =\ndb.createCollection(\"collection_vector1\", 14, Product.class);\n// Retrieve a products from its id\nOptional&lt;DocumentResult&lt;Product&gt;&gt; res1 = productRepository.findById(\"id1\");\n// Retrieve a product from its vector\nfloat[] vector = new float[]{1f, 0f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f};\nOptional&lt;DocumentResult&lt;Product&gt;&gt; res2 = productRepository.findByVector(vector);\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#find_1","title":"Find","text":"<ul> <li> To perform search use <code>find()</code></li> </ul> ObjectMappingFind.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBCollection;\nimport io.stargate.sdk.data.domain.query.Filter;\nimport io.stargate.sdk.data.domain.query.SelectQuery;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\npublic class ObjectMappingFind {\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBCollection collection = db.createCollection(\"collection_vector1\", 14);\n// Retrieve the first document with a product_price\nFilter filter = new Filter()\n.where(\"product_price\")\n.exists();\ncollection.find(\nSelectQuery.builder()\n.filter(filter)\n.build())\n.forEach(System.out::println);\n// Retrieve the first document where product_price is 12.99\nFilter filter2 = new Filter()\n.where(\"product_price\")\n.isEqualsTo(12.99);\ncollection.find(\nSelectQuery.builder()\n.filter(filter2)\n.build())\n.forEach(System.out::println);\n// Only retrieve the product_name and product_price fields\ncollection.find(\nSelectQuery.builder()\n.select(\"product_name\", \"product_price\")\n.filter(filter2)\n.build())\n.forEach(System.out::println);\n// Order the results by similarity\ncollection.find(\nSelectQuery.builder()\n.filter(filter2)\n.orderByAnn(new float[]{1f, 0f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f})\n.build())\n.forEach(System.out::println);\n// Order the results by a specific field\ncollection.find(\nSelectQuery.builder()\n.filter(filter2)\n.orderBy(\"product_price\", 1)\n.build())\n.forEach(System.out::println);\n// Complex query with AND and OR:\n//     (product_price == 9.99 OR product_name == \"HealthyFresh - Beef raw dog food\")\n// AND (product_price == 12.99 OR product_name == \"HealthyFresh - Beef raw dog food\")\nSelectQuery sq2 = new SelectQuery();\nsq2.setFilter(new HashMap&lt;&gt;());\nMap&lt;String, List&lt;Map&lt;String, Object&gt;&gt;&gt; or1Criteria = new HashMap&lt;&gt;();\nor1Criteria.put(\"$or\", new ArrayList&lt;Map&lt;String, Object&gt;&gt;());\nor1Criteria.get(\"$or\").add(Map.of(\"product_price\", 9.99));\nor1Criteria.get(\"$or\").add(Map.of(\"product_name\", \"HealthyFresh - Beef raw dog food\"));\nMap&lt;String, List&lt;Map&lt;String, Object&gt;&gt;&gt; or2Criteria = new HashMap&lt;&gt;();\nor2Criteria.put(\"$or\", new ArrayList&lt;Map&lt;String, Object&gt;&gt;());\nor2Criteria.get(\"$or\").add(Map.of(\"product_price\", 12.99));\nor2Criteria.get(\"$or\").add(Map.of(\"product_name\", \"HealthyFresh - Beef raw dog food\"));\nList&lt;Map&lt;String, List&lt;Map&lt;String, Object&gt;&gt;&gt;&gt; andCriteria = new ArrayList&lt;&gt;();\nandCriteria.add(or1Criteria);\nandCriteria.add(or2Criteria);\nsq2.getFilter().put(\"$and\", andCriteria);\ncollection.find(sq2).forEach(System.out::println);\n}\n}\n</code></pre> <ul> <li> To perform semantic search use <code>findVector()</code></li> </ul> ObjectMappingFindVector.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBRepository;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport io.stargate.sdk.data.domain.odm.DocumentResult;\nimport io.stargate.sdk.data.domain.query.Filter;\nimport java.util.List;\npublic class ObjectMappingFindVector {\nstatic class Product {\n@JsonProperty(\"product_name\") private String name;\n@JsonProperty(\"product_price\") private Double price;\n}\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBRepository&lt;Product&gt; productRepository =\ndb.createCollection(\"collection_vector1\", 14, Product.class);\n// Perform a semantic search\nfloat[] embeddings = new float[]{1f, 0f, 1f, 1f, 1f, 1f, 0f, 0f, 0f, 0f, 0f, 0f, 0f, 0f};\nFilter metadataFilter = new Filter().where(\"product_price\").isEqualsTo(9.99);\nint maxRecord = 10;\nList&lt;DocumentResult&lt;Product&gt;&gt; res = productRepository.findVector(embeddings, metadataFilter, maxRecord);\n// If you do not have max record or metadata filter, you can use the following\nproductRepository.findVector(embeddings, maxRecord);\nproductRepository.findVector(embeddings, metadataFilter);\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#update-one_1","title":"Update One","text":"ObjectMappingUpdateOne.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBRepository;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport io.stargate.sdk.data.domain.odm.Document;\npublic class ObjectMappingUpdateOne {\nstatic class Product {\n@JsonProperty(\"product_name\") private String name;\n@JsonProperty(\"product_price\") private Double price;\nProduct(String name, Double price) {\nthis.name = name;\nthis.price = price;\n}\n}\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBRepository&lt;Product&gt; productRepository =\ndb.createCollection(\"collection_vector1\", 14, Product.class);\n// Upsert a document\nproductRepository.save(new Document&lt;Product&gt;()\n.id(\"product1\")\n.vector(new float[]{1f, 0f, 1f, 1f, .5f, 1f, 0f, 0.3f, 0f, 0f, 0f, 0f, 0f, 0f})\n.data(new Product(\"product1\", 9.99)));\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#update-many_1","title":"Update Many","text":"ObjectMappingUpdateMany.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBRepository;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport dev.langchain4j.agent.tool.P;\nimport io.stargate.sdk.data.domain.DocumentMutationResult;\nimport io.stargate.sdk.data.domain.odm.Document;\nimport java.util.List;\npublic class ObjectMappingUpdateMany {\nstatic class Product {\n@JsonProperty(\"product_name\") private String name;\n@JsonProperty(\"product_price\") private Double price;\nProduct(String name, Double price) {\nthis.name = name;\nthis.price = price;\n}\n}\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBRepository&lt;Product&gt; productRepository =\ndb.createCollection(\"collection_vector1\", 14, Product.class);\n// Insert documents into the collection (IDs are generated automatically)\nList&lt;DocumentMutationResult&lt;Product&gt;&gt; identifiers = productRepository.saveAll(\nList.of(\nnew Document&lt;Product&gt;()\n.vector(new float[]{1f, 0f, 1f, 1f, .5f, 1f, 0f, 0.3f, 0f, 0f, 0f, 0f, 0f, 0f})\n.data(new Product(\"product1\", 9.99)),\nnew Document&lt;Product&gt;()\n.vector(new float[]{1f, 0f, 1f, 1f, .5f, 1f, 0f, 0.3f, 0f, 0f, 0f, 0f, 0f, 0f})\n.data(new Product(\"product2\", 12.99))));\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#delete-one_1","title":"Delete One","text":"ObjectMappingDeleteOne.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBRepository;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport io.stargate.sdk.data.domain.odm.Document;\npublic class ObjectMappingDeleteOne {\nstatic class Product {\n@JsonProperty(\"product_name\") private String name;\n@JsonProperty(\"product_price\") private Double price;\n}\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBRepository&lt;Product&gt; collection1 =\ndb.createCollection(\"collection_simple\", Product.class);\n// Delete a document by ID\ncollection1.deleteById(\"id1\");\n// Delete a specific document\ncollection1.delete(new Document&lt;Product&gt;().id(\"id2\"));\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#delete-many_1","title":"Delete Many","text":"ObjectMappingDeleteMany.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBRepository;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport io.stargate.sdk.data.domain.query.DeleteQuery;\nimport io.stargate.sdk.data.domain.query.DeleteResult;\nimport static io.stargate.sdk.http.domain.FilterOperator.EQUALS_TO;\npublic class ObjectMappingDeleteMany {\nstatic class Product {\n@JsonProperty(\"product_name\") private String name;\n@JsonProperty(\"product_price\") private Double price;\n}\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\n// Create a vector collection\nAstraDBRepository&lt;Product&gt; collection1 =\ndb.createCollection(\"collection_simple\", Product.class);\n// Delete rows based on a query\nDeleteQuery q = DeleteQuery.builder()\n.where(\"product_price\", EQUALS_TO, 9.99)\n.build();\nDeleteResult res = collection1.deleteAll(q);\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#clear_1","title":"Clear","text":"ObjectMappingClearCollection.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDB;\nimport com.dtsx.astra.sdk.AstraDBRepository;\nimport com.fasterxml.jackson.annotation.JsonProperty;\npublic class ObjectMappingClearCollection {\nstatic class Product {\n@JsonProperty(\"product_name\") private String name;\n@JsonProperty(\"product_price\") private Double price;\n}\npublic static void main(String[] args) {\nAstraDB db = new AstraDB(\"TOKEN\", \"API_ENDPOINT\");\nAstraDBRepository&lt;Product&gt; collection1 =\ndb.createCollection(\"collection_simple\", Product.class);\n// Delete all rows in a collection\ncollection1.deleteAll();\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#working-with-databases","title":"Working with databases","text":""},{"location":"pages/develop/sdk/astra-db-client-java/#connection_1","title":"Connection","text":"About token permissions <p>To work with Databases you need to use a token with organization level permissions. You will work with the class <code>AstraDBClient</code></p> <p>To establish a connection with AstraDB using the client SDK, you are required to supply a token. This token enables two primary connection modes:</p> <ul> <li> <p>Direct database-level connection, facilitating access to a specific database. It is the one decribe above and primay way of working with the SDK.</p> </li> <li> <p>Organization-level connection, which allows interaction with multiple databases under your organization. This is what we will detailed now</p> </li> </ul> <p><code>AstraDBClient</code> class is used to facilitate interactions with all components within your Astra organization, rather than limiting operations to a single database. This approach enables a broader scope of management and control across the organization's databases. The token used for this connection must be scoped to the organization with</p> Properties Values Token Role <code>Organization Administrator</code> ConnectingAdmin.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDBAdmin;\npublic class ConnectingAdmin {\npublic static void main(String[] args) {\n// Default Initialization\nAstraDBAdmin client = new AstraDBAdmin(\"TOKEN\");\n// You can omit the token if you defined the `ASTRA_DB_APPLICATION_TOKEN`\n// environment variable or if you are using the Astra CLI.\nAstraDBAdmin defaultClient=new AstraDBAdmin();\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#list-databases","title":"List databases","text":"FindAllDatabases.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDBAdmin;\nimport com.dtsx.astra.sdk.db.domain.Database;\nimport java.util.stream.Stream;\npublic class FindAllDatabases {\npublic static void main(String[] args) {\nAstraDBAdmin client = new AstraDBAdmin(\"TOKEN\");\nboolean exists = client.isDatabaseExists(\"&lt;database_name&gt;\");\n// List all available databases\nStream&lt;Database&gt; dbStream = client.findAllDatabases();\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#create-database","title":"Create database","text":"<p>To create a database you need to use a token with organization level permissions. You will work with the class <code>AstraDBClient</code></p> CreateDatabase.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDBAdmin;\nimport com.dtsx.astra.sdk.db.domain.CloudProviderType;\nimport java.util.UUID;\npublic class CreateDatabase {\npublic static void main(String[] args) {\nAstraDBAdmin client = new AstraDBAdmin(\"TOKEN\");\n// Choose a cloud provider (GCP, AZURE, AWS) and a region\nCloudProviderType cloudProvider = CloudProviderType.GCP;\nString cloudRegion = \"us-east1\";\n// Create a database\nUUID newDbId = client.createDatabase(\"&lt;database_name&gt;\", cloudProvider, cloudRegion);\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#find-database","title":"Find database","text":"FindDatabase.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDBAdmin;\nimport com.dtsx.astra.sdk.db.domain.Database;\nimport java.util.Optional;\nimport java.util.UUID;\nimport java.util.stream.Stream;\npublic class FindDatabase {\npublic static void main(String[] args) {\nAstraDBAdmin client = new AstraDBAdmin(\"TOKEN\");\n// Check if a database exists\nboolean exists = client.isDatabaseExists(\"&lt;database_name&gt;\");\n// Find a database by name (names may not be unique)\nStream&lt;Database&gt; dbStream = client.findDatabaseByName(\"&lt;database_name&gt;\");\nOptional&lt;Database&gt; dbByName = dbStream.findFirst();\n// Find a database by ID\nOptional&lt;Database&gt; dbById = client\n.findDatabaseById(UUID.fromString(\"&lt;replace_with_db_uuid&gt;\"));\n}\n}\n</code></pre> <ul> <li> Accessing object <code>AstraDB</code></li> </ul> <pre><code>AstraDB myDB = client.database(\"getting_started\");\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#delete-database","title":"Delete database","text":"<ul> <li> Delete Databases with <code>deleteDatabase</code></li> </ul> <p>The function can take a database identifier (uuid) or the database name.</p> DeleteDatabase.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDBAdmin;\nimport java.util.UUID;\npublic class DeleteDatabase {\npublic static void main(String[] args) {\nAstraDBAdmin client = new AstraDBAdmin(\"TOKEN\");\n// Delete an existing database\nclient.deleteDatabaseByName(\"&lt;database_name&gt;\");\n// Delete an existing database by ID\nclient.deleteDatabaseById(\nUUID.fromString(\"&lt;replace_with_db_uuid&gt;\"));\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#working-with-keyspaces","title":"Working with Keyspaces","text":""},{"location":"pages/develop/sdk/astra-db-client-java/#create-keyspace","title":"Create Keyspace","text":"<p>Create a keyspace in the current database with the given name.</p> General Information <ul> <li>Default keyspace is <code>default_keyspace</code></li> <li>If the keyspace already exist, the method will return 'KeyspaceAlreadyExistException'</li> </ul> <ul> <li> Signature and Javadoc \ud83d\udd17</li> </ul> <pre><code>void createKeyspace(String databaseName, String keyspaceName);\nvoid createKeyspace(UUID databaseId, String keyspaceName);\n</code></pre> <ul> <li> Sample Code</li> </ul> CreateKeyspace.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDBAdmin;\nimport com.dtsx.astra.sdk.db.domain.CloudProviderType;\nimport java.util.UUID;\npublic class CreateKeyspace {\npublic static void main(String[] args) {\nAstraDBAdmin client = new AstraDBAdmin(\"TOKEN\");\n// Create a Keyspace\nclient.createKeyspace(\"&lt;db_name&gt;\", \"&lt;keyspace_name&gt;\");\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#delete-keyspace","title":"Delete Keyspace","text":"<p>Delete a keyspace in the current database from its name.</p> General Information <ul> <li>Default keyspace is <code>default_keyspace</code></li> <li>If the keyspace does not exist, the method will return 'KeyspaceNotFoundException'</li> </ul> <ul> <li> Signature and Javadoc \ud83d\udd17</li> </ul> <pre><code>void deleteKeyspace(String databaseName, String keyspaceName);\nvoid deleteKeyspace(UUID databaseId, String keyspaceName);\n</code></pre> <ul> <li> Sample Code</li> </ul> DeleteKeyspace.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDBAdmin;\npublic class DeleteKeyspace {\npublic static void main(String[] args) {\nAstraDBAdmin client = new AstraDBAdmin(\"TOKEN\");\n// Create a Keyspace\nclient.deleteKeyspace(\"&lt;db_name&gt;\", \"&lt;keyspace_name&gt;\");\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#find-keyspace","title":"Find Keyspace","text":"General Information <ul> <li>A database is not limited in number of keyspaces.</li> <li>A keyspace is a logical grouping of collections.</li> <li>Default keyspace name is <code>default_keyspace</code></li> </ul> <ul> <li> Signature and Javadoc \ud83d\udd17</li> </ul> <pre><code>boolean isKeyspaceExists(String keyspaceName);\nStream&lt;String&gt; findAllKeyspaceNames();\nString getCurrentKeyspace(String keyspaceName);\nvoid changeKeyspace(String keyspaceName);\n</code></pre> <ul> <li> Sample Code</li> </ul> FindKeyspace.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.dtsx.astra.sdk.AstraDBAdmin;\npublic class DeleteKeyspace {\npublic static void main(String[] args) {\nAstraDBAdmin client = new AstraDBAdmin(\"TOKEN\");\n// Create a Keyspace\nclient.deleteKeyspace(\"&lt;db_name&gt;\", \"&lt;keyspace_name&gt;\");\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#6-class-diagram","title":"6. Class Diagram","text":""},{"location":"pages/develop/sdk/astra-db-client-java/#7-working-with-cassio","title":"7. Working with CassIO","text":"<p>Cassio is framework originally implement in Python to use Open Source Cassandra as a Vector Store. It has been partially ported in Java. Idea is java to use the same table created by CassIO.</p>"},{"location":"pages/develop/sdk/astra-db-client-java/#connection_2","title":"Connection","text":"General Information <ul> <li>CassIO is a framework to use Open Source Cassandra as a Vector Store.</li> <li>Java portage is only 2 tables <code>metadata_vector</code> and <code>clustered_metadata_vector</code></li> <li>The tables are created with a specific schema to store vectors and metadata</li> <li>The indices are created to perform efficient search on the vector</li> </ul> <ul> <li> Signature and Javadoc \ud83d\udd17</li> </ul> <pre><code>CqlSession init(String token, UUID databaseId, String databaseRegion, String keyspace);\n</code></pre> <ul> <li> Sample Code</li> </ul> CassIOConnection.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.datastax.oss.driver.api.core.CqlSession;\nimport com.dtsx.astra.sdk.AstraDBAdmin;\nimport com.dtsx.astra.sdk.cassio.CassIO;\nimport com.dtsx.astra.sdk.utils.TestUtils;\nimport java.util.UUID;\npublic class CassIOConnection {\npublic static void main(String[] args) {\n// Create db if not exists\nUUID databaseId = new AstraDBAdmin(\"TOKEN\")\n.createDatabase(\"database\");\n// Initializing CqlSession\ntry (CqlSession cqlSession = CassIO.init(\"TOKEN\",\ndatabaseId, TestUtils.TEST_REGION,\nAstraDBAdmin.DEFAULT_KEYSPACE)) {\ncqlSession\n.execute(\"SELECT datacenter FROM system.local;\")\n.one()\n.get(\"datacenter\", String.class);\n}\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#metadatavectortable","title":"MetadataVectorTable","text":"General Information <ul> <li>Creating a Cassandra table with the following schema and associated indices</li> </ul> <pre><code>CREATE TABLE vector_store (\nrow_id          timeuuid,\nattributes_blob text,\nbody_blob       text,\nmetadata_s      map&lt;text, text&gt;,\nvector          vector&lt;float, 1536&gt;,\nPRIMARY KEY (row_id)\n);\n</code></pre> <ul> <li> Sample Code</li> </ul> CassIOMetadataVectorTable.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.datastax.oss.driver.api.core.CqlSession;\nimport com.dtsx.astra.sdk.AstraDBAdmin;\nimport com.dtsx.astra.sdk.cassio.AnnQuery;\nimport com.dtsx.astra.sdk.cassio.CassIO;\nimport com.dtsx.astra.sdk.cassio.MetadataVectorRecord;\nimport com.dtsx.astra.sdk.cassio.MetadataVectorTable;\nimport com.dtsx.astra.sdk.utils.TestUtils;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.UUID;\npublic class CassIOMetadataVectorTable {\npublic static void main(String[] args) {\n// Create db if not exists\nUUID databaseId = new AstraDBAdmin(\"TOKEN\")\n.createDatabase(\"database\");\n// Initializing CqlSession\ntry (CqlSession cqlSession = CassIO.init(\"TOKEN\",\ndatabaseId, TestUtils.TEST_REGION,\nAstraDBAdmin.DEFAULT_KEYSPACE)) {\n// Initializing table with the dimension\nMetadataVectorTable vector_Store = CassIO\n.metadataVectorTable(\"vector_store\", 1536);\nvector_Store.create();\n// Insert Vectors\nString partitionId = UUID.randomUUID().toString();\nMetadataVectorRecord record = new MetadataVectorRecord();\nrecord.setVector(List.of(0.1f, 0.2f, 0.3f, 0.4f));\nrecord.setMetadata(Map.of(\"key\", \"value\"));\nrecord.setBody(\"Sample text fragment\");\nrecord.setAttributes(\"handy field for special attributes\");\nvector_Store.put(record);\n// Semantic Search\nAnnQuery query = AnnQuery\n.builder()\n.embeddings(List.of(0.1f, 0.2f, 0.3f, 0.4f))\n.metaData(Map.of(\"key\", \"value\"))\n.build();\nvector_Store.similaritySearch(query).forEach(result -&gt; {\nSystem.out.println(\"Similarity : \" + result.getSimilarity());\nSystem.out.println(\"Record : \" + result.getEmbedded().getBody());\n});\n}\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#clusteredmetadatavectortable","title":"ClusteredMetadataVectorTable","text":"General Information <ul> <li>Creating a Cassandra table</li> </ul> <pre><code>CREATE TABLE goodbards.vector_store_openai_by_tenant (\npartition_id text,\nrow_id timeuuid,\nattributes_blob text,\nbody_blob text,\nmetadata_s map&lt;text, text&gt;,\nvector vector&lt;float, 1536&gt;,\nPRIMARY KEY (partition_id, row_id)\n) WITH CLUSTERING ORDER BY (row_id DESC)\n</code></pre> <ul> <li> Sample Code</li> </ul> CassIOClusteredMetadataVectorTable.java<pre><code>package com.dtsx.astra.sdk.documentation;\nimport com.datastax.oss.driver.api.core.CqlSession;\nimport com.dtsx.astra.sdk.AstraDBAdmin;\nimport com.dtsx.astra.sdk.cassio.AnnQuery;\nimport com.dtsx.astra.sdk.cassio.AnnResult;\nimport com.dtsx.astra.sdk.cassio.CassIO;\nimport com.dtsx.astra.sdk.cassio.ClusteredMetadataVectorRecord;\nimport com.dtsx.astra.sdk.cassio.ClusteredMetadataVectorTable;\nimport com.dtsx.astra.sdk.utils.TestUtils;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.UUID;\nimport static com.dtsx.astra.sdk.cassio.AbstractCassandraTable.PARTITION_ID;\nimport static com.dtsx.astra.sdk.utils.TestUtils.getAstraToken;\npublic class CassIOClusteredMetadataVectorTable {\npublic static void main(String[] args) {\n// Create db if not exists\nUUID databaseId = new AstraDBAdmin(\"TOKEN\")\n.createDatabase(\"database\");\n// Initializing CqlSession\ntry (CqlSession cqlSession = CassIO.init(\"TOKEN\",\ndatabaseId, TestUtils.TEST_REGION,\nAstraDBAdmin.DEFAULT_KEYSPACE)) {\n// Initializing table with the dimension\nClusteredMetadataVectorTable vector_Store = CassIO\n.clusteredMetadataVectorTable(\"vector_store\", 1536);\nvector_Store.create();\n// Insert Vectors\nString partitionId = UUID.randomUUID().toString();\nClusteredMetadataVectorRecord record = new ClusteredMetadataVectorRecord();\nrecord.setVector(List.of(0.1f, 0.2f, 0.3f, 0.4f));\nrecord.setMetadata(Map.of(\"key\", \"value\"));\nrecord.setPartitionId(partitionId);\nrecord.setBody(\"Sample text fragment\");\nrecord.setAttributes(\"handy field for special attributes\");\nvector_Store.put(record);\n// Semantic Search\nAnnQuery query = AnnQuery\n.builder()\n.embeddings(List.of(0.1f, 0.2f, 0.3f, 0.4f))\n.metaData(Map.of(PARTITION_ID, partitionId))\n.build();\nvector_Store.similaritySearch(query).forEach(result -&gt; {\nSystem.out.println(\"Similarity : \" + result.getSimilarity());\nSystem.out.println(\"Record : \" + result.getEmbedded().getBody());\n});\n}\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-db-client-java/#8-working-with-langchain4j","title":"8. Working with Langchain4j","text":""},{"location":"pages/develop/sdk/astra-sdk-java/","title":"\u2022 Astra SDK Java","text":"<p>Astra Java SDK is a framework that wraps different apis and interfaces exposed by Astra and provides fluent API and ease of usage.</p> <p></p>"},{"location":"pages/develop/sdk/astra-sdk-java/#1-pre-requisites","title":"1. Pre-requisites","text":"Setup your <code>JAVA</code> Development environment <ul> <li> Install Java Development Kit (JDK) 8+</li> </ul> <p>Use java reference documentation targetting your operating system to install a Java Development Kit. You can then validate your installation with the following command.</p> <pre><code>java --version\n</code></pre> <ul> <li> Install Apache Maven (3.8+)</li> </ul> <p>Samples and tutorials have been designed with <code>Apache Maven</code>. Use the reference documentation top install maven validate your installation with </p> <pre><code>mvn -version\n</code></pre> Setup Datastax <code>Astra DB</code> <ul> <li> Create your DataStax Astra account: </li> </ul> <p>Sign Up</p> <ul> <li> Create an Astra Token</li> </ul> <p>An astra token acts as your credentials, it holds the different permissions. The scope of a token is the whole organization (tenant) but permissions can be edited to limit usage to a single database.</p> <p>To create a token, please follow this guide</p> <p>The Token is in fact three separate strings: a <code>Client ID</code>, a <code>Client Secret</code> and the <code>token</code> proper. You will need some of these strings to access the database, depending on the type of access you plan. Although the Client ID, strictly speaking, is not a secret, you should regard this whole object as a secret and make sure not to share it inadvertently (e.g. committing it to a Git repository) as it grants access to your databases.</p> <pre><code>{\n\"ClientId\": \"ROkiiDZdvPOvHRSgoZtyAapp\",\n\"ClientSecret\": \"fakedfaked\",\n\"Token\":\"AstraCS:fake\"\n}\n</code></pre> <p>It is handy to have your token declare as an environment variable (replace with proper value):</p> <pre><code>export ASTRA_TOKEN=\"AstraCS:replace_me\"\n</code></pre> <ul> <li> Create a Database and a keyspace</li> </ul> <p>With your account you can run multiple databases, a Databases is an Apache Cassandra cluster. It can live in one or multiple regions (dc). In each Database you can have multiple keyspaces. In the page we will use the database name <code>db_demo</code> and the keyspace <code>keyspace_demo</code>.</p> <p>You can create the DB using the user interface and here is a tutorial. You can also use Astra command line interface. To install and setup the CLI run the following:</p> <pre><code>curl -Ls \"https://dtsx.io/get-astra-cli\" | bash\nsource ~/.astra/cli/astra-init.sh\nastra setup --token ${ASTRA_TOKEN}\n</code></pre> <p>To create DB and keyspace with the CLI:</p> <pre><code>astra db create db_demo -k keyspace_demo --if-not-exists\n</code></pre> <ul> <li> Download the Secure Connect Bundle for current database</li> </ul> <p>A Secure Connect Bundle contains the certificates and endpoints informations to open a mTLS connection. Often mentionned as <code>scb</code> its scope is a database AND a region. If your database is deployed on multiple regions you will have to download the bundle for each one and initiate the connection accordingly. Instructions to download Secure Connect Bundle are here</p> <p></p> <p>You can download the secure connect bundle from the user interface and here is a tutorial. You can also use Astra command line interface.</p> <pre><code>astra db download-scb db_demo -f /tmp/secure-connect-bundle-db-demo.zip\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#2-astra-db","title":"2. Astra DB","text":""},{"location":"pages/develop/sdk/astra-sdk-java/#21-project-setup","title":"2.1 - Project Setup","text":"Project Setup <ul> <li> If you create a project from scratch you can start with our template that include the <code>astra-sdk</code> dependency.: </li> </ul> <pre><code>mvn archetype:generate \\\n  -DarchetypeGroupId=com.datastax.astra \\\n  -DarchetypeArtifactId=astra-sdk-quickstart \\\n  -DarchetypeVersion=0.6.2 \\\n  -DinteractiveMode=false \\\n  -DgroupId=foo.bar \\\n  -DartifactId=astra-quickstart\\\n  -Dversion=1.0-SNAPSHOT\n</code></pre> <ul> <li> OR, if you already have a project simple Update your <code>pom.xml</code> file with the latest version of the SDK </li> </ul> <pre><code>&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.astra&lt;/groupId&gt;\n&lt;artifactId&gt;astra-sdk&lt;/artifactId&gt;\n&lt;version&gt;${latestSDK}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#22-cql-native-drivers","title":"2.2 - CQL Native Drivers","text":"Sample Code <ul> <li> Create a class <code>AstraSdkDrivers.java</code> with the following code.</li> </ul> AstraSdkDrivers.java<pre><code>package com.datastax.astra;\nimport com.datastax.astra.sdk.AstraClient;\nimport com.datastax.oss.driver.api.core.CqlSession;\npublic class AstraSdkDrivers {\npublic static void main(String[] args) {\n// Connect\ntry (AstraClient astraClient = AstraClient.builder()\n.withClientId(\"client_id\")\n.withClientSecret(\"client_secret\")\n.withDatabaseId(\"database_id\")\n.withDatabaseRegion(\"database_region\")\n.withCqlKeyspace(\"demo\")\n.enableCql()\n.build()) {\ntry(CqlSession cqlSession = astraClient.cqlSession()) {\nSystem.out.println(\"+ Cql Version (cql)   : \" + cqlSession\n.execute(\"SELECT cql_version from system.local\")\n.one().getString(\"cql_version\"));\n}\n}\n}\n}\n</code></pre> <p> \u00a0Download Project   </p> What you need to know <p>\ud83d\udd11 About Credentials</p> <ul> <li>The pair <code>clientId</code>/ <code>clientSecret</code> hold your credentials. It can be replaced by the value of the token only.</li> </ul> <pre><code>AstraClient.builder().withToken(\"AstraCS:...\");\n</code></pre> <ul> <li>There is no need to download the cloud securebundle in advance as it will be downloaded for you in folder <code>~.astra/scb</code> by default. Stil, but you can also provide the file location with <code>.withCqlCloudSecureConnectBundle()</code>:</li> </ul> <pre><code>AstraClient.builder().withCqlCloudSecureConnectBundle(\"/tmp/scb.zio\");\n</code></pre> <ul> <li>Notice than <code>enableCQL()</code> must be explictely provided. The <code>sdk</code> will open only the asked interfaces in order to limit the resource consumption.</li> </ul> <p>\u2699\ufe0f About Database identifiers</p> <ul> <li><code>databaseId</code>/<code>databaseRegion</code> will be required to locate the proper endpoint. You can find them for a particular database with either the cli.</li> </ul> <pre><code>$astra db list\n\n+---------------------+--------------------------------------+---------------------+-----------+\n| Name                | id                                   | Default Region      | Status    |\n+---------------------+--------------------------------------+---------------------+-----------+\n| db_demo             | 3043a40f-39bf-464e-8337-dc283167b2c3 | us-east1            | ACTIVE    |\n+---------------------+--------------------------------------+---------------------+-----------+\n\n$astra db describe db_demo\n\n+------------------------+-----------------------------------------+\n| Attribute              | Value                                   |\n+------------------------+-----------------------------------------+\n| Name                   | db_demo                                 |\n| id                     | 3043a40f-39bf-464e-8337-dc283167b2c3    |\n| Status                 | ACTIVE                                  |\n| Default Cloud Provider | GCP                                     |\n| Default Region         | us-east1                                |\n| Default Keyspace       | keyspace_demo                           |\n| Creation Time          | 2023-04-17T09:03:14Z                    |\n| Keyspaces              | [0] demo                                |\n| Regions                | [0] us-east1                            |\n+------------------------+-----------------------------------------+\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#23-stargate-rest-api","title":"2.3 - Stargate Rest API","text":"<p>The <code>REST API</code> (also known as <code>api Data</code>) is wrapping exposing CQL language as Rest Resources. To get more information about this API check the dedicated page</p> Sample Code <ul> <li> Create a class <code>AstraSdkRestApi.java</code> with the following code.</li> </ul> AstraSdkDrivers.java<pre><code>package com.datastax.astra;\nimport com.datastax.astra.sdk.AstraClient;\nimport io.stargate.sdk.core.domain.RowResultPage;\nimport io.stargate.sdk.rest.StargateRestApiClient;\nimport io.stargate.sdk.rest.TableClient;\nimport io.stargate.sdk.rest.domain.CreateTable;\nimport io.stargate.sdk.rest.domain.SearchTableQuery;\nimport java.util.HashMap;\nimport java.util.Map;\npublic class AstraSdkRestApi {\npublic static void main(String[] args) {\n// Connect\ntry (AstraClient astraClient = AstraClient.builder()\n.withClientId(\"client_id\")\n.withClientSecret(\"client_secret\")\n.withDatabaseId(\"database_id\")\n.withDatabaseRegion(\"database_region\")\n.build()) {\nStargateRestApiClient restApiClient =\nastraClient.apiStargateData();\n// -- Create a table\nCreateTable createTable = CreateTable.builder()\n.name(\"my_table\")\n.addPartitionKey(\"foo\", \"text\")\n.addColumn(\"bar\", \"text\")\n.ifNotExist(true)\n.build();\nrestApiClient.keyspace(\"demo\")\n.createTable(\"my_table\", createTable);\n// we can now work on the table\nTableClient tableClient = restApiClient\n.keyspace(\"demo\")\n.table(\"my_table\");\n// -- Insert a row\nMap&lt;String, Object&gt; record = new HashMap&lt;&gt;();\nrecord.put(\"foo\", \"Hello\");\nrecord.put(\"bar\", \"World\");\ntableClient.upsert(record);\n// -- Retrieve rows\nSearchTableQuery query = SearchTableQuery.builder().\nselect(\"foo\", \"bar\")\n.where(\"foo\")\n.isEqualsTo(\"Hello\")\n.build();\nRowResultPage result = tableClient.search(query);\nSystem.out.println(result.getResults()\n.get(0).get(\"bar\"));\n}\n}\n}\n</code></pre> <p> \u00a0Download Project   </p> <p>More information about Rest API</p>"},{"location":"pages/develop/sdk/astra-sdk-java/#24-stargate-document-api","title":"2.4 - Stargate Document API","text":"<p>The <code>DOCUMENT API</code> exposes an Rest Resources to use Cassandra as a document-oriented database To get more information about this API check the dedicated page.</p> Sample Code <ul> <li> Create a class <code>AstraSdkDocApi.java</code> with the following code.</li> </ul> AstraSdkDrivers.java<pre><code>package com.datastax.astra;\nimport com.datastax.astra.sdk.AstraClient;\nimport io.stargate.sdk.doc.CollectionClient;\nimport io.stargate.sdk.doc.Document;\nimport io.stargate.sdk.doc.StargateDocumentApiClient;\nimport java.util.stream.Collectors;\npublic class AstraSdkDocApi {\n// Given a Java POJO\npublic static final class User {\nString email;\npublic String getEmail() { return email; }\npublic void setEmail(String email) { this.email = email; }\n}\npublic static void main(String[] args) {\n// Connect\ntry (AstraClient astraClient = AstraClient.builder()\n.withClientId(\"client_id\")\n.withClientSecret(\"client_secret\")\n.withDatabaseId(\"database_id\")\n.withDatabaseRegion(\"database_region\")\n.build()) {\nStargateDocumentApiClient apiDoc =\nastraClient.apiStargateDocument();\n// List namespaces\nSystem.out.println(\"- List namespaces  : \" + apiDoc\n.namespaceNames()\n.collect(Collectors.toList()));\n// List Collection in a Keyspace\nSystem.out.println(\"- List collections : \" + apiDoc\n.namespace(\"demo\")\n.collectionNames()\n.collect(Collectors.toList()));\n// Create collection\nCollectionClient userCollection = apiDoc\n.namespace(\"demo\")\n.collection(\"user\");\nif (!userCollection.exist()) userCollection.create();\n// Working with documents\nUser userA = new User();userA.setEmail(\"a@a.com\");\nUser userB = new User();userB.setEmail(\"b@b.com\");\n// Create a Document and let Astra create the ID\nString userADocumentId = userCollection.create(userA);\nSystem.out.println(\"Document Created: \" + userADocumentId);\n// Create a Document with a specific ID\nuserCollection.document(\"b@b.com\").upsert(userB);\n// List Documents\nSystem.out.println(\"Documents:\");\nfor(Document&lt;User&gt; doc : userCollection.findPage(User.class).getResults()) {\nSystem.out.println(\"\" +\n\"- id: \" + doc.getDocumentId() +\n\", email : \" + doc.getDocument().getEmail());\n}\n}\n}\n}\n</code></pre> <p> \u00a0Download Project   </p> <p>Document Repository</p> <p>With modern java applications you want to interact with the DB using the repository pattern where most operations have been implemented for you <code>findAll(), findById()...</code>. Astra SDK provide this feature for the document API.</p> Sample Document Repository <ul> <li> Create a class <code>AstraSdkDocRepository.java</code> with the following code.</li> </ul> AstraSdkDrivers.java<pre><code>package com.datastax.astra;\nimport com.datastax.astra.sdk.AstraClient;\nimport io.stargate.sdk.doc.StargateDocumentRepository;\npublic class AstraSdkDocRepository {\n// Given a Java POJO\npublic static final class User {\nString email;\npublic String getEmail() { return email; }\npublic void setEmail(String email) { this.email = email; }\n}\npublic static void main(String[] args) {\n// Connect\ntry (AstraClient astraClient = AstraClient.builder()\n.withClientId(\"client_id\")\n.withClientSecret(\"client_secret\")\n.withDatabaseId(\"database_id\")\n.withDatabaseRegion(\"database_region\")\n.withCqlKeyspace(\"demo\")\n.enableCql()\n.build()) {\n// Doc Repository\nStargateDocumentRepository&lt;User&gt; userRepo =\nnew StargateDocumentRepository&lt;User&gt;(\nastraClient.apiStargateDocument().namespace(\"demo\"),\nUser.class);\n// Working with documents\nUser userA = new User();\nuserA.setEmail(\"a@a.com\");\nUser userB = new User();\nuserB.setEmail(\"b@b.com\");\n// Create Documents\nuserRepo.insert(userA);\nuserRepo.insert(\"b@b.com\", userB);\n// List Documents\nuserRepo.findAll().forEach(doc -&gt; {\nSystem.out.println(\n\"id=\" + doc.getDocumentId()\n+ \", email= \" + doc.getDocument().getEmail());\n});\n}\n}\n}\n</code></pre> <p> \u00a0Download Project   </p>"},{"location":"pages/develop/sdk/astra-sdk-java/#25-stargate-grpc-api","title":"2.5 - Stargate Grpc APi","text":"<p>The <code>GRPC API</code> exposes a grpc endpoint to query some CQL. From there it is very similar from native drivers. To know more about it check the dedicated page.</p> Sample Code <ul> <li> Create a class <code>AstraSdkGrpcApi.java</code> with the following code.</li> </ul> AstraSdkGrpcApi.java<pre><code>package com.datastax.astra;\nimport com.datastax.astra.sdk.AstraClient;\nimport io.stargate.sdk.grpc.StargateGrpcApiClient;\npublic class AstraSdkGrpcApi {\npublic static void main(String[] args) {\n// Connect\ntry (AstraClient astraClient = AstraClient.builder()\n.withClientId(\"client_id\")\n.withClientSecret(\"client_secret\")\n.withDatabaseId(\"database_id\")\n.withDatabaseRegion(\"database_region\")\n.build()) {\nStargateGrpcApiClient grpcClient = astraClient.apiStargateGrpc();\nSystem.out.println(\"+ Cql Version (grpc)  : \" + grpcClient\n.execute(\"SELECT cql_version from system.local\")\n.one().getString(\"cql_version\"));\n}\n}\n}\n</code></pre> <p> \u00a0Download Project    </p>"},{"location":"pages/develop/sdk/astra-sdk-java/#26-stargate-graphql-api","title":"2.6 - Stargate GraphQL Api","text":"<p>The <code>GRAPHQL API</code> exposes a graphQL endpoint to query CQL over graphQL. To know more about this api please check the dedicated page.</p> Sample Code <ul> <li> Create a class <code>AstraSdkGraphQLApi.java</code> with the following code.</li> </ul> AstraSdkGraphQLApi.java<pre><code>package com.datastax.astra;\nimport com.datastax.astra.sdk.AstraClient;\nimport io.stargate.sdk.gql.StargateGraphQLApiClient;\npublic class AstraSdkGraphQLApi {\npublic static void main(String[] args) {\n// Connect\ntry (AstraClient astraClient = AstraClient.builder()\n.withClientId(\"client_id\")\n.withClientSecret(\"client_secret\")\n.withDatabaseId(\"database_id\")\n.withDatabaseRegion(\"database_region\")\n.build()) {\nStargateGraphQLApiClient graphClient = astraClient.apiStargateGraphQL();\ngraphClient.keyspaceDDL().keyspaces().forEach(k-&gt; System.out.println(k.getName()));\n}\n}\n}\n</code></pre> <p> \u00a0Download Project   </p>"},{"location":"pages/develop/sdk/astra-sdk-java/#3-astra-streaming","title":"3. Astra Streaming","text":""},{"location":"pages/develop/sdk/astra-sdk-java/#31-project-setup","title":"3.1 - Project setup","text":"Project Setup <ul> <li> Update your <code>pom.xml</code> file with the latest version of the SDK </li> </ul> <pre><code>&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.astra&lt;/groupId&gt;\n&lt;artifactId&gt;astra-sdk-pulsar&lt;/artifactId&gt;\n&lt;version&gt;${latestSDK}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#32-pulsarclient","title":"3.2 - PulsarClient","text":"<p>Pulsar Api are very simple and the need of an sdk is limited. It could be used to get the proper version of <code>pulsar-client</code>, there are no extra setup steps.</p> PulsarClient Setup <pre><code>String pulsarUrl   = \"pulsar+ssl://&lt;cluster&gt;.streaming.datastax.com:6651\";\nString pulsarToken = \"&lt;your_token&gt;\"\ntry(PulsarClient cli = new PulsarClientProvider(pulsarUrl, pulsarToken).get()) {\n// work with client\n};\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#33-pulsar-admin","title":"3.3 - Pulsar Admin","text":"<p>Pulsar Api are very simple and the need of an sdk is limited. It could be used to to setup the <code>pulsar-admin</code> for you.</p> Pulsar Admin Setup <pre><code>String pulsarUrlAdmin = \"https://&lt;cluster&gt;.api.streaming.datastax.com\";\nString pulsarToken = \"&lt;your_token&gt;\"\nPulsarAdmin pa = new PulsarAdminProvider(pulsarUrlAdmin,pulsarToken).get();\n// Get Infos\nSystem.out.println(pa.tenants().getTenantInfo(\"tenant_name\").toString());\n// List Namespaces\npa.namespaces().getNamespaces(\"clun-gcp-east1\").forEach(System.out::println);\n// List Topics\npa.topics().getList(\"clun-gcp-east1/astracdc\").forEach(System.out::println);\n// Details on a topic\nPersistentTopicInternalStats stats = pa.topics()\n.getInternalStats(\"persistent://&lt;tenant&gt;/&lt;namespace&gt;/&lt;topic&gt;\");\nSystem.out.println(stats.totalSize);\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#43-producer","title":"4.3 - Producer","text":"<p>The Astra user interface already shows the expected code. There is very little an SDK can provide here. A sample to code to copy.</p> Sample Code <pre><code>import org.apache.pulsar.client.api.*;\nimport java.io.IOException;\npublic class SimpleProducer {\nprivate static final String CLUSTER      = \"&lt;cluster&gt;\";\nprivate static final String TENANT       = \"&lt;yourtenant&gt;\";\nprivate static final String NAMESPACE    = \"&lt;yournamespace&gt;\";\nprivate static final String TOPIC        = \"&lt;your_topic&gt;\";\nprivate static final String PULSAR_TOKEN = \"&lt;your_token&gt;\";\nprivate static final String SERVICE_URL  = \"pulsar+ssl://\" + CLUSTER + \"streaming.datastax.com:6651\";\npublic static void main(String[] args) throws IOException {\ntry(PulsarClient cli = new PulsarClientProvider(pulsarUrl, pulsarToken).get()) {\ntry( Producer&lt;byte[]&gt; producer = cli\n.newProducer()\n.topic(\"persistent://\" + TENANT + \"/\"+NAMESPACE+\"/\" + TOPIC)\n.create()) {\n// Send a message to the topic\nproducer.send(\"Hello World\".getBytes());\n}\n};\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#44-consumer","title":"4.4 - Consumer","text":"<p>The Astra user interface already shows the expected code. There is very little an SDK can provide here. A sample to code to copy.</p> Sample Consumer <pre><code>import org.apache.pulsar.client.api.*;\nimport java.io.IOException;\nimport java.util.concurrent.TimeUnit;\npublic class SimpleConsumer {\nprivate static final String CLUSTER      = \"&lt;cluster&gt;\";\nprivate static final String TENANT       = \"&lt;yourtenant&gt;\";\nprivate static final String NAMESPACE    = \"&lt;yournamespace&gt;\";\nprivate static final String TOPIC        = \"&lt;your_topic&gt;\";\nprivate static final String SUBSCRIPTION = \"&lt;your_subscription&gt;\";\nprivate static final String PULSAR_TOKEN = \"&lt;your_token&gt;\";\npublic static void main(String[] args) throws IOException {\ntry(PulsarClient cli = new PulsarClientProvider(\n\"pulsar+ssl://\" + CLUSTER + \"streaming.datastax.com:6651\", pulsarToken).get()) {\ntry(Consumer consumer = cli\n.newConsumer()\n.topic(\"persistent://\" + TENANT + \"/\"+NAMESPACE+\"/\" + TOPIC)\n.subscriptionName(SUBSCRIPTION)\n.subscribe()) {\nboolean receivedMsg = false;\n// Loop until a message is received\ndo {\n// Block for up to 1 second for a message\nMessage msg = consumer.receive(1, TimeUnit.SECONDS);\nif(msg != null){\nSystem.out.printf(\"Message received: %s\", new String(msg.getData()));\n// Acknowledge the message to remove it from the message backlog\nconsumer.acknowledge(msg);\nreceivedMsg = true;\n}\n} while (!receivedMsg);\n}\n}\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#4-spring-boot","title":"4. Spring Boot","text":""},{"location":"pages/develop/sdk/astra-sdk-java/#41-create-project","title":"4.1 - Create Project","text":"Spring Boot 2x <ul> <li> To Create a project from scratch start with our template:</li> </ul> <p>The 3 last parameters in this command define your project groupId, artifactId and version. </p> <p><code>0.6</code> is the latest version of the starter  </p> <pre><code>mvn archetype:generate \\\n-DarchetypeGroupId=com.datastax.astra \\\n-DarchetypeArtifactId=spring-boot-2x-archetype \\\n-DarchetypeVersion=0.6 \\\n-DinteractiveMode=false \\\n-DgroupId=fr.clunven \\\n-DartifactId=my-demo-springboot2x \\\n-Dversion=1.0-SNAPSHOT\n</code></pre> Spring Boot 3x <ul> <li> To Create a project from scratch start with our template:</li> </ul> <p>The 3 last parameters in this command define your project groupId, artifactId and version. </p> <p><code>0.6</code> is the latest version of the starter  </p> <pre><code>mvn archetype:generate \\\n-DarchetypeGroupId=com.datastax.astra \\\n-DarchetypeArtifactId=spring-boot-3x-archetype \\\n-DarchetypeVersion=0.6 \\\n-DinteractiveMode=false \\\n-DgroupId=fr.clunven \\\n-DartifactId=my-demo-springboot3x \\\n-Dversion=1.0-SNAPSHOT\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#42-setup-project","title":"4.2 - Setup Project","text":"Spring Boot 2x <p>If you created your project with the archetypes the <code>pom.xml</code> is already correct. </p> <ul> <li> Update your <code>pom.xml</code> file with the latest version of the SDK </li> </ul> <pre><code>&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.astra&lt;/groupId&gt;\n&lt;artifactId&gt;astra-spring-boot-starter&lt;/artifactId&gt;\n&lt;version&gt;${latest-stater}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> Spring Boot 3x <p>If you created your project with the archetypes the <code>pom.xml</code> is already correct. </p> <ul> <li> Update your <code>pom.xml</code> file with the latest version of the SDK </li> </ul> <pre><code>&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.astra&lt;/groupId&gt;\n&lt;artifactId&gt;astra-spring-boot-3x-starter&lt;/artifactId&gt;\n&lt;version&gt;${latest-stater}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#43-configuration","title":"4.3 - Configuration","text":"Configuration of  <code>application.xml</code> <p>If you created the project with the archetypes the file is already populated. </p> <ul> <li> Update your <code>application.xml</code> with the following keys</li> </ul> <pre><code>astra:\napi:\napplication-token: ${ASTRA_DB_APPLICATION_TOKEN}\ndatabase-id: ${ASTRA_DB_ID}\ndatabase-region: ${ASTRA_DB_REGION}\ncross-region-failback: false\ncql:\nenabled: true\ndownload-scb:\nenabled: true\ndriver-config:\nbasic:\nsession-keyspace: ${ASTRA_DB_KEYSPACE}\n</code></pre> <p>As you notice there are 4 variables to be replaced to point to your Astra Database. You can create environment variables or do the substition manually.</p> Param Description <code>ASTRA_DB_APPLICATION_TOKEN</code> Your authentication token starting with <code>AstraCS:..</code> It MUST HAVE ADMINISTRATION PRIVILEGES in order to download the secure connect bundle from devops API. <code>ASTRA_DB_ID</code> Your database identifier, it is a UUID available at the top of your db page in the user interface or in the CLI with <code>astra db list</code> <code>ASTRA_DB_REGION</code> Your database main region. Even if the database lives in multiple region you need to provide the main one (to reduce latency). The id of the region also provides information on the cloud provide in used. There is no need for an extra parameter. It is a value available in the details database page in the user interface or in the CLI with <code>astra db list</code> <code>ASTRA_DB_KEYSPACE</code> The keyspace where to connect your application. <p>The ASTRA CLI can help you defining those environment variables with the following:</p> <pre><code># Create a file .env with all needed keys\nastra db create-dotenv demo -d `pwd`\n# Load Those keys as environment variables\nset -a\nsource .env\nset +a\n\n# Display variables in the console\nenv | grep ASTRA_DB\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#44-run-application","title":"4.4 - Run Application","text":"<p>You can now start your application with:</p> <pre><code>mvn spring-boot:run\n</code></pre> <p>If you created the application with archetype a firs table <code>todos</code> has been created and populated. An REST APi is also available for your</p> <pre><code>curl localhost:8080/todos\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#5-astra-devops-api","title":"5. Astra Devops Api","text":"Reminders <p>To work with the devops Apis it is recommanded to have an <code>Organization Administrator</code> role token. Most of the operation indeed required an high level of permissions.</p> <ul> <li> Create an Astra Token</li> </ul> <p>To create a token, please follow this guide</p> Project Setup <p>If you added the <code>astra-sdk</code> dependency your are good to go. Now if you want to work only work with the Devops API and not the stargate APis you might want to only import this subset. </p> <ul> <li> Update your <code>pom.xml</code> file with the latest version of the SDK </li> </ul> <pre><code>&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.astra&lt;/groupId&gt;\n&lt;artifactId&gt;astra-sdk-devops&lt;/artifactId&gt;\n&lt;version&gt;${latestSDK}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> <ul> <li> Initializing the Devopis Api Client</li> </ul> <pre><code>// It can be accessed from the global AstraClient\nAstraClient          astraClient = ....;\nAstraDevopsApiClient apiDevopsClient          =  astraClient.apiDevops();\nAstraDbClient        apiDevopsClientDb        = astraClient.apiDevopsDatabases();\nAstraStreamingClient apiDevopsClientStreaming = astraClient.apiDevopsStreaming();\n// You can only setup the devops part\nAstraDevopsApiClient apiDevopsDirect = new AstraDevopsApiClient(getToken());\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#51-astra-db","title":"5.1 - Astra DB","text":"<p>The SDK is extensively tested a lot of samples can be found in the database unit tests</p> Sample Code <pre><code>String token = \"&lt;your_token&gt;\";\nAstraDbClient devopsDbsClient = new AstraDbClient(token);\n// Create DB\ndevopsDbsClient.create(DatabaseCreationRequest\n.builder()\n.name(\"my_db\")\n.keyspace(\"my_keyspace\")\n.cloudRegion(\"my_region\")\n//.withVector() to enable vector preview\n.build());\nStream&lt;Database&gt; dbList = devopsDbsClient.findByName(\"my_db\");\n// Working with 1 db\nDatabaseClient dbCli = devopsDbsClient.databaseByName(\"my_db\"));\n// Keyspaces\ndbCli.keyspaces().exist(SDK_TEST_KEYSPACE2)\ndbCli.keyspaces().create(SDK_TEST_KEYSPACE2)\ndbCli.keyspaces().delete(\"invalid\")\n// Scb\ndbCli.downloadDefaultSecureConnectBundle(randomFile);\ndbCli.downloadSecureConnectBundle(SDK_TEST_DB_REGION, randomFile);\ndbCli.downloadAllSecureConnectBundles(\"/invalid\"));\n// Multi-Regions\ndbCli.datacenters().create(\"serverless\", CloudProviderType.AWS, \"eu-central-1\");\ndbCli.datacenters().delete(\"eu-central-1\");\n// Classic Ops\ndbCli.park()\ndbCli.unpark()\ndbCli.resize(2)\ndbCli.resetPassword(\"token\", \"cedrick1\")\n// Access-list\ndbCli.accessLists().addAddress(a1, a2);\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#52-astra-streaming","title":"5.2 - Astra Streaming","text":"<p>The SDK is extensively tested a lot of samples can be found in the streaming unit tests</p> Sample Code <pre><code>// Lists Tenants for a users\nSet&lt;String&gt; tenants = cli.findAll()\n.map(Tenant::getTenantName)\n.collect(Collectors.toSet());\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#53-organization","title":"5.3 - Organization","text":"<p>The SDK is extensively tested a lot of samples can be found in the control unit tests</p> Sample Code <pre><code>Organization org = getApiDevopsClient().getOrganization();\n</code></pre>"},{"location":"pages/develop/sdk/astra-sdk-java/#6-extra-resources","title":"6. Extra Resources","text":""},{"location":"pages/develop/sdk/astra-sdk-java/#61-source-code-and-issues","title":"6.1 - Source Code and Issues","text":"<ul> <li> <p>The source code is open and available at https://github.com/datastax/astra-sdk-java. If you like the solution please give consider to give us a star on github.</p> </li> <li> <p>To open an issue please use the Github Repository Issues</p> </li> </ul>"},{"location":"pages/develop/sdk/astra-sdk-java/#62-javadoc","title":"6.2 - JavaDoc","text":"<p>The javadocs are available here:</p> <ul> <li> <p>0.6.2 (latest)</p> </li> <li> <p>0.6.1</p> </li> </ul>"},{"location":"pages/develop/sdk/astra-vector-client-python/","title":"\u2022 AstraPy","text":""},{"location":"pages/develop/sdk/astra-vector-client-python/#1-overview","title":"1. Overview","text":""},{"location":"pages/develop/sdk/astra-vector-client-python/#2-prerequisites","title":"2. Prerequisites","text":"<ul> <li> Create your DataStax Astra account:</li> </ul> <p>Sign Up to Datastax Astra</p> <ul> <li> Create an Astra Token</li> </ul> <p>Once connected on the user interface, select <code>settings</code> on the left menu and tab <code>tokens</code> to create a new token.</p> <p></p> <p>You want to pick the following role:</p> Properties Values Token Role <code>Organization Administrator</code> <p>The Token contains properties <code>Client ID</code>, <code>Client Secret</code> and the <code>token</code>. You will only need the third (starting with <code>AstraCS:</code>)</p> <pre><code>{\n  \"ClientId\": \"ROkiiDZdvPOvHRSgoZtyAapp\",\n\n  \"ClientSecret\": \"fakedfaked\",\n\n  \"Token\":\"AstraCS:fake\" &lt;========== use this field\n}\n</code></pre>"},{"location":"pages/develop/sdk/astra-vector-client-python/#3-setup-project","title":"3. Setup project","text":""},{"location":"pages/develop/sdk/astra-vector-client-python/#4-getting-started","title":"4. Getting Started","text":""},{"location":"pages/develop/sdk/astra-vector-client-python/#5-reference-guide","title":"5. Reference Guide","text":""},{"location":"pages/develop/sdk/astra-vector-client-python/#51-initialization","title":"5.1. Initialization","text":""},{"location":"pages/develop/sdk/astra-vector-client-python/#52-working-with-databases","title":"5.2. Working with Databases","text":""},{"location":"pages/develop/sdk/astra-vector-client-python/#53-working-with-namespaces","title":"5.3. Working with Namespaces","text":""},{"location":"pages/develop/sdk/astra-vector-client-python/#54-working-with-collections","title":"5.4. Working with Collections","text":""},{"location":"pages/develop/sdk/astra-vector-client-python/#55-working-with-vectors","title":"5.5. Working with Vectors","text":""},{"location":"pages/develop/sdk/astra-vector-client-python/#6-troubleshooting","title":"6. Troubleshooting","text":"<ul> <li> Common Errors and Solutions</li> </ul> <p>List typical issues users might face and their resolutions.</p> <ul> <li> 6.2. FAQ</li> </ul> <p>Address frequently asked questions.</p>"},{"location":"pages/develop/sdk/astra-vector-client-python/#7-best-practices","title":"7. Best Practices","text":"<ul> <li> 7.1. Performance Tips</li> </ul> <p>Offer guidance on optimizing usage for better performance.</p> <ul> <li> 7.2. Security Recommendations</li> </ul> <p>Share advice on secure practices when using the library.</p>"},{"location":"pages/develop/sdk/astra-vector-client-python/#8-contribution-guide","title":"8. Contribution Guide","text":"<ul> <li> 8.1. Code of Conduct</li> </ul> <p>Outline the behavior expected from contributors.</p> <ul> <li> 8.2. Contribution Steps</li> </ul> <p>Describe how one can contribute to the library, e.g., via pull requests.</p>"},{"location":"pages/develop/sdk/astra-vector-client-python/#9-release-noteschangelog","title":"9. Release Notes/Changelog","text":"<p>Track changes made in each version of the library.</p>"},{"location":"pages/develop/sdk/astra-vector-client-python/#10-contact-and-support","title":"10. Contact and Support","text":"<ul> <li> 10.1. Reporting Bugs</li> </ul> <p>Provide a link or method for users to report issues.</p> <ul> <li> 10.2. Getting Help</li> </ul> <p>Point users to forums, support channels, or other resources.</p>"},{"location":"pages/tools/","title":"Tools List","text":"<ul> <li> <p>Apache Airflow: Apache Airflow is an open source workflow management system. It provides components which allow engineers to build data pipelines between different systems.</p> </li> <li> <p>Apache Beam is an open-source, unified programming model for batch and streaming data processing pipelines that simplifies large-scale data processing dynamics. Thousands of organizations around the world choose Apache Beam due to its unique data processing features, proven scale, and powerful yet extensible capabilities.</p> </li> <li> <p>Apache Flink: Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. </p> </li> <li> <p>Apache Nifi: NiFi was built to automate the flow of data between systems. While the term 'dataflow' is used in a variety of contexts, we use it here to mean the automated and managed flow of information between systems.</p> </li> <li> <p>Apache Spark: Apache Spark is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size. Use Apache Spark to connect to your database and begin accessing your Astra DB tables using Scala in spark-shell.</p> </li> <li> <p>Authorizer: Authorizer is an open source auth solution for application.  It works with many different databases, allowing the developers to use a single datastore for the entire application stack and have complete control over all user data.</p> </li> <li> <p>Celery: Celery is an open-source, distributed task queue written in Python. With Celery you can run tasks (e.g. processing of messages) in an asynchronous fashion. Celery supports a variety of message buses and backends; among the supported backends are Cassandra and Astra DB.</p> </li> <li> <p>Cloud Functions (Python Driver): Google's function-as-a-service offering that provides a serverless execution environment for your code. Cloud Functions are commonly used to extend Astra DB with additional data processing capabilities and connect Astra DB with other cloud services into data pipelines.</p> </li> <li> <p>Cloud Functions (Python SDK): Google's function-as-a-service offering that provides a serverless execution environment for your code. Cloud Functions are commonly used to extend Astra DB with additional data processing capabilities and connect Astra DB with other cloud services into data pipelines.</p> </li> <li> <p>CQL Proxy: cql-proxy is designed to forward your application's CQL traffic to an appropriate database service. It listens on a local address and securely forwards that traffic.</p> </li> <li> <p>CQL Shell: the standalone CQLSH client is a separate, lightweight tool you can use to interact with your database.</p> </li> <li> <p>Datagrip Jetbrains: DataGrip is a database management environment for developers. It is designed to query, create, and manage databases. Databases can work locally, on a server, or in the cloud. Supports MySQL, PostgreSQL, Microsoft SQL Server, Oracle, and more. If you have a JDBC driver, add it to DataGrip, connect to your DBMS, and start working.</p> </li> <li> <p>DataStation: DataStation is an open-source data IDE for developers.</p> </li> <li> <p>DataStax Bulk: The DataStax Bulk Loader tool (DSBulk) is a unified tool for loading into and unloading from Cassandra-compatible storage engines, such as OSS Apache Cassandra\u00ae, DataStax Astra and DataStax Enterprise (DSE).</p> </li> <li> <p>DBeaver: DBeaver is a universal database management tool for everyone who needs to work with data in a professional way. With DBeaver you are able to manipulate with your data like in a regular spreadsheet, create analytical reports based on records from different data storages, export information in an appropriate format.</p> </li> <li> <p>Feast: Feast is a feature store for machine learning whose goal is to provide a (mostly cloud-based) infrastructure for managing, versioning and sharing features for training and serving ML models.</p> </li> <li> <p>Github Actions: GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production.</p> </li> <li> <p>Google DataFlow Google Dataflow is an hosted version of <code>Apache Beam</code> running in google cloud platform. It allows users to build and execute data pipelines. It enables the processing of large amounts of data in a parallel and distributed manner, making it scalable and efficient. Dataflow supports both batch and streaming processing, allowing for real-time data analysis.</p> </li> <li> <p>Grafana: Grafana is an industry standard tool for data visualisation. With Grafana, you can explore your time-series data using different visualisations: charts, plots, diagrams and even configure alerting if a value exceeds some desired range.</p> </li> <li> <p>HashiCorp Vault: Vault is an identity-based secrets and encryption management system. A secret is anything that you want to tightly control access to, such as API encryption keys, passwords, or certificates. Vault provides encryption services that are gated by authentication and authorization methods.</p> <ul> <li>Astra DB Plugin</li> </ul> </li> <li> <p>IntelliJ IDEA: The Capable &amp; Ergonomic Java IDE by JetBrains</p> </li> <li> <p>JanusGraph: JanusGraph is designed to support the processing of graphs so large that they require storage and computational capacities beyond what a single machine can provide. Scaling graph data processing for real time traversals and analytical queries is JanusGraph\u2019s foundational benefit. This section will discuss the various specific benefits of JanusGraph and its underlying, supported persistence solutions.</p> </li> <li> <p>Liquibase: Liquibase is a database schema change management solution that enables you to revise and release database changes faster and safer from development to production.</p> </li> <li> <p>Micronaut: Micronaut is a modern, JVM-based, full stack Java framework designed for building modular, easily testable JVM applications with support for Java, Kotlin, and Groovy. Micronaut is developed by the creators of the Grails framework and takes inspiration from lessons learnt over the years building real-world applications from monoliths to microservices using Spring, Spring Boot and Grails.</p> </li> <li> <p>MindsDB: MindsDB enables you to use ML predictions in your database using SQL.</p> </li> <li> <p>Pentaho Data Integration: Pentaho Data Integration (PDI) provides the Extract, Transform, and Load (ETL) capabilities that facilitate the process of capturing, cleansing, and storing data using a uniform and consistent format that is accessible and relevant to end users and IoT technologies.</p> </li> <li> <p>Power Query: Microsoft Power Query is a data preparation and transformation ETL engine that lets you connect to various data sources. Power Query is available in Microsoft Excel, Power BI, Power BI dataflows, Azure Data Factory wrangling dataflows, SQL Server Analysis Services, and much more.</p> </li> <li> <p>Quine: Quine.io is a streaming graph capable of building high-volumes of data into a stateful graph.  It allows for real-time traversals on a graph, as well as for the data to be streamed-out for event processing.</p> </li> <li> <p>StepZen: StepZen helps developers build GraphQL faster, deploy in seconds, and run on StepZen. It simplifies how you access the data you need, and with zero infrastructure to build or manage, you can focus on crafting modern data-driven experiences.</p> </li> <li> <p>TablePlus: TablePlus is a modern, native tool with elegant UI that allows you to simultaneously manage multiple databases such as MySQL, PostgreSQL, SQLite, Microsoft SQL Server and more.</p> </li> <li> <p>Temporal: Temporal.io is an open source microservice orchestration platform that assists in tracking workflows in your application development. It provides the user with a plug-and-play persistence layer that lets the user choose and configure their Temporal Server with their preferred backend.</p> </li> </ul>"},{"location":"pages/tools/cloud-providers/google-dataflow/","title":"GCP Dataflow","text":""},{"location":"pages/tools/cloud-providers/google-dataflow/#overview","title":"Overview","text":"<p>GCP DataFlow is a managed service for batch and streaming data processing pipelines and is based on Apache Beam. Apache Beam is an open-source, unified programming model for batch and streaming data processing pipelines which simplifies large-scale data processing dynamics. Thousands of organizations around the world choose Apache Beam due to its unique data processing features, proven scale, and powerful yet extensible capabilities.</p> <ul> <li>Learn more about Apache Beam</li> </ul> Apache Beam Overview <p></p> <p>Objectives</p> <p></p> <p>Main Concepts</p> <p></p> <ul> <li> <p><code>Pipeline</code>: A <code>Pipeline</code> encapsulates your entire data processing task, from start to finish. This includes reading input data, transforming that data, and writing output data. All Beam driver programs must create a Pipeline. When you create the Pipeline, you must also specify the execution options that tell the Pipeline where and how to run.</p> </li> <li> <p><code>PCollection</code>: A <code>PCollection</code> represents a distributed data set that your Beam pipeline operates on. The data set can be bounded, meaning it comes from a fixed source like a file, or unbounded, meaning it comes from a continuously updating source via a subscription or other mechanisms. Your pipeline typically creates an initial PCollection by reading data from an external data source, but you can also create a PCollection from in-memory data within your driver program. From there, PCollections are the inputs and outputs for each step in your pipeline.</p> </li> <li> <p><code>PTransform</code>: A <code>PTransform</code> represents a data processing operation, or a step, in your pipeline. Every PTransform takes one or more PCollection objects as input, performs a processing function that you provide on the elements of that PCollection, and produces zero or more output PCollection objects.</p> </li> <li> <p>Input and Output so called <code>I/O transforms</code>: Beam comes with a number of \u201cIOs\u201d - library PTransforms that read or write data to various external storage systems.</p> </li> </ul> <p>I/O Connectors</p> <p>Apache Beam I/O connectors provide read and write transforms for the most popular data storage systems so that Beam users can benefit from natively optimised connectivity. With the available I/Os, Apache Beam pipelines can read and write data to and from an external storage type in a unified and distributed way.</p> <p>Integration with DataStax Astra is inspired by the built-in <code>CassandraIO</code> and <code>PulsarIO</code> connectors. This integration leverages a new <code>AstraIO</code> connector.</p> <p>Runners</p> <p>A runner in Apache Beam is responsible for executing pipelines on a particular processing engine or framework, such as Apache Flink or Google Cloud Dataflow. The runner translates the Beam pipeline into the appropriate format for the underlying engine, manages job execution, and provides feedback on job progress and status.</p> <p></p> <ul> <li>Learn more about Google DataFlow</li> </ul> Rational on technical integrations Choices <p>Astra allows both bulk and real time operations through AstraDB and Astra Streaming. For each service there are multiple interfaces available and integration with Apache Beam/Google Dataflow is possible in different ways. Some of the design choices for this integration are below:</p> <p></p> <p>Data Bulk Operations</p> <p>The Astra service which handles massive amount of data is <code>Astra DB</code>. It provides multiples ways to load data but some methods are preferred over others.</p> <ul> <li> <p><code>Cassandra and CQL</code>: This is the way to go. It is the most mature and provides an efficient way to execute queries. With the native drivers you can run reactive queries and token range queries to distribute the load across the nodes.  This is the approach that was taken with the original <code>CassandraIO</code> connector. The existing <code>CassandraIO</code> connector does not support Astra but we leveraged it to create a new <code>AstraIO</code> connector.</p> </li> <li> <p><code>CQL over REST</code>: This interface can be use with any HTTP client. While the Astra SDKs provides a built-in client, this interface is not the best for bulk loading as it introduces an extra layer of serialization.</p> </li> <li> <p><code>CQL over GraphQL</code>: This interface can be used with any HTTP Client. While the Astra SDKs provides a built-in client, this interface is not the best for bulk loading as it introduces an extra layer of serialization.</p> </li> <li> <p><code>CQL over GRPC</code>: This interface is stateless, with an optimized serialization component (grpc), and reactive interfaces so it is a viable option. Currently, the operations exposed are CQL and the token metadata information is not available to perform range queries.</p> </li> </ul> <p>Data Streaming Operations</p> <p>The Astra service to handle streaming data is <code>Astra Streaming</code>. It provides multiple interfaces like <code>JMS</code>, <code>RabbitMQ</code>, <code>Kafka</code>, and built-in Apache Beam support is available in standard connectors.</p> <p>To leverage the split capabilities of Pulsar, a <code>PulsarIO</code> connector was released in 2022. To learn more about its development you can follow this video from the Beam Summit 2022.</p> <ul> <li>Connectivity to Astra is implemented through a custom I/O Connector named <code>beam-sdks-java-io-astra</code> available on central Maven.</li> </ul> AstraDBIO Connector <p>The Astra I/O connectors implements the different operations needed across the pipelines in the page: Read, Write, Delete, ReadAll.</p> <p>This is the dependency to add to the project.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;com.datastax.astra&lt;/groupId&gt;\n&lt;artifactId&gt;beam-sdks-java-io-astra&lt;/artifactId&gt;\n&lt;version&gt;${latest-version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <ul> <li>Read From Astra</li> </ul> <p>To Read Data from AstraDb use <code>AstraDbIO.Read&lt;Entity&gt;</code> where the entity should be a <code>Serializable</code> object. In the sample we can leverage the cassandra object mapping of Driver 4x.</p> <pre><code>byte[] scbZip = ...\nAstraDbIO.Read&lt;LanguageCode&gt; read = AstraDbIO.&lt;LanguageCode&gt;read()\n.withToken(\"token\")\n.withKeyspace(\"keyspace\")\n.withSecureConnectBundle(scbZip)\n.withTable(\"table\")\n.withMinNumberOfSplits(20)\n.withCoder(SerializableCoder.of(LanguageCode.class))\n.withMapperFactoryFn(new LanguageCodeDaoMapperFactoryFn())\n.withEntity(LanguageCode.class);\n</code></pre> <ul> <li> <p>The <code>mapperFactoryFn</code> should implements <code>SerializableFunction&lt;CqlSession, AstraDbMapper&lt;Entity&gt;&gt;</code></p> </li> <li> <p>You can also specify a query, the table is name is not mandatory anymore</p> </li> </ul> <pre><code> AstraDbIO.Read&lt;LanguageCode&gt; read2 = AstraDbIO.&lt;LanguageCode&gt;read()\n.withToken(\"token\")\n.withKeyspace(\"keyspace\")\n.withSecureConnectBundle(scbZip)\n.withQuery(\"select * from table where ...\")\n.withMinNumberOfSplits(20)\n.withCoder(SerializableCoder.of(LanguageCode.class))\n.withMapperFactoryFn(new LanguageCodeDaoMapperFactoryFn())\n.withEntity(LanguageCode.class);\n</code></pre> <ul> <li>Write data into Astra</li> </ul> <p>To write Data into Astra use the <code>AstraDbIO.Write&lt;Entity&gt;</code></p> <pre><code>AstraDbIO.Write&lt;LanguageCode&gt; write = AstraDbIO.&lt;LanguageCode&gt;write()\n.withToken(\"token\")\n.withKeyspace(\"keyspace\")\n.withSecureConnectBundle(scbZip)\n.withMapperFactoryFn(new LanguageCodeDaoMapperFactoryFn())\n.withEntity(LanguageCode.class);\n</code></pre>"},{"location":"pages/tools/cloud-providers/google-dataflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have Java11+, Maven, and Git installed</li> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle</li> </ul> Setup your <code>JAVA</code> Development environment <ul> <li> Install Java Development Kit (JDK) 8+</li> </ul> <p>Use java reference documentation targetting your operating system to install a Java Development Kit. You can then validate your installation with the following command.</p> <pre><code>java --version\n</code></pre> <ul> <li> Install Apache Maven (3.8+)</li> </ul> <p>Samples and tutorials have been designed with <code>Apache Maven</code>. Use the reference documentation top install maven validate your installation with </p> <pre><code>mvn -version\n</code></pre> Setup Datastax <code>Astra DB</code> <ul> <li> Create your DataStax Astra account: </li> </ul> <p>Sign Up</p> <ul> <li> Create an Astra Token</li> </ul> <p>An astra token acts as your credentials, it holds the different permissions. The scope of a token is the whole organization (tenant) but permissions can be edited to limit usage to a single database.</p> <p>To create a token, please follow this guide</p> <p>The Token is in fact three separate strings: a <code>Client ID</code>, a <code>Client Secret</code> and the <code>token</code> proper. You will need some of these strings to access the database, depending on the type of access you plan. Although the Client ID, strictly speaking, is not a secret, you should regard this whole object as a secret and make sure not to share it inadvertently (e.g. committing it to a Git repository) as it grants access to your databases.</p> <pre><code>{\n\"ClientId\": \"ROkiiDZdvPOvHRSgoZtyAapp\",\n\"ClientSecret\": \"fakedfaked\",\n\"Token\":\"AstraCS:fake\"\n}\n</code></pre> <p>It is handy to have your token declare as an environment variable (replace with proper value):</p> <pre><code>export ASTRA_TOKEN=\"AstraCS:replace_me\"\n</code></pre> <ul> <li> Create a Database and a keyspace</li> </ul> <p>With your account you can run multiple databases, a Databases is an Apache Cassandra cluster. It can live in one or multiple regions (dc). In each Database you can have multiple keyspaces. In the page we will use the database name <code>db_demo</code> and the keyspace <code>keyspace_demo</code>.</p> <p>You can create the DB using the user interface and here is a tutorial. You can also use Astra command line interface. To install and setup the CLI run the following:</p> <pre><code>curl -Ls \"https://dtsx.io/get-astra-cli\" | bash\nsource ~/.astra/cli/astra-init.sh\nastra setup --token ${ASTRA_TOKEN}\n</code></pre> <p>To create DB and keyspace with the CLI:</p> <pre><code>astra db create db_demo -k keyspace_demo --if-not-exists\n</code></pre> <ul> <li> Download the Secure Connect Bundle for current database</li> </ul> <p>A Secure Connect Bundle contains the certificates and endpoints informations to open a mTLS connection. Often mentionned as <code>scb</code> its scope is a database AND a region. If your database is deployed on multiple regions you will have to download the bundle for each one and initiate the connection accordingly. Instructions to download Secure Connect Bundle are here</p> <p></p> <p>You can download the secure connect bundle from the user interface and here is a tutorial. You can also use Astra command line interface.</p> <pre><code>astra db download-scb db_demo -f /tmp/secure-connect-bundle-db-demo.zip\n</code></pre>"},{"location":"pages/tools/cloud-providers/google-dataflow/#installation-and-setup","title":"Installation and Setup","text":"<ul> <li>Clone the Repository with sample flows. The different flows are distributed in 2 different modules. <code>sample-beams</code> contains flows that do not interact with Google Cloud solutions and will be run with a direct runner. <code>sample-dataflows</code> contains flows that could be executed.</li> </ul> <pre><code>git clone https://github.com/DataStax-Examples/astra-dataflow-starter.git\n</code></pre> <ul> <li>Navigate to the repository and build the project with maven.</li> </ul> <pre><code>cd astra-dataflow-starter\nmvn clean install -Dmaven.test.skip=true\n</code></pre> More on the <code>maven</code> project setup locally <ul> <li> Clone the Repository with <code>AstraIO</code> and sample flows</li> </ul> <pre><code>git clone https://github.com/DataStax-Examples/astra-dataflow-starter.git\n</code></pre> <ul> <li> Build the project with maven</li> </ul> <pre><code>cd astra-dataflow-starter\nmvn clean install -Dmaven.test.skip=true\n</code></pre> <p>The different flows are distributed in 2 different modules: </p> <ul> <li><code>sample-beams</code> contains flows that do not interact with Google Cloud solutions and will be run with a direct runner. </li> </ul> <p></p> <ul> <li><code>sample-dataflows</code> contains flow that could be executed </li> </ul> <p></p> <pre><code>\n</code></pre>"},{"location":"pages/tools/cloud-providers/google-dataflow/#beam-samples","title":"Beam Samples","text":""},{"location":"pages/tools/cloud-providers/google-dataflow/#1-import-a-csv-file","title":"1. Import a CSV File","text":"<p>In this flow a <code>CSV</code> file is parsed to populate a table with same structure in Astra. The mapping from CSV to the table is done manually. The dataset is a list of languages.</p> <p></p> <ul> <li>Access folder <code>samples-beam</code> in the project.</li> </ul> <pre><code>cd samples-beam\npwd\n</code></pre> <ul> <li>Setup Environment variables</li> </ul> <pre><code># Database name (use with CLI)\nexport ASTRA_DB=&lt;your-db-name&gt;\n# Keyspace name \nexport ASTRA_KEYSPACE=&lt;your-keyspace-name&gt;\n# Path of local secure connect bundle\nexport ASTRA_SCB_PATH=&lt;your-secure-connect-bundle&gt;\n# Astra Token starting by AstraCS:...\nexport ASTRA_TOKEN=&lt;your-token&gt;\n</code></pre> <ul> <li>Run Beam pipeline</li> </ul> <pre><code> mvn clean compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.beam.Csv_to_AstraDb \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SCB_PATH} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --csvInput=`pwd`/src/test/resources/language-codes.csv\"\n</code></pre> <ul> <li>Check output data in Astra</li> </ul> <pre><code>astra db cqlsh ${ASTRA_DB} \\\n-k ${ASTRA_KEYSPACE} \\\n-e \"SELECT * FROM languages LIMIT 10;\"\n</code></pre>"},{"location":"pages/tools/cloud-providers/google-dataflow/#2-export-table-as-csv","title":"2. Export Table as CSV","text":"<p>In this flow a Cassandra table is exported as a CSV file. The mapping from table to csv row is done manually. The same objects are reused from <code>#1</code></p> <p></p> <ul> <li>Access folder</li> </ul> <pre><code>cd samples-beam\npwd\n</code></pre> <ul> <li>Setup Environment variables</li> </ul> <pre><code># Database name (use with CLI)\nexport ASTRA_DB=&lt;your-db-name&gt;\n# Keyspace name \nexport ASTRA_KEYSPACE=&lt;your-keyspace-name&gt;\n# Path of local secure connect bundle\nexport ASTRA_SCB_PATH=&lt;your-secure-connect-bundle&gt;\n# Astra Token starting by AstraCS:...\nexport ASTRA_TOKEN=&lt;your-token&gt;\n</code></pre> <ul> <li>Run Beam pipeline</li> </ul> <pre><code> mvn clean compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.beam.AstraDb_To_Csv \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SCB_PATH} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --table=languages \\\n --csvOutput=`pwd`/src/test/resources/out/language\"\n</code></pre> <ul> <li>Check output data in astra</li> </ul> <pre><code>ls -l `pwd`/src/test/resources/out\ncat `pwd`/src/test/resources/out/language-00001-of-00004\n</code></pre>"},{"location":"pages/tools/cloud-providers/google-dataflow/#3-import-cassandra-table","title":"3. Import Cassandra Table","text":"<p>Similar to ZDM a cassandra Table is imported into Astra. We are reusing the same data model as before. Mapping is manual. We can note that Cassandra reading is operated with <code>CassandraIO</code> (driver3x) where the load is done with <code>AstraDbIO</code> (drivers4x).</p> <p></p> <ul> <li>Access folder</li> </ul> <pre><code>cd samples-beam\npwd\n</code></pre> <ul> <li>Start Cassandra as a docker image with docker compose: Project propose a docker-compose to run Cassandra locally. Use <code>docker-compose</code> to start the containers</li> </ul> <pre><code>docker-compose -f ./src/main/docker/docker-compose.yml up -d\n</code></pre> <ul> <li>Wait a few seconds for Cassandra to Start. The following command give you the status of the container</li> </ul> <pre><code>docker-compose -f ./src/main/docker/docker-compose.yml ps | cut -b 55-61\n</code></pre> <ul> <li>Validate Cassandra is ready: By connecting with <code>cqlsh</code> and displaying the datacenter.</li> </ul> <pre><code>docker exec -it `docker ps | \\\ngrep cassandra:4.1.1 | \\\ncut -b 1-12` cqlsh -e \"SELECT data_center FROM system.local;\"\n</code></pre> <ul> <li>Setup Env variables</li> </ul> <pre><code>export ASTRA_DB=&lt;your-db-name&gt;\nexport ASTRA_KEYSPACE=&lt;your-keyspace-name&gt;\nexport ASTRA_SCB_PATH=&lt;your-secure-connect-bundle&gt;\nexport ASTRA_TOKEN=&lt;your-token&gt;\n</code></pre> <ul> <li>Run the pipeline: Keyspaces and Tables are created in local cassandra before starting the copy into Astra.</li> </ul> <pre><code> mvn clean compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.beam.Cassandra_To_AstraDb \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SCB_PATH} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --cassandraHost=localhost \\\n --cassandraKeyspace=demo \\\n --cassandraTableName=languages \\\n --cassandraPort=9042 \\\n --tableName=languages\"\n</code></pre> <ul> <li>Check data in Cassandra with <code>cqlsh</code></li> </ul> <pre><code>docker exec -it `docker ps \\\n| grep cassandra:4.1.1 \\\n| cut -b 1-12` \\\ncqlsh -e \"SELECT *  FROM samples_beam.languages LIMIT 10;\"\n</code></pre> <ul> <li>Check data in Astra destination with <code>cqlsh</code> (CLI)</li> </ul> <pre><code>astra db cqlsh ${ASTRA_DB} \\\n-k ${ASTRA_KEYSPACE} \\\n-e \"SELECT * FROM languages LIMIT 10;\"\n</code></pre>"},{"location":"pages/tools/cloud-providers/google-dataflow/#4-generative-ai","title":"4. Generative AI","text":"<p>This use cases is divided in 2 flows. In the first step we will import a CSV file as before mapping the CSV schema in destination table. Second flow will alter the table to add the embeddings vector and populate it after calling OpenAI Embedding API</p> <p></p> <ul> <li>Access folder</li> </ul> <pre><code>cd samples-beam\npwd\n</code></pre> <ul> <li>Setup Env variables</li> </ul> <pre><code>export ASTRA_DB=&lt;your-db-name&gt;\nexport ASTRA_KEYSPACE=&lt;your-keyspace-name&gt;\nexport ASTRA_SCB_PATH=&lt;your-secure-connect-bundle&gt;\nexport ASTRA_TOKEN=&lt;your-token&gt;\n</code></pre> <ul> <li>Import Data with first flow</li> </ul> <pre><code> mvn clean compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.beam.genai.GenAI_01_ImportData \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SCB_PATH} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --csvInput=`pwd`/src/main/resources/fables_of_fontaine.csv\"\n</code></pre> <p>A table is created with the following structure:</p> <pre><code>CREATE TABLE IF NOT EXISTS ai.fable (\ndocument_id text PRIMARY KEY,\ndocument text,\ntitle text\n);\n</code></pre> <ul> <li>Check output data in Astra</li> </ul> <pre><code>astra db cqlsh ${ASTRA_DB} \\\n-k ${ASTRA_KEYSPACE} \\\n-e \"SELECT * FROM fable LIMIT 10;\"\n</code></pre> <p></p> <ul> <li>Add extra environment variables</li> </ul> <pre><code>export ASTRA_TABLE=fable\nexport OPENAI_KEY=&lt;change_me&gt;\n</code></pre> <ul> <li>Run pipeline</li> </ul> <pre><code> mvn clean compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.beam.genai.GenAI_02_CreateEmbeddings \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SCB_PATH} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --openAiKey=${OPENAI_KEY} \\\n --table=${ASTRA_TABLE}\"\n</code></pre>"},{"location":"pages/tools/cloud-providers/google-dataflow/#google-dataflow-samples","title":"Google Dataflow Samples","text":""},{"location":"pages/tools/cloud-providers/google-dataflow/#1-setup-gcloud-cli","title":"1. Setup gCloud CLI","text":"<ul> <li>Create GCP Project</li> </ul> <p>Note: If you don't plan to keep the resources that you create in this guide, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project. Create a new Project in Google Cloud Console or select an existing one.</p> <p>In the Google Cloud console, on the project selector page, select or create a Google Cloud project</p> <ul> <li>Enable Billing</li> </ul> <p>Make sure that billing is enabled for your Cloud project. Learn how to check if billing is enabled on a project</p> <ul> <li>Save project ID</li> </ul> <p>The project identifier is available in the column <code>ID</code>. We will need it so let's save it as an environment variable</p> <pre><code>export GCP_PROJECT_ID=&lt;your-gcp-project-id&gt;\nexport GCP_PROJECT_CODE=&lt;your-gcp-project-code&gt;\nexport GCP_USER=&lt;your-gcp-email&gt;\nexport GCP_COMPUTE_ENGINE=${GCP_PROJECT_CODE}-compute@developer.gserviceaccount.com\n</code></pre> <ul> <li>Install gCloud CLI</li> </ul> <pre><code>curl https://sdk.cloud.google.com | bash\n</code></pre> <ul> <li>Login to gCloud: Run the following command to authenticate with Google Cloud:</li> </ul> <pre><code>gcloud auth login\n</code></pre> <ul> <li>Setup your project: If you haven't set your project yet, use the following command to set your project ID:</li> </ul> <pre><code>gcloud config set project ${GCP_PROJECT_ID}\ngcloud projects describe ${GCP_PROJECT_ID}\n</code></pre> <ul> <li>Enable needed APIs</li> </ul> <pre><code>gcloud services enable dataflow compute_component \\\n   logging storage_component storage_api \\\n   bigquery pubsub datastore.googleapis.com \\\n   cloudresourcemanager.googleapis.com\n</code></pre> <ul> <li>Add Roles</li> </ul> <p>To complete the steps, your user account must have the Dataflow Admin role and the Service Account User role. The Compute Engine default service account must have the Dataflow Worker role. To add the required roles in the Google Cloud console:</p> <pre><code>gcloud projects add-iam-policy-binding ${GCP_PROJECT_ID} \\\n    --member=\"user:${GCP_USER}\" \\\n    --role=roles/iam.serviceAccountUser\ngcloud projects add-iam-policy-binding ${GCP_PROJECT_ID}  \\\n    --member=\"serviceAccount:${GCP_COMPUTE_ENGINE}\" \\\n    --role=roles/dataflow.admin\ngcloud projects add-iam-policy-binding ${GCP_PROJECT_ID}  \\\n    --member=\"serviceAccount:${GCP_COMPUTE_ENGINE}\" \\\n    --role=roles/dataflow.worker\ngcloud projects add-iam-policy-binding ${GCP_PROJECT_ID}  \\\n    --member=\"serviceAccount:${GCP_COMPUTE_ENGINE}\" \\\n    --role=roles/storage.objectAdmin\n</code></pre>"},{"location":"pages/tools/cloud-providers/google-dataflow/#2-gcs-to-astradb","title":"2. GCS to AstraDB","text":"<ul> <li> <p>Access Folder</p> </li> <li> <p>Make sure you are in <code>samples-dataflow</code> folder</p> </li> </ul> <pre><code>cd samples-dataflow\npwd\n</code></pre> <ul> <li>Create <code>buckets</code>: Create the bucket for the for the project in cloud storage:</li> </ul> <pre><code>export GCP_BUCKET_INPUT=gs://astra_dataflow_inputs\ngsutil mb -c STANDARD -l US ${GCP_BUCKET_INPUT}\n</code></pre> <p>Copy the CSV file in the bucket</p> <pre><code>gsutil cp src/test/resources/language-codes.csv ${GCP_BUCKET_INPUT}/csv/\ngsutil ls\n</code></pre> <ul> <li>Create Secrets</li> </ul> <p>Create secrets for the project in secret manager**. To connect to <code>AstraDB</code> you need a token (credentials) and a zip used to secure the transport. Those two inputs should be defined as secrets.</p> <pre><code>export GCP_SECRET_TOKEN=token\nexport GCP_SECRET_SECURE_BUNDLE=cedrick-demo-scb\ngcloud secrets create ${GCP_SECRET_TOKEN} \\\n--data-file &lt;(echo -n \"${ASTRA_TOKEN}\") \\\n--replication-policy=\"automatic\"\ngcloud secrets add-iam-policy-binding ${GCP_SECRET_TOKEN} \\\n--member=\"serviceAccount:${GCP_COMPUTE_ENGINE}\" \\\n--role='roles/secretmanager.secretAccessor'\ngcloud secrets create ${GCP_SECRET_SECURE_BUNDLE} \\\n--data-file ${ASTRA_SCB_PATH} \\\n--replication-policy=\"automatic\"\ngcloud secrets add-iam-policy-binding ${GCP_SECRET_SECURE_BUNDLE} \\\n--member=\"serviceAccount:${GCP_COMPUTE_ENGINE}\" \\\n--role='roles/secretmanager.secretAccessor'\ngcloud secrets list\n</code></pre> <ul> <li>Create Keyspace</li> </ul> <pre><code>astra db create-keyspace demo \\\n-k samples_dataflow \\\n--if-not-exist\n</code></pre> <ul> <li>Setup Env. variables</li> </ul> <pre><code>export ASTRA_DB=&lt;your-db-name&gt;\nexport ASTRA_KEYSPACE=&lt;your-keyspace&gt;\nexport ASTRA_SECRET_TOKEN=projects/${GCP_PROJECT_CODE}/secrets/${GCP_SECRET_TOKEN}/versions/1\nexport ASTRA_SECRET_SECURE_BUNDLE=projects/${GCP_PROJECT_CODE}/secrets/${GCP_SECRET_SECURE_BUNDLE}/versions/1\nexport GCP_INPUT_CSV=${GCP_BUCKET_INPUT}/csv/language-codes.csv\n</code></pre> <ul> <li>Run the demo</li> </ul> <pre><code> mvn compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.dataflow.Gcs_To_AstraDb \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_SECRET_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SECRET_SECURE_BUNDLE} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --csvInput=${GCP_INPUT_CSV} \\\n --project=${GCP_PROJECT_ID} \\\n --runner=DataflowRunner \\\n --region=us-central1\"\n</code></pre> <ul> <li>Check Astra table is populated</li> </ul> <pre><code>astra db cqlsh ${ASTRA_DB} \\\n-k ${ASTRA_KEYSPACE} \\\n-e \"SELECT * FROM languages LIMIT 10;\"\n</code></pre>"},{"location":"pages/tools/cloud-providers/google-dataflow/#3-astradb-to-gcs","title":"3. AstraDb to GCS","text":"<p> <p>Note</p> <p>We assume that you have already executed pipeline described in <code>D.1</code> to <code>D.5</code> and that gcloud is set up.</p> <p></p> <ul> <li>Environment Variables</li> </ul> <pre><code>export ASTRA_DB=&lt;your-db-name&gt;\nexport ASTRA_KEYSPACE=&lt;your-keyspace&gt;\nexport ASTRA_TABLE=&lt;your-table&gt;\nexport ASTRA_SECRET_TOKEN=projects/${GCP_PROJECT_CODE}/secrets/${GCP_SECRET_TOKEN}/versions/1\nexport ASTRA_SECRET_SECURE_BUNDLE=projects/${GCP_PROJECT_CODE}/secrets/${GCP_SECRET_SECURE_BUNDLE}/versions/1\nexport GCP_PROJECT_ID=&lt;your-gcp-project-id&gt;\n</code></pre> <ul> <li>Create output bucket</li> </ul> <pre><code>export GCP_OUTPUT_CSV=gs://astra_dataflow_outputs\ngsutil mb -c STANDARD -l US ${GCP_OUTPUT_CSV}\n</code></pre> <ul> <li>Access folder</li> </ul> <pre><code>cd samples-dataflow\npwd\n</code></pre> <ul> <li>Run the pipeline</li> </ul> <pre><code> mvn compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.dataflow.AstraDb_To_Gcs \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_SECRET_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SECRET_SECURE_BUNDLE} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --table=${ASTRA_TABLE} \\\n --outputFolder=${GCP_OUTPUT_CSV} \\\n --project=${GCP_PROJECT_ID} \\\n --runner=DataflowRunner \\\n --region=us-central1\"\n</code></pre>"},{"location":"pages/tools/cloud-providers/google-dataflow/#4-astradb-to-bigquery","title":"4. AstraDb to BigQuery","text":"<p> <p>Note</p> <p>We assume that you have already executed pipeline described in <code>D.1</code> to <code>D.5</code> and that gcloud is set up.</p> <p></p> <p></p> <ul> <li>Access Folder</li> </ul> <pre><code>cd samples-dataflow\npwd\n</code></pre> <ul> <li>Setup Env. Variables</li> </ul> <pre><code>export ASTRA_DB=&lt;your-db-name&gt;\nexport ASTRA_KEYSPACE=&lt;your-keyspace&gt;\nexport ASTRA_SECRET_TOKEN=projects/${GCP_PROJECT_CODE}/secrets/${GCP_SECRET_TOKEN}/versions/1\nexport ASTRA_SECRET_SECURE_BUNDLE=projects/${GCP_PROJECT_CODE}/secrets/${GCP_SECRET_SECURE_BUNDLE}/versions/1\nexport GCP_PROJECT_ID=&lt;your-gcp-project-id&gt;\n</code></pre> <ul> <li> <p>Create BigQuery dataset</p> </li> <li> <p>Create a dataset in <code>dataflow_input_us</code> BigQuery with the following command</p> </li> </ul> <pre><code>export GCP_BIGQUERY_DATASET=dataflow_input_us\nbq mk ${GCP_BIGQUERY_DATASET}\nbq ls --format=pretty\n</code></pre> <ul> <li> <p>Create BigQuery Schema</p> </li> <li> <p>Create a json <code>schema_language_codes.json</code> file with the schema of the table** We have created it for you here</p> </li> </ul> <pre><code>[\n{\n\"mode\": \"REQUIRED\",\n\"name\": \"code\",\n\"type\": \"STRING\"\n},\n{\n\"mode\": \"REQUIRED\",\n\"name\": \"language\",\n\"type\": \"STRING\"\n}\n]\n</code></pre> <ul> <li>Create BigQuery Table</li> </ul> <pre><code>export GCP_BIGQUERY_TABLE=destination\nbq mk --table --schema src/main/resources/schema_language_codes.json ${GCP_BIGQUERY_DATASET}.${GCP_BIGQUERY_TABLE}\n</code></pre> <ul> <li>List tables dataset</li> </ul> <pre><code>bq ls --format=pretty ${GCP_PROJECT_ID}:${GCP_BIGQUERY_DATASET}\n</code></pre> <ul> <li>Show Table schema</li> </ul> <pre><code>bq show --schema --format=prettyjson ${GCP_PROJECT_ID}:${GCP_BIGQUERY_DATASET}.${GCP_BIGQUERY_TABLE}\n</code></pre> <ul> <li>&gt;Run the pipeline</li> </ul> <pre><code>mvn compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.dataflow.AstraDb_To_BigQuery \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_SECRET_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SECRET_SECURE_BUNDLE} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --table=languages \\\n --bigQueryDataset=${GCP_BIGQUERY_DATASET} \\\n --bigQueryTable=${GCP_BIGQUERY_TABLE} \\\n --runner=DataflowRunner \\\n --project=${GCP_PROJECT_ID} \\\n --region=us-central1\"\n</code></pre> <ul> <li>Show Output Table</li> </ul> <pre><code>bq head -n 10 ${GCP_BIGQUERY_DATASET}.${GCP_BIGQUERY_TABLE}\n</code></pre>"},{"location":"pages/tools/cloud-providers/google-dataflow/#5-bigquery-to-astradb","title":"5. BigQuery to AstraDb","text":"<p> <p>Note</p> <p>We assume that you have already executed pipeline described in <code>D.1</code> to <code>D.5</code> and that gcloud is set up. We also assume that you have a bigquery table populated as describe in <code>#F</code>,</p> <p></p> <ul> <li>Access Folder</li> </ul> <pre><code>cd samples-dataflow\npwd\n</code></pre> <ul> <li>Setup Env. Variables</li> </ul> <p>Replace with values coming from your gcp project.  The destination table has been created in flow <code>3.3</code></p> <pre><code>export ASTRA_DB=&lt;your-db-name&gt;\nexport ASTRA_KEYSPACE=&lt;your-keyspace&gt;\nexport ASTRA_TABLE=languages\nexport ASTRA_SECRET_TOKEN=projects/${GCP_PROJECT_CODE}/secrets/${GCP_SECRET_TOKEN}/versions/1\nexport ASTRA_SECRET_SECURE_BUNDLE=projects/${GCP_PROJECT_CODE}/secrets/${GCP_SECRET_SECURE_BUNDLE}/versions/1\nexport GCP_PROJECT_ID=&lt;your-gcp-project-id&gt;\nexport GCP_BIGQUERY_DATASET=dataflow_input_us\nexport GCP_BIGQUERY_TABLE=destination\n</code></pre> <ul> <li>Clear astra table</li> </ul> <pre><code>astra db cqlsh ${ASTRA_DB} \\\n-k ${ASTRA_KEYSPACE} \\\n-e \"TRUNCATE ${ASTRA_TABLE};\"\n</code></pre> <ul> <li>Clear the Pipeline</li> </ul> <pre><code>mvn compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.dataflow.BigQuery_To_AstraDb \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_SECRET_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SECRET_SECURE_BUNDLE} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --bigQueryDataset=${GCP_BIGQUERY_DATASET} \\\n --bigQueryTable=${GCP_BIGQUERY_TABLE} \\\n --runner=DataflowRunner \\\n --project=${GCP_PROJECT_ID} \\\n --region=us-central1\"\n</code></pre> <ul> <li>Check output data</li> </ul> <pre><code>astra db cqlsh ${ASTRA_DB} \\\n-k ${ASTRA_KEYSPACE} \\\n-e \"select * FROM languages LIMIT 10;\"\n</code></pre>"},{"location":"pages/tools/cloud-providers/google-dataflow/#6-bigquery-dynamic-mapping","title":"6. BigQuery Dynamic Mapping","text":"<p> <p>Note</p> <p>We assume that you have already executed pipeline described in <code>D.1</code> to <code>D.5</code> and that gcloud is set up.</p> <p></p> <ul> <li>Access Folder</li> </ul> <pre><code>cd samples-dataflow\npwd\n</code></pre> <ul> <li>Setup Env. Variables</li> </ul> <pre><code>export GCP_PROJECT_ID=&lt;your-gcp-project-id&gt;\nexport GCP_PROJECT_CODE=&lt;your-gcp-project-code&gt;\nexport ASTRA_DB=&lt;your-db-name&gt;\nexport ASTRA_KEYSPACE=&lt;your-keyspace&gt;\nexport ASTRA_TABLE=&lt;your-table&gt;\nexport ASTRA_SECRET_TOKEN=projects/${GCP_PROJECT_CODE}/secrets/${GCP_SECRET_TOKEN}/versions/1\nexport ASTRA_SECRET_SECURE_BUNDLE=projects/${GCP_PROJECT_CODE}/secrets/${GCP_SECRET_SECURE_BUNDLE}/versions/1\n</code></pre> <ul> <li>Run the pipeline</li> </ul> <pre><code>mvn compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.dataflow.AstraDb_To_BigQuery_Dynamic \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_SECRET_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SECRET_SECURE_BUNDLE} \\\n --keyspace=${ASTRA_KEYSPACE} \\\n --table=${ASTRA_TABLE} \\\n --runner=DataflowRunner \\\n --project=${GCP_PROJECT_ID} \\\n --region=us-central1\"\n</code></pre> <ul> <li>Show Content of Table</li> </ul> <p>A dataset with the keyspace name and a table  with the table name have been created in BigQuery.</p> <pre><code>bq head -n 10 ${ASTRA_KEYSPACE}.${ASTRA_TABLE}\n</code></pre>"},{"location":"pages/tools/databases/janusgraph/","title":"JanusGraph","text":"<ul> <li>Documented on JanusGraph official documentation</li> </ul>"},{"location":"pages/tools/databases/janusgraph/#overview","title":"Overview","text":"<p>JanusGraph is designed to support the processing of graphs so large that they require storage and computational capacities beyond those that a single machine can provide. Scaling graph data processing for real time traversals and analytical queries is JanusGraph\u2019s foundational benefit. This section will discuss the various specific benefits of JanusGraph and its underlying, supported persistence solutions.</p> <ul> <li>\u2139\ufe0f Introduction to JanusGraph</li> <li>\ud83d\udce5 JanusGraph Installation</li> </ul> <p>JanusGraph uses the Java driver to connect to Cassandra as the storage backend. The Java driver itself supports connections to Astra DB natively. For example: <pre><code>CqlSession session = CqlSession.builder()\n  .withCloudSecureConnectBundle(Paths.get(\"/path/to/secure-connect-db_name.zip\"))\n  .withAuthCredentials(\"CLIENT_ID\", \"CLIENT_SECRET\")\n  .withKeyspace(\"keyspace_name\")\n  .build();\n</code></pre></p>"},{"location":"pages/tools/databases/janusgraph/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle</li> </ul> <p>This article assumes you have a running installation of JanusGraph server. This was written and tested on JanusGraph <code>v0.6.2</code>. If JanusGraph <code>v0.6.0</code> is used instead, refer to this article. It has not been tested on older versions of JanusGraph. </p> <p>You will need to choose which keyspace to use to store your graph. If it doesn't exist, you will need to create the keyspace on the Astra UI. For simplicity, the keyspace is created as <code>janusgraph</code>.</p> <p> <p>IMPORTANT</p> <p>If you want to connect JanusGraph server to ZDM, it is recommended to use the Internal Configuration File to avoid any credentials issues. Just comment out the secure bundle path part from the external file.</p> <p></p>"},{"location":"pages/tools/databases/janusgraph/#installation-and-setup","title":"Installation and Setup","text":"<p>Note: For simplicity, the secure connect bundle has been placed in <code>/path/to/scb</code></p>"},{"location":"pages/tools/databases/janusgraph/#step-1-db-information","title":"\u2705 Step 1: DB Information","text":"<p>On the JanusGraph server, move your secure bundle using secure copy or other techniques. For example: <pre><code>$ cd /path/to/scb\n$ ls -l secure-connect-janusgraph.zip\n</code></pre></p> <p>We will use this information to configure Astra DB as the storage backend for JanusGraph.</p>"},{"location":"pages/tools/databases/janusgraph/#step-2-graph-storage","title":"\u2705 Step 2: Graph Storage","text":"<p>While connecting to Astra DB from JanusGraph, it is preferred to make use of the secure connect bundle file as-is without extracting it. There are multiple ways in which a secure connect bundle file can be passed on to the JanusGraph configuration to connect to Astra DB using the DataStax driver.</p> <p>On the JanusGraph server, modify the CQL storage configuration file: <pre><code>$ cd janusgraph-0.6.2\n$ vi conf/janusgraph-cql.properties\n</code></pre> Make the necessary changes using one of the two templates:</p>"},{"location":"pages/tools/databases/janusgraph/#step-2a-internal-string-configuration","title":"\u2705 Step 2a: Internal String Configuration","text":"<p>Set the property <code>storage.cql.internal.string-configuration</code> to <code>datastax-java-driver { basic.cloud.secure-connect-bundle=/path/to/scb/secure-connect-janusgraph.zip }</code> and set the username, password and keyspace details.</p> <p>For example: <pre><code>gremlin.graph=org.janusgraph.core.JanusGraphFactory\nstorage.backend=cql\nstorage.cql.keyspace=&lt;keyspace name which was created in AstraDB&gt;\nstorage.username=&lt;clientID&gt;\nstorage.password=&lt;clientSecret&gt;\nstorage.cql.internal.string-configuration=datastax-java-driver { basic.cloud.secure-connect-bundle=/path/to/scb/secure-connect-janusgraph.zip }\n</code></pre></p> <p>Also, you can set a JVM argument to pass the secure connect bundle file as shown below and remove that property <code>(storage.cql.internal.string-configuration)</code> from the list above.</p> <pre><code>-Ddatastax-java-driver.basic.cloud.secure-connect-bundle=/path/to/scb/secure-connect-janusgraph.zip\n</code></pre>"},{"location":"pages/tools/databases/janusgraph/#step-2b-internal-file-configuration","title":"\u2705 Step 2b: Internal File Configuration","text":"<p>Set the property <code>storage.cql.internal.file-configuration</code> to an external configuration file if you would like to externalize the astra connection related properties to a separate file and specify the secure bundle and credentials information on that file.</p> <p>For example: <pre><code>gremlin.graph=org.janusgraph.core.JanusGraphFactory\nstorage.backend=cql\nstorage.cql.keyspace=janusgraph\n# Link to the external file that DataStax driver understands\nstorage.cql.internal.file-configuration=/path/to/scb/astra.conf\n</code></pre></p> <p><code>astra.conf</code> (external file) to contain: <pre><code>datastax-java-driver {\n  basic.cloud {\n    secure-connect-bundle = \"/path/to/scb/secure-connect-janusgraph.zip\"\n  }\n  basic.request {\n    timeout = \"10 seconds\"\n  }\n  advanced.auth-provider {\n    class = PlainTextAuthProvider\n    username = \"&lt;ClientID&gt;\"\n    password = \"&lt;ClientSecret&gt;\"\n  }\n}\n</code></pre></p> <p> <p>IMPORTANT</p> <p>The ClientID and ClientSecret are from the token you generated in the Prerequisites section above.</p> <p></p>"},{"location":"pages/tools/databases/janusgraph/#step-3-final-test","title":"\u2705 Step 3: Final Test","text":"<p>Start a Gremlin console: <pre><code>$ bin/gremlin.sh\n\n         \\,,,/\n         (o o)\n-----oOOo-(3)-oOOo-----\ngremlin&gt;\n</code></pre> Load a graph using Astra as the storage backend with: <pre><code>gremlin&gt; graph = JanusGraphFactory.open('conf/janusgraph-cql.properties')\n==&gt;standardjanusgraph[cql:[70bf8560-105f-11ec-a3ea-0800200c9a66-us-west1.db.astra.datastax.com]]\n</code></pre></p> <p> <p>Note</p> <p>It is normal to see some warnings on the Gremlin console. I have attached a text file with a sample output so you know what to expect.</p> <p></p> <p>In the Astra CQL Console, I can see JanusGraph created the following tables in the <code>janusgraph</code> keyspace: <pre><code>token@cqlsh&gt; USE janusgraph;\ntoken@cqlsh:janusgraph&gt; DESCRIBE TABLES;\n\nedgestore_lock_  graphindex_lock_         janusgraph_ids   \ntxlog            systemlog                graphindex       \nedgestore        system_properties_lock_  system_properties\n</code></pre></p>"},{"location":"pages/tools/ide/datastation/","title":"DataStation","text":""},{"location":"pages/tools/ide/datastation/#overview","title":"Overview","text":"<p>DataStation is an open-source data IDE for developers. It allows you to easily build graphs and tables with data pulled from SQL databases, logging databases, metrics databases, HTTP servers, and all kinds of text and binary files. Need to join or munge data? Write embedded scripts as needed in languages like Python, JavaScript, R or SQL. All in one application. This tutorial will show you step-by-step how to connect your Astra DB with DataStation. </p> <ul> <li>\u2139\ufe0f Introduction to DataStation</li> <li>\ud83d\udce5 DataStation Quick Install</li> </ul>"},{"location":"pages/tools/ide/datastation/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should Install DataStation</li> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>Clone this repository to use to set up CQL-Proxy which is a sidecar that enables unsupported CQL drivers to work with DataStax Astra</li> <li>You need your Astra Token and Astra Database ID to use CQL-Proxy</li> <li>Follow the steps in the repo to spin up CQL-Proxy using Terminal/Command Line. Once successfully running, you should see the following output:</li> <pre><code>{\"level\":\"info\",\"ts\":1651012815.176512,\"caller\":\"proxy/proxy.go:222\",\"msg\":\"proxy is listening\",\"address\":\"[::]:9042\"}\n</code></pre> </ul>"},{"location":"pages/tools/ide/datastation/#installation-and-setup","title":"Installation and Setup","text":"<p>Once you have completed all of the Prerequisites and confirmed CQL Proxy is running, you are now able to move on to setting up your Astra DB with DataStation IDE. </p> <ol> <li> <p>First, launch your DataStation IDE.  </p> </li> <li> <p>Click Add Data Source and select Cassandra</p> </li> <li> <p>A dialog will appear that will prompt you to name this connection (in this example I used Test) and your Cassandra credentials:</p> <ul> <li>Host - Use <code>localhost:9042</code> or <code>127.0.0.1:9042</code></li> <li>Keyspace - Enter the name of the keyspace that you want to use from your Astra DB. </li> <li>Username - Use <code>token</code></li> <li> <p>Password - From your Astra Token creation in the Prerequisites, find your Token. </p> <ul> <li>Ex. <code>AstraCS:BWsdjhdf...</code> </li> </ul> <p></p> </li> </ul> </li> <li> <p>Once you've entered your credentials, click Add Panel and select Database under IMPORT FROM. This will allow DataStation to connect with the database you are trying to view through the credentials you had just entered in the previous step. </p> </li> </ol> <p> Note <p>DataStation IDE currently doesn't show a success message after the credentials have been entered. The Password field will also show up blank once you've minimized the credentials panel on the sidebar, but this does not necessarily mean you have to re-enter your password.</p> <p></p> <ol> <li>You will know your database has been added once your display looks like this. You can name this Panel what you'd like. In this example, it is titled Test Panel.      </li> <li>To test and validate, you can run a quick query to one of the tables that are present in the Keyspace that you provided in Step 3.      </li> </ol> <p>...and you're done! This tutorial quickly shows you how you can easily integrate your Astra DB with the DataStation IDE to run queries, build tables, and further enhance how you interact with your data. </p>"},{"location":"pages/tools/ide/eclipse/","title":"Eclipse","text":"<p>Check back soon</p> <p>Nothing to see here yet! Check back for updates! </p>"},{"location":"pages/tools/ide/intellij/","title":"IntelliJ","text":"<ul> <li>This content has been built using Reference Documentation</li> </ul>"},{"location":"pages/tools/ide/intellij/#overview","title":"Overview","text":"<p>IntelliJ IDEA is an integrated development environment written in Java for developing computer software written in Java, Kotlin, Groovy, and other JVM-based languages. It is developed by JetBrains, and is available as an Apache 2 Licensed community edition, and in a proprietary commercial edition.</p>"},{"location":"pages/tools/ide/intellij/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should download either Community or ultimate edition of intelliJ from the Download Page</li> </ul>"},{"location":"pages/tools/ide/intellij/#installation-guide","title":"Installation Guide","text":""},{"location":"pages/tools/ide/intellij/#1-download-plugin","title":"\u2705 1. Download Plugin","text":"<p>Astra DB Explorer Installation Page</p> <ul> <li>Open the plugin panel and search for <code>astra</code></li> </ul> <pre><code>File &gt; Preferences &gt; Plugins\n</code></pre> <ul> <li>Click the <code>[INSTALL]</code> button</li> </ul> <p></p> <ul> <li>Once the plugin is downloaded and installed you will be asked to restart</li> </ul> <p></p>"},{"location":"pages/tools/ide/intellij/#2-setup-plugin","title":"\u2705 2. Setup Plugin","text":"<ul> <li>During the first restart you will got an <code>IDE error occured</code> message it is expected we will now configure the plugin</li> </ul> <ul> <li> <p>The plugin configuration is defined in a file on disk located at <code>${user.home}/.astra/config</code>. Fortunately you can do it directly in the IDE</p> </li> <li> <p>In the bottom left hand corner locate the panel <code>astra.explorer</code> and open it</p> </li> </ul> <p></p>"},{"location":"pages/tools/ide/intellij/#3-edit-profiles","title":"\u2705 3. Edit Profiles","text":"<ul> <li>In the drop down menu select <code>Edit Profiles</code> the configuration file is referred as a profile</li> </ul> <ul> <li>You will be asked if you want to create the file, click <code>[CREATE]</code></li> </ul> <ul> <li>Also pick the first option in the radio button Edit this file anyway</li> </ul> <ul> <li>The file open and the content should look like. Not that the value used for the <code>bearerToken</code> is the one starting by <code>AstraCS:....</code>. Save the file</li> </ul> <pre><code>[astraProfileFile.profiles]\ndefault = \"AstraCS:XXXX\"\n</code></pre>"},{"location":"pages/tools/ide/intellij/#4-reload-profiles","title":"\u2705 4. Reload Profiles","text":"<ul> <li>Now on the drop down menu select <code>Reload Profiles</code></li> </ul> <ul> <li>Et voila! You can now list databases on your Astra organization and for each you can see the different keyspaces</li> </ul>"},{"location":"pages/tools/ide/postman/","title":"Postman","text":""},{"location":"pages/tools/ide/postman/#overview","title":"Overview","text":"<p>Overview of Postman. What is it? Why is it important and useful with Astra?</p>"},{"location":"pages/tools/ide/postman/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> </ul>"},{"location":"pages/tools/ide/postman/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"pages/tools/ide/postman/#step-1","title":"\u2705 Step 1:","text":""},{"location":"pages/tools/ide/postman/#step-2","title":"\u2705 Step 2:","text":""},{"location":"pages/tools/ide/vscode/","title":"VSCode","text":"<p> <p>Check back soon</p> <p>Nothing to see here yet! Check back for updates! </p>"},{"location":"pages/tools/integration/apache-airflow/","title":"Apache Airflow","text":""},{"location":"pages/tools/integration/apache-airflow/#overview","title":"Overview","text":"<p>Apache Airflow is an open source workflow management system. It provides components which allow engineers to build data pipelines between different systems. These instructions will step through tasks/adjustments to be done in each product (Astra DB, cql-proxy, Apache Airflow), ultimately resulting in Airflow being able to work with AstraDB in its directed acyclic graphs (DAG).</p> <ul> <li>\u2139\ufe0f Apache Airflow Documentation</li> </ul>"},{"location":"pages/tools/integration/apache-airflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle</li> <li>You should install `python3` and `pip3` (local deployment of Airflow) or Docker (docker)</li> </ul> <p>This article was written for Apache Airflow version <code>2.2.3</code> on <code>MacOS</code> with Python <code>3.9</code>.</p>"},{"location":"pages/tools/integration/apache-airflow/#installation","title":"Installation","text":""},{"location":"pages/tools/integration/apache-airflow/#download-and-install","title":"\u2705 Download and install","text":"<p>Following the Apache Airflow reference documentation download and install the software.</p>"},{"location":"pages/tools/integration/apache-airflow/#create-the-keyspace-airflow","title":"\u2705 Create the keyspace <code>airflow</code>","text":"<p>From the Astra DB dashboard, click on your database name. Scroll down to where the keyspaces are listed, and click the <code>Add Keyspace</code> button to create a new keyspace. Name this keyspace <code>airflow</code>.</p>"},{"location":"pages/tools/integration/apache-airflow/#start-cql-proxy","title":"\u2705 Start Cql Proxy","text":"<p>DataStax\u2019s cql-proxy is designed to function as an intermediate connection point to allow legacy Apache Cassandra applications to connect to DataStax Astra DB using its new Secure Connect Bundle. There are a few ways to install and run DataStax\u2019s cql-proxy, as outlined in <code>cql-proxy</code>.</p> <p>Be sure to start <code>cql-proxy</code> with the following settings:</p> <ul> <li>Using the Secure Connect Bundle downloaded in the previous section</li> <li>Binding it to the listen IP of the server instance</li> <li>Specifying the username of \u201ctoken\u201d</li> <li>Specifying the Astra Token created for the user in Astra DB as the password</li> </ul> <p>You can run <code>cql-proxy</code> (in the foreground) from the command line in this way, like this:</p> <pre><code>./cql-proxy --bundle ~/local/astraCreds/secure-connect.zip \\\n--bind 127.0.0.1 \\\n--username token \\\n--password AstraCS:rtFckUZblahblahblahblahblahblaha3953d799a525\n</code></pre> <p>Important to note that the command shown above binds <code>cql-proxy</code> to localhost (127.0.0.1), meaning it is not reachable (by Airflow) from outside the server instance.</p>"},{"location":"pages/tools/integration/apache-airflow/#create-a-new-connection-in-apache-airflow","title":"\u2705 Create a new connection in Apache Airflow","text":"<p>Inside Apache Airflow, click <code>Connections</code> from underneath the <code>Admin</code> drop-down menu. Then click on the blue button labeled with the plus sign (<code>+</code>) to add a new connection. Fill out the form as shown in Figure 2:</p> <ul> <li>Connection Id: A unique identifier for the connection in Apache Airflow, which will be referenced inside the DAG code. We will use \u201ccassandra_cqlproxy.\u201d</li> <li>Connection Type: Select \u201cCassandra\u201d from the drop-down. If it is not present, you will have to install Airflow\u2019s Cassandra provider.</li> <li>Host: The listen address that cql-proxy is bound to. In this case, that is \u201c127.0.0.1.\u201d</li> <li>Schema: The Cassandra keyspace which we created in Astra DB. We\u2019ll set that to \u201cairflow\u201d in this case.</li> <li>Login: Your Astra DB client id.</li> <li>Password: Your Astra DB client secret.</li> <li>Port: The port that cql-proxy is listening on for the CQL native binary protocol, most likely 9042.</li> </ul> <p></p> <p>Figure 2 - Create a new Cassandra connection for Apache Airflow.</p> <p>Click the blue <code>Save</code> button to persist the new connection.</p>"},{"location":"pages/tools/integration/apache-airflow/#create-a-new-dag-in-apache-airflow","title":"\u2705 Create a new DAG in Apache Airflow","text":"<p>A directed acyclic graph (DAG) is essentially a Python script which imports one or more libraries specific to Airflow. To create a new DAG, first locate your DAG directory. By default, Airflow looks for custom DAGs in the <code>~/airflow/dags/</code> directory.</p> <p>For testing, there is a sample DAG out in the following GitHub repository: https://github.com/aar0np/DS_Python_stuff/blob/main/cassandra_test_dag.py</p> <p>This DAG uses the following line to reference the Cassandra connection we created in the above step:</p> <pre><code>hook = CassandraHook('cassandra_cqlproxy')\n</code></pre> <p>The other important aspect is that this DAG sets its unique identifier as <code>cass_hooks_tutorial</code>:</p> <pre><code>with DAG(\n    'cass_hooks_tutorial',\n</code></pre> <p>It also specifically creates two unique tasks:</p> <ul> <li><code>check_table_exists</code></li> <li><code>query_system_local</code></li> </ul>"},{"location":"pages/tools/integration/apache-airflow/#final-test","title":"\u2705 Final Test","text":"<p>To test the connection, copy the DAG mentioned above into the <code>/dags/</code> directory. Then we will invoke Airflow\u2019s task testing functionality, by running airflow tasks test and specifying:</p> <ul> <li>The DAG\u2019s unique identifier</li> <li>The name of the task to be run</li> <li>The execution date</li> </ul> <p>If today\u2019s date is 2022-02-08, the command looks like this:</p> <pre><code>airflow tasks test cass_hooks_tutorial check_table_exists 2022-02-08\n</code></pre> <p>Many messages will go by quickly. If it worked, the final messages should look something like this:</p> <pre><code>INFO - Done. Returned value was: True\nINFO - Marking task as SUCCESS. dag_id=cass_hooks_tutorial, task_id=check_table_exists, execution_date=20220208T000000, start_date=20220208T195333, end_date=20220208T195334\n</code></pre>"},{"location":"pages/tools/integration/apache-airflow/#acknowledgements","title":"Acknowledgements","text":"<p>Special thanks goes out to Obioma Anomnachi of Anant. Obi\u2019s video and GitHub repo proved quite helpful in building out this tutorial.</p>"},{"location":"pages/tools/integration/apache-beam/","title":"Apache Beam","text":""},{"location":"pages/tools/integration/apache-beam/#overview","title":"Overview","text":"<p>Beam Official documentation</p> <p>Apache Beam is an open-source, unified programming model for batch and streaming data processing pipelines which simplifies large-scale data processing dynamics. Thousands of organizations around the world choose Apache Beam due to its unique data processing features, proven scale, and powerful yet extensible capabilities.</p> <p></p> <p>Main Concepts</p> <p></p> <ul> <li> <p><code>Pipeline</code>: A <code>Pipeline</code> encapsulates your entire data processing task, from start to finish. This includes reading input data, transforming that data, and writing output data. All Beam driver programs must create a Pipeline. When you create the Pipeline, you must also specify the execution options that tell the Pipeline where and how to run.</p> </li> <li> <p><code>PCollection</code>: A <code>PCollection</code> represents a distributed data set that your Beam pipeline operates on. The data set can be bounded, meaning it comes from a fixed source like a file, or unbounded, meaning it comes from a continuously updating source via a subscription or other mechanisms. Your pipeline typically creates an initial PCollection by reading data from an external data source, but you can also create a PCollection from in-memory data within your driver program. From there, PCollections are the inputs and outputs for each step in your pipeline.</p> </li> <li> <p><code>PTransform</code>: A <code>PTransform</code> represents a data processing operation, or a step, in your pipeline. Every PTransform takes one or more PCollection objects as input, performs a processing function that you provide on the elements of that PCollection, and produces zero or more output PCollection objects.</p> </li> <li> <p>Input and Output so called <code>I/O transforms</code>: Beam comes with a number of \u201cIOs\u201d - library PTransforms that read or write data to various external storage systems.</p> </li> </ul> <p>I/O Connectors</p> <p>Apache Beam I/O connectors provide read and write transforms for the most popular data storage systems so that Beam users can benefit from natively optimised connectivity. With the available I/Os, Apache Beam pipelines can read and write data to and from an external storage type in a unified and distributed way.</p> <p>Integration with DataStax Astra is inspired by the built-in <code>CassandraIO</code> and <code>PulsarIO</code> connectors. This integration leverages a new <code>AstraIO</code> connector.</p> <p>Runners</p> <p>A runner in Apache Beam is responsible for executing pipelines on a particular processing engine or framework, such as Apache Flink or Google Cloud Dataflow. The runner translates the Beam pipeline into the appropriate format for the underlying engine, manages job execution, and provides feedback on job progress and status.</p> <p></p>"},{"location":"pages/tools/integration/apache-beam/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have Java11+, Maven, and Git installed</li> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle</li> </ul> Setup your <code>JAVA</code> Development environment <ul> <li> Install Java Development Kit (JDK) 8+</li> </ul> <p>Use java reference documentation targetting your operating system to install a Java Development Kit. You can then validate your installation with the following command.</p> <pre><code>java --version\n</code></pre> <ul> <li> Install Apache Maven (3.8+)</li> </ul> <p>Samples and tutorials have been designed with <code>Apache Maven</code>. Use the reference documentation top install maven validate your installation with </p> <pre><code>mvn -version\n</code></pre> Setup Datastax <code>Astra DB</code> <ul> <li> Create your DataStax Astra account: </li> </ul> <p>Sign Up</p> <ul> <li> Create an Astra Token</li> </ul> <p>An astra token acts as your credentials, it holds the different permissions. The scope of a token is the whole organization (tenant) but permissions can be edited to limit usage to a single database.</p> <p>To create a token, please follow this guide</p> <p>The Token is in fact three separate strings: a <code>Client ID</code>, a <code>Client Secret</code> and the <code>token</code> proper. You will need some of these strings to access the database, depending on the type of access you plan. Although the Client ID, strictly speaking, is not a secret, you should regard this whole object as a secret and make sure not to share it inadvertently (e.g. committing it to a Git repository) as it grants access to your databases.</p> <pre><code>{\n\"ClientId\": \"ROkiiDZdvPOvHRSgoZtyAapp\",\n\"ClientSecret\": \"fakedfaked\",\n\"Token\":\"AstraCS:fake\"\n}\n</code></pre> <p>It is handy to have your token declare as an environment variable (replace with proper value):</p> <pre><code>export ASTRA_TOKEN=\"AstraCS:replace_me\"\n</code></pre> <ul> <li> Create a Database and a keyspace</li> </ul> <p>With your account you can run multiple databases, a Databases is an Apache Cassandra cluster. It can live in one or multiple regions (dc). In each Database you can have multiple keyspaces. In the page we will use the database name <code>db_demo</code> and the keyspace <code>keyspace_demo</code>.</p> <p>You can create the DB using the user interface and here is a tutorial. You can also use Astra command line interface. To install and setup the CLI run the following:</p> <pre><code>curl -Ls \"https://dtsx.io/get-astra-cli\" | bash\nsource ~/.astra/cli/astra-init.sh\nastra setup --token ${ASTRA_TOKEN}\n</code></pre> <p>To create DB and keyspace with the CLI:</p> <pre><code>astra db create db_demo -k keyspace_demo --if-not-exists\n</code></pre> <ul> <li> Download the Secure Connect Bundle for current database</li> </ul> <p>A Secure Connect Bundle contains the certificates and endpoints informations to open a mTLS connection. Often mentionned as <code>scb</code> its scope is a database AND a region. If your database is deployed on multiple regions you will have to download the bundle for each one and initiate the connection accordingly. Instructions to download Secure Connect Bundle are here</p> <p></p> <p>You can download the secure connect bundle from the user interface and here is a tutorial. You can also use Astra command line interface.</p> <pre><code>astra db download-scb db_demo -f /tmp/secure-connect-bundle-db-demo.zip\n</code></pre>"},{"location":"pages/tools/integration/apache-beam/#installation-and-setup","title":"Installation and Setup","text":"<ul> <li>Clone the Repository with sample flows. The different flows are distributed in 2 different modules. <code>sample-beams</code> contains flows that do not interact with Google Cloud solutions and will be run with a direct runner. <code>sample-dataflows</code> contains flows that could be executed.</li> </ul> <pre><code>git clone https://github.com/DataStax-Examples/astra-dataflow-starter.git\n</code></pre> <ul> <li>Navigate to the repository and build the project with maven.</li> </ul> <pre><code>cd astra-dataflow-starter\nmvn clean install -Dmaven.test.skip=true\n</code></pre> More on the <code>maven</code> project setup locally <ul> <li> Clone the Repository with <code>AstraIO</code> and sample flows</li> </ul> <pre><code>git clone https://github.com/DataStax-Examples/astra-dataflow-starter.git\n</code></pre> <ul> <li> Build the project with maven</li> </ul> <pre><code>cd astra-dataflow-starter\nmvn clean install -Dmaven.test.skip=true\n</code></pre> <p>The different flows are distributed in 2 different modules: </p> <ul> <li><code>sample-beams</code> contains flows that do not interact with Google Cloud solutions and will be run with a direct runner. </li> </ul> <p></p> <ul> <li><code>sample-dataflows</code> contains flow that could be executed </li> </ul> <p></p> <pre><code>\n</code></pre>"},{"location":"pages/tools/integration/apache-beam/#examples","title":"Examples","text":""},{"location":"pages/tools/integration/apache-beam/#1-import-a-csv-file","title":"1. Import a CSV File","text":"<p>In this flow a <code>CSV</code> file is parsed to populate a table with same structure in Astra. The mapping from CSV to the table is done manually. The dataset is a list of languages.</p> <p></p> <ul> <li>Access folder <code>samples-beam</code> in the project.</li> </ul> <pre><code>cd samples-beam\npwd\n</code></pre> <ul> <li>Setup Environment variables</li> </ul> <pre><code># Database name (use with CLI)\nexport ASTRA_DB=&lt;your-db-name&gt;\n# Keyspace name \nexport ASTRA_KEYSPACE=&lt;your-keyspace-name&gt;\n# Path of local secure connect bundle\nexport ASTRA_SCB_PATH=&lt;your-secure-connect-bundle&gt;\n# Astra Token starting by AstraCS:...\nexport ASTRA_TOKEN=&lt;your-token&gt;\n</code></pre> <ul> <li>Run Beam pipeline</li> </ul> <pre><code> mvn clean compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.beam.Csv_to_AstraDb \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SCB_PATH} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --csvInput=`pwd`/src/test/resources/language-codes.csv\"\n</code></pre> <ul> <li>Check output data in Astra</li> </ul> <pre><code>astra db cqlsh ${ASTRA_DB} \\\n-k ${ASTRA_KEYSPACE} \\\n-e \"SELECT * FROM languages LIMIT 10;\"\n</code></pre>"},{"location":"pages/tools/integration/apache-beam/#2-export-table-as-csv","title":"2. Export Table as CSV","text":"<p>In this flow a Cassandra table is exported as a CSV file. The mapping from table to csv row is done manually. The same objects are reused from <code>#1</code></p> <p></p> <ul> <li>Access folder</li> </ul> <pre><code>cd samples-beam\npwd\n</code></pre> <ul> <li>Setup Environment variables</li> </ul> <pre><code># Database name (use with CLI)\nexport ASTRA_DB=&lt;your-db-name&gt;\n# Keyspace name \nexport ASTRA_KEYSPACE=&lt;your-keyspace-name&gt;\n# Path of local secure connect bundle\nexport ASTRA_SCB_PATH=&lt;your-secure-connect-bundle&gt;\n# Astra Token starting by AstraCS:...\nexport ASTRA_TOKEN=&lt;your-token&gt;\n</code></pre> <ul> <li>Run Beam pipeline</li> </ul> <pre><code> mvn clean compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.beam.AstraDb_To_Csv \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SCB_PATH} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --table=languages \\\n --csvOutput=`pwd`/src/test/resources/out/language\"\n</code></pre> <ul> <li>Check output data in astra</li> </ul> <pre><code>ls -l `pwd`/src/test/resources/out\ncat `pwd`/src/test/resources/out/language-00001-of-00004\n</code></pre>"},{"location":"pages/tools/integration/apache-beam/#3-import-cassandra-table","title":"3. Import Cassandra Table","text":"<p>Similar to ZDM a cassandra Table is imported into Astra. We are reusing the same data model as before. Mapping is manual. We can note that Cassandra reading is operated with <code>CassandraIO</code> (driver3x) where the load is done with <code>AstraDbIO</code> (drivers4x).</p> <p></p> <ul> <li>Access folder</li> </ul> <pre><code>cd samples-beam\npwd\n</code></pre> <ul> <li>Start Cassandra as a docker image with docker compose: Project propose a docker-compose to run Cassandra locally. Use <code>docker-compose</code> to start the containers</li> </ul> <pre><code>docker-compose -f ./src/main/docker/docker-compose.yml up -d\n</code></pre> <ul> <li>Wait a few seconds for Cassandra to Start. The following command give you the status of the container</li> </ul> <pre><code>docker-compose -f ./src/main/docker/docker-compose.yml ps | cut -b 55-61\n</code></pre> <ul> <li>Validate Cassandra is ready: By connecting with <code>cqlsh</code> and displaying the datacenter.</li> </ul> <pre><code>docker exec -it `docker ps | \\\ngrep cassandra:4.1.1 | \\\ncut -b 1-12` cqlsh -e \"SELECT data_center FROM system.local;\"\n</code></pre> <ul> <li>Setup Env variables</li> </ul> <pre><code>export ASTRA_DB=&lt;your-db-name&gt;\nexport ASTRA_KEYSPACE=&lt;your-keyspace-name&gt;\nexport ASTRA_SCB_PATH=&lt;your-secure-connect-bundle&gt;\nexport ASTRA_TOKEN=&lt;your-token&gt;\n</code></pre> <ul> <li>Run the pipeline: Keyspaces and Tables are created in local cassandra before starting the copy into Astra.</li> </ul> <pre><code> mvn clean compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.beam.Cassandra_To_AstraDb \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SCB_PATH} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --cassandraHost=localhost \\\n --cassandraKeyspace=demo \\\n --cassandraTableName=languages \\\n --cassandraPort=9042 \\\n --tableName=languages\"\n</code></pre> <ul> <li>Check data in Cassandra with <code>cqlsh</code></li> </ul> <pre><code>docker exec -it `docker ps \\\n| grep cassandra:4.1.1 \\\n| cut -b 1-12` \\\ncqlsh -e \"SELECT *  FROM samples_beam.languages LIMIT 10;\"\n</code></pre> <ul> <li>Check data in Astra destination with <code>cqlsh</code> (CLI)</li> </ul> <pre><code>astra db cqlsh ${ASTRA_DB} \\\n-k ${ASTRA_KEYSPACE} \\\n-e \"SELECT * FROM languages LIMIT 10;\"\n</code></pre>"},{"location":"pages/tools/integration/apache-beam/#4-generative-ai","title":"4. Generative AI","text":"<p>This use cases is divided in 2 flows. In the first step we will import a CSV file as before mapping the CSV schema in destination table. Second flow will alter the table to add the embeddings vector and populate it after calling OpenAI Embedding API</p> <p></p> <ul> <li>Access folder</li> </ul> <pre><code>cd samples-beam\npwd\n</code></pre> <ul> <li>Setup Env variables</li> </ul> <pre><code>export ASTRA_DB=&lt;your-db-name&gt;\nexport ASTRA_KEYSPACE=&lt;your-keyspace-name&gt;\nexport ASTRA_SCB_PATH=&lt;your-secure-connect-bundle&gt;\nexport ASTRA_TOKEN=&lt;your-token&gt;\n</code></pre> <ul> <li>Import Data with first flow</li> </ul> <pre><code> mvn clean compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.beam.genai.GenAI_01_ImportData \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SCB_PATH} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --csvInput=`pwd`/src/main/resources/fables_of_fontaine.csv\"\n</code></pre> <p>A table is created with the following structure:</p> <pre><code>CREATE TABLE IF NOT EXISTS ai.fable (\ndocument_id text PRIMARY KEY,\ndocument text,\ntitle text\n);\n</code></pre> <ul> <li>Check output data in Astra</li> </ul> <pre><code>astra db cqlsh ${ASTRA_DB} \\\n-k ${ASTRA_KEYSPACE} \\\n-e \"SELECT * FROM fable LIMIT 10;\"\n</code></pre> <p></p> <ul> <li>Add extra environment variables</li> </ul> <pre><code>export ASTRA_TABLE=fable\nexport OPENAI_KEY=&lt;change_me&gt;\n</code></pre> <ul> <li>Run pipeline</li> </ul> <pre><code> mvn clean compile exec:java \\\n-Dexec.mainClass=com.datastax.astra.beam.genai.GenAI_02_CreateEmbeddings \\\n-Dexec.args=\"\\\n --astraToken=${ASTRA_TOKEN} \\\n --astraSecureConnectBundle=${ASTRA_SCB_PATH} \\\n --astraKeyspace=${ASTRA_KEYSPACE} \\\n --openAiKey=${OPENAI_KEY} \\\n --table=${ASTRA_TABLE}\"\n</code></pre>"},{"location":"pages/tools/integration/apache-nifi/","title":"Apache NiFi","text":"<p>This is an adaptation of the Steven Matison Blogpost</p> <p>\ud83d\udccb On this page</p> <ul> <li>A - Overview</li> <li>B - Prerequisites</li> <li>C - Log Ingestion to Astra with Stargate Document Api</li> </ul>"},{"location":"pages/tools/integration/apache-nifi/#overview","title":"Overview","text":""},{"location":"pages/tools/integration/apache-nifi/#what-is-nifi","title":"\ud83d\udcd8  What is NiFi?","text":"<p>Apache NiFi is a software project from the Apache Software Foundation designed to automate the flow of data between software systems. It is super powerful tool I have been using for a few years to develop data flows and data pipelines. With NiFi I can do just about anything without writing a single line of code.</p> <p>You can use NiFi\u2019s <code>invokeHttp processor</code> for any Astra API calls.</p> <p>You can also use native NiFi Cassandra Processors:</p> <ul> <li>QueryCassandraRecord</li> <li>PutCassandraRecord</li> <li>and PutCassandraQL against Astra.</li> </ul>"},{"location":"pages/tools/integration/apache-nifi/#my-astra-nifi-templates","title":"\ud83d\udcd8  My Astra NiFi Templates","text":"<ul> <li> <p>You can find my official NiFi Astra Cassandra templates here</p> </li> <li> <p>You can also find templates from my previous life here</p> </li> </ul>"},{"location":"pages/tools/integration/apache-nifi/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should install a `Java JDK 1.8+` and Apache Maven</li> <li>Download and install Apache Nifi</li> <li>You should add `invokeHttp` and `Cassandra` [processors](https://nifi.apache.org/docs/nifi-docs/html/getting-started.html#adding-a-processor)</li> </ul>"},{"location":"pages/tools/integration/apache-nifi/#log-ingestion-to-astra-with-nifi","title":"Log Ingestion to Astra with NiFi","text":"<p>In this blog I am going to show you how to ingest raw log data into cassandra using NiFi and Astra. With NiFi ingesting data from any source is super easy. With Astra and Cassandra ingesting raw data can be a challenge due to data model constraints (primary keys and clustering columns).</p> <p>In this demo I am going to remove that constraint and ingest all raw data using Astra &amp; Stargate Document API which is accepting of schemaless JSON data. Although not a focus off this blog, it is fully possible to build a cassandra data model and do this log ingestion using NiFi Cassandra Processors or Astra REST API against standard cassandra database tables.</p> <p>In this demo we are going to communicate with Astra via Stargate\u2019s Documement APIs.</p>"},{"location":"pages/tools/integration/apache-nifi/#step-1-get-nifi-authorized-for-astra-calls","title":"\u2705 Step 1 :  Get NiFi Authorized for Astra Calls","text":""},{"location":"pages/tools/integration/apache-nifi/#getauthtoken","title":"GetAuthToken","text":"<ul> <li>Upload and add Get Astra Get Auth Token Template to your canvas. Record the Process Group Id for later.</li> <li>Collect Astra details needed: astra databaseid, region, api url, username, password.</li> <li>Update Process Group variables with API url, process Group Id, Username and Password.</li> <li>Configure and Enable SSL Context Services. For simple demo purposes we use java cacerts and in my environment I have copied ca certs to local path /nifi/ssl/cacerts. You will need to locate your path to cacerts and adjust. The cacerts password is \u201cchangeit\u201d. You can also use Astra Secure Bundle and keystore/trustore found within that bundled zip file.</li> <li>Confirm NiFi host:port in the Blue InvokeHTTP Processors.</li> <li>Play the data flow and confirm variable astraToken is filled with authorization token.</li> </ul> <p>\u2139\ufe0f Things to Note:</p> <ul> <li>Top of flow (GenerateFlowFile) will kick off the auth process every 30 minutes.</li> <li>For sake of this demo, all variables are included in GetAuthToken Process Group. In production or in your data flow you will want those variables in the parent location. Adjust your own flow accordingly.</li> <li>For demo purposes failure routes are visible. In production, these may be auto terminated or routed to exception handling.</li> </ul>"},{"location":"pages/tools/integration/apache-nifi/#step-2-create-data-flow-for-log-ingestion","title":"\u2705 Step 2 :  Create Data Flow for Log Ingestion","text":"<p>In this first example, we are going to ingest apache log data from a custom log file. Reference the template Astra Apache Logs to Cassandra with Stargate for this data flow. This log data happens to be on the same NiFi host in the normal /var/log/httpd/ location. The custom apache log file is in the format of:</p> <pre><code>&lt;IfModule log_config_module&gt;\nLogFormat \"%&gt;s  %U  %h  %{%Y-%m-%d %H:%M:%S}t\" urlsdetails\n        CustomLog \"/var/log/httpd/access-file-details.log\" urlsdetails\n&lt;/IfModule&gt;\n</code></pre> <p>The CSVReader Schema used in QueryRecord is as follows:</p> <pre><code>{\n\"name\": \"apache_logs\",\n\"type\": \"record\",\n\"fields\": [\n{ \"name\": \"http_status\", \"type\": \"string\" },\n{ \"name\": \"access_url\", \"type\": \"string\" },\n{ \"name\": \"ip\", \"type\": \"string\" },\n{ \"name\": \"apachetime\", \"type\": \"string\" }\n]\n}\n</code></pre> <p>And the output of the JSON Writer is as follows:</p> <pre><code>{\n\"http_status\": \"200\",\n\"access_url\": \"/INTROV8.mp3\",\n\"ip\": \"115.164.45.55\",\n\"apachetime\": \"2021-01-26 14:16:58\"\n}\n</code></pre> <p>\u26a0\ufe0f Notice this JSON structure is exactly what we need to insert into Astra. We do not have to create the collection or schema ahead of time. This collection creation will automatically happen with the delivery of the first document. \ud83d\udca1</p> <p>\u2139\ufe0f Things to Note:</p> <ul> <li>For portability of the log data flow template, the SSL Context Service is duplicated. You can adjust your flow to use a single context service at the root canvas level.</li> <li>Some of the NiFi Variables from above template are referenced in this template. Adjust your flow accordingly with root level variables or import this template into same Process Group above.</li> </ul>"},{"location":"pages/tools/integration/apache-nifi/#step-3-verify-log-data-with-cql-console","title":"\u2705 Step 3 :  Verify Log Data With Cql Console","text":"<p>Login to the Astra and navigate to your Cql Consoe and execute the following query:</p> <pre><code>select count(*) FROM apache_log;\n</code></pre> <p>\u26a0\ufe0f For this demo it is not important to look at the data, only important to verify results are in Astra. In future updates I will go into Postman and show how to access the data in a meaningful manner. For now, let us just bask in the glory of being able to ingest log data to cassandra without data modeling.</p>"},{"location":"pages/tools/integration/apache-nifi/#whats-next","title":"What\u2019s Next","text":"<p>We can now use Stargate Document API to query this data source and even search into the JSON Object. We can have conversations about the raw data, build cassandra data models, and investigate how this log data can be used downtream from cassandra. Stay tuned as I add other Log Ingestion Use Cases, a UUID Generator, and more Astra NiFi content here.</p>"},{"location":"pages/tools/integration/apache-spark/","title":"Apache Spark","text":"<ul> <li>This article includes information that was originally written by Arpan Patel on Anant Github and Astra DataStax</li> </ul>"},{"location":"pages/tools/integration/apache-spark/#overview","title":"Overview","text":"<p>Apache Spark is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size. Use Apache Spark to connect to your database and begin accessing your Astra DB tables using Scala in spark-shell.</p> <ul> <li>\u2139\ufe0f Introduction to Apache Spark</li> <li>\ud83d\udce5 Apache Spark Download Link</li> </ul>"},{"location":"pages/tools/integration/apache-spark/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle and unpack it.</li> <li>Download and install the latest version of Spark Cassandra Connector that matches with your Apache Spark and Scala version from the maven central repository. To find the right version of SCC, please check SCC compatibility here.</li> </ul>"},{"location":"pages/tools/integration/apache-spark/#installation-and-setup","title":"Installation and Setup","text":"<p>These steps assume you will be using Apache Spark in local mode. For help using Spark cluster mode click the chat button on the bottom of the screen.</p>"},{"location":"pages/tools/integration/apache-spark/#steps","title":"\u2705  Steps:","text":"<ol> <li> <p>Expand the downloaded Apache Spark package into a directory, and assign the directory name to <code>$SPARK_HOME</code>.</p> </li> <li> <p>Navigate to this directory using <code>cd $SPARK_HOME</code></p> </li> <li> <p>Append the following lines at the end of a file called <code>$SPARK_HOME/conf/spark-defaults.conf</code> (you may be able to find a template under $SPARK_HOME/conf directory), and replace the second column (value) with the first four lines:</p> </li> </ol> <pre><code>spark.files $SECURE_CONNECT_BUNDLE_FILE_PATH/secure-connect-astraiscool.zip\nspark.cassandra.connection.config.cloud.path secure-connect-astraiscool.zip\nspark.cassandra.auth.username &lt;&lt;CLIENT ID&gt;&gt;\nspark.cassandra.auth.password &lt;&lt;CLIENT SECRET&gt;&gt;\nspark.dse.continuousPagingEnabled false\n</code></pre> <ol> <li>Launch spark-shell and enter the following scala commands:</li> </ol> <pre><code>import com.datastax.spark.connector._\nimport org.apache.spark.sql.cassandra._\nspark.read.cassandraFormat(\"tables\", \"system_schema\").load().count()\n</code></pre> <p>You should expect to see the following output:</p> <pre><code>$ bin/spark-shell\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nSpark context Web UI available at http://localhost:4040\nSpark context available as 'sc' (master = local[*], app id = local-1608781805157).\nSpark session available as 'spark'.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.0.1\n      /_/\n\nUsing Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.9.1)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala&gt; import com.datastax.spark.connector._\nimport com.datastax.spark.connector._\n\nscala&gt; import org.apache.spark.sql.cassandra._\nimport org.apache.spark.sql.cassandra._\n\nscala&gt; spark.read.cassandraFormat(\"tables\", \"system_schema\").load().count()\nres0: Long = 25\nscala&gt; :quit\n</code></pre>"},{"location":"pages/tools/integration/authorizer/","title":"Authorizer","text":""},{"location":"pages/tools/integration/authorizer/#overview","title":"Overview","text":"<p>Authorizer is an open source auth solution for application.  It works with many different databases, allowing the developers to use a single datastore for the entire application stack and have complete control over all user data.</p> <ul> <li>\u2139\ufe0f Authorizer Documentation</li> </ul>"},{"location":"pages/tools/integration/authorizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle</li> </ul>"},{"location":"pages/tools/integration/authorizer/#installation","title":"Installation","text":""},{"location":"pages/tools/integration/authorizer/#step-0-download-and-install","title":"\u2705 Step 0  Download and install","text":"<p>Following the Authorizer documentation download and untar the software where you would like to install it.</p>"},{"location":"pages/tools/integration/authorizer/#step-1-create-the-keyspace-authorizer","title":"\u2705 Step 1  Create the keyspace <code>authorizer</code>","text":"<p>From the Astra DB dashboard, click on your database name. Scroll down to where the keyspaces are listed, and click the <code>Add Keyspace</code> button to create a new keyspace. Name this keyspace <code>authorizer</code>.</p>"},{"location":"pages/tools/integration/authorizer/#step-2-create-configuration-file","title":"\u2705 Step 2  Create configuration file","text":"<p>Use the delivered <code>.env.sample</code> file to create a new <code>.env</code> file for your configuration.  Edit this file with Atom, Vi, or whichever editor you choose. <pre><code>    cd authorizer\n    cp .env.sample .env\n    atom .env\n</code></pre></p>"},{"location":"pages/tools/integration/authorizer/#step-3-create-base64-encoded-strings-from-your-cert-cacrt-and-key-files","title":"\u2705 Step 3  Create base64 encoded strings from your cert, ca.crt, and key files","text":"<p>To successfully connect with Astra DB, you will need to open the secure bundle and convert the following files into base64 encoded strings:</p> <ul> <li>cert</li> <li>ca.crt</li> <li>key</li> </ul> <p>You can accomplish this with the <code>base64</code> command:</p> <pre><code>base64 cert cert_base64_file\nbase64 ca.crt ca_base64_file\nbase64 key key_base64_file\n</code></pre> <p>Note that you can omit the file parameter and output the base64 encoded string to STDOUT for easy copy/paste accessibility.</p>"},{"location":"pages/tools/integration/authorizer/#step-4-connect-to-astra-db","title":"\u2705 Step 4  Connect to Astra DB","text":"<p>To connect to Astra DB, you will need to specify the following variables in the <code>.env</code> file:</p> <pre><code>DATABASE_HOST=\"ASTRA_DB_ID-ASTRA_DB_REGION.db.astra.datastax.com\"\nDATABASE_TYPE=\"cassandradb\"\nDATABASE_PORT=29042\nDATABASE_USERNAME=\"token\"\nDATABASE_PASSWORD=\"AstraCS:yourAstraT0ken\"\n\nDATABASE_CERT=\"LS0tLS1CRUdJTiBDblahblahblahnotrealRVJUSUZJQ0FURS0tLS0\"\nDATABASE_CERT_KEY=\"RXNRNVcKYXkwblahblahblahnotrealkt4b1FnL2s4K29IaD\"\nDATABASE_CA_CERT=\"WVhneERqQU1CZblahblahblahnotrealWQkFzVEJVTnNiM1Z\"\n</code></pre>"},{"location":"pages/tools/integration/authorizer/#step-5-start-authorizer","title":"\u2705 Step 5  Start Authorizer","text":"<p>From the <code>authorizer</code> directory, run the <code>server</code> binary from the <code>build</code> directory.  It will run in the foreground. <pre><code>build/server\n</code></pre></p> <p>Verify that it is running by bringing up the Authorizer dashboard in a browser: http://127.0.0.1:8080/dashboard/</p>"},{"location":"pages/tools/integration/authorizer/#acknowledgements","title":"Acknowledgements","text":"<p>Special thanks goes out to Lakhan Samani of Authorizer. YouTube channel GitHub repo</p>"},{"location":"pages/tools/integration/cadence/","title":"Cadence","text":""},{"location":"pages/tools/integration/cadence/#overview","title":"Overview","text":"<p>Cadence is a multi-tenant orchestration framework that helps with managing workflows. It scales horizontally to handle millions of concurrent executions from various customers. Cadence Open Sources uses docker compose to run their server, and uses Apache Cassandra\u24c7 as its default backend dependency. Using docker compose, users are able to also use Cadence with MySQL, PostgreSQL, Statsd+Graphite, and Elasticsearch.</p> <ul> <li>\u2139\ufe0f Introduction to Cadence</li> <li>\ud83d\udce5 Cadence Quick Install</li> </ul>"},{"location":"pages/tools/integration/cadence/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> </ul> <p> <p>Note</p> <p>This runbook was written using Mac OS Monterey but it will also work with Windows. Any Windows-specific instructions will be noted as such.</p> <p></p>"},{"location":"pages/tools/integration/cadence/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"pages/tools/integration/cadence/#1-setup-astra","title":"\u2705 1.  Setup Astra","text":"<ol> <li>In your Astra database, create two new keyspaces called \"cadence\" and \"cadence_visibility\". You will be using both of these in the next steps.</li> <li>Make sure to create an Astra token with Admin Role</li> <li>Get your Database ID</li> </ol> <p> Find your Database ID in one of two ways <p><ol><li>Navigate to your your database and get the last ID in the URL: <code>https://astra.datastax.com/org/.../database/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</code></li> <li>Copy and paste the Datacenter ID without the trailing <code>-1</code> from the Regions section of your Astra Dashboard.</li></ol></p> <p></p>"},{"location":"pages/tools/integration/cadence/#2-cadence-pre-setup","title":"\u2705 2.  Cadence Pre-setup","text":"<ol> <li>Clone this GitHub repository</li> <li>Navigate to your cloned repository and using your preferred text editor (e.g. VisualStudio or Sublime), update the .env file with your Astra Token and Astra Database ID that you obtained above.</li> </ol> <pre><code>ASTRA_TOKEN=&lt;your Astra token&gt;\nASTRA_DATABASE_ID=&lt;your DB ID&gt;\n</code></pre>"},{"location":"pages/tools/integration/cadence/#3-cadence-schema-migration-to-astra-db","title":"\u2705 3.  Cadence Schema Migration to Astra DB","text":"<p>For this step, you will set up the keyspaces you created earlier in the Astra prerequisites (cadence and cadence_visibility). You will be using <code>cadence-cassandra-tool</code> which is part of the Temporal repo and it relies on schema definition.</p> <ol> <li>Navigate to your cloned <code>cadence-astra-cql-proxy</code> directory</li> <li>Run the following commands to initialize the keyspaces that we created through Astra. Note that there are two sets of commands, one for <code>cadence</code> keyspace and one for <code>cadence_visibility</code> keyspace:</li> </ol> <pre><code>docker-compose -f docker-compose-schema.yaml run cadence \\\n-ep cqlproxy-cadence -k cadence setup-schema -v 0.0\ndocker-compose -f docker-compose-schema.yaml run cadence \\\n-ep cql-proxy -k cadence update-schema -d schema/cassandra/cadence/versioned/\n\ndocker-compose -f docker-compose-schema.yaml run cadence \\\n-ep cql-proxy -k cadence_visibility setup-schema -v 0.0\ndocker-compose -f docker-compose-schema.yaml run cadence \\\n-ep cql-proxy -k cadence_visibility update-schema -d schema/cassandra/visibility/versioned/\n</code></pre> <p>Once the process is completed, you should see a message similar to this:</p> <pre><code>2022/04/05 21:50:24 Starting schema setup, config=&amp;{SchemaFilePath: InitialVersion:0.0 Overwrite:false DisableVersioning:false}\n2022/04/05 21:50:24 Setting up version tables\n2022/04/05 21:50:25 Setting initial schema version to 0.0\n2022/04/05 21:50:25 Updating schema update log\n2022/04/05 21:50:26 Schema setup complete\n...\n2022/04/05 22:13:16 ---- Done ----\n2022/04/05 22:13:16 Schema updated from 0.32 to 0.33, elapsed 1.4960138s\n2022/04/05 22:13:16 All schema changes completed in 32.5941245s\n2022/04/05 22:13:16 UpdateSchemeTask done\n</code></pre> <p>Great! Your schemas have been migrated with Astra DB.</p> <p> <p>Confirm your tables exist in Astra</p> <p>You can double-check to make sure the correct tables have been created by querying your database in Astra DB\u2019s CQL Console. Run <code>DESC tables;</code> in both your <code>cadence</code> and <code>cadence_visibility</code> keyspaces. You should see there are tables loaded in that were created by the schema migration with <code>cadence-cassandra-tool</code>.</p> <p></p> <pre><code>token@cqlsh&gt; use cadence;\ntoken@cqlsh:cadence&gt; desc tables;\nhistory_node        schema_version  tasks           history_tree\ndomains_by_name_v2  executions      domains         events\ncluster_config      queue           queue_metadata  schema_update_history\n\ntoken@cqlsh:cadence&gt; use cadence_visibility ;\ntoken@cqlsh:cadence_visibility&gt; desc tables;\nopen_executions        closed_executions_v2  closed_executions\nschema_update_history  schema_version\n</code></pre>"},{"location":"pages/tools/integration/cadence/#4-run-docker-compose","title":"\u2705 4.  Run Docker Compose","text":"<p>In this step, the <code>docker-compose.yaml</code> file is already provided for you in the <code>cadence-astra-cql-proxy</code> repo. This file creates different docker containers to run Temporal server. The persistence layer is configured for you to connect with cql-proxy, and it should pull your Astra credentials from when you set it earlier.</p> <pre><code>services:\n cql-proxy:\n   container_name: cqlproxy\n   image: datastax/cql-proxy:v${CQL_PROXY_VERSION}\n...\n   environment:\n     - ASTRA_TOKEN=${ASTRA_TOKEN}\n- ASTRA_DATABASE_ID=${ASTRA_DATABASE_ID}\n- HEALTH_CHECK=true\n</code></pre> <p>Now you can run the docker-compose command to start up Cadence:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"pages/tools/integration/cadence/#5-test-and-validate","title":"\u2705 5.  Test and Validate","text":"<p>You can test your connection and play with your Cadence cluster with these instructions. Using Cadence\u2019s Command Line tool, you will be able to interact with your local Temporal server.</p> <ol> <li>Create a domain <code>samples-domain</code> by running the following command. You should see the success message once the domain is created:</li> </ol> <pre><code>% cadence --do samples-domain d re\nDomain samples-domain successfully registered.\n</code></pre> <ol> <li>Clone the sample project repository to your machine. Navigate to this project and run make to build all the projects.</li> <li>Once this is complete, you can start by running the sample Hello World project by following the instructions in that repository.</li> </ol> <p>Once you have this all running, you should be able to see your workflows reflect on both the Cadence UI and Astra UI. You can see the domain on the top left is samples-domain, the domain we created, as well as the Status of each workflow as \u201cCompleted\u201d.</p> <p></p>"},{"location":"pages/tools/integration/celery/","title":"Celery","text":""},{"location":"pages/tools/integration/celery/#overview","title":"Overview","text":"<p>Celery is a (BSD-licensed) open source, simple and flexible distributed task queue for asynchronous processing of messages. With Celery one can define units of work called \"tasks\" and dispatch them for execution, in a distributed way if desired. Celery is a Python package and as such is easily integrated in any Python project.</p> <p>Typical use cases might be: a queue of uploaded images to resize in the background, long-running tasks initiated by a Web application's API, a batch of emails scheduled for sending, ...</p> <p>Celery is composed of two parts: on one side, one or more clients define the tasks to be run and enqueue/schedule them for execution; on the other side, one or more workers pick up these tasks, execute them and optionally store the resulting values. Communication between these two parts happens through a message bus (such as RabbitMQ) acting as broker, while the return value of a task is made available back to the caller through a backend (de/serialization is transparently handled by the Celery infrastructure).</p> <p>Celery supports several backends for storing and exposing task results. Among the supported backends are Cassandra and (starting with <code>v5.3</code>) Astra DB.</p> <p>In the following we assume familiarity with the <code>celeryconfig</code> configuration object for Celery and with the usage of Cassandra as backend. See the Celery documentation for more details:</p> <ul> <li>\u2139\ufe0f Celery documentation</li> <li>\u2139\ufe0f The <code>celeryconfig</code> object</li> <li>\u2139\ufe0f Cassandra/AstraDB backend configuration guide (which covers the instructions on this page as well)</li> <li>\ud83d\udce5 Celery installation instructions</li> </ul>"},{"location":"pages/tools/integration/celery/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create an Astra Database. In the following example, a keyspace called <code>celeryks</code> is created in the database.</li> <li>Create an Astra Token with the role \"Database Administrator\" (it is desirable to leave table creation to Celery). You should have received your token while creating the database in the previous step.</li> <li>Download your secure connect bundle ZIP.   <li>Install Celery with the Cassandra backend in your local Python environment, e.g. <code>pip install celery[cassandra]</code>. See the backend-settings page for additional info.</li> <p>Keep the token information and the bundle file location ready: these will be soon provided in the Celery configuration.</p>"},{"location":"pages/tools/integration/celery/#installation-and-setup","title":"Installation and Setup","text":"<p>Here a minimal Celery setup that makes use of the Astra DB backend is described start-to-end.</p> <p>A task will be defined and executed through Celery: afterwards, its return value will be retrieved on the client side. For this example to work, a message bus is needed - here, in line with a quickstart on Celery's documentation, a dockerized RabbitMQ is used.</p>"},{"location":"pages/tools/integration/celery/#1-start-a-message-broker","title":"1.  Start a message broker","text":"<p>Make sure you have a RabbitMQ instance running in Docker with <code>docker run -d -p 5672:5672 rabbitmq</code> (it might take a while for the image to be downloaded and complete startup).</p>"},{"location":"pages/tools/integration/celery/#2-define-a-task","title":"2.  Define a task","text":"<p>Create a <code>tasks.py</code> module with the definition of a task, to be later executed through Celery:</p> <pre><code>from celery import Celery\napp = Celery('tasks')\napp.config_from_object('celeryconfig')\n@app.task\ndef sortWords(text, capitalize):\n# Rearrange the text so that words are in alphabetical order.\nwords = text.split(' ')\nsortedWords = sorted(words, key=str.upper)\nreturn ' '.join([\nw if not capitalize else w.upper()\nfor w in sortedWords\n])\n</code></pre>"},{"location":"pages/tools/integration/celery/#3-configure-celery","title":"3.  Configure Celery","text":"<p>Create a module <code>celeryconfig.py</code> in the same directory, providing (among other things) the broker and backend configuration:</p> <pre><code>broker_url = 'pyamqp://guest@localhost//'\nbroker_connection_retry_on_startup = True\ntask_serializer = 'json'\nresult_serializer = 'json'\naccept_content = ['json']\nenable_utc = True\nresult_backend = 'cassandra://'\ncassandra_keyspace = 'celeryks'                       # REPLACE_ME\ncassandra_table = 'celery_tasks'                      # REPLACE_ME\ncassandra_read_consistency = 'quorum'\ncassandra_write_consistency = 'quorum'\ncassandra_auth_provider = 'PlainTextAuthProvider'\ncassandra_auth_kwargs = {\n'username': 'client-id-from-astra-token',           # REPLACE_ME\n'password': 'client-secret-from-astra-token',       # REPLACE_ME\n}\ncassandra_secure_bundle_path = '/path/to/secure-connect-database.zip'   # REPLACE_ME\n</code></pre> <p>In the above, take care of inserting your values for:</p> <ul> <li>the keyspace name you created earlier in Astra DB;</li> <li>the table name you want Celery to store results in (no need to create it beforehand);</li> <li>the Client ID and Client Secret generated in your Astra DB token earlier (resp. as username and password in <code>cassandra_auth_kwargs</code>);</li> <li>the path to the Secure Connect Bundle you downloaded earlier.</li> </ul>"},{"location":"pages/tools/integration/celery/#4-start-the-worker","title":"4.  Start the worker","text":"<p>Start a Celery worker with:</p> <pre><code>celery -A tasks worker --loglevel=INFO\n</code></pre>"},{"location":"pages/tools/integration/celery/#5-run-and-check-a-task","title":"5.  Run and check a task","text":"<p>In a different shell, open a Python REPL and type the following commands to run a couple of tasks and retrieve their result:</p> <pre><code>from tasks import sortWords\nsorted1 = sortWords.delay('storage yay my DB is powerful results Astra', False)\nsorted1.ready()\n# Returns:     True\n# (as soon as the function completes, which here is almost immediately)\nsorted1.get()\n# Returns:     'Astra DB is my powerful results storage yay'\nsorted2 = sortWords.delay('In the land of another wizards day', capitalize=True)\nsorted2.get()\n# Returns:     'ANOTHER DAY IN LAND OF THE WIZARDS'\n</code></pre>"},{"location":"pages/tools/integration/celery/#6-optional-look-at-the-database","title":"6.  (Optional) Look at the database","text":"<p>Check the corresponding data stored on Astra DB. Navigate to the CQL Console for the database you created and enter the following commands:</p> <pre><code>USE celeryks;               // &lt;== enter your keyspace name here\n\nDESCRIBE TABLES;            // the output, e.g. \"celery_tasks\", lists the tables\n\nSELECT * FROM celery_tasks; // &lt;== enter your table name here\n</code></pre> <p></p>"},{"location":"pages/tools/integration/celery/#additional-configuration","title":"Additional configuration","text":"<p>Celery uses the DataStax Python driver for Cassandra; hence, the choice of connection parameters is that for the generic driver-based usage of Cassandra in Python.</p> <p>In particular, one may want to specify additional parameters through the <code>celeryconfig</code> such as protocol level, load-balancing policy and so on. Refer to the \"Additional configuration\" section in the Celery documentation for a more comprehensive setup.</p>"},{"location":"pages/tools/integration/feast/","title":"Feast","text":""},{"location":"pages/tools/integration/feast/#overview","title":"Overview","text":"<p>Feast is a (Apache-licensed) open-source feature store for machine learning. Feast aims at providing a fast solution to the typical MLOps needs one encounters when bringing ML applications to production.</p> <p>Feast offers a solution to the problem of training/serving skew, provides tools to standardize the data engineering workflows (thus avoiding having to \"re-invent the features\" every time), and ensures reproducible feature sets with point-in-time historical retrievals.</p> <p>This feature store supports several backends, both as offline store (for historical time-series data) and online store (with the latest features, synced from the former by Feast itself). Besides a few core backends, the Feast project features additional backends contributed by the community.</p> <p>Feast is built with the cloud in mind: one of its goals is to free MLOps practitioners and data engineers from having to manage their own infrastructure. In this spirit, starting with version <code>0.24</code>, the Feast online store for Cassandra contribution flexibly supports both Cassandra and Astra DB, as will be explained below.</p> <p>Reference documentation:</p> <ul> <li>\u2139\ufe0f Feast Documentation</li> <li>\u2139\ufe0f Cassandra online store</li> <li>\u2139\ufe0f Feast minimal quickstart</li> </ul>"},{"location":"pages/tools/integration/feast/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database In the following example, a keyspace called `feastks` is created in the database.</li> <li>You should Create an Astra Token with the role \"Database Administrator\" (Feast will have to dynamically create and delete tables in the keyspace).</li> <li>You should Download your Secure Connect Bundle.</li> <li>Install Feast, including the dependencies for the Cassandra/Astra DB backend, in your local Python environment: <code>pip install feast[cassandra]</code>.</li> </ul> <p>Keep the token information and the bundle file location ready: these will be soon provided in the Feast configuration.</p>"},{"location":"pages/tools/integration/feast/#quickstart","title":"Quickstart","text":"<p> <p>Note</p> <p>In this minimal quickstart, modeled after the one found in the Feast documentation, you will be providing the store configuration file by hand.</p> <p>Alternatively, an interactive command-line procedure to help you set up your store is available by launching <code>feast init REPO_NAME -t cassandra</code>.</p> <p>All credits for the sample code given here goes to the Feast documentation.</p> <p></p> <p>A new feature store is created and configured to use Astra DB as online store; next, a few sample features will be materialized to database; finally, historical/online feature retrieval is demonstrated.</p>"},{"location":"pages/tools/integration/feast/#install-feast","title":"Install Feast","text":"<p>See last item in the \"Prerequisites\" above.</p>"},{"location":"pages/tools/integration/feast/#create-a-feature-repository","title":"Create a feature repository","text":"<p>In a directory of your choice, create a new repository and <code>cd</code> to the corresponding repo-definition directory:</p> <pre><code>feast init astraFeatures\ncd astraFeatures/feature_repo\n</code></pre> <p>As you can see, the new feature store already contains sample data and a sample feature definition. These will be used in this walkthrough, so don't delete them.</p>"},{"location":"pages/tools/integration/feast/#configure-astra-db-as-online-store","title":"Configure Astra DB as online store","text":"<p>Locate and open the store configuration file, <code>feature_store.yaml</code>. Replace the <code>online_store</code> portion of the file with something like the following. Make sure you use your values for the Secure Bundle file full path, the Client ID and Client Secret from your token and the keyspace name:</p> <pre><code>online_store:\n    type: cassandra\n    secure_bundle_path: /path/to/secure/bundle.zip\n    username: Client_ID\n    password: Client_Secret\n    keyspace: feastks\n</code></pre> <p> <p>Settings in 'feature_store.yaml' for usage with Cassandra</p> <p>If using regular Cassandra as opposed to Astra DB, the \"online_store\" portion might look like:</p> <pre><code>online_store:\n    type: cassandra\n    hosts:\n        - 192.168.1.1\n        - 192.168.1.2\n        - 192.168.1.3\n    keyspace: feastks\n    port: 9042        # optional\n    username: user    # optional\n    password: 123456  # optional\n</code></pre> <p></p> <p>Additional settings are available when configuring your Cassandra/Astra DB online store: check out the full examples on the Feast documentation.</p>"},{"location":"pages/tools/integration/feast/#register-feature-definitions-and-deploy-the-store","title":"Register feature definitions and deploy the store","text":"<p>With the <code>apply</code> command, features defined in Python modules (in this case, <code>example.py</code>) are scanned and used for actual deployment of the infrastructure.</p> <p>Run the command</p> <pre><code>feast apply\n</code></pre> <p>This is the step that actually accesses the database. After running it, you may want to check directly the presence of a new table in the Astra DB keyspace.</p>"},{"location":"pages/tools/integration/feast/#generate-training-data","title":"Generate training data","text":"<p>This illustrates the <code>get_historical_features</code> store method, which directly scans the offline source data and performs a point-in-time join to construct the features requested up to a certain provided timestamp.</p> <p>Create a file <code>generate.py</code> and run it with <code>python generate.py</code>:</p> <pre><code>from datetime import datetime, timedelta\nimport pandas as pd\nfrom feast import FeatureStore\n# The entity dataframe is the dataframe we want to enrich with feature values\nentity_df = pd.DataFrame.from_dict(\n{\n# entity's join key -&gt; entity values\n\"driver_id\": [1001, 1002, 1003],\n# label name -&gt; label values\n\"label_driver_reported_satisfaction\": [1, 5, 3], \n# \"event_timestamp\" (reserved key) -&gt; timestamps\n\"event_timestamp\": [\ndatetime.now() - timedelta(minutes=11),\ndatetime.now() - timedelta(minutes=36),\ndatetime.now() - timedelta(minutes=73),\n],\n}\n)\nstore = FeatureStore(repo_path=\".\")\ntraining_df = store.get_historical_features(\nentity_df=entity_df,\nfeatures=[\n\"driver_hourly_stats:conv_rate\",\n\"driver_hourly_stats:acc_rate\",\n\"driver_hourly_stats:avg_daily_trips\",\n],\n).to_df()\nprint(\"----- Feature schema -----\\n\")\nprint(training_df.info())\nprint()\nprint(\"----- Example features -----\\n\")\nprint(training_df.head())\n</code></pre>"},{"location":"pages/tools/integration/feast/#load-features-in-the-online-store","title":"Load features in the online store","text":"<p>With the <code>materialize-incremental</code> command, Feast is instructed to carry the latest feature values over to the online store, for quick access during feature serving:</p> <pre><code>CURRENT_TIME=$(date -u +\"%Y-%m-%dT%H:%M:%S\")\nfeast materialize-incremental $CURRENT_TIME\n</code></pre> <p>At this point, inspection of the Astra DB table will show the presence of newly-inserted rows.</p>"},{"location":"pages/tools/integration/feast/#fetch-feature-vectors-from-the-online-store","title":"Fetch feature vectors from the online store","text":"<p>The <code>get_online_features</code> store method will query the online store and return the required features, as resulting from the last \"materialize\" operation.</p> <p>Create a <code>fetch_online.py</code> script and run it with <code>python fetch_online.py</code>:</p> <pre><code>from pprint import pprint\nfrom feast import FeatureStore\nstore = FeatureStore(repo_path=\".\")\nfeature_vector = store.get_online_features(\nfeatures=[\n\"driver_hourly_stats:conv_rate\",\n\"driver_hourly_stats:acc_rate\",\n\"driver_hourly_stats:avg_daily_trips\",\n],\nentity_rows=[\n# {join_key: entity_value}\n{\"driver_id\": 1004},\n{\"driver_id\": 1005},\n],\n).to_dict()\npprint(feature_vector)\n</code></pre>"},{"location":"pages/tools/integration/feast/#next-steps","title":"Next steps","text":"<p>Have a look at the <code>feature_store.yaml</code> examples for Cassandra and Astra DB to check the full set of options available.</p> <p>Head over to the Feast documentation to find out what you can do with your newly-deployed feature store.</p>"},{"location":"pages/tools/integration/flink/","title":"Flink","text":"<ul> <li>This article includes information that was originally written by Bret McGuire on GitHub </li> </ul>"},{"location":"pages/tools/integration/flink/#overview","title":"Overview","text":"<p>Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. This tutorial will show you step-by-step how to use Astra as a sink for results computed by Flink. These instructions are intended to demonstrate how to enable such support when using a Flink DataStream.</p> <p>This code is intended as a fairly simple demonstration of how to enable an Apache Flink job to interact with DataStax Astra. There is certainly room for optimization here. A simple example: Flink's CassandraSink will open a new Session on each open() call even though these Session objects are thread-safe. A more robust implementation would be more aggressive about memoizing Sessions, encouraging a minimal number of open sessions for multiple operations on the same JVM. This work may be undertaken in the future, but for the moment it is beyond the scope of what we're aiming for here.</p> <ul> <li>\u2139\ufe0f Introduction to Apache Flink</li> <li>\ud83d\udce5 Download Apache Flink</li> </ul>"},{"location":"pages/tools/integration/flink/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should clone this GitHub Repository</li> <li>You should have Apache Flink, Gradle, and Java installed in your system. </li> </ul> <p> <p>Note</p> <p>For this tutorial, you will need either Java 8 or Java 11 to run it. Any other version might run into an exception and cause build failure.</p> <p></p>"},{"location":"pages/tools/integration/flink/#installation-and-setup","title":"Installation and Setup","text":"<p>Now that you have gathered all of your prerequisites, you are ready to configure and setup for this example.</p> <ol> <li>Create a keyspace named <code>example</code> in your Astra database. At the moment, this name will be hard-coded.</li> <li>Download the secure connect bundle (SCB) for your database. You can find this under the \"Connect\" tab in the UI. </li> <li>Once you have downloaded your secure connect bundle, place it in <code>app/src/main/resources</code> in your GitHub directory (You do not have to unzip the file).</li> <li>Create a properties file titled <code>app.properties</code>, and place it in <code>app/src/main/resources/</code>.</li> <li>Add properties specifying your Astra client ID, Astra secret, and SCB file name. These should map to the \"astra.clientid\", \"astra.secret\", and \"astra.scb\" properties respectively. Your <code>app.properties</code> file should look something like this: <pre><code>astra.clientid=Bwy...\nastra.secret=E4dfE...\nastra.scb=secure-connect-test.zip\n</code></pre></li> </ol>"},{"location":"pages/tools/integration/flink/#test-and-validate","title":"Test and Validate","text":"<p>Once you have completed all of the prerequisites along with the section above, you can move on to this section to run the sample app and validate the connection between Flink and Astra.</p> <ol> <li>In your <code>flink-astra</code> cloned GitHub directory, run <code>./gradlew run</code></li> <li>Verify that the application runs and exits normally. If this completed successfully you should see the following message: <pre><code>BUILD SUCCESSFUL in 31s\n3 actionable tasks: 2 executed, 1 up-to-date\n</code></pre></li> <li>Navigate back to the Astra UI to use the CQL Console. You can run this sample query to confirm that the defined data from the sample app has been loaded properly: <pre><code>token@cqlsh:example&gt; select * from wordcount ;\n\n word   | count\n--------+-------\n   dogs |     1\n lazier |     1\n  least |     1\n  foxes |     1\n jumped |     1\n     at |     1\n    are |     1\n   just |     1\n  quick |     1\n   than |     1\n    fox |     1\n    our |     1\n    dog |     2\n     or |     1\n   over |     1\n  brown |     1\n   lazy |     1\n    the |     2\n\n(18 rows)\ntoken@cqlsh:example&gt; \n</code></pre></li> </ol>"},{"location":"pages/tools/integration/grafana/","title":"Grafana","text":""},{"location":"pages/tools/integration/grafana/#overview","title":"Overview","text":"<p>Grafana is a multi-platform open source analytics and interactive visualization web application. It provides charts, graphs, and alerts for the web when connected to supported data sources. A licensed Grafana Enterprise version with additional capabilities is also available as a self-hosted installation or an account on the Grafana Labs cloud service. It is expandable through a plug-in system. End users can create complex dashboards using interactive query builders.</p> <p>Community-developed Cassandra Datasource for Grafana supports both Apache Cassandra as well as DataStax AstraDB, allowing to use Cassandra as a data backend for Grafana. Data can be pulled using simple Query Configurator or more advanced but powerful Query Editor.</p> <p></p> <p>(On the picture: Query Editor at work)</p>"},{"location":"pages/tools/integration/grafana/#prerequisites","title":"Prerequisites","text":"<ul> <li>To use Grafana, you will need a running Grafana instance deployed locally or in a cloud. Locally launched Grafana in Docker works well too.</li> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle and unpack it.</li> </ul> <p>Keep the token information and the bundle file location ready: these will be soon provided in the datasource configuration.</p>"},{"location":"pages/tools/integration/grafana/#quickstart","title":"Quickstart","text":""},{"location":"pages/tools/integration/grafana/#install-the-plugin-using-cli-or-using-web-interface","title":"Install the plugin using CLI or using web-interface","text":"<ul> <li>Install the plugin using grafana console tool:</li> </ul> <pre><code>grafana-cli plugins install hadesarchitect-cassandra-datasource\n</code></pre> <p>It will be installed into your grafana plugins directory; the default is /var/lib/grafana/plugins. Alternatively, enable it using Grafana Web UI.</p>"},{"location":"pages/tools/integration/grafana/#create-a-datasource","title":"Create a Datasource","text":"<ul> <li>Add the Apache Cassandra Data Source as a data source at the datasource configuration page. </li> <li> <p>Enable <code>Custom TLS Settings</code> button and configure the datasource using following details:</p> </li> <li> <p>Host: specify the <code>host:cql_port</code> values from the <code>config.json</code> file from the SecureConnectBundle. It should look like <code>1234567890qwerty-eu-central-1.db.astra.datastax.com:29402</code> IMPORTANT Notice, it has to be the <code>cql_port</code> value, not just <code>port</code></p> </li> <li>User: <code>Client ID</code> of the API Token</li> <li>Password: <code>Client Secret</code> of the API Token</li> <li>Certificate Path: <code>/path/to/cert</code> (use <code>cert</code> file from SecureConnectBundle)</li> <li>Root Certificate Path: <code>/path/to/key</code> (use <code>key</code> file from SecureConnectBundle)</li> <li>RootCA Certificate Path: <code>/path/to/ca.crt</code> (use <code>ca.crt</code> file from SecureConnectBundle)</li> </ul> <p>Push the <code>Save and Test</code> button, if everything is right, you will see a <code>Database Connection OK</code> message.</p> <p></p> <p>If the database cannot be connected, check the following known common issues:</p>"},{"location":"pages/tools/integration/grafana/#known-issues","title":"Known issues:","text":"<p>Misconfigured Port (Using <code>port</code> instead of <code>cql-port</code>)</p> <p>Sometimes users specify the wrong port and a connection cannot be established. If you can't connect to your Astra instance, please check if the correct port specified in the datasource config (See step 3 above)</p> <p>Unavailable TLS files</p> <p>if you have an error message like <code>[ERROR] cassandra-backend-datasource: Unable create tls config, open /cert: permission denied</code>, it means that Grafana cannot open TLS certificate files. Set the proper permission f.e. using <code>chown</code> command. If you copied the files using <code>docker cp</code> command, they'll be copied by a root user and grafana will have no access to them.</p>"},{"location":"pages/tools/integration/grafana/#usage","title":"Usage","text":"<p>First, to visualize the data, you have to create a panel. Choose or create a dashboard and create a panel. In the panel setup, choose the correct datasource from the previous steps.</p> <p>There are two ways to query data from Cassandra: Query Configurator and Query Editor. Configurator is easier to use but has limited capabilities, Editor is more powerful but requires an understanding of CQL. </p>"},{"location":"pages/tools/integration/grafana/#query-configurator","title":"Query Configurator","text":"<p>Query Configurator is the easiest way to query data. At first, enter the keyspace and table name, then pick proper columns. If keyspace and table names are given correctly, the datasource will suggest the column names automatically.</p> <ul> <li>Time Column - the column storing the timestamp value, it's used to answer \"when\" question. </li> <li>Value Column - the column storing the value you'd like to show. It can be the <code>value</code>, <code>temperature</code> or whatever property you need.</li> <li>ID Column - the column to uniquely identify the source of the data, e.g. <code>sensor_id</code>, <code>shop_id</code>, or whatever allows you to identify the origin of data.</li> </ul> <p>After that, you have to specify the <code>ID Value</code>, the particular ID of the data origin you want to show. You may need to enable \"ALLOW FILTERING\" although we recommend avoiding it.</p> <p>Example Imagine you want to visualise reports of a temperature sensor installed in your smart home. Given the sensor reports its ID, time, location and temperature every minute, we create a table to store the data and put some values there:</p> <pre><code>CREATE TABLE IF NOT EXISTS temperature (\n    sensor_id uuid,\n    registered_at timestamp,\n    temperature int,\n    location text,\n    PRIMARY KEY ((sensor_id), registered_at)\n);\n\ninsert into temperature (sensor_id, registered_at, temperature, location) values (99051fe9-6a9c-46c2-b949-38ef78858dd0, '2020-04-01T11:21:59.001+0000', 18, 'kitchen');\ninsert into temperature (sensor_id, registered_at, temperature, location) values (99051fe9-6a9c-46c2-b949-38ef78858dd0, '2020-04-01T11:22:59.001+0000', 19, 'kitchen');\ninsert into temperature (sensor_id, registered_at, temperature, location) values (99051fe9-6a9c-46c2-b949-38ef78858dd0, '2020-04-01T11:23:59.001+0000', 20, 'kitchen');\n</code></pre> <p>In this case, we have to fill the configurator fields the following way to get the results:</p> <ul> <li>Keyspace - smarthome (keyspace name)</li> <li>Table - temperature (table name)</li> <li>Time Column - registered_at (occurence)</li> <li>Value Column - temperature (value to show)</li> <li>ID Column - sensor_id (ID of the data origin)</li> <li>ID Value - 99051fe9-6a9c-46c2-b949-38ef78858dd0 ID of the sensor</li> <li>ALLOW FILTERING - FALSE (not required, so we are happy to avoid)</li> </ul> <p>In the case of a few origins (multiple sensors), you will need to add more rows. If your case is as simple as that, a query configurator will be a good choice, otherwise please proceed to the query editor.</p>"},{"location":"pages/tools/integration/grafana/#query-editor","title":"Query Editor","text":"<p>Query Editor is a more powerful way to query data. To enable query editor, press the \"toggle text edit mode\" button.</p> <p></p> <p>Query Editor unlocks all possibilities of CQL including aggregations, etc. </p> <pre><code>SELECT sensor_id, CAST(temperature as double), registered_at FROM test.test WHERE id IN (99051fe9-6a9c-46c2-b949-38ef78858dd1, 99051fe9-6a9c-46c2-b949-38ef78858dd0) AND created_at &gt; $__timeFrom and created_at &lt; $__timeTo\n</code></pre> <ol> <li>Follow the order of the SELECT expressions, it's important! </li> <li>Identifier - the first property in the SELECT expression must be the ID, something that uniquely identifies the data (e.g. <code>sensor_id</code>)</li> <li>Value - The second property must be the value that you are going to show </li> <li> <p>Timestamp - The third value must be a timestamp of the value. All other properties will be ignored</p> </li> <li> <p>To filter data by time, use <code>$__timeFrom</code> and <code>$__timeTo</code> placeholders as in the example. The datasource will replace them with time values from the panel. Notice It's important to add the placeholders otherwise query will try to fetch data for the whole period of time. Don't try to specify the timeframe on your own, just put the placeholders. It's grafana's job to specify time limits.</p> </li> </ol> <p></p>"},{"location":"pages/tools/integration/grafana/#contacts","title":"Contacts","text":"<p>We hope it works well for you! In case of any questions please contact developers using GitHub Discussions.</p>"},{"location":"pages/tools/integration/liquibase/","title":"Liquibase","text":""},{"location":"pages/tools/integration/liquibase/#overview","title":"Overview","text":"<p>The purpose of this document is to guide you through the process of creating a new Liquibase project with Cassandra. In this tutorial, you will generate an example project and follow the instructions to apply and learn concepts associated with creating new Liquibase projects with Cassandra on DataStax Astra. </p> <ul> <li>\u2139\ufe0f Introduction to Liquibase</li> <li>\ud83d\udce5 Liquibase Quick Install</li> </ul>"},{"location":"pages/tools/integration/liquibase/#prerequisites","title":"Prerequisites","text":""},{"location":"pages/tools/integration/liquibase/#liquibase-prerequisites","title":"Liquibase Prerequisites","text":"<ul> <li>Install the latest version of Liquibase</li> <li>Ensure the Liquibase install directory path is set to a location in the PATH System variable</li> <li>Download the liquibase-cassandra-.jar latest release extension jar file and place this file in the `liquibase/lib` install directory"},{"location":"pages/tools/integration/liquibase/#astra-prerequisites","title":"Astra Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>Download the Simba JDBC Jar driver file for Apache Cassandra and place this file in the `liquibase/lib` install directory</li> <li>Clone this repository to use to set up CQL-Proxy which is a sidecar that enables unsupported CQL drivers to work with DataStax Astra</li> <li>You need your Astra Token and Astra Database ID to use CQL-Proxy</li> <li>Follow the steps in the repo to spin up CQL-Proxy using Terminal/Command Line. Once successfully running, you should see the following output:</li> <pre><code>{\"level\":\"info\",\"ts\":1651012815.176512,\"caller\":\"proxy/proxy.go:222\",\"msg\":\"proxy is listening\",\"address\":\"[::]:9042\"}\n</code></pre> </ul>"},{"location":"pages/tools/integration/liquibase/#installation-and-setup","title":"Installation and Setup","text":"<p>To create a Liquibase project with Cassandra on DataStax Astra on your machine, begin with the following steps:</p> <ol> <li> <p>Create a new project folder and name it LiquibaseProj.</p> </li> <li> <p>In your LiquibaseProj folder, create a new text file and name it dbchangelog.sql.</p> </li> <li> <p>Open the dbchangelog.sql file and update the changelog file with the following code snippet:  <code>--liquibase formatted sql</code> </p> </li> <li> <p>In your LiquibaseProj folder, create a new text file and name it liquibase.properties.</p> </li> <li> <p>Edit the liquibase.properties file to add the following properties:</p> </li> </ol> <p><pre><code>changelog-file: dbchangelog.sql\nurl: jdbc:cassandra://localhost:9042/test;DefaultKeyspace=test;TunableConsistency=6\ndriver: com.simba.cassandra.jdbc42.Driver\ndefaultSchemaName: test\nliquibase.hub.mode=off \n</code></pre> In <code>liquibase.properties</code> above, replace test with the name of your own keyspace.</p> <ol> <li>Add a changeset to the changelog \u2013 changeset are uniquely identified by author and id attributes. Liquibase attempts to execute each changeset in a transaction that is committed at the end. In the <code>dbchangelog.sql</code> file, add a new changeset with a create table statement. We will create a new table department using a changeset as follows:</li> </ol> <pre><code>--liquibase formatted sql\n\n--changeset bob:1\nCREATE TABLE test.DEPARTMENT (id int PRIMARY KEY, NAME text, ACTIVE BOOLEAN);\n</code></pre> <ol> <li> <p>Open the command prompt. Navigate to the LiquibaseProj directory. Run the following command: liquibase update</p> </li> <li> <p>From a SQL Client User Interface, check your database changes. You should see a new department table added to the database. For example: <code>SELECT * FROM \"keyspace\".\"department\";</code></p> </li> </ol> ID NAME ACTIVE NULL NULL NULL <p>After your first update, your database will contain the table you added along with the DATABASECHANGELOG and DATABASECHANGELOGLOCK tables:</p> <ul> <li>DATABASECHANGELOG table. This table keeps a record of all the changesets that were deployed. When you deploy, the changesets in the changelog are compared with the DATABASECHANGELOG tracking table, and only the new changesets that were not found in the DATABASECHANGELOG will be deployed.</li> <li>DATABASECHANGELOGLOCK table. This table is used internally by Liquibase to manage access to the DATABASECHANGELOG table during deployment and ensure only one instance of Liquibase is updating the database at a time, whether that is creating, updating, or deleting changes.</li> </ul>"},{"location":"pages/tools/integration/microsoft-powerquery/","title":"Microsoft Power Query","text":""},{"location":"pages/tools/integration/microsoft-powerquery/#overview","title":"Overview","text":"<p>Microsoft Power Query is a data preparation and transformation ETL engine that lets you connect to various data sources. Power Query is available in Microsoft Excel, Power BI, Power BI dataflows, Azure Data Factory wrangling dataflows, SQL Server Analysis Services, and much more. A great number of disparate data sources is available thanks to its support for third-party \"plugins\" (i.e. Connectors).</p> <p>You have two options to connect Power Query to Astra: 1. either through a standard Power Query ODBC connector, paired with the DataStax ODBC Driver for Apache Cassandra; 2. or directly through our Power Query custom connector. Keep reading to find out which one is best suited to your needs.</p> <p> <p>Note</p> <p>Power Query and the related data products run only on Microsoft Windows.  In the following, out of several products and services, usage with Microsoft Power BI (Desktop and Service) is assumed. Depending on the product/service you are using, the appearance will vary; also, some of the features might also differ.</p> <p></p> <p>The Power Query engine, which operates the Connector, can run in three different modes: with reference to Power BI, the possible setups are:</p> <ul> <li>Local, where the engine is contained in Power BI Desktop, hence the connector runs on-premises;</li> <li>Data Gateway, where the cloud product Power BI Service is used, which receives the connector data through an installed On-Premises Data Gateway (whether in \"personal mode\" or not);</li> <li>Cloud, where the whole stack runs in Azure's cloud. This requires a connector certified by Microsoft; moreover, for security reasons, it cannot make use of any external dependency (such as a custom ODBC driver).</li> </ul> <p></p> <p>Here is the support matrix for these options:</p> Mode ODBC Connector Custom Connector Local OK (requires Simba ODBC driver) OK (currently self-signed) Data Gateway OK (requires Simba ODBC driver) OK (currently self-signed) Cloud NO (security limitations from Azure) pending (certification in progress) <p>In the following, the various ways to connect with data from Astra DB are outlined: keep in mind that, reagardless whether through ODBC or the Custom connector, you will need to successfully create a report locally before publishing it to Power BI Service.</p> <p> Precautions about very large tables <p>Regardless of whether you use the ODBC or the Custom connector, reading indiscriminately from a very large table is a process that can last a long time. It is discouraged to fully import huge tables through Power Query. If you do, chances are you will see something like this \"preview\" dialog for a long time:  In essence, this is a manifestation of Cassandra's take on data models and its OLTP-first nature, whereby tables should be generally designed to support single-partition queries only (and not whole-table scans). In practice, this potential issue can reasonably be ignored below the 10k-100k-rows mark (depending on factors such as your latency requirements, the network bandwidth and cost, and the average row size). The ODBC connector lets you specify a query string in order to <code>SELECT</code> a subset of rows from very large tables: as long as the query complies with Cassandra's data modeling best practices, this is a sensible approach. Keep reading for details. The Custom connector, conversely, is not suitable for very large tables as it only supports reading a table in full. In any case, keep in mind that by reading from massive tables one might unwittingly consume a sizeable amount of Astra credits.</p> <p></p>"},{"location":"pages/tools/integration/microsoft-powerquery/#odbc-connection-local","title":"ODBC connection (local)","text":"<p>This way of connecting works by first creating and configuring, outside of Power Query, an ODBC connection right to your specific target database and then simply connecting to it (without specifying additional connection parameters anymore) from Power Query (e.g. from Power BI). Let's see how this works.</p>"},{"location":"pages/tools/integration/microsoft-powerquery/#pre-requisites","title":"Pre-requisites","text":"<p> <p>Note</p> <p>In the following it is assumed that you have a local installation of Power BI Desktop. Consult the Microsoft documentation if your goal is to use another of the products that support Power Query.</p> <p></p>"},{"location":"pages/tools/integration/microsoft-powerquery/#database","title":"Database","text":"<ul> <li>You should have an Astra account.</li> <li>You should Create an Astra Database.</li> <li>You should Have an Astra Token. You will need the various string fields contained therein.</li> <li>You should Download your Secure Connect Bundle.</li> </ul> <p> Minimal token permissions <p>While you can certainly use a standard \"Database Administrator\" token, you may want to use a least-privilege token for this data connection through the ODBC connector. These are the specifications for a minimal Custom Role for this purpose:</p> <p><ul><li>The token must have, in Table Permissions, (1) Select Table and (2) Describe Table; and (3) in API Access it needs CQL;</li> <li>It is OK if the token is scoped to just the one DB that is being used;</li> <li>If the token is disallowed on certain keyspaces, you will still be able to list the tables they contain, but you will get a permission error if trying to read data from them.</li> <p></p> <p></p> <p> <p>Note</p> <p>You might find it convenient to have some tables with data in your database in order to ensure the connection works all right. See Awesome Astra's \"Load and Export\" page for suggestions on how to load data in Astra DB.</p> <p></p>"},{"location":"pages/tools/integration/microsoft-powerquery/#odbc-driver","title":"ODBC Driver","text":"<p>You need to install and configure the \"DataStax ODBC Driver for Apache Cassandra\" on your Windows machine. A useful reference during these steps is the Install Guide.</p> <p>First, visit this download link and select your architecture (most probably the 64-bit one will do).</p> <p>Second install the driver (by double-clicking on the <code>*.msi</code> Windows installer file you just downloaded and following the instructions).</p> <p>Third you need to download and install the \"Visual C++ 2013 redistributable bundle\" on your Windows machine.</p> <p>Fourth. Run the ODBC Data Source Administrator program on Windows (choose Run as administrator). In the taskbar the program will show as \"ODBC Data Sources (64-bit)\".</p> <ol> <li>Go to the \"System DSN\" tab click \"Add...\" to create a new Data Source, selecting the DataStax Cassandra ODBC Driver;</li> <li>Configure the source (check the Install Guide linked above for details):<ul> <li>Authentication mechanisms is Cloud secure connect bundle;</li> <li>User name is <code>token</code> (the literal lower-case word \"token\"!);</li> <li>Password is the string starting with <code>AstraCS:...</code> in your Database Token;</li> <li>Upload your Secure Connect Bundle zip file;</li> <li>Choose a \"Data source name\";</li> <li>Choose a \"Description\";</li> <li>Set the \"Default keyspace\" to a keyspace in your database.</li> </ul> </li> <li>Hit the \"Test ...\" button and make sure you get back a \"Test completed successfully\" message.</li> </ol> <p> Visual guide <p>Starting the Manager:  Creating the Data source:  Configuring the Data source:  The \"Advanced settings\" can be left to their defaults:  Testing the Data source: </p> <p></p> <p>Close the ODBC Administrator. You are ready to launch Power BI Desktop and head for the next section.</p>"},{"location":"pages/tools/integration/microsoft-powerquery/#how-to","title":"How-to","text":"<p>Now that you have configured your Astra DB as a specific ODBC data source, all that remains is to channel the data coming from it into a report.</p> <p>Open Power BI Desktop and go through the \"Get Data\" action (usually the first choice when starting the program). Choose the standard ODBC source among the proposed connectors (tip: you can restrict the list by typing a search term).</p> <p> Visual guide <p>Click \"Get Data\":  Choose the ODBC connector: </p> <p></p> <p>In the configuration of the ODBC connector, pick the \"Data source name (DSN)\" you just created, i.e. your Astra DB connection.</p> <p>You can leave the \"Advanced options\" as they are; however, if you need to specify a custom query (typically to restrict the data ingestion to a subset of the whole table, such as a single partition), expand the options and write an appropriate <code>SELECT</code> CQL query.</p> <p> Visual guide <p>Choosing the Data Source Name (DSN) for the ODBC connection:  (Optional) adding a CQL query in the Advanced options: </p> <p></p> <p>You will need to provide authentication credentials once more at this point: enter again <code>token</code> as user and your <code>AstraCS:...</code> string as password, and leave the connection string empty. Confirm and wait a few seconds for the connection to be established.</p> <p> Visual guide <p></p> <p></p> <p>You will finally be able to explore the data in your database in Power BI's \"Navigator\" preview, in the form of a \"database / keyspaces / tables\" navigable hierarchy.</p> <p> Visual guide <p></p> <p></p> <p>Now you can select a table and hit \"Load\" (or \"Transform data\"): the data will be available in Power BI Desktop for you, e.g. to create a report which you can save to (local) file. See the \"Power BI Service\" section below if you want to bring the report to the cloud.</p> <p> Visual guide <p></p> <p></p>"},{"location":"pages/tools/integration/microsoft-powerquery/#astra-db-custom-connector-local","title":"Astra DB Custom connector (local)","text":"<p>When using the Custom connector, no ODBC is involved: the connector uses directly the REST API endpoints to access your database data. Pending completion of the certification process, you need to install the connector as a self-signed plugin and authorize Power BI Desktop to run it.</p> <p></p>"},{"location":"pages/tools/integration/microsoft-powerquery/#pre-requisites_1","title":"Pre-requisites","text":"<p> <p>Note</p> <p>In the following it is assumed that you have a local installation of Power BI Desktop. Consult the Microsoft documentation if your goal is to use another of the products that support Power Query.</p> <p></p>"},{"location":"pages/tools/integration/microsoft-powerquery/#database_1","title":"Database","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token. Note that in this case you only need the \"token\" string, i.e. the one starting with \"AstraCS:...\".</li> </ul> <p> Minimal token permissions <p>While you can certainly use a standard \"Database Administrator\" token, you may want to use a least-privilege token for this data connection through the Custom connector. These are the specifications for a minimal Custom Role for this purpose:</p> <p><ul><li>The token must have, in Table Permissions, (1) Select Table and (2) Describe Table; and (3) in API Access it needs REST;</li> <li>It is OK if the token is scoped to just the one DB that is being used;</li> <li>If the token is disallowed on certain keyspaces, they will show up as empty in the connector's resulting navigation table.</li> <p></p> <p></p> <p> <p>Note</p> <p>You might find it convenient to have some tables with data in your database in order to ensure the connection works all right. See Awesome Astra's \"Load and Export\" page for suggestions on how to load data in Astra DB.</p> <p></p>"},{"location":"pages/tools/integration/microsoft-powerquery/#custom-connector-setup","title":"Custom connector setup","text":"<p>This section explains how the Astra DB Custom connector is installed locally. For more information, check the connector project on GitHub.</p> <p> <p>Note</p> <p>As soon as the connector will be certified by Microsoft, manual installation will be unnecessary, as the connector will ship bundled with Power BI already.</p> <p></p> <p>First, obtain the latest <code>PQX</code> file from the releases page and place the file in (your equivalent for) directory <code>C:\\Users\\USER\\Documents\\Power BI Desktop\\Custom Connectors</code>.</p> <p>Now, as long as the connector awaits certification, it will run as \"self-signed\", so you need a way to tell Power BI that you do indeed trust it to run. To this aim, you can either list the certificate thumbprint as \"trusted\" in your system (recommended) or alternatively enable untrusted extensions in PowerBI.</p>"},{"location":"pages/tools/integration/microsoft-powerquery/#trusted-certificate-thumbprint","title":"Trusted certificate thumbprint","text":"<p>To mark the thumbprint as trusted, the steps are outlined at this link:</p> <ul> <li>Open <code>regedit</code> as admin;</li> <li>Locate the key <code>HKEY_LOCAL_MACHINE\\Software\\Policies\\Microsoft\\Power BI Desktop</code>, creating it if absent;</li> <li>In this key, create a multi-string (<code>REG_MULTI_SZ</code>) entry named <code>TrustedCertificateThumbprints</code>;</li> <li>The value must be a newline-separated string (right-click, Modify to edit as text), each with a trusted thumbprint;</li> <li>Enter the following thumbprint of the certificate used to sign the connector releases:</li> </ul> <pre><code>1BB690F359432E849D06FDEA4E82573B279AAD75\n</code></pre> <p> Visual guide <p></p> <p></p> <p>You can now close <code>regedit</code> and move on.</p>"},{"location":"pages/tools/integration/microsoft-powerquery/#enable-untrusted-connectors","title":"Enable untrusted connectors","text":"<p>Note: you don't need to do this if you marked the signer's thumbprint as trusted as per the instructions above.</p> <p>Alternatively, if you don't have admin access to <code>regedit</code>, you can lower the overall security level of PowerBI Desktop as outlined here:</p> <p><code>File</code> =&gt; <code>Options and Settings</code> =&gt; <code>Options</code> =&gt; <code>Security</code> =&gt; in \"Data Extensions\", choose Allow any data extension to load without validation or warning. Then restart PowerBI Desktop.</p> <p> Visual guide <p>Go to Power BI Desktop's Settings:  Enable untrusted extensions: </p> <p></p>"},{"location":"pages/tools/integration/microsoft-powerquery/#how-to_1","title":"How-to","text":"<p>Now you can start PowerBI Desktop, choose \"Get Data\", search for the \"Astra DB\" connector and select it. A warning will show up about the connector being a third-party plugin in beta version: you can dismiss it and move on.</p> <p> Visual guide <p>Click \"Get Data\":  Choose the \"Astra DB\" connector:  Dismiss the warning about a third-party connector: </p> <p></p> <p>You will then be asked for the connection details: database ID and region.</p> Visual guide <p></p> <p>Next, you will provide the \"Database Token\" (the string starting with <code>AstraCS:...</code>) as credentials.</p> <p> Visual guide <p></p> <p></p> <p>At this point you will be able to explore the data in your database in Power BI's \"Navigator\" preview, in the form of a \"keyspaces / tables\" navigable hierarchy.</p> <p> Visual guide <p></p> <p></p> <p>Now you can select a table and hit \"Load\" (or \"Transform data\"): the data will be available in Power BI Desktop for you, e.g. to create a report which you can save to (local) file. See the \"Power BI Service\" section below if you want to bring the report to the cloud.</p> <p> Visual guide <p></p> <p></p>"},{"location":"pages/tools/integration/microsoft-powerquery/#power-bi-service-with-a-data-gateway","title":"Power BI Service (with a Data Gateway)","text":"<p>You can publish the report to the cloud product Power BI Service with the help of an On-Premises Data Gateway.</p> <p> <p>Note</p> <p>In the following it is assumed that you have a Power Query Service account (logged to the same account as the Desktop version). Consult the Microsoft documentation if your goal is to use another of the products that support Power Query.</p> <p></p>"},{"location":"pages/tools/integration/microsoft-powerquery/#pre-requisites_2","title":"Pre-requisites","text":""},{"location":"pages/tools/integration/microsoft-powerquery/#desktop-setup","title":"Desktop setup","text":"<p>First complete one of the Desktop flows described above (i.e. either the ODBC option or the Custom connector option) and successfully create a report powered by data from Astra DB.</p> <p>You need to additionally install a Data Gateway which will serve as data bridge between the locally-running connector and the data source as seen on Power BI Service (running in Azure's cloud). See here for the \"personal mode\" installation (easiest) or here for an enterprise, production-grade setup. (See also this community blog post for gateway troubleshooting tips, also covering how to make sure that the custom-connector directory is the same as for Power BI Desktop.)</p> <p> Visual guide <p>Starting the Data Gateway interface:  Checking the \"Custom data connectors\" it has detected: </p> <p></p>"},{"location":"pages/tools/integration/microsoft-powerquery/#how-to_2","title":"How-to","text":"<p>Make sure you save the report you created to a local <code>pbix</code> file.</p> <p>From the Power BI Desktop main menu, pick \"File\" / \"Publish to Power BI\", choosing your destination workspace. This will upload the report, and the associated \"Dataset\", to the cloud service in your account.</p> <p> Visual guide <p>The \"Publish\" item in Power BI Desktop's menu:  Publishing a report: </p> <p></p> <p>Open app.powerbi.com and check that you are logged in with the correct account. Navigate to the chosen workspace, where you should see the newly-uploaded report. Click on its name to open it.</p> <p> Visual guide <p>Your workspace on Power BI Service:  Viewing a report: </p> <p></p> <p>Now you have to check that Power BI Service can read from Astra DB. Go back to the workspace and click the \"Refresh now\" button next to the dataset name (you must hover on the dataset for the  button to show up).</p> <p> Visual guide <p>Hover on the data source name to reveal the buttons: </p> <p></p> <p>This will fail, as signaled by a tiny \"danger\" icon next to the date in the \"refreshed\" column, for lack of credentials (indeed, the Desktop and the Service product do not share any credential store). To provide the credentials, click the \"Schedule refresh\" button for the dataset (hover with the mouse again to reveal the  button) and look for the \"Data source credentials\" section in the settings page you just reached.</p> <p> Visual guide <p>A failed data refresh:  The Data Source settings for the ODBC method:  The Data Source settings for the Custom connector method: </p> <p></p> <p>Choose \"Edit credentials\" to insert the required secrets, right as you did for the Desktop setup. Note that depending on the connection method you are using, the Credentials dialog will require either the <code>token</code>/<code>AstraCS:...</code> pair or just the <code>AstraCS:...</code> token string (for ODBC and Custom connector, respectively).</p> <p> Visual guide <p>Entering the credentials for the ODBC method:  Entering the credentials for the Custom connector method:  Credentials are updated (ODBC):  Credentials are updated (Custom connector): </p> <p></p> <p>Then click \"Sign in\" to confirm the credentials, go back to the workspace and try \"Refresh now\" again. There should be no errors anymore.</p> <p>As a confirmation exercise, you can try changing some data on the table in an obvious way, then triggering a refresh, and finally opening the report again (you may need to reload the browser page to see the change finally reflected in your report).</p> <p> Further problems during data refresh <p>If you notice the small \"danger\" icon next to the dataset persists, there is presumably something wrong with the data connection. Please check the above-mentioned community blog post as a valid starting point for your troubleshooting journey.</p> <p></p>"},{"location":"pages/tools/integration/pentaho/","title":"Pentaho","text":"<p>This article was originally written by Erick Ramirez on community.datastax.com</p> <p></p>"},{"location":"pages/tools/integration/pentaho/#overview","title":"Overview","text":"<p>Pentaho Data Integration (PDI) provides the Extract, Transform, and Load (ETL) capabilities that facilitate the process of capturing, cleansing, and storing data using a uniform and consistent format that is accessible and relevant to end users and IoT technologies.</p> <ul> <li>\u2139\ufe0f Introduction to PDI</li> <li>\ud83d\udce5 PDI Download Link</li> <li>\ud83d\udcd8 Installation Guide on Linux</li> <li>\ud83d\udcd8 Installation Guide on Windows</li> </ul>"},{"location":"pages/tools/integration/pentaho/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure bundle</li> <li>You should Download and install PDI</li> </ul> <p>This article was written for version <code>9.1</code> on <code>MacOS</code> but it should also work for the Windows version.</p>"},{"location":"pages/tools/integration/pentaho/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"pages/tools/integration/pentaho/#step-1-download-jdbc-driver","title":"\u2705 Step 1: Download JDBC Driver","text":"<p>Download the JDBC driver from the DataStax website:</p> <ol> <li>Go to https://downloads.datastax.com/#odbc-jdbc-drivers.</li> <li>Select Simba JDBC Driver for Apache Cassandra.</li> <li>Select JDBC 4.2.</li> <li>Read the license terms and accept it (click the checkbox).</li> <li>Hit the blue Download button.</li> <li>Once the download completes, unzip the downloaded file.</li> </ol>"},{"location":"pages/tools/integration/pentaho/#step-2-import-driver-jar-in-pentaho","title":"\u2705 Step 2: Import Driver JAR in Pentaho","text":"<p>Deploy the Simba driver to Pentaho servers using the distribution tool:</p> <ol> <li> <p>On your laptop or PC, copy the Simba JAR to the JDBC distribution directory:</p> <pre><code>$ cp CassandraJDBC42.jar pentaho/jdbc-distribution/\n</code></pre> </li> <li> <p>Run the distribution tool (<code>distribute-files.bat</code> on Windows)</p> <pre><code>$ cd /Applications/Pentaho/jdbc-distribution\n$ ./distribute-files.sh CassandraJDBC42.jar\n</code></pre> </li> <li> <p>Verify that the JAR has been copied to the PDI library:</p> <pre><code>$ cd /Applications/Pentaho\n$ ls -lh design-tools/data-integration/lib/CassandraJDBC42.jar\n</code></pre> <ul> <li>Expected output:</li> </ul> <pre><code>-rw-r--r--  1 erick  vaxxed   16M 14 Sep 22:18 design-tools/data-integration/lib/CassandraJDBC42.jar\n</code></pre> <pre><code>$ file design-tools/data-integration/lib/CassandraJDBC42.jar\n</code></pre> <ul> <li>Expected output:</li> </ul> <pre><code>design-tools/data-integration/lib/CassandraJDBC42.jar: Java archive data (JAR)\n</code></pre> </li> <li> <p>Restart Pentaho on your workstation for the Simba driver to be loaded.</p> </li> </ol>"},{"location":"pages/tools/integration/pentaho/#step-3-define-a-connection-in-pentaho","title":"\u2705 Step 3: Define a connection in Pentaho","text":"<p>In this section we assume that your database in Astra is called <code>pentaho</code> and as such the download secure bundle is called <code>secure-connect-pentaho.zip</code></p> <ol> <li>Create a new Transformation.</li> <li>Open a new Database Connection dialog box.</li> <li>In the Connection name field, give your DB connection a name.</li> <li>Under Connection type, select Generic database.</li> <li> <p>Set the Custom connection URL. (Note that you will need to specify the full path to your secure bundle and adapt to your database name)</p> <pre><code> jdbc:cassandra://;AuthMech=2;TunableConsistency=6;SecureConnectionBundlePath=/path/to/secure-connect-pentaho.zip\n</code></pre> </li> <li> <p>In the Username field, enter the string <code>token</code>.</p> </li> <li>In the Password field, paste the value of the token you created in the Prerequisites section above. The token looks like <code>AstraCS:AbC...XYz:123...edf0</code> </li> <li>Click on the Test Connection button to confirm that the driver configuration is working:    </li> <li>Click on the OK button to save the connection settings.</li> </ol>"},{"location":"pages/tools/integration/pentaho/#step-4-final-test","title":"\u2705 Step 4: Final Test","text":"<p>Connect to your Astra DB by launching the SQL Editor in Pentaho and run a simple CQL statement. For example:</p> <p></p> <p>Here's an example output:</p> <p></p> <p>You should also be able to browse the keyspaces in your Astra DB using the DataBase Explorer. Here's an example output:</p> <p></p>"},{"location":"pages/tools/integration/quine.io/","title":"Quine","text":""},{"location":"pages/tools/integration/quine.io/#overview","title":"Overview","text":"<p>Quine is a streaming graph capable of building high-volumes of data into a stateful graph.  It allows for real-time traversals on a graph, as well as for the data to be streamed-out for event processing.</p> <ul> <li>\u2139\ufe0f Quine Documentation - Core Concepts</li> </ul>"},{"location":"pages/tools/integration/quine.io/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Connect Bundle</li> <li>You should install a JDK (version 11 or higher).</li> </ul> <p>This article was written for Quine version <code>1.2.1</code> on <code>MacOS</code> with Java <code>11.10</code>.</p>"},{"location":"pages/tools/integration/quine.io/#installation","title":"Installation","text":"<p>\u2705 Step 1 Download and install</p> <p>Follow the Download Quine page to download the JAR.  Choose/create a directory for Quine, and copy the JAR to this location:</p> <pre><code>mkdir ~/local/quine\ncp ~/Downloads/quine-1.2.1.jar ~/local/quine\n</code></pre> <p>\u2705Step 2 Create the keyspace <code>quine</code></p> <p>From the Astra DB console, click on your database name, or create a new one called <code>quine</code> (or another name of your preference). Scroll down to where the keyspaces are listed, and click the <code>Add Keyspace</code> button to create a new keyspace. Name this keyspace <code>quine</code>.</p> <p>\u2705 Step 3 Configuration</p> <p>Create a <code>quine.conf</code> file inside the <code>quine</code> directory:</p> <pre><code>cd ~/local/quine\ntouch quine.conf\n</code></pre> <p>Edit the <code>quine.conf</code> file to look like the following:</p> <pre><code>quine.store {\n  # store data in an Apache Cassandra instance\n  type = cassandra\n\n  # the keyspace to use\n  keyspace = quine\n\n  should-create-keyspace = false\n  should-create-tables = true\n\n  replication-factor = 3\n\n  write-consistency = LOCAL_QUORUM\n  read-consistency = LOCAL_QUORUM\n\n  local-datacenter = \"us-east1\"\n\n  write-timeout = \"10s\"\n  read-timeout = \"10s\"\n}\ndatastax-java-driver {\n  advanced {\n    auth-provider {\n      class = PlainTextAuthProvider\n      username = \"token\"\n      password = \"AstraCS:qFDPGZEgBlahBlahYourTokenGoesHerecff15fc\"\n    }\n  }\n  basic {\n    cloud {\n      secure-connect-bundle = \"/Users/aaronploetz/local/secure-connect-quine.zip\"\n    }\n  }\n}\n</code></pre> <p>Astra-Specific Settings:</p> <p><code>type = cassandra</code> - If the <code>type</code> is not specified, Quine defaults to use RocksDB.</p> <p><code>should-create-keyspace = false</code> - Remember keyspaces can only be created in Astra via the dashboard.</p> <p><code>replication-factor = 3</code> - Defaults to <code>1</code> if not set, which will not work with Astra DB.</p> <p><code>write-consistency = LOCAL_QUORUM</code> - Minimum consistency level required by Astra.</p> <p><code>read-consistency = LOCAL_QUORUM</code> - Minimum consistency level required by Astra.</p> <p><code>local-datacenter = \"us-east1\"</code> - Set Astra DB's cloud region as the local DC.</p> <p><code>username = \"token\"</code> - No need to mess with this.  Just leave it as the literal word \"token.\"</p> <p><code>password</code> - A valid token for an Astra DB cluster.</p> <p><code>secure-connect-bundle</code> - A valid, local file location of a downloaded secure connect bundle.  Also, the driver gets the Astra DB hostname and Cloud provider from the secure bundle, so there is no need to specify endpoints separately.</p> <p>\u2705 Step 4 Download Secure Connect Bundle (SCB) In your Astra DB console navigate to your database in the dashboard, then the connect tab.  In the 'Connect using a Driver' , and then the click 'Java' Java section. Then click the 'download bundle' on the right. Without unzipping it, move the downloaded file to the directory you created in step 1, that contains the quine-1.2.1.jar.  The file will be named <code>secure-connect-[your databasename].zip</code>, so in this example <code>secure-connect-quine.zip</code>.  You will reference this file directly in the previous configation file step above.</p> <p>\u2705 Step 5 Run Quine</p> <p>To run Quine, invoke the JAR with Java, while passing the <code>quine.conf</code> in the <code>config.file</code> parameter:</p> <pre><code>$ java -Dconfig.file=quine.conf -jar quine-1.2.1.jar\n\n2022-06-15 15:11:52,666 WARN [NotFromActor] [s0-io-4] com.datastax.oss.driver.internal.core.cql.CqlRequestHandler - Query '[0 values] CREATE TABLE IF NOT EXISTS journals (quine_id blob,timestamp bigint,data blob,PRIMARY KEY(quine_id,timestamp)) WITH CLUSTERING ORDER BY (timestamp ASC) AND compaction={'class':'TimeWindowCompactionStrategy'}' generated server side warning(s): Ignoring provided values [compaction] as they are not supported for Table Properties (ignored values are: [additional_write_policy, bloom_filter_fp_chance, caching, cdc, compaction, compression, crc_check_chance, dclocal_read_repair_chance, extensions, gc_grace_seconds, id, max_index_interval, memtable_flush_period_in_ms, min_index_interval, nodesync, read_repair, read_repair_chance, speculative_retry])\nGraph is ready!\nApplication state loaded.\nQuine app web server available at http://localhost:8080\n</code></pre> <p>As shown above, Astra DB will return a warning about table valid options which it will ignore.</p> <p>You can now use Quine's visual graph explorer in a web browser, and create/traverse data with either Gremlin or Cypher: http://localhost:8080/</p> <p></p> <p>The Swagger spec for the Quine API can also be found locally at: http://localhost:8080/docs</p> <p>\u2705 Optional Step 6: Loading some sample data</p> <p>Download attempts.json (74.MB) from the Quine Password Spraying example and locate it in the root of your Quine directory alongside the quine-1.2.1.jar file. Make sure the Quine server is not running -- it's requires graceful shutdown.  Simply issue a curl -X \"POST\" \"http://127.0.0.1:8080/api/v1/admin/shutdown\" in a seperate terminal or command prompt window to do so.</p> <p>Then execute </p> <pre><code>java -Dconfig.file=quine.conf -jar quine-1.2.1.jar -r passwordspraying\n</code></pre> <p>Note that it will take a few minutes to load!  When it is completed successfully you will see:</p> <pre><code>INGEST-1 status is completed and ingested 55000\n</code></pre> <p>\u2705 Troubleshooting</p> <p>If the output does not read: </p> <pre><code>Graph is ready!\nApplication state loaded.\nQuine app web server available at http://locahost:8080\n</code></pre> <p>Then look for exceptions.</p> <p>If you see an error:</p> <pre><code>com.datastax.oss.driver.api.core.servererrors.InvalidQueryException: Clustering key columns must exactly match columns in CLUSTERING ORDER BY directive\n</code></pre> <p>Check to ensure the snapshots table exists:</p> <pre><code>cqlsh&gt; use quine;\n\ncqlsh&gt; desc quine;\n</code></pre> <p>If not, execute this command in CQLSH to create it:</p> <pre><code>CREATE TABLE quine.snapshots (\n    quine_id blob,\n    timestamp bigint,\n    multipart_index int,\n    data blob,\n    multipart_count int,\n    PRIMARY KEY (quine_id, timestamp, multipart_index)\n) WITH CLUSTERING ORDER BY (timestamp DESC, multipart_index ASC)\n    AND additional_write_policy = '99PERCENTILE'\n    AND bloom_filter_fp_chance = 0.01\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\n    AND comment = ''\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.UnifiedCompactionStrategy'}\n    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\n    AND crc_check_chance = 1.0\n    AND default_time_to_live = 0\n    AND gc_grace_seconds = 864000\n    AND max_index_interval = 2048\n    AND memtable_flush_period_in_ms = 0\n    AND min_index_interval = 128\n    AND read_repair = 'BLOCKING'\n    AND speculative_retry = '99PERCENTILE';\n</code></pre>"},{"location":"pages/tools/integration/quine.io/#acknowledgements","title":"Acknowledgements","text":"<p>Special thanks goes out to Ryan Wright and Leif Warner of thatDot for their help with getting Quine running and connected.</p>"},{"location":"pages/tools/integration/stepzen/","title":"StepZen","text":""},{"location":"pages/tools/integration/stepzen/#overview","title":"Overview","text":"<p>StepZen helps developers build GraphQL faster, deploy in seconds, and run on StepZen. It simplifies how you access the data you need, and with zero infrastructure to build or manage, you can focus on crafting modern data-driven experiences. </p> <ul> <li>\u2139\ufe0f Introduction to StepZen</li> <li>\ud83d\udce5 StepZen Quick Install</li> </ul>"},{"location":"pages/tools/integration/stepzen/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should create a StepZen account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should retrieve your **Database ID** and **Region** from your Astra DB dashboard</li> </ul>"},{"location":"pages/tools/integration/stepzen/#installation-and-setup","title":"Installation and Setup","text":"<p>After logging into your StepZen account and have all of your credentials ready, you can navigate to this page or follow the steps below to setup StepZen.</p> <ol> <li> <p>First, install the StepZen CLI  <pre><code>npm install -g stepzen\n</code></pre></p> </li> <li> <p>Log in with your StepZen account <pre><code>stepzen login -a YOUR_ACCOUNT\n</code></pre></p> </li> <li> <p>Enter your Admin Key when prompted <pre><code>YOUR_ADMIN_KEY\n</code></pre> Note: For steps #2 and #3, if it does not autopopulate, you can find this information in your StepZen account under \"My Stepzen\".</p> </li> <li> <p>Import a DataStax Astra DB GraphQL API from your terminal <pre><code>stepzen import graphql\n</code></pre></p> </li> <li>When prompted, enter your GraphQL API details:</li> </ol> What is the GraphQL endpoint URL? <code>https://&lt;ASTRA_DB_ID&gt;-&lt;ASTRA_DB_REGION&gt;.apps.astra.datastax.com/api/graphql/&lt;KEYSPACE_NAME&gt;</code> Prefix to add to all generated type names (leave blank for none) Optional. Adviced to use when you're importing multiple data sources. Add an HTTP header, e.g. Header-Name: header value (leave blank for none) <code>X-Cassandra-Token: &lt;APPLICATION_TOKEN&gt;</code> <p>Once successful, you should see the following output: <pre><code>Generating schemas...... done\nSuccessfully imported schema graphql from StepZen\n</code></pre> 6. Type <code>stepzen start</code> in your terminal</p> <p>StepZen introspects your DataStax Astra DB GraphQL API and builds your endpoint. </p> <p>You should receive something similar to this output: <pre><code>File changed: /your/path/.DS_Store\nDeploying api/coy-aardwolf to StepZen... done in 4.8s\n\nYour API url is  https://&lt;YOUR_ACCOUNT&gt;.stepzen.net/api/&lt;YOUR_ENDPOINT_NAME&gt;/__graphql\n\nYou can test your hosted API with cURL:\n\ncurl https://&lt;YOUR_ACCOUNT&gt;.stepzen.net/api/&lt;YOUR_ENDPOINT_NAME&gt;/__graphql \\\n   --header \"Authorization: Apikey $(stepzen whoami --apikey)\" \\\n   --header \"Content-Type: application/json\" \\\n   --data '{\"query\": \"your graphql query\"}'\n\nor explore it with GraphiQL at  http://localhost:5001/api/&lt;YOUR_ENDPOINT_NAME&gt;\n\nWatching ~/your/path/here for GraphQL changes...\n</code></pre></p>"},{"location":"pages/tools/integration/stepzen/#test-and-validate","title":"Test and Validate","text":"<ol> <li>To quickly validate that the previous steps went smoothly, navigate to your local host to view the StepZen UI.  <code>http://localhost:5001/api/&lt;YOUR_ENDPOINT_NAME&gt;</code></li> <li>Using the Explorer you can visualize what tables are in your keyspace, and by selecting each box, you are building your GraphQL query which shows up in the middle console.  </li> </ol> <p>...and you're done! You can now use StepZen to build your GraphQL queries with ease to use with your applications. </p>"},{"location":"pages/tools/integration/strapi/","title":"Strapi","text":""},{"location":"pages/tools/integration/strapi/#overview","title":"Overview","text":"<p>Strapi is an open-source headless CMS that gives developers the freedom to choose their favorite tools and frameworks and allows editors to manage and distribute their content using their application\u2019s admin panel. Based on a plugin system, its admin panel and API are extensible. Every part is customizable to match any use case. Strapi also has a built-in user system to manage what the administrators and end users can access.</p> <ul> <li>\u2139\ufe0f Introduction to Strapi</li> <li>\ud83d\udce5 Strapi Quick Install</li> </ul>"},{"location":"pages/tools/integration/strapi/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should retrieve your **Database ID** and **Region** from your Astra DB dashboard</li> <li>Install node (14.17.3 version).</li> </ul>"},{"location":"pages/tools/integration/strapi/#installation-and-setup","title":"Installation and Setup","text":"<p>Follow the steps below to setup Strapi locally.</p> <ol> <li> <p>First, install Strapi locally:  <pre><code>npx create-strapi-app my-project\n</code></pre> You can view your Strapi project as it is hosted locally at http://localhost:1337/admin.</p> </li> <li> <p>Install the Strapi hook: <pre><code>npm i strapi-hook-astra\n</code></pre></p> </li> <li> <p>Activate the hook by adding the following to <code>./config/hook.js</code> of the sample Strapi Project: <pre><code>module.exports = {\n    settings: {\n        astra: {\n            enabled: true,\n            token: 'REPLACE_ME',\n            databaseId: 'REPLACE_ME',\n            databaseRegion: 'REPLACE_ME',\n            keyspace: 'REPLACE_ME',\n            collection: 'REPLACE_ME'\n        },\n    }\n};\n</code></pre> Where:</p> </li> <li><code>token</code>: Generate a token from Astra DB.</li> <li><code>databaseId</code>: Enter your Astra DB database ID from your database URL.</li> <li><code>databaseRegion</code>: Enter your Astra DB database region</li> <li><code>keyspace</code>: Enter your Astra DB keyspace name.</li> <li><code>collection</code>: Enter your Astra DB collection name.</li> </ol>"},{"location":"pages/tools/integration/strapi/#test-and-validate","title":"Test and Validate","text":"<ol> <li> <p>Create a document: <pre><code>strapi.services.astra.create(document);\n</code></pre> |Parameter|Type|Explanation|Values| |:---|:---|:---|:---| |document|json|Create a document|var dataString = '{ \"name\": \"John\", \"last_name\": \"Doe\" }'|</p> </li> <li> <p>Get document by ID: <pre><code>strapi.services.astra.getById(documentId);\n</code></pre> |Parameter|Type|Explanation|Values| |:---|:---|:---|:---| |documentId|string|Get document by documentId|var documentId = \"your_document_id\"|</p> </li> <li> <p>Get document by path: <pre><code>strapi.services.astra.getByPath();\n</code></pre></p> </li> <li> <p>Search a collection: <pre><code>strapi.services.astra.searchCollection(query,pagesize);\n</code></pre> |Parameter|Type|Explanation|Values| |:---|:---|:---|:---| |query|string|Search collection via query|var query = {\"name\": { \"$eq\": \"John\" }}| |pagesize|int|Number of documents to fetch|int page_size = 3|</p> </li> </ol> <p>For more, see the Strapi documentation.</p>"},{"location":"pages/tools/integration/temporal/","title":"Temporal","text":""},{"location":"pages/tools/integration/temporal/#overview","title":"Overview","text":"<p>Temporal.io is an open source microservice orchestration platform that assists in tracking workflows in your application development. It provides the user with a plug-and-play persistence layer that lets the user choose and configure their Temporal Server with their preferred backend. Currently, Temporal is compatible with Postgres, MySQL, and Apache Cassandra\u24c7 as backend dependencies. </p> <ul> <li>\ud83d\udce5 Temporal Quick Install</li> <li>\u2139\ufe0f Introduction to Temporal</li> <li>\u2139\ufe0f Part 1: Introduction to Temporal and Cassandra, Astra DB</li> <li>\u2139\ufe0f Part 2: Connect Temporalio to Astra DB in 5 Easy Steps</li> <li>\u2139\ufe0f Part 3: Connect Temporalio to Astra DB With Kubernetes</li> <li>\ud83c\udfa5 Workflow with Temporal and Astra DB in 5 minutes</li> </ul>"},{"location":"pages/tools/integration/temporal/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> </ul> <p> <p>Note</p> <p>This runbook was written using Mac OS Monterey but it will also work with Windows. Any Windows-specific instructions will be noted as such. </p> <p> </p>"},{"location":"pages/tools/integration/temporal/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"pages/tools/integration/temporal/#step-1-setup-astra","title":"\u2705 Step 1: Setup Astra","text":"<ol> <li>In your Astra database, create two new keyspaces called \"temporal\" and \"temporal_visibility\". You will be using both of these in the next steps.</li> <li>Make sure to create an Astra token with Admin Role</li> <li>Get your Database ID</li> </ol> <p> Find your Database ID in one of two ways: <ol> <li>Navigate to your your database and get the last ID in the URL: <code>https://astra.datastax.com/org/.../database/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</code></li> <li>Copy and paste the Datacenter ID without the trailing <code>-1</code> from the Regions section of your Astra Dashboard. </li> </ol> <p></p>"},{"location":"pages/tools/integration/temporal/#step-2-temporal-pre-setup","title":"\u2705 Step 2: Temporal Pre-setup","text":"<ol> <li>Clone this GitHub repository</li> <li>Navigate to your cloned repository and using your preferred text editor (e.g. VisualStudio or Sublime), update the .env file with your Astra Token and Astra Database ID that you obtained above. </li> </ol> <pre><code>ASTRA_TOKEN=&lt;your Astra token&gt;\nASTRA_DATABASE_ID=&lt;your DB ID&gt;\n</code></pre>"},{"location":"pages/tools/integration/temporal/#step-3-temporal-schema-migration-to-astra-db","title":"\u2705 Step 3: Temporal Schema Migration to Astra DB","text":"<p>For this step, you will set up the keyspaces you created earlier in the Astra prerequisites (temporal and temporal_visibility). You will be using <code>temporal-cassandra-tool</code> which is part of the Temporal repo and it relies on schema definition. </p> <ol> <li>Navigate to your cloned <code>temporal-astra-cql-proxy</code> directory</li> <li>Run the following commands to initialize the keyspaces that we created through Astra. Note that there are two sets of commands, one for <code>temporal</code> keyspace and one for <code>temporal_visibility</code> keyspace:</li> </ol> <pre><code>docker-compose -f docker-compose-schema.yaml run temporal-admin-tools \\\n-ep cql-proxy -k temporal setup-schema -v 0.0\ndocker-compose -f docker-compose-schema.yaml run temporal-admin-tools \\\n-ep cql-proxy -k temporal update-schema -d schema/cassandra/temporal/versioned/\n\ndocker-compose -f docker-compose-schema.yaml run temporal-admin-tools \\\n-ep cql-proxy -k temporal_visibility setup-schema -v 0.0\ndocker-compose -f docker-compose-schema.yaml run temporal-admin-tools \\\n-ep cql-proxy -k temporal_visibility update-schema -d schema/cassandra/visibility/versioned/\n</code></pre> <p>Once the process is completed, you should see a message similar to this: </p> <pre><code>2022-03-02T22:23:27.618Z    INFO    Validating connection to cassandra cluster. {\"logging-call-at\": \"cqlclient.go:112\"}\n2022-03-02T22:42:53.526Z    INFO    Connection validation succeeded.    {\"logging-call-at\": \"cqlclient.go:118\"}\n2022-03-02T22:42:53.526Z    INFO    Starting schema setup   {\"config\": {\"SchemaFilePath\":\"\",\"InitialVersion\":\"0.0\",\"Overwrite\":false,\"DisableVersioning\":false}, \"logging-call-at\": \"setuptask.go:57\"}\n2022-03-02T22:42:53.526Z    DEBUG   Setting up version tables   {\"logging-call-at\": \"setuptask.go:67\"}\n2022-03-02T22:42:54.120Z    DEBUG   Current database schema version 1.6 is greater than initial schema version 0.0. Skip version upgrade    {\"logging-call-at\": \"setuptask.go:116\"}\n2022-03-02T22:42:54.120Z    INFO    Schema setup complete   {\"logging-call-at\": \"setuptask.go:131\"}\n</code></pre> <p>Great! Your schemas have been migrated with Astra DB. </p> <p> Confirm your tables exist in Astra <ul> <li>You can double-check to make sure the correct tables have been created by querying your database in Astra DB\u2019s CQL Console.</li> <li>Run <code>DESC tables;</code> in both your <code>temporal</code> and <code>temporal_visibility</code> keyspaces. You should see there are tables loaded in that were created by the schema migration with <code>temporal-cassandra-tool</code>. </li> </ul> <p></p> <pre><code>token@cqlsh&gt; use temporal;\ntoken@cqlsh:temporal&gt; desc tables;\nhistory_node        tasks             cluster_metadata_info\ncluster_membership  namespaces        cluster_metadata     schema_version      namespaces_by_id  schema_update_history\nexecutions          queue_metadata  queue               history_tree    token@cqlsh:temporal&gt; use temporal_visibility;\ntoken@cqlsh:temporal_visibility&gt; desc tables;\nopen_executions  schema_update_history  schema_version  closed_executions\n</code></pre>"},{"location":"pages/tools/integration/temporal/#step-4-run-docker-compose","title":"\u2705 Step 4: Run Docker Compose","text":"<p>In this step, the <code>docker-compose.yaml</code> file is already provided for you in the <code>temporal-astra-cql-proxy</code> repo. This file creates different docker containers to run Temporal server. The persistence layer is configured for you to connect with <code>cql-proxy</code>, and it should pull your Astra credentials from when you set it earlier:</p> <pre><code>services:\n cql-proxy:\n   container_name: cqlproxy\n   image: datastax/cql-proxy:v${CQL_PROXY_VERSION}\n...\n   environment:\n     - ASTRA_TOKEN=${ASTRA_TOKEN}\n- ASTRA_DATABASE_ID=${ASTRA_DATABASE_ID}\n- HEALTH_CHECK=true\n</code></pre> <p>Now you can run the docker-compose command to start up Temporal:  <pre><code>docker-compose up\n</code></pre></p>"},{"location":"pages/tools/integration/temporal/#step-5-test-and-validate","title":"\u2705 Step 5: Test and Validate","text":"<p>You can test your connection and play with your Temporal cluster with these instructions.</p> <ol> <li>Make sure to use tctl to create namespaces dedicated to certain workflows: <pre><code>bash-5.0# tctl --namespace test namespace re\nNamespace test successfully registered.\n</code></pre></li> <li>When using the sample apps, keep in mind that you want to modify the starter and worker code so that it points to this specific Temporal deployment. For example: <pre><code>c, err := client.NewClient(client.Options{HostPort: \"127.0.0.1:7233\", Namespace: \"test\"})\n</code></pre></li> </ol> <p>Once you have this all running, you should be able to see your workflows reflect on both the Temporal UI and Astra UI.</p> <p></p>"},{"location":"pages/tools/integration/vault/","title":"Vault","text":""},{"location":"pages/tools/integration/vault/#overview","title":"Overview","text":"<p>The purpose of this document is to guide you through the process using Astra DB as the storage configuration for your  HashiCorp Vault instance. In this tutorial, you will install Vault and edit the configuration file to point to Astra DB.</p> <ul> <li>\u2139\ufe0f Introduction to Vault</li> <li>\ud83d\udce5 Vault Quick Install</li> </ul>"},{"location":"pages/tools/integration/vault/#prerequisites","title":"Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> <li>You should Download your Secure Bundle</li> <li>You should Install Vault</li> <li>Clone this repository to use to set up CQL-Proxy which is a sidecar that enables unsupported CQL drivers to work with DataStax Astra   <li>You need your Astra Token and Astra Database ID to use CQL-Proxy</li> <li>Follow the steps in the repo to spin up CQL-Proxy using Terminal/Command Line. Once successfully running, you should see the following output: <code>{\"level\":\"info\",\"ts\":1651012815.176512,\"caller\":\"proxy/proxy.go:222\",\"msg\":\"proxy is listening\",\"address\":\"[::]:9042\"}</code></li>"},{"location":"pages/tools/integration/vault/#installation-and-setup","title":"Installation and Setup","text":"<ol> <li>In the Astra UI, create a keyspace called vault. </li> <li>Navigate to your CQL Console in the Astra UI. Issue the following statement to create a table called entries <pre><code>CREATE TABLE vault.\"entries\" (\nbucket text,\n    key text,\n    value blob,\n    PRIMARY KEY (bucket, key)\n) WITH CLUSTERING ORDER BY (key ASC);\n</code></pre></li> <li>Navigate to your terminal. Create a Vault configuration file <code>config.hcl</code> in your local directory.</li> <li>Edit your <code>config.hcl</code> file. Copy and paste the following to your configuration file:</li> </ol> <p><pre><code>storage \"cassandra\" {\nhosts            = \"localhost\"\nconsistency      = \"LOCAL_QUORUM\"\nprotocol_version = 3\n}\nlistener \"tcp\" {\naddress     = \"127.0.0.1:8200\"\ntls_disable = \"true\"\n}\napi_addr = \"http://127.0.0.1:8200\"\ncluster_addr = \"https://127.0.0.1:8201\"\nui = true\n</code></pre> 4. Run Vault from your terminal with the following command:</p> <p><code>vault server -config=config.hcl</code></p> <p>Successful output should look like this: <pre><code>==&gt; Vault server configuration:\n\nApi Address: http://127.0.0.1:8200\n                     Cgo: disabled\n         Cluster Address: https://127.0.0.1:8201\n              Go Version: go1.17.9\n              Listener 1: tcp (addr: \"127.0.0.1:8200\", cluster address: \"127.0.0.1:8201\", max_request_duration: \"1m30s\", max_request_size: \"33554432\", tls: \"disabled\")\nLog Level: info\n                   Mlock: supported: false, enabled: false\nRecovery Mode: false\nStorage: cassandra\n                 Version: Vault v1.10.2\n             Version Sha: 94325865b12662cb72efa3003d6aaa4f5ae57f3a\n==&gt; Vault server started! Log data will stream in below:\n</code></pre></p> <p> <p>Note</p> <p>If you get a warning message about mlock not being supported, that is okay. However, for maximum security you should run Vault on a system that supports mlock.</p> <p></p>"},{"location":"pages/tools/integration/vault/#test-and-validate","title":"Test and Validate","text":"<ol> <li>Once you see the above message that you successfully started Vault server, open a new terminal window.</li> <li>Run <code>vault operator init</code>. This will give you 5 Unseal Keys and a Root Token. Vault needs 3 Unseal Keys to properly unseal. </li> </ol> <p> <p>Note</p> <p>You may get an error that looks like this: <code>Error initializing: Put \"https://127.0.0.1:8200/v1/sys/init\": http: server gave HTTP response to HTTPS client</code> This is because Vault runs on localhost, but the default address is HTTPS. Instead, you might need to specify the explicit address with the follow command: <code>vault operator init -address=http://127.0.0.1:8200</code></p> <p></p> <p>Once Vault is initialized, it should give you an output of your Unseal Keys:</p> <pre><code>% vault operator init\nUnseal Key 1: rVRPym...\nUnseal Key 2: 71tY5X...\nUnseal Key 3: ETYWDf...\nUnseal Key 4: 4mDtrr...\nUnseal Key 5: o9X46m...\n\nInitial Root Token: hvs.gF14F...\n\nVault initialized with 5 key shares and a key threshold of 3. Please securely\ndistribute the key shares printed above. When the Vault is re-sealed,\nrestarted, or stopped, you must supply at least 3 of these keys to unseal it\nbefore it can start servicing requests.\n\nVault does not store the generated root key. Without at least 3 keys to\nreconstruct the root key, Vault will remain permanently sealed!\n</code></pre> <p> <p>Note</p> <p>Make sure to save these keys somewhere safe. This is the only time that Vault will generate these keys. </p> <p></p> <ol> <li>Run the Vault UI at http://127.0.0.1:8200</li> <li>Enter your Unseal Keys and Root Token</li> </ol> <p> </p> <ol> <li>You should now be able to access the Vault UI as well as cross-reference your CQL Console to make sure the requests are properly being written to your entries table! </li> </ol> <p>Note: When querying from the entries table, you must use double-quotes as <code>entries</code> is a reserved word for CQL.</p> <p> </p> <pre><code>token@cqlsh:vault&gt; use vault;                           //Switches to Vault keyspace\ntoken@cqlsh:vault&gt; expand on;                           //Prints output in readable format\ntoken@cqlsh:vault&gt; select * from \"entries\" limit 1;     //Select statement from \"entries\" table\n\n@ Row 1\n--------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n bucket | logical/59ca7834-32f6-a70a-8a61-53dce6dd9c18/oidc_provider/provider\n key    | logical/59ca7834-32f6-a70a-8a61-53dce6dd9c18/oidc_provider/provider/default\n value  | 0x0000000102002d31867373d44b1ae2412b4a1a2bd895c3eec2b2db671ec6a8e323e69539cf6d5e1b43e2e11fabc9cc76ad3c77a722caac47cc3f877013df200e4e6d268e6dbff10ba4007cef042643721101e669ae35ff08842e2d1f70e19de2\n\n(1 rows)\n</code></pre>"},{"location":"pages/tools/notebooks/datastax-studio/","title":"DataStax Studio","text":""},{"location":"pages/tools/notebooks/datastax-studio/#overview","title":"Overview","text":"<p>DataStax Studio is an interactive developer tool for CQL (Cassandra Query Language), Spark SQL, and DSE Graph. Developers and analysts collaborate by mixing code, documentation, query results, and visualizations in self-documenting notebooks.</p> <ul> <li>\u2139\ufe0f Introduction to DataStax Studio</li> <li>\ud83d\udce5 DataStax Studio Quick Install</li> </ul>"},{"location":"pages/tools/notebooks/datastax-studio/#prerequisites","title":"Prerequisites","text":""},{"location":"pages/tools/notebooks/datastax-studio/#datastax-studio-prerequisites","title":"DataStax Studio Prerequisites","text":"<ul> <li>You should have a supported browser</li> <li>You should have a supported version of Java</li> <ul> <li>Recommended: OpenJDK 8</li> <li>Supported: Oracle Java SE 8 (JRE or JDK)</li> </ul> </ul>"},{"location":"pages/tools/notebooks/datastax-studio/#astra-prerequisites","title":"Astra Prerequisites","text":"<ul> <li>You should have an Astra account</li> <li>You should Create an Astra Database</li> <li>You should Have an Astra Token</li> </ul>"},{"location":"pages/tools/notebooks/datastax-studio/#installation-and-setup","title":"Installation and Setup","text":"<p>As mentioned in the Prerequisites above, you must have DataStax Studio already installed. You can follow the quick installation steps here. Once you have successfully installed DataStax Studio, you may proceed to the following steps. </p> <ol> <li>Start up DataStax Studio by running the Studio Server shell script:<ul> <li>Linux:  <pre><code>cd installation_location/datastax-studio-6.8.0\n./bin/server.sh\n</code></pre></li> <li>Windows: <pre><code>C:/&gt; cd installation_location\\datastax-studio-6.8.0\\bin\\\nC:/&gt; server.bat\n</code></pre> Once Studio is running, your output should look something similar to this: <pre><code>Studio is now running at: http://127.0.0.1:9091\n</code></pre></li> </ul> </li> <li> <p>You may now use the <code>localhost</code> URL provided in your terminal or command line to navigate to the DataStax Studio UI. This should look something like this: </p> </li> <li> <p>For this example, we will use the Getting Started with Astra notebook. A notebook is essentially a workspace used to visualize queries from your database, test and run different commands, and more.  </p> </li> <li> <p>On the top right corner of the notebook, click <code>default localhost</code> and then <code>Add Connection</code> to configure a new connection for the notebook. </p> </li> <li> <p>A screen should appear with the options <code>Standard Connection</code> and <code>Astra Connection</code>. For this example, you will select <code>Astra Connection</code>. </p> </li> <li> <p>Here, you will need the credentials that you gathered in the Astra Prerequisites.      <pre><code>Name: &lt;Your Database Name&gt;\nSecure Connection Bundle path: &lt;The path to your SCB locally&gt;\nClient ID: &lt;Your Client ID&gt;\nClient Secret: &lt;Your Client Secret&gt;\n</code></pre> </p> </li> <li> <p>Once you have filled this information out, you can select Test in the bottom right corner. If this is successful, you should see a message that says <code>CQL connected successfully</code>. Once this is completed, click Save. </p> </li> <li> <p>In the upper right hand corner, you should be able to switch the connection to the name of the database you just configured. </p> </li> </ol>"},{"location":"pages/tools/notebooks/datastax-studio/#test-and-validate","title":"Test and Validate","text":"<p>Finally, we will test and validate once more that the connection is validated by submitting a couple test queries.</p> <ol> <li> <p>Click the + symbol in the top-middle of the screen to add a new cell.  </p> </li> <li> <p>In the cell, you can select which Keyspace that you want to query from. </p> </li> <li> <p>Run the following queries to confirm that the connection to your Astra Database is successful. </p> </li> </ol> <p><pre><code>describe tables;\nselect * from &lt;YOUR_TABLE&gt;;\n</code></pre> </p> <p>Once you have received the correct results back, that's it! You have successfully connected DataStax Studio to Astra DB and can use this as a tool to help model your queries. You may also scroll down within the Getting Started with Astra notebook for more examples and recommendations. </p>"},{"location":"pages/tools/notebooks/jupyter/","title":"Jupyter Notebooks","text":""},{"location":"pages/tools/notebooks/jupyter/#overview","title":"Overview","text":"<p>Jupyter Notebooks are an execution environment for Julia, Python and R code (hence, Ju-Py-te-R) in technical computing domains, but are especially popular with data scientists working on machine learning models. Jupyter blends the ability to present explanatory text and imagery with interactive code blocks. Jupyter notebooks can run on your own computer, or via a hoted service like Google Colab. </p> <ul> <li> <p>\u2139\ufe0f Google Colab FAQ</p> </li> <li> <p>\u2139\ufe0f Project Jupyter Homepage</p> </li> </ul>"},{"location":"pages/tools/notebooks/jupyter/#get-started","title":"Get Started","text":"<p>Note: for notebooks specific to Vector Search functionalities, please head over to the dedicated Vector Demos page.</p> Overview Prerequisites Links Astra can be a little tricky to get started with when working inside a Jupyter notebook. This sample notebook shows how to connect to Astra, create a new database, download the secure connect bundle, and load and index data into tables. Finally, just because it's a notebook, we'll train a model and plot the test error from the sample dataset. <ul> <li>You should have an Astra account</li> <li>You should Have an Astra Token (with Administrator privileges)</li> </ul>  Or, download the notebook. Learn about Kaskada from this (non-runnable) notebook that provides an overview of using the Kaskada language to perform feature engineering for a popular Kaggle data set. None  Or, download the notebook."},{"location":"pages/tools/plugins/astradb-vault-plugin/","title":"HashiCorp Vault","text":""},{"location":"pages/tools/plugins/astradb-vault-plugin/#overview","title":"Overview","text":"<p>DataStax Astra DB Plugin for HashiCorp Vault is an open-source project that adds robust token lifecycle management features for Astra DB. Due to the nature of the Astra DB object hierarchy, by default, API tokens are not associated with specific users and currently the tokens do not have metadata descriptions.</p> <p>For more details, see the full Astra DB Plugin for HashiCorp Vault documentation in the plugin\u2019s open-source GitHub repo.</p> <p>Without the plugin, it\u2019s easy to lose track of:</p> <ul> <li>Who created tokens</li> <li>The purpose of each token</li> <li>Which tokens are being used actively</li> </ul> <p>Consequently, there\u2019s no audit trail of who has downloaded and used tokens, and there\u2019s no tracking regarding who may have manually shared tokens with others.</p> <p>Astra DB Plugin for HashiCorp Vault solves these security management issues. To ensure that your token ownership and usage are well understood, the plugin gives you the ability to associate metadata with tokens\u2014such as the user who created each token, and what it is being used for. The plugin also logs who has accessed the tokens.</p>"},{"location":"pages/tools/plugins/astradb-vault-plugin/#what-is-hashi-vault","title":"What is Hashi Vault?","text":"<p>HashiCorp Vault is a widely-used solution across the tech industry. It\u2019s an identity-based secrets and encryption management system. HashiCorp Vault from HashiCorp provides key-value encryption services that are gated by authentication and authorization methods. Access to tokens, secrets, and other sensitive data are securely stored, managed, and tightly controlled. Audit trails are provided. HashiCorp Vault is also extensible via a variety of interfaces, allowing plugins (including Astra DB Plugin for HashiCorp Vault) to contribute to this ecosystem.</p>"},{"location":"pages/tools/plugins/astradb-vault-plugin/#whats-next","title":"What's next?","text":"<p>See the full Astra DB Plugin for HashiCorp Vault documentation in the plugin\u2019s open-source GitHub repo.</p>"},{"location":"pages/tools/plugins/github-actions/","title":"GitHub Actions","text":""},{"location":"pages/tools/plugins/github-actions/#overview","title":"Overview","text":"<p>GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production.</p> <p>GitHub Actions goes beyond just DevOps and lets you run workflows when other events happen in your repository. For example, you can run a workflow to automatically add the appropriate labels whenever someone creates a new issue in your repository.</p> <p>GitHub provides Linux, Windows, and macOS virtual machines to run your workflows, or you can host your own self-hosted runners in your own data center or cloud infrastructure.</p> <p>Reference documentation:</p> <ul> <li>\u2139\ufe0f GitHub Documentation</li> <li>\u2139\ufe0f Understanding GitHub Actions</li> </ul>"},{"location":"pages/tools/plugins/github-actions/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create an Astra Database. In the following example, a keyspace called <code>demo</code> is created in the <code>demo</code> database.</li> <li>Create an Astra Token. You should have received your token while creating the database in the previous step.</li> <li>Have a GitHub account and a repository in it.</li> </ul>"},{"location":"pages/tools/plugins/github-actions/#github-repository-configuration","title":"GitHub Repository Configuration","text":"<p> <p>Note</p> <p>Getting started is simple. Ask GitHub for an ubuntu environment, install the Astra CLI, and use the <code>CLI</code> to create a database.</p> <p></p>"},{"location":"pages/tools/plugins/github-actions/#create-a-secret-for-token","title":"Create a secret for token","text":"<ol> <li>Open your github Repository and locate the tabs <code>Settings</code></li> <li>On the left menu locate `Secrets and variables` and expand to list different options. Click on <code>Actions</code></li> <li>Click on the <code>[New Repository Secret]</code> on the top right hand corner </li> <li>Enter the new secret name <code>ASTRA_DB_APPLICATION_TOKEN</code> and provide the value for your secret </li> <li>The Secret should now be visible in your secret list </li> </ol>"},{"location":"pages/tools/plugins/github-actions/#create-a-new-action","title":"Create a new action","text":"<ul> <li> <p>Create a new folder in your github repository <code>.github/actions</code></p> </li> <li> <p>Create a new Yaml file and name it as you like here i use <code>cli-db-create</code></p> </li> </ul> <p></p> <ul> <li>Populate the file as follows</li> </ul> <pre><code>name: Astra Cli Sample\non:\npush:\nbranches:\n- main\njobs:\ncli-create-db:\nenv:\nASTRA_DB_APPLICATION_TOKEN: ${{ secrets.ASTRA_DB_APPLICATION_TOKEN }}\nASTRA_DB_NAME: demo\nASTRA_DB_KEYSPACE: demo\nTERM: linux\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout\nuses: actions/checkout@v3\n- name: Install Astra CLI\nrun: curl -Ls \"https://dtsx.io/get-astra-cli\" | bash\n- name: Create DB\nrun: |\necho $ASTRA_DB_NAME\n/home/runner/.astra/cli/astra db create $ASTRA_DB_NAME -k $ASTRA_DB_KEYSPACE --token $ASTRA_DB_APPLICATION_TOKEN --if-not-exists \n</code></pre> <p> <p>Information Regarding the variables</p> <p>ASTRA_DB_NAME: The name of your database. ASTRA_DB_KEYSPACE: The keyspace (if not provided defaulting to same name as db). ASTRA_DB_APPLICATION_TOKEN: The secret we defined before.</p> <p></p>"},{"location":"pages/tools/plugins/github-actions/#run-the-action","title":"Run the action","text":"<p>With the configuration above the CLI is triggered at each commit. As we provided the options <code>--if-not-exists</code> in the command line, it is not harmful.</p> <ul> <li>Locate the tab <code>ACTIONS</code> in your github repository and select the last execution</li> </ul> <p></p> <ul> <li>You will get the following output</li> </ul> <p></p> <p>Here is a sample repository you can use as a reference.</p>"},{"location":"pages/tools/plugins/github-actions/#whats-next","title":"What's NEXT ?","text":"<p>If you are not familiar with the CLI all available commands are available in the documentation or in the help of the tools.</p> <pre><code>$&gt; astra help db create\n\nNAME\n        astra db create - Create a database with cli\n\nSYNOPSIS\n        astra db create [ --async ] [ {-cf | --config-file} &lt;CONFIG_FILE&gt; ]\n                [ {-conf | --config} &lt;CONFIG_SECTION&gt; ]\n                [ {--if-not-exist | --if-not-exists} ]\n                [ {-k | --keyspace} &lt;KEYSPACE&gt; ] [ --no-color ]\n                [ {-o | --output} &lt;FORMAT&gt; ] [ {-r | --region} &lt;DB_REGION&gt; ]\n                [ --timeout &lt;timeout&gt; ] [ --token &lt;AUTH_TOKEN&gt; ]\n                [ {-v | --verbose} ] [ --wait ] [--] &lt;DB&gt;\n\nOPTIONS\n        --async\n            Will not wait for the resource to become available\n\n        -cf &lt;CONFIG_FILE&gt;, --config-file &lt;CONFIG_FILE&gt;\n            Configuration file (default = ~/.astrarc)\n\n        -conf &lt;CONFIG_SECTION&gt;, --config &lt;CONFIG_SECTION&gt;\n            Section in configuration file (default = ~/.astrarc)\n\n        --if-not-exist, --if-not-exists\n            will create a new DB only if none with same name\n\n        -k &lt;KEYSPACE&gt;, --keyspace &lt;KEYSPACE&gt;\n            Default keyspace created with the Db\n\n        --no-color\n            Remove all colors in output\n\n        -o &lt;FORMAT&gt;, --output &lt;FORMAT&gt;\n            Output format, valid values are: human,json,csv\n\n        -r &lt;DB_REGION&gt;, --region &lt;DB_REGION&gt;\n            Cloud provider region to provision\n\n        --timeout &lt;timeout&gt;\n            Provide a limit to the wait period in seconds, default is 300s.\n\n        --token &lt;AUTH_TOKEN&gt;\n            Key to use authenticate each call.\n\n        -v, --verbose\n            Verbose mode with log in console\n\n        --wait\n            Will wait until the database become ACTIVE\n\n        --\n            This option can be used to separate command-line options from the\n            list of arguments (useful when arguments might be mistaken for\n            command-line options)\n\n        &lt;DB&gt;\n            Database name (not unique)\n</code></pre>"}]}