{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"pages/cheatsheet/","text":"Executable Code (REPLIT) \u00b6 Change values ASTRA_DB_TOKEN , ASTRA_DB_ID , ASTRA_DB_REGION , ASTRA_DB_KEYSPACE in the code below and execute with Mermaids \u00b6 1\ufe0f\u20e3 Flow \u00b6 Cassandra Graph Code graph LR user>fa:fa-user Developer]-- Create Database --> cassandra[(fa:fa-database Cassandra)] user-- Design -->usecase{{fa:fa-cube Use Case}} usecase-- Workflow -->queries[fa:fa-bezier-curve queries] usecase-- MCD -->entities[fa:fa-grip-vertical entities] queries-- Chebotko modelization -->schema[fa:fa-list schema] entities-- Chebotko modelization -->schema[fa:fa-list schema] schema[fa:fa-list schema]-- Inject -->cassandra[(fa:fa-database Cassandra)] user-- prepare -->dataset{{fa:fa-coings DataSet}} dataset-- input -->dsbulk-- load data -->cassandra user-- Create Token -->token{{fa:fa-key Token}} usecase-->API API-->Request token-->Request schema-->Request Request-- invoke -->cassandra graph LR user>fa:fa-user Developer ] -- Create Database --> cassandra [( fa:fa-database Cassandra )] user-- Design -->usecase {{ fa:fa-cube Use Case }} usecase-- Workflow -->queries [ fa:fa-bezier-curve queries ] usecase-- MCD -->entities [ fa:fa-grip-vertical entities ] queries-- Chebotko modelization -->schema [ fa:fa-list schema ] entities-- Chebotko modelization -->schema [ fa:fa-list schema ] schema [ fa:fa-list schema ] -- Inject -->cassandra [( fa:fa-database Cassandra )] user-- prepare -->dataset {{ fa:fa-coings DataSet }} dataset-- input -->dsbulk-- load data -->cassandra user-- Create Token -->token {{ fa:fa-key Token }} usecase-->API API-->Request token-->Request schema-->Request Request-- invoke -->cassandra Example #1 Output Markdown graph TD; A-->B; A-->C; B-->D; C-->D; ```mermaid graph TD; A-->B; A-->C; B-->D; C-->D; ``` Example3 Output Markdown graph TD A[Hard] -->|Text| B(Round) B --> C{Decision} C -->|One| D[Result 1] C -->|Two| E[Result 2] ```mermaid graph TD A[Hard] -->|Text| B(Round) B --> C{Decision} C -->|One| D[Result 1] C -->|Two| E[Result 2] ``` 2\ufe0f\u20e3 Sequence \u00b6 Output Markdown sequenceDiagram Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! ```mermaid sequenceDiagram Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! ``` 3\ufe0f\u20e3 Gantt \u00b6 Output Markdown gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` 4\ufe0f\u20e3 Class \u00b6 Output Markdown classDiagram Class01 <|-- AveryLongClass : Cool <<interface>> Class01 Class09 --> C2 : Where am i? Class09 --* C3 Class09 --|> Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { <<service>> int id size() } ```mermaid classDiagram Class01 <|-- AveryLongClass : Cool <<interface>> Class01 Class09 --> C2 : Where am i? Class09 --* C3 Class09 --|> Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { <<service>> int id size() } ``` 5\ufe0f\u20e3 State \u00b6 Output Markdown stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*] ```mermaid stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*] ``` 6\ufe0f\u20e3 Pie \u00b6 Output Markdown pie \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15 ```mermaid pie \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15 ``` 7\ufe0f\u20e3 Journey \u00b6 Output Markdown journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 3: Me ```mermaid journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 3: Me ``` 8\ufe0f\u20e3 ER \u00b6 Output Markdown erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses ```mermaid erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses ``` Sample Blocs \u00b6 THis is a note my note abstract my note info info Sample tip tip How to add plugins to the Docker image? Import Stuff Success my note Sample warning This is so cool. failure my note danger danger bug bug Sample example example Sample warning warning Sample wuote quote Tooltip \u00b6 wanna a tooltip ? # (1)! Cedrick rock mkdocs serve Icons \u00b6 Material \u00b6 HERE is the full list Font Awesome \u00b6 HTML = fa-camera-retro MARKDOWN HERE is the full list Opticons \u00b6 Sample Adding buttons \u00b6 In order to render a link as a button, suffix it with curly braces and add the .md-button class selector to it. The button will receive the selected [primary color] and [accent color] if active. Button [ Subscribe to our newsletter ]( # ){ .md-button } [Subscribe to our newsletter][demo]{ .md-button } [Subscribe to our newsletter][demo]{ .md-button .md-button--primary }","title":"Cheatsheet"},{"location":"pages/cheatsheet/#executable-code-replit","text":"Change values ASTRA_DB_TOKEN , ASTRA_DB_ID , ASTRA_DB_REGION , ASTRA_DB_KEYSPACE in the code below and execute with","title":"Executable Code (REPLIT)"},{"location":"pages/cheatsheet/#mermaids","text":"","title":"Mermaids"},{"location":"pages/cheatsheet/#1-flow","text":"Cassandra Graph Code graph LR user>fa:fa-user Developer]-- Create Database --> cassandra[(fa:fa-database Cassandra)] user-- Design -->usecase{{fa:fa-cube Use Case}} usecase-- Workflow -->queries[fa:fa-bezier-curve queries] usecase-- MCD -->entities[fa:fa-grip-vertical entities] queries-- Chebotko modelization -->schema[fa:fa-list schema] entities-- Chebotko modelization -->schema[fa:fa-list schema] schema[fa:fa-list schema]-- Inject -->cassandra[(fa:fa-database Cassandra)] user-- prepare -->dataset{{fa:fa-coings DataSet}} dataset-- input -->dsbulk-- load data -->cassandra user-- Create Token -->token{{fa:fa-key Token}} usecase-->API API-->Request token-->Request schema-->Request Request-- invoke -->cassandra graph LR user>fa:fa-user Developer ] -- Create Database --> cassandra [( fa:fa-database Cassandra )] user-- Design -->usecase {{ fa:fa-cube Use Case }} usecase-- Workflow -->queries [ fa:fa-bezier-curve queries ] usecase-- MCD -->entities [ fa:fa-grip-vertical entities ] queries-- Chebotko modelization -->schema [ fa:fa-list schema ] entities-- Chebotko modelization -->schema [ fa:fa-list schema ] schema [ fa:fa-list schema ] -- Inject -->cassandra [( fa:fa-database Cassandra )] user-- prepare -->dataset {{ fa:fa-coings DataSet }} dataset-- input -->dsbulk-- load data -->cassandra user-- Create Token -->token {{ fa:fa-key Token }} usecase-->API API-->Request token-->Request schema-->Request Request-- invoke -->cassandra Example #1 Output Markdown graph TD; A-->B; A-->C; B-->D; C-->D; ```mermaid graph TD; A-->B; A-->C; B-->D; C-->D; ``` Example3 Output Markdown graph TD A[Hard] -->|Text| B(Round) B --> C{Decision} C -->|One| D[Result 1] C -->|Two| E[Result 2] ```mermaid graph TD A[Hard] -->|Text| B(Round) B --> C{Decision} C -->|One| D[Result 1] C -->|Two| E[Result 2] ```","title":"1\ufe0f\u20e3 Flow"},{"location":"pages/cheatsheet/#2-sequence","text":"Output Markdown sequenceDiagram Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! ```mermaid sequenceDiagram Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! ```","title":"2\ufe0f\u20e3 Sequence"},{"location":"pages/cheatsheet/#3-gantt","text":"Output Markdown gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```","title":"3\ufe0f\u20e3 Gantt"},{"location":"pages/cheatsheet/#4-class","text":"Output Markdown classDiagram Class01 <|-- AveryLongClass : Cool <<interface>> Class01 Class09 --> C2 : Where am i? Class09 --* C3 Class09 --|> Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { <<service>> int id size() } ```mermaid classDiagram Class01 <|-- AveryLongClass : Cool <<interface>> Class01 Class09 --> C2 : Where am i? Class09 --* C3 Class09 --|> Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { <<service>> int id size() } ```","title":"4\ufe0f\u20e3 Class"},{"location":"pages/cheatsheet/#5-state","text":"Output Markdown stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*] ```mermaid stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*] ```","title":"5\ufe0f\u20e3 State"},{"location":"pages/cheatsheet/#6-pie","text":"Output Markdown pie \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15 ```mermaid pie \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15 ```","title":"6\ufe0f\u20e3 Pie"},{"location":"pages/cheatsheet/#7-journey","text":"Output Markdown journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 3: Me ```mermaid journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 3: Me ```","title":"7\ufe0f\u20e3 Journey"},{"location":"pages/cheatsheet/#8-er","text":"Output Markdown erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses ```mermaid erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses ```","title":"8\ufe0f\u20e3 ER"},{"location":"pages/cheatsheet/#sample-blocs","text":"THis is a note my note abstract my note info info Sample tip tip How to add plugins to the Docker image? Import Stuff Success my note Sample warning This is so cool. failure my note danger danger bug bug Sample example example Sample warning warning Sample wuote quote","title":"Sample Blocs"},{"location":"pages/cheatsheet/#tooltip","text":"wanna a tooltip ? # (1)! Cedrick rock mkdocs serve","title":"Tooltip"},{"location":"pages/cheatsheet/#icons","text":"","title":"Icons"},{"location":"pages/cheatsheet/#material","text":"HERE is the full list","title":"Material"},{"location":"pages/cheatsheet/#font-awesome","text":"HTML = fa-camera-retro MARKDOWN HERE is the full list","title":"Font Awesome"},{"location":"pages/cheatsheet/#opticons","text":"Sample","title":"Opticons"},{"location":"pages/cheatsheet/#adding-buttons","text":"In order to render a link as a button, suffix it with curly braces and add the .md-button class selector to it. The button will receive the selected [primary color] and [accent color] if active. Button [ Subscribe to our newsletter ]( # ){ .md-button } [Subscribe to our newsletter][demo]{ .md-button } [Subscribe to our newsletter][demo]{ .md-button .md-button--primary }","title":"Adding buttons"},{"location":"pages/astra/","text":"Greetings Developers ! \u00b6 Datastax created Astra which provides Apache Cassandra databases in the cloud. You can start for free with no credit card and no time limits. A lot of effort has been put to ease the usage of the database with some developer friendly apis . What's NEXT ? \u00b6 With your database running, you may want to learn more about Apis exposed by running Quick Start guides. info The Getting started guides for each Api are being created as we speak, please come back in a week to get them. Then when you feel confident, pick the language of your choice in friendly apis and start building amazing apps.","title":"Getting Started With Astra"},{"location":"pages/astra/#greetings-developers","text":"Datastax created Astra which provides Apache Cassandra databases in the cloud. You can start for free with no credit card and no time limits. A lot of effort has been put to ease the usage of the database with some developer friendly apis .","title":"Greetings Developers !"},{"location":"pages/astra/#whats-next","text":"With your database running, you may want to learn more about Apis exposed by running Quick Start guides. info The Getting started guides for each Api are being created as we speak, please come back in a week to get them. Then when you feel confident, pick the language of your choice in friendly apis and start building amazing apps.","title":"What's NEXT ?"},{"location":"pages/astra/astra-cli/","text":"Astra Cli is not released yet. It is in active developement Astra CLI provides a command line interface in a terminal to operate Datastax Astra. The goal is to offer access to any feature without accessing the user interface. Getting Started \u00b6 1. Installation \u00b6 \ud83d\udcd8 1a Prequisites On your machine you will need the following components A bash shell ( Bourne Again SHell ) The following commands untar , unzip , curl A Java JRE or JDK version 8+ installed You will also need an Astra token. As such, make sure you completed those 2 steps before Create an Astra account Generate an Astra Token \u2705 1b Installation To install or reinstall the CLI use the following command. Previous installations will be cleaned but configuration will NOT be lost. The cli is installed in ~/.astra/cli folder whereas your configuration is saved in ~/.astrarc file. curl -Ls \"https://dtsx.io/get-astra-cli\" | bash You can notice that the installation script also check the pre-requisites. It will download expected archive and update your bash profile to have astra in your path. \ud83d\udda5\ufe0f Installation script output \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d Installing Astra Cli 0.1.alpha3, please wait... Checking prerequisites: [OK] - Ready to install. [OK] - unzip command is available [OK] - zip command is available [OK] - curl command is available Preparing directories: [OK] - Created /Users/cedricklunven/.astra/tmp [OK] - Created /Users/cedricklunven/.astra/cli [OK] - Created /Users/cedricklunven/.astra/scb Downloading archive: ######################################################################## 100.0% [OK] - File downloaded [OK] - Integrity of the archive checked Extracting and installation: [OK] - Extraction is successful [OK] - File moved to /Users/cedricklunven/.astra/cli [OK] - Installation cleaned up [OK] - Installation Successful Open A NEW TERMINAL and run: astra setup You can close this window. 2. Setup \u00b6 After installing, make sure to open a new terminal to have astra in the PATH . \u2705 2a - Run Setup Before issuing commands to init to initialize the configuration file ~/.astrarc . To to so run the following command. You will be asked to provide your token (AstraCS:...). It will be saved and reuse for your commands. astra setup \ud83d\udda5\ufe0f astra setup command output +-------------------------------+ +- Astra CLI SETUP -+ +-------------------------------+ Welcome to Astra Cli. We will guide you to start. [Astra Setup] To use the cli you need to: \u2022 Create an Astra account on : https://astra.datastax.com \u2022 Create an Authentication token following: https://dtsx.io/create-astra-token [Cli Setup] You will be asked to enter your token, it will be saved locally in ~/.astrarc \u2022 Enter your token (starting with AstraCS) : AstraCS:lorem_ipsum Copy paste the value of your token and press enter. \ud83d\udda5\ufe0f astra setup command output [What's NEXT ?] You are all set.(configuration is stored in ~/.astrarc) You can now: \u2022 Use any command, 'astra help' will get you the list \u2022 Try with 'astra db list' \u2022 Enter interactive mode using 'astra' Happy Coding ! \u2705 2b - Setup validation You are all set. The configuration (mainly your token) is stored in file ~/.astrarc . Display current version of the cli, validating setup is complete. astra --version \ud83d\udda5\ufe0f Sample output 0.1.alpha5 You created a default configuration pointing to your organization. If you want to work with multiple organizations look at Config Management chapter below. After setup you have 1 configuration automaticall set as default. You can have more than one configuration is you work with multiple organizations for instance. To know more about multi-organizations configuration check chapter 5.1 . astra config list \ud83d\udda5\ufe0f Sample output +-----------------------------------------+ | configuration | +-----------------------------------------+ | default (cedrick.lunven@datastax.com) | | cedrick.lunven@datastax.com | +-----------------------------------------+ 3. Get Help \u00b6 The solution provides extensive documentation for any command. It also provides som bash autocompletion, use the TAB key twice to get a list of propositions. \u2705 3a - Autocompletion astra <TAB> <TAB> \ud83d\udda5\ufe0f Sample output --no-color config db help role setup shell user \u2705 3b - Documentation Groups of command will get you the different command avalable. Display main help astra help \ud83d\udda5\ufe0f Sample output usage: astra <command> [ <args> ] Commands are: help View help for any command setup Initialize configuration file shell Interactive mode (default if no command provided) config Manage configuration file db Manage databases role Manage roles (RBAC) user Manage users See 'astra help <command>' for more information on a specific command. Display help for command group astra db astra help db \ud83d\udda5\ufe0f Sample output NAME astra db - Manage databases SYNOPSIS astra db { cqlsh | create | create-keyspace | delete | dsbulk | get | list } [--] [ --token <AUTH_TOKEN> ] [ --config-file <CONFIG_FILE> ] [ --no-color ] [ {-v | --verbose} ] [ {-conf | --config} <CONFIG_SECTION> ] [ --log <LOG_FILE> ] [ {-o | --output} <FORMAT> ] [cmd-options] <cmd-args> Where command-specific options [cmd-options] are: cqlsh: [ --debug ] [ {-f | --file} <FILE> ] [ {-k | --keyspace} <KEYSPACE> ] [ --version ] [ {-e | --execute} <STATEMENT> ] [ --encoding <ENCODING> ] create: [ {-k | --keyspace} <KEYSPACE> ] [ --if-not-exist ] [ {-r | --region} <DB_REGION> ] create-keyspace: {-k | --keyspace} <KEYSPACE> [ --if-not-exist ] delete: dsbulk: get: list: Where command-specific arguments <cmd-args> are: cqlsh: <DB> create: <DB_NAME> create-keyspace: <DB> delete: <DB> dsbulk: [ <dsbulkArguments>... ] get: <DB> list: See 'astra help db <command>' for more information on a specific command. Display help for unitary command astra db list For unitary commands all options details are provided. astra help db list \ud83d\udda5\ufe0f Sample output NAME astra db list - Display the list of Databases in an organization SYNOPSIS astra db list [ {-conf | --config} <CONFIG_SECTION> ] [ --config-file <CONFIG_FILE> ] [ --log <LOG_FILE> ] [ --no-color ] [ {-o | --output} <FORMAT> ] [ --token <AUTH_TOKEN> ] [ {-v | --verbose} ] OPTIONS -conf <CONFIG_SECTION>, --config <CONFIG_SECTION> Section in configuration file (default = ~/.astrarc) --config-file <CONFIG_FILE> Configuration file (default = ~/.astrarc) --log <LOG_FILE> Logs will go in the file plus on console --no-color Remove all colors in output -o <FORMAT>, --output <FORMAT> Output format, valid values are: human,json,csv --token <AUTH_TOKEN> Key to use authenticate each call. -v, --verbose Verbose mode with log in console Astra DB \u00b6 1. List databases \u00b6 \u2705 1a - list To get the list of non terminated database in your oganization use the command list in the group db . astra db list \ud83d\udda5\ufe0f Sample output +---------------------+--------------------------------------+---------------------+----------------+ | Name | id | Default Region | Status | +---------------------+--------------------------------------+---------------------+----------------+ | mtg | dde308f5-a8b0-474d-afd6-81e5689e3e25 | eu-central-1 | ACTIVE | | workshops | 3ed83de7-d97f-4fb6-bf9f-82e9f7eafa23 | eu-west-1 | ACTIVE | | sdk_tests | 06a9675a-ca62-4cd0-9b94-aefaf395922b | us-east-1 | ACTIVE | | test | 7677a789-bd57-455d-ab2c-a3bdfa35ba68 | eu-central-1 | ACTIVE | | demo | 071d7059-d55b-4cdb-90c6-41c26da1a029 | us-east-1 | ACTIVE | | ac201 | 48c7178c-58cb-4657-b3d2-8a9e3cc89461 | us-east-1 | ACTIVE | +---------------------+--------------------------------------+---------------------+----------------+ \u2705 1b - Get Help To get help on a command always prefix with astra help XXX astra help db list \ud83d\udda5\ufe0f Sample output NAME astra db list - Display the list of Databases in an organization SYNOPSIS astra db list [ {-conf | --config} <CONFIG_SECTION> ] [ --config-file <CONFIG_FILE> ] [ --log <LOG_FILE> ] [ --no-color ] [ {-o | --output} <FORMAT> ] [ --token <AUTH_TOKEN> ] [ {-v | --verbose} ] OPTIONS -conf <CONFIG_SECTION>, --config <CONFIG_SECTION> Section in configuration file (default = ~/.astrarc) --config-file <CONFIG_FILE> Configuration file (default = ~/.astrarc) --log <LOG_FILE> Logs will go in the file plus on console --no-color Remove all colors in output -o <FORMAT>, --output <FORMAT> Output format, valid values are: human,json,csv --token <AUTH_TOKEN> Key to use authenticate each call. -v, --verbose Verbose mode with log in console \u2705 1c - Change output astra db list -o csv \ud83d\udda5\ufe0f Sample output Name,id,Default Region,Status mtg,dde308f5-a8b0-474d-afd6-81e5689e3e25,eu-central-1,ACTIVE workshops,3ed83de7-d97f-4fb6-bf9f-82e9f7eafa23,eu-west-1,ACTIVE sdk_tests,06a9675a-ca62-4cd0-9b94-aefaf395922b,us-east-1,ACTIVE test,7677a789-bd57-455d-ab2c-a3bdfa35ba68,eu-central-1,ACTIVE demo,071d7059-d55b-4cdb-90c6-41c26da1a029,us-east-1,ACTIVE ac201,48c7178c-58cb-4657-b3d2-8a9e3cc89461,us-east-1,ACTIVE astra db list -o json \ud83d\udda5\ufe0f Sample output { \"code\" : 0 , \"message\" : \"astra db list -o json\" , \"data\" : [ { \"Status\" : \"ACTIVE\" , \"Default Region\" : \"eu-central-1\" , \"id\" : \"dde308f5-a8b0-474d-afd6-81e5689e3e25\" , \"Name\" : \"mtg\" }, { \"Status\" : \"ACTIVE\" , \"Default Region\" : \"eu-west-1\" , \"id\" : \"3ed83de7-d97f-4fb6-bf9f-82e9f7eafa23\" , \"Name\" : \"workshops\" }, { \"Status\" : \"ACTIVE\" , \"Default Region\" : \"us-east-1\" , \"id\" : \"06a9675a-ca62-4cd0-9b94-aefaf395922b\" , \"Name\" : \"sdk_tests\" }, { \"Status\" : \"ACTIVE\" , \"Default Region\" : \"eu-central-1\" , \"id\" : \"7677a789-bd57-455d-ab2c-a3bdfa35ba68\" , \"Name\" : \"test\" }, { \"Status\" : \"ACTIVE\" , \"Default Region\" : \"us-east-1\" , \"id\" : \"071d7059-d55b-4cdb-90c6-41c26da1a029\" , \"Name\" : \"demo\" }, { \"Status\" : \"ACTIVE\" , \"Default Region\" : \"us-east-1\" , \"id\" : \"48c7178c-58cb-4657-b3d2-8a9e3cc89461\" , \"Name\" : \"ac201\" } ] } 2. Create database \u00b6 \u2705 2a - Create Database If not provided the region will be the default free region and the keyspace will be the database name but you can change then with -r and -k respectivitely. astra db create demo \u2705 2b - Options --if-not-exist and --wait Database name does not ensure unicity (database id does) as such if you issue the command multiple times you will end up with multiple instances. To change this behaviour you can use --if-not-exist Database creation is asynchronous operation. Still during your CI/CD you want the Db to be ACTIVE before moving forward. The option --wait will trigger a blocking command until the db is ready On the free tier, after a period of inactivity the database moves to HIBERNATED state. The creation command, will resume the db when needed. astra db create demo -k ks2 --if-not-exist --wait \u2705 2c - Get help Better doc the cli itself. astra help db create 3. Resume database \u00b6 In the free tier, after 23H of inactivity your database got hibernated. To wake up the db you can use the resume command \u2705 2a - Resuming Assuming you have an hibernating database. astra db list +---------------------+--------------------------------------+---------------------+----------------+ | Name | id | Default Region | Status | +---------------------+--------------------------------------+---------------------+----------------+ | hemidactylus | 643c6bb8-2336-4649-97d5-39c33491f5c1 | eu-central-1 | HIBERNATED | +---------------------+--------------------------------------+---------------------+----------------+ Trigger an explicit resuming with: astra db resume hemidactylus \ud83d\udda5\ufe0f Sample output +---------------------+--------------------------------------+---------------------+----------------+ | Name | id | Default Region | Status | +---------------------+--------------------------------------+---------------------+----------------+ | hemidactylus | 643c6bb8-2336-4649-97d5-39c33491f5c1 | eu-central-1 | RESUMING | +---------------------+--------------------------------------+---------------------+----------------+ And after a few time +---------------------+--------------------------------------+---------------------+----------------+ | Name | id | Default Region | Status | +---------------------+--------------------------------------+---------------------+----------------+ | hemidactylus | 643c6bb8-2336-4649-97d5-39c33491f5c1 | eu-central-1 | ACTIVE | +---------------------+--------------------------------------+---------------------+----------------+ 4. Get database details \u00b6 \u2705 4a. To get general information or details on an entity use the command get . astra db get demo In the output you specially see the list of keyspaces available and the different regions. \ud83d\udda5\ufe0f Sample output +------------------------+-----------------------------------------+ | Attribute | Value | +------------------------+-----------------------------------------+ | Name | demo | | id | 071d7059-d55b-4cdb-90c6-41c26da1a029 | | Status | ACTIVE | | Default Cloud Provider | AWS | | Default Region | us-east-1 | | Default Keyspace | demo | | Creation Time | 2022-07-26T15:41:18Z | | | | | Keyspaces | [0] demo | | | | | Regions | [0] us-east-1 | +------------------------+-----------------------------------------+ \u2705 4b. To get a special property you can add the option --key . Multiple keys are available: id , status , cloud , keyspace , keyspaces , region , regions . Notice that the output is raw. This command is expected to be used in scripts astra db get demo --key id \ud83d\udda5\ufe0f Sample output dde308f5-a8b0-474d-afd6-81e5689e3e25 \u2705 4c. To get database status in a human readble for use status command astra db status demo \ud83d\udda5\ufe0f Sample output [ INFO ] - Database 'demo' has status 'ACTIVE' 5. Delete Database \u00b6 \u2705 5a. To delete a db use the command delete . astra db delete demo2 6. Working with keyspaces \u00b6 A keyspace is created when you create the database. Default CLI behaviour is to provide same values for keyspace and database names. But you can define your own keyspace name with the flag -k . \u2705 6a - Create new keyspace To add a keyspace ks2 to an existing database demo use the following. The option --if-not-exist is optional but could help you providing idempotent scripts. astra db create-keyspace demo -k ks2 --if-not-exist If the database is not found you will get a warning message and dedicated code returned. To see your new keyspace you can display your database details astra db list-keyspaces demo \u2705 6b - Get help astra help db create-keyspace 7. Cqlsh \u00b6 Cqlsh is a standalone shell to work with Apache Cassandra\u2122. It is compliant with Astra but requires a few extra steps of configuration. The purpose of the CLI is to integrate with cqlsh and do the integration for you. Astra Cli will download , install , setup and wrap cqlsh for you to interact with Astra. \u2705 7a - Interactive mode If no option are provided, you enter cqlsh interactive mode astra db cqlsh demo \ud83d\udda5\ufe0f Sample output Cqlsh is starting please wait for connection establishment... Connected to cndb at 127.0.0.1:9042. [cqlsh 6.8.0 | Cassandra 4.0.0.6816 | CQL spec 3.4.5 | Native protocol v4] Use HELP for help. token@cqlsh> \u2705 7b - Execute CQL To execute CQL Statement with cqlsh use the flag -e . astra db cqlsh demo -e \"describe keyspaces;\" \u2705 7b - Execute CQL Files To execute CQL Files with cqlsh use the flag -f . You could also use the CQL syntax SOURCE. astra db cqlsh demo -f sample.cql 8. DSBulk \u00b6 \u2705 8a - Setup Dsbulk stands for Datastax bulk loader. It is a standalone program to load, unload and count data in an efficient way with Apache Cassandra\u2122. It is compliant with Datastax Astra. As for Cqlsh the cli will download , install , setup and wrap the dsbulk command for you. All options are available. To give you an idea let's tak a simple example. Make sure we have a db demo with a keyspace demo astra db create demo Looking at a dataset of cities in the world. cities.csv . We can show here the first lines of the file. id,name,state_id,state_code,state_name,country_id,country_code,country_name,latitude,longitude,wikiDataId 52,Ashk\u0101sham,3901,BDS,Badakhshan,1,AF,Afghanistan,36.68333000,71.53333000,Q4805192 68,Fayzabad,3901,BDS,Badakhshan,1,AF,Afghanistan,37.11664000,70.58002000,Q156558 ... Let's create a table to store those values. Connect to CQHSH astra db cqlsh demo -k demo Create the table CREATE TABLE cities_by_country ( country_name text , name text , id int , state_id text , state_code text , state_name text , country_id text , country_code text , latitude double , longitude double , wikiDataId text , PRIMARY KEY (( country_name ), name ) ); describe table cities_by_country ; quit \u2705 8b - Load Data astra db dsbulk demo load \\ -url https://raw.githubusercontent.com/awesome-astra/docs/main/docs/assets/cities.csv \\ -k demo \\ -t cities_by_country \\ --schema.allowMissingFields true The first time the line DSBulk is starting please wait can take a few seconds to appear. The reason is the cli is download dsbulk if not downloaded before. \ud83d\udda5\ufe0f Sample output DSBulk is starting please wait ... Username and password provided but auth provider not specified, inferring PlainTextAuthProvider A cloud secure connect bundle was provided: ignoring all explicit contact points. A cloud secure connect bundle was provided and selected operation performs writes: changing default consistency level to LOCAL_QUORUM. Operation directory: /Users/cedricklunven/Downloads/logs/LOAD_20220823-182343-074618 Setting executor.maxPerSecond not set when connecting to DataStax Astra: applying a limit of 9,000 ops/second based on the number of coordinators (3). If your Astra database has higher limits, please define executor.maxPerSecond explicitly. total | failed | rows/s | p50ms | p99ms | p999ms | batches 148,266 | 0 | 8,361 | 663.86 | 767.56 | 817.89 | 30.91 Operation LOAD_20220823-182343-074618 completed successfully in 17 seconds. Last processed positions can be found in positions.txt \u2705 8c - Count Check than the data has been imported with cqlsh SH astra db cqlsh demo -e \"select * from demo.cities_by_country LIMIT 20;\" \ud83d\udda5\ufe0f Sample output Cqlsh is starting please wait for connection establishment... country_name | name | country_code | country_id | id | latitude | longitude | state_code | state_id | state_name | wikidataid --------------+---------------------+--------------+------------+------+----------+-----------+------------+----------+---------------------+------------ Bangladesh | Azimpur | BD | 19 | 8454 | 23.7298 | 90.3854 | 13 | 771 | Dhaka District | null Bangladesh | Badarganj | BD | 19 | 8455 | 25.67419 | 89.05377 | 55 | 759 | Rangpur District | null Bangladesh | Bagerhat | BD | 19 | 8456 | 22.4 | 89.75 | 27 | 811 | Khulna District | null Bangladesh | Bandarban | BD | 19 | 8457 | 22 | 92.33333 | B | 803 | Chittagong Division | null Bangladesh | Baniachang | BD | 19 | 8458 | 24.51863 | 91.35787 | 60 | 767 | Sylhet District | null Bangladesh | Barguna | BD | 19 | 8459 | 22.13333 | 90.13333 | 06 | 818 | Barisal District | null Bangladesh | Barisal | BD | 19 | 8460 | 22.8 | 90.5 | 06 | 818 | Barisal District | null Bangladesh | Bera | BD | 19 | 8462 | 24.07821 | 89.63262 | 54 | 813 | Rajshahi District | null Bangladesh | Bhairab B\u0101z\u0101r | BD | 19 | 8463 | 24.0524 | 90.9764 | 13 | 771 | Dhaka District | null Bangladesh | Bher\u0101m\u0101ra | BD | 19 | 8464 | 24.02452 | 88.99234 | 27 | 811 | Khulna District | null Bangladesh | Bhola | BD | 19 | 8465 | 22.36667 | 90.81667 | 06 | 818 | Barisal District | null Bangladesh | Bh\u0101nd\u0101ria | BD | 19 | 8466 | 22.48898 | 90.06273 | 06 | 818 | Barisal District | null Bangladesh | Bh\u0101tp\u0101ra Abhaynagar | BD | 19 | 8467 | 23.01472 | 89.43936 | 27 | 811 | Khulna District | null Bangladesh | Bibir Hat | BD | 19 | 8468 | 22.68347 | 91.79058 | B | 803 | Chittagong Division | null Bangladesh | Bogra | BD | 19 | 8469 | 24.78333 | 89.35 | 54 | 813 | Rajshahi District | null Bangladesh | Brahmanbaria | BD | 19 | 8470 | 23.98333 | 91.16667 | B | 803 | Chittagong Division | null Bangladesh | Burh\u0101nuddin | BD | 19 | 8471 | 22.49518 | 90.72391 | 06 | 818 | Barisal District | null Bangladesh | B\u0101jitpur | BD | 19 | 8472 | 24.21623 | 90.95002 | 13 | 771 | Dhaka District | null Bangladesh | Chandpur | BD | 19 | 8474 | 23.25 | 90.83333 | B | 803 | Chittagong Division | null Bangladesh | Chapai Nababganj | BD | 19 | 8475 | 24.68333 | 88.25 | 54 | 813 | Rajshahi District | null Count with ds bulkd astra db dsbulk demo count -k demo -t cities_by_country \ud83d\udda5\ufe0f Sample output DSBulk is starting please wait ... [INFO ] - RUNNING: /Users/cedricklunven/.astra/dsbulk-1.9.1/bin/dsbulk count -k demo -t cities_by_country -u token -p AstraCS:gdZaqzmFZszaBTOlLgeecuPs:edd25600df1c01506f5388340f138f277cece2c93cb70f4b5fa386490daa5d44 -b /Users/cedricklunven/.astra/scb/scb_071d7059-d55b-4cdb-90c6-41c26da1a029_us-east-1.zip Username and password provided but auth provider not specified, inferring PlainTextAuthProvider A cloud secure connect bundle was provided: ignoring all explicit contact points. Operation directory: /Users/cedricklunven/Downloads/logs/COUNT_20220823-182833-197954 total | failed | rows/s | p50ms | p99ms | p999ms 134,574 | 0 | 43,307 | 315.71 | 457.18 | 457.18 \u2705 8d - Unload Data astra db dsbulk demo unload -k demo -t cities_by_country -url /tmp/unload \ud83d\udda5\ufe0f Sample output DSBulk is starting please wait ... Username and password provided but auth provider not specified, inferring PlainTextAuthProvider A cloud secure connect bundle was provided: ignoring all explicit contact points. Operation directory: /Users/cedricklunven/Downloads/logs/UNLOAD_20220823-183054-208353 total | failed | rows/s | p50ms | p99ms | p999ms 134,574 | 0 | 14,103 | 927.51 | 1,853.88 | 1,853.88 Operation UNLOAD_20220823-183054-208353 completed successfully in 9 seconds. 9. Download Secure bundle \u00b6 \u2705 9a - Default values Download the different secure bundles (one per region) with the pattern scb_${dbid}-${dbregion}.zip in the current folder. mkdir db-demo cd db-demo astra db download-scb demo ls \u2705 9b - Download in target folder Download the different secure bundles (one per region) with the pattern scb_${dbid}-${dbregion}.zip in the folder provide with option -d ( --output-director ). astra db download-scb demo -d /tmp \u2705 9c - Download in target folder Provide the target filename with -f ( --output-file ). It will work only if you have a SINGLE REGION for your database (or you will have to use the flag -d ) astra db download-scb demo -f /tmp/demo.zip Astra STREAMING \u00b6 1. List tenants \u00b6 \u2705 1a - list To get the list of tenant in your oganization use the command list in the group streaming . astra streaming list \ud83d\udda5\ufe0f Sample output +---------------------+-----------+----------------+----------------+ | name | cloud | region | Status | +---------------------+-----------+----------------+----------------+ | cedrick-20220910 | aws | useast2 | active | | trollsquad-2022 | aws | useast2 | active | +---------------------+-----------+----------------+----------------+ \u2705 1b - Change output as csv amd json astra streaming list -o csv \ud83d\udda5\ufe0f Sample output name,cloud,region,Status cedrick-20220910,aws,useast2,active trollsquad-2022,aws,useast2,active astra streaming list -o json \ud83d\udda5\ufe0f Sample output { \"code\" : 0 , \"message\" : \"astra streaming list -o json\" , \"data\" : [ { \"cloud\" : \"aws\" , \"Status\" : \"active\" , \"name\" : \"cedrick-20220910\" , \"region\" : \"useast2\" }, { \"cloud\" : \"aws\" , \"Status\" : \"active\" , \"name\" : \"trollsquad-2022\" , \"region\" : \"useast2\" } ] } 2. Create tenant \u00b6 \u2705 2a - Check tenant existence with exist The tenant name needs to be unique for the cluster (Cloud provider / region). It may be useful to check if the name is already in use by somebody else. astra streaming exist new_tenant_from_cli \ud83d\udda5\ufe0f Sample output [ INFO ] - Tenant 'new_tenant_from_cli' does not exist. \u2705 2b - Create tenant To create a tenant with default cloud ( aws ), default region ( useast2 ), plan ( free ) and namespace ( default ): astra streaming create new_tenant_from_cli To know all supported option please use astra help streaming create 3. Get tenant details \u00b6 \u2705 3a - To get i nformation or details on an entity use the command get . astra streaming get trollsquad-2022 The pulsar token is not displayed in this view as too loong, there are dedicated command to display it. \ud83d\udda5\ufe0f Sample output +------------------+-------------------------------------------------------------+ | Attribute | Value | +------------------+-------------------------------------------------------------+ | Name | trollsquad-2022 | | Status | active | | Cloud Provider | aws | | Cloud region | useast2 | | Cluster Name | pulsar-aws-useast2 | | Pulsar Version | 2.10 | | Jvm Version | JDK11 | | Plan | payg | | WebServiceUrl | https://pulsar-aws-useast2.api.streaming.datastax.com | | BrokerServiceUrl | pulsar+ssl://pulsar-aws-useast2.streaming.datastax.com:6651 | | WebSocketUrl | wss://pulsar-aws-useast2.streaming.datastax.com:8001/ws/v2 | +------------------+-------------------------------------------------------------+ \u2705 3b. To get a special property you can add the option --key . Multiple keys are available: status , cloud , pulsar_token . Notice that the output is raw. This command is expected to be used in scripts astra streaming get trollsquad-2022 --key cloud \ud83d\udda5\ufe0f Sample output aws \u2705 3c. To get tenant pulsar-token please use pulsar-token command astra streaming pulsar-token trollsquad-2022 \ud83d\udda5\ufe0f Sample output eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE2NjI5NzcyNzksImlzcyI6ImRhdGFzdGF4Iiwic3ViIjoiY2xpZW50O2Y5NDYwZjE0LTk4NzktNGViZS04M2YyLTQ4ZDNmM2RjZTEzYztkSEp2Ykd4emNYVmhaQzB5TURJeTsxOTZlYjg0YTMzIiwidG9rZW5pZCI6IjE5NmViODRhMzMifQ.rjJYDG_nJu0YpgATfjeKeUUAqwJGyVlvzpA5iP-d5-bReQf1FPaDlGxo40ADHHn2kx2NOdgMsm-Ys4K... \u2705 3d. To get tenant status in a human readble for use status command astra streaming status trollsquad-2022 \ud83d\udda5\ufe0f Sample output [ INFO ] - Tenant 'trollsquad-2022' has status 'active' 4. Delete Tenant \u00b6 \u2705 4a. To delete a tenant simply use the command delete astra streaming delete trollsquad 5. Pulsar-Shell \u00b6 Pulsar-Shell is a standalone shell to work with Apache Pulsar. It is compliant with Astra but requires a few extra steps of configuration. The purpose of the CLI is to integrate with pulsar-shell and do the integration for you. Astra Cli will download , install , setup and wrap pulsar-shell for you to interact with Astra. \u2705 5a - Interactive mode If no option are provided, you enter pulsar-shell interactive mode astra streaming pulsar-shell trollsquad-2022 \ud83d\udda5\ufe0f Sample output /Users/cedricklunven/.astra/lunastreaming-shell-2.10.1.1/conf/client-aws-useast2-trollsquad-2022.conf Pulsar-shell is starting please wait for connection establishment... Using directory: /Users/cedricklunven/.pulsar-shell Welcome to Pulsar shell! Service URL: pulsar+ssl://pulsar-aws-useast2.streaming.datastax.com:6651 Admin URL: https://pulsar-aws-useast2.api.streaming.datastax.com Type help to get started or try the autocompletion (TAB button). Type exit or quit to end the shell session. default(pulsar-aws-useast2.streaming.datastax.com)> You can quit with exit. \u2705 5b - Execute Pulsar Shell command To execute command with pushar-shell use the flag -e . astra streaming pulsar-shell trollsquad-2022 -e \"admin namespaces list trollsquad-2022\" \ud83d\udda5\ufe0f Sample output /Users/cedricklunven/.astra/lunastreaming-shell-2.10.1.1/conf/client-aws-useast2-trollsquad-2022.conf Pulsar-shell is starting please wait for connection establishment... Using directory: /Users/cedricklunven/.pulsar-shell [1/1] Executing admin namespaces list trollsquad-2022 [1/1] \u2714 admin namespaces list trollsquad-2022 \u2705 5c - Execute Pulsar Shell files To execute CQL Files with pushar-shell use the flag -e . astra streaming pulsar-shell trollsquad-2022 -f create_topics.txt 6. Pulsar-client and Admin \u00b6 Pulsar client and admin are provided within pulsar-shell. This section simply provide some examples to write and read in a topic with client. \u2705 6a - Create a topic demo . First start the pulsar-shell on 2 different terminal astra streaming pulsar-shell trollsquad-2022 Then on first terminal create a topic demo in namespace default admin topics create persistent://trollsquad-2022/default/demo You can now list the different topics in namespace default admin topics list trollsquad-2022/default \ud83d\udda5\ufe0f Sample output persistent://trollsquad-2022/default/demo Start a consumer on this topic client consume persistent://trollsquad-2022/default/demo -s astra_cli_tuto -n 0 \ud83d\udda5\ufe0f Sample output .. init ... 83 - R:pulsar-aws-useast2.streaming.datastax.com/3.16.119.226:6651]] Connected to server 2022-09-12T12:28:34,869+0200 [pulsar-client-io-1-1] INFO org.apache.pulsar.client.impl.ClientCnx - [id: 0xc5ce3ec4, L:/192.168.82.1:53683 - R:pulsar-aws-useast2.streaming.datastax.com/3.16.119.226:6651] Connected through proxy to target broker at 192.168.7.141:6650 2022-09-12T12:28:35,460+0200 [pulsar-client-io-1-1] INFO org.apache.pulsar.client.impl.ConsumerImpl - [persistent://trollsquad-2022/default/demo][astra_cli_tuto] Subscribing to topic on cnx [id: 0xc5ce3ec4, L:/192.168.82.1:53683 - R:pulsar-aws-useast2.streaming.datastax.com/3.16.119.226:6651], consumerId 0 2022-09-12T12:28:35,645+0200 [pulsar-client-io-1-1] INFO org.apache.pulsar.client.impl.ConsumerImpl - [persistent://trollsquad-2022/default/demo][astra_cli_tuto] Subscribed to topic on pulsar-aws-useast2.streaming.datastax.com/3.16.119.226:6651 -- consumer: 0 On the second terminal you can now start a producer client produce persistent://trollsquad-2022/default/demo -m \"hello,world\" -n 20 \ud83d\udda5\ufe0f Sample output 2022-09-12T12:36:28,684+0200 [pulsar-client-io-14-1] INFO org.apache.pulsar.client.impl.ClientCnx - [id: 0x682890b5, L:/192.168.1.106:53796 ! R:pulsar-aws-useast2.streaming.datastax.com/3.138.177.230:6651] Disconnected 2022-09-12T12:36:30,756+0200 [main] INFO org.apache.pulsar.client.cli.PulsarClientTool - 40 messages successfully produced And on the client side key:[null], properties:[], content:world ----- got message ----- key:[null], properties:[], content:hello User and Roles \u00b6 1. List users \u00b6 astra user list \ud83d\udda5\ufe0f Sample output +--------------------------------------+-----------------------------+---------------------+ | User Id | User Email | Status | +--------------------------------------+-----------------------------+---------------------+ | b665658a-ae6a-4f30-a740-2342a7fb469c | cedrick.lunven@datastax.com | active | +--------------------------------------+-----------------------------+---------------------+ 2. Invite User \u00b6 astra user invite cedrick.lunven@gmail.com Check the list of user and notice the new user invited. astra user list \ud83d\udda5\ufe0f Sample output +--------------------------------------+-----------------------------+---------------------+ | User Id | User Email | Status | +--------------------------------------+-----------------------------+---------------------+ | 825bd3d3-82ae-404b-9aad-bbb4c53da315 | cedrick.lunven@gmail.com | invited | | b665658a-ae6a-4f30-a740-2342a7fb469c | cedrick.lunven@datastax.com | active | +--------------------------------------+-----------------------------+---------------------+ 3. Revoke User \u00b6 astra user delete cedrick.lunven@gmail.com \ud83d\udda5\ufe0f Sample output +--------------------------------------+-----------------------------+---------------------+ | User Id | User Email | Status | +--------------------------------------+-----------------------------+---------------------+ | b665658a-ae6a-4f30-a740-2342a7fb469c | cedrick.lunven@datastax.com | active | +--------------------------------------+-----------------------------+---------------------+ 4. List roles \u00b6 astra role list 6. Get role infos \u00b6 astra role get \"Database Administrator\" Advanced Topics \u00b6 1 Config Management \u00b6 If you work with multiple organizations it could be useful to switch from configuration to another, one token to another. The Cli provides a configuration management solution to handle this use case. \u2705 1a - List available configuration astra config list \u2705 1b - Create a new section astra config create dev --token <token_of_org_2> \u2705 1c - Use your section config anywhere You can use any organization anytime with --config <onfig_name> . astra user list --config dev \u2705 1d - Select a section as defaul Change the current org astra config use dev See your new list astra config list \u2705 1e - Delete a section You can delete any organization. If you delete the selected organization you will have to pick a new one. Delete you config astra config delete dev See the new list astra config list","title":"\u2023 Astra Cli"},{"location":"pages/astra/astra-cli/#getting-started","text":"","title":"Getting Started"},{"location":"pages/astra/astra-cli/#1-installation","text":"\ud83d\udcd8 1a Prequisites On your machine you will need the following components A bash shell ( Bourne Again SHell ) The following commands untar , unzip , curl A Java JRE or JDK version 8+ installed You will also need an Astra token. As such, make sure you completed those 2 steps before Create an Astra account Generate an Astra Token \u2705 1b Installation To install or reinstall the CLI use the following command. Previous installations will be cleaned but configuration will NOT be lost. The cli is installed in ~/.astra/cli folder whereas your configuration is saved in ~/.astrarc file. curl -Ls \"https://dtsx.io/get-astra-cli\" | bash You can notice that the installation script also check the pre-requisites. It will download expected archive and update your bash profile to have astra in your path. \ud83d\udda5\ufe0f Installation script output \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557 \u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551 \u255a\u2550\u2550\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d Installing Astra Cli 0.1.alpha3, please wait... Checking prerequisites: [OK] - Ready to install. [OK] - unzip command is available [OK] - zip command is available [OK] - curl command is available Preparing directories: [OK] - Created /Users/cedricklunven/.astra/tmp [OK] - Created /Users/cedricklunven/.astra/cli [OK] - Created /Users/cedricklunven/.astra/scb Downloading archive: ######################################################################## 100.0% [OK] - File downloaded [OK] - Integrity of the archive checked Extracting and installation: [OK] - Extraction is successful [OK] - File moved to /Users/cedricklunven/.astra/cli [OK] - Installation cleaned up [OK] - Installation Successful Open A NEW TERMINAL and run: astra setup You can close this window.","title":"1. Installation"},{"location":"pages/astra/astra-cli/#2-setup","text":"After installing, make sure to open a new terminal to have astra in the PATH . \u2705 2a - Run Setup Before issuing commands to init to initialize the configuration file ~/.astrarc . To to so run the following command. You will be asked to provide your token (AstraCS:...). It will be saved and reuse for your commands. astra setup \ud83d\udda5\ufe0f astra setup command output +-------------------------------+ +- Astra CLI SETUP -+ +-------------------------------+ Welcome to Astra Cli. We will guide you to start. [Astra Setup] To use the cli you need to: \u2022 Create an Astra account on : https://astra.datastax.com \u2022 Create an Authentication token following: https://dtsx.io/create-astra-token [Cli Setup] You will be asked to enter your token, it will be saved locally in ~/.astrarc \u2022 Enter your token (starting with AstraCS) : AstraCS:lorem_ipsum Copy paste the value of your token and press enter. \ud83d\udda5\ufe0f astra setup command output [What's NEXT ?] You are all set.(configuration is stored in ~/.astrarc) You can now: \u2022 Use any command, 'astra help' will get you the list \u2022 Try with 'astra db list' \u2022 Enter interactive mode using 'astra' Happy Coding ! \u2705 2b - Setup validation You are all set. The configuration (mainly your token) is stored in file ~/.astrarc . Display current version of the cli, validating setup is complete. astra --version \ud83d\udda5\ufe0f Sample output 0.1.alpha5 You created a default configuration pointing to your organization. If you want to work with multiple organizations look at Config Management chapter below. After setup you have 1 configuration automaticall set as default. You can have more than one configuration is you work with multiple organizations for instance. To know more about multi-organizations configuration check chapter 5.1 . astra config list \ud83d\udda5\ufe0f Sample output +-----------------------------------------+ | configuration | +-----------------------------------------+ | default (cedrick.lunven@datastax.com) | | cedrick.lunven@datastax.com | +-----------------------------------------+","title":"2. Setup"},{"location":"pages/astra/astra-cli/#3-get-help","text":"The solution provides extensive documentation for any command. It also provides som bash autocompletion, use the TAB key twice to get a list of propositions. \u2705 3a - Autocompletion astra <TAB> <TAB> \ud83d\udda5\ufe0f Sample output --no-color config db help role setup shell user \u2705 3b - Documentation Groups of command will get you the different command avalable. Display main help astra help \ud83d\udda5\ufe0f Sample output usage: astra <command> [ <args> ] Commands are: help View help for any command setup Initialize configuration file shell Interactive mode (default if no command provided) config Manage configuration file db Manage databases role Manage roles (RBAC) user Manage users See 'astra help <command>' for more information on a specific command. Display help for command group astra db astra help db \ud83d\udda5\ufe0f Sample output NAME astra db - Manage databases SYNOPSIS astra db { cqlsh | create | create-keyspace | delete | dsbulk | get | list } [--] [ --token <AUTH_TOKEN> ] [ --config-file <CONFIG_FILE> ] [ --no-color ] [ {-v | --verbose} ] [ {-conf | --config} <CONFIG_SECTION> ] [ --log <LOG_FILE> ] [ {-o | --output} <FORMAT> ] [cmd-options] <cmd-args> Where command-specific options [cmd-options] are: cqlsh: [ --debug ] [ {-f | --file} <FILE> ] [ {-k | --keyspace} <KEYSPACE> ] [ --version ] [ {-e | --execute} <STATEMENT> ] [ --encoding <ENCODING> ] create: [ {-k | --keyspace} <KEYSPACE> ] [ --if-not-exist ] [ {-r | --region} <DB_REGION> ] create-keyspace: {-k | --keyspace} <KEYSPACE> [ --if-not-exist ] delete: dsbulk: get: list: Where command-specific arguments <cmd-args> are: cqlsh: <DB> create: <DB_NAME> create-keyspace: <DB> delete: <DB> dsbulk: [ <dsbulkArguments>... ] get: <DB> list: See 'astra help db <command>' for more information on a specific command. Display help for unitary command astra db list For unitary commands all options details are provided. astra help db list \ud83d\udda5\ufe0f Sample output NAME astra db list - Display the list of Databases in an organization SYNOPSIS astra db list [ {-conf | --config} <CONFIG_SECTION> ] [ --config-file <CONFIG_FILE> ] [ --log <LOG_FILE> ] [ --no-color ] [ {-o | --output} <FORMAT> ] [ --token <AUTH_TOKEN> ] [ {-v | --verbose} ] OPTIONS -conf <CONFIG_SECTION>, --config <CONFIG_SECTION> Section in configuration file (default = ~/.astrarc) --config-file <CONFIG_FILE> Configuration file (default = ~/.astrarc) --log <LOG_FILE> Logs will go in the file plus on console --no-color Remove all colors in output -o <FORMAT>, --output <FORMAT> Output format, valid values are: human,json,csv --token <AUTH_TOKEN> Key to use authenticate each call. -v, --verbose Verbose mode with log in console","title":"3. Get Help"},{"location":"pages/astra/astra-cli/#astra-db","text":"","title":"Astra DB"},{"location":"pages/astra/astra-cli/#1-list-databases","text":"\u2705 1a - list To get the list of non terminated database in your oganization use the command list in the group db . astra db list \ud83d\udda5\ufe0f Sample output +---------------------+--------------------------------------+---------------------+----------------+ | Name | id | Default Region | Status | +---------------------+--------------------------------------+---------------------+----------------+ | mtg | dde308f5-a8b0-474d-afd6-81e5689e3e25 | eu-central-1 | ACTIVE | | workshops | 3ed83de7-d97f-4fb6-bf9f-82e9f7eafa23 | eu-west-1 | ACTIVE | | sdk_tests | 06a9675a-ca62-4cd0-9b94-aefaf395922b | us-east-1 | ACTIVE | | test | 7677a789-bd57-455d-ab2c-a3bdfa35ba68 | eu-central-1 | ACTIVE | | demo | 071d7059-d55b-4cdb-90c6-41c26da1a029 | us-east-1 | ACTIVE | | ac201 | 48c7178c-58cb-4657-b3d2-8a9e3cc89461 | us-east-1 | ACTIVE | +---------------------+--------------------------------------+---------------------+----------------+ \u2705 1b - Get Help To get help on a command always prefix with astra help XXX astra help db list \ud83d\udda5\ufe0f Sample output NAME astra db list - Display the list of Databases in an organization SYNOPSIS astra db list [ {-conf | --config} <CONFIG_SECTION> ] [ --config-file <CONFIG_FILE> ] [ --log <LOG_FILE> ] [ --no-color ] [ {-o | --output} <FORMAT> ] [ --token <AUTH_TOKEN> ] [ {-v | --verbose} ] OPTIONS -conf <CONFIG_SECTION>, --config <CONFIG_SECTION> Section in configuration file (default = ~/.astrarc) --config-file <CONFIG_FILE> Configuration file (default = ~/.astrarc) --log <LOG_FILE> Logs will go in the file plus on console --no-color Remove all colors in output -o <FORMAT>, --output <FORMAT> Output format, valid values are: human,json,csv --token <AUTH_TOKEN> Key to use authenticate each call. -v, --verbose Verbose mode with log in console \u2705 1c - Change output astra db list -o csv \ud83d\udda5\ufe0f Sample output Name,id,Default Region,Status mtg,dde308f5-a8b0-474d-afd6-81e5689e3e25,eu-central-1,ACTIVE workshops,3ed83de7-d97f-4fb6-bf9f-82e9f7eafa23,eu-west-1,ACTIVE sdk_tests,06a9675a-ca62-4cd0-9b94-aefaf395922b,us-east-1,ACTIVE test,7677a789-bd57-455d-ab2c-a3bdfa35ba68,eu-central-1,ACTIVE demo,071d7059-d55b-4cdb-90c6-41c26da1a029,us-east-1,ACTIVE ac201,48c7178c-58cb-4657-b3d2-8a9e3cc89461,us-east-1,ACTIVE astra db list -o json \ud83d\udda5\ufe0f Sample output { \"code\" : 0 , \"message\" : \"astra db list -o json\" , \"data\" : [ { \"Status\" : \"ACTIVE\" , \"Default Region\" : \"eu-central-1\" , \"id\" : \"dde308f5-a8b0-474d-afd6-81e5689e3e25\" , \"Name\" : \"mtg\" }, { \"Status\" : \"ACTIVE\" , \"Default Region\" : \"eu-west-1\" , \"id\" : \"3ed83de7-d97f-4fb6-bf9f-82e9f7eafa23\" , \"Name\" : \"workshops\" }, { \"Status\" : \"ACTIVE\" , \"Default Region\" : \"us-east-1\" , \"id\" : \"06a9675a-ca62-4cd0-9b94-aefaf395922b\" , \"Name\" : \"sdk_tests\" }, { \"Status\" : \"ACTIVE\" , \"Default Region\" : \"eu-central-1\" , \"id\" : \"7677a789-bd57-455d-ab2c-a3bdfa35ba68\" , \"Name\" : \"test\" }, { \"Status\" : \"ACTIVE\" , \"Default Region\" : \"us-east-1\" , \"id\" : \"071d7059-d55b-4cdb-90c6-41c26da1a029\" , \"Name\" : \"demo\" }, { \"Status\" : \"ACTIVE\" , \"Default Region\" : \"us-east-1\" , \"id\" : \"48c7178c-58cb-4657-b3d2-8a9e3cc89461\" , \"Name\" : \"ac201\" } ] }","title":"1. List databases"},{"location":"pages/astra/astra-cli/#2-create-database","text":"\u2705 2a - Create Database If not provided the region will be the default free region and the keyspace will be the database name but you can change then with -r and -k respectivitely. astra db create demo \u2705 2b - Options --if-not-exist and --wait Database name does not ensure unicity (database id does) as such if you issue the command multiple times you will end up with multiple instances. To change this behaviour you can use --if-not-exist Database creation is asynchronous operation. Still during your CI/CD you want the Db to be ACTIVE before moving forward. The option --wait will trigger a blocking command until the db is ready On the free tier, after a period of inactivity the database moves to HIBERNATED state. The creation command, will resume the db when needed. astra db create demo -k ks2 --if-not-exist --wait \u2705 2c - Get help Better doc the cli itself. astra help db create","title":"2. Create database"},{"location":"pages/astra/astra-cli/#3-resume-database","text":"In the free tier, after 23H of inactivity your database got hibernated. To wake up the db you can use the resume command \u2705 2a - Resuming Assuming you have an hibernating database. astra db list +---------------------+--------------------------------------+---------------------+----------------+ | Name | id | Default Region | Status | +---------------------+--------------------------------------+---------------------+----------------+ | hemidactylus | 643c6bb8-2336-4649-97d5-39c33491f5c1 | eu-central-1 | HIBERNATED | +---------------------+--------------------------------------+---------------------+----------------+ Trigger an explicit resuming with: astra db resume hemidactylus \ud83d\udda5\ufe0f Sample output +---------------------+--------------------------------------+---------------------+----------------+ | Name | id | Default Region | Status | +---------------------+--------------------------------------+---------------------+----------------+ | hemidactylus | 643c6bb8-2336-4649-97d5-39c33491f5c1 | eu-central-1 | RESUMING | +---------------------+--------------------------------------+---------------------+----------------+ And after a few time +---------------------+--------------------------------------+---------------------+----------------+ | Name | id | Default Region | Status | +---------------------+--------------------------------------+---------------------+----------------+ | hemidactylus | 643c6bb8-2336-4649-97d5-39c33491f5c1 | eu-central-1 | ACTIVE | +---------------------+--------------------------------------+---------------------+----------------+","title":"3. Resume database"},{"location":"pages/astra/astra-cli/#4-get-database-details","text":"\u2705 4a. To get general information or details on an entity use the command get . astra db get demo In the output you specially see the list of keyspaces available and the different regions. \ud83d\udda5\ufe0f Sample output +------------------------+-----------------------------------------+ | Attribute | Value | +------------------------+-----------------------------------------+ | Name | demo | | id | 071d7059-d55b-4cdb-90c6-41c26da1a029 | | Status | ACTIVE | | Default Cloud Provider | AWS | | Default Region | us-east-1 | | Default Keyspace | demo | | Creation Time | 2022-07-26T15:41:18Z | | | | | Keyspaces | [0] demo | | | | | Regions | [0] us-east-1 | +------------------------+-----------------------------------------+ \u2705 4b. To get a special property you can add the option --key . Multiple keys are available: id , status , cloud , keyspace , keyspaces , region , regions . Notice that the output is raw. This command is expected to be used in scripts astra db get demo --key id \ud83d\udda5\ufe0f Sample output dde308f5-a8b0-474d-afd6-81e5689e3e25 \u2705 4c. To get database status in a human readble for use status command astra db status demo \ud83d\udda5\ufe0f Sample output [ INFO ] - Database 'demo' has status 'ACTIVE'","title":"4. Get database details"},{"location":"pages/astra/astra-cli/#5-delete-database","text":"\u2705 5a. To delete a db use the command delete . astra db delete demo2","title":"5. Delete Database"},{"location":"pages/astra/astra-cli/#6-working-with-keyspaces","text":"A keyspace is created when you create the database. Default CLI behaviour is to provide same values for keyspace and database names. But you can define your own keyspace name with the flag -k . \u2705 6a - Create new keyspace To add a keyspace ks2 to an existing database demo use the following. The option --if-not-exist is optional but could help you providing idempotent scripts. astra db create-keyspace demo -k ks2 --if-not-exist If the database is not found you will get a warning message and dedicated code returned. To see your new keyspace you can display your database details astra db list-keyspaces demo \u2705 6b - Get help astra help db create-keyspace","title":"6. Working with keyspaces"},{"location":"pages/astra/astra-cli/#7-cqlsh","text":"Cqlsh is a standalone shell to work with Apache Cassandra\u2122. It is compliant with Astra but requires a few extra steps of configuration. The purpose of the CLI is to integrate with cqlsh and do the integration for you. Astra Cli will download , install , setup and wrap cqlsh for you to interact with Astra. \u2705 7a - Interactive mode If no option are provided, you enter cqlsh interactive mode astra db cqlsh demo \ud83d\udda5\ufe0f Sample output Cqlsh is starting please wait for connection establishment... Connected to cndb at 127.0.0.1:9042. [cqlsh 6.8.0 | Cassandra 4.0.0.6816 | CQL spec 3.4.5 | Native protocol v4] Use HELP for help. token@cqlsh> \u2705 7b - Execute CQL To execute CQL Statement with cqlsh use the flag -e . astra db cqlsh demo -e \"describe keyspaces;\" \u2705 7b - Execute CQL Files To execute CQL Files with cqlsh use the flag -f . You could also use the CQL syntax SOURCE. astra db cqlsh demo -f sample.cql","title":"7. Cqlsh"},{"location":"pages/astra/astra-cli/#8-dsbulk","text":"\u2705 8a - Setup Dsbulk stands for Datastax bulk loader. It is a standalone program to load, unload and count data in an efficient way with Apache Cassandra\u2122. It is compliant with Datastax Astra. As for Cqlsh the cli will download , install , setup and wrap the dsbulk command for you. All options are available. To give you an idea let's tak a simple example. Make sure we have a db demo with a keyspace demo astra db create demo Looking at a dataset of cities in the world. cities.csv . We can show here the first lines of the file. id,name,state_id,state_code,state_name,country_id,country_code,country_name,latitude,longitude,wikiDataId 52,Ashk\u0101sham,3901,BDS,Badakhshan,1,AF,Afghanistan,36.68333000,71.53333000,Q4805192 68,Fayzabad,3901,BDS,Badakhshan,1,AF,Afghanistan,37.11664000,70.58002000,Q156558 ... Let's create a table to store those values. Connect to CQHSH astra db cqlsh demo -k demo Create the table CREATE TABLE cities_by_country ( country_name text , name text , id int , state_id text , state_code text , state_name text , country_id text , country_code text , latitude double , longitude double , wikiDataId text , PRIMARY KEY (( country_name ), name ) ); describe table cities_by_country ; quit \u2705 8b - Load Data astra db dsbulk demo load \\ -url https://raw.githubusercontent.com/awesome-astra/docs/main/docs/assets/cities.csv \\ -k demo \\ -t cities_by_country \\ --schema.allowMissingFields true The first time the line DSBulk is starting please wait can take a few seconds to appear. The reason is the cli is download dsbulk if not downloaded before. \ud83d\udda5\ufe0f Sample output DSBulk is starting please wait ... Username and password provided but auth provider not specified, inferring PlainTextAuthProvider A cloud secure connect bundle was provided: ignoring all explicit contact points. A cloud secure connect bundle was provided and selected operation performs writes: changing default consistency level to LOCAL_QUORUM. Operation directory: /Users/cedricklunven/Downloads/logs/LOAD_20220823-182343-074618 Setting executor.maxPerSecond not set when connecting to DataStax Astra: applying a limit of 9,000 ops/second based on the number of coordinators (3). If your Astra database has higher limits, please define executor.maxPerSecond explicitly. total | failed | rows/s | p50ms | p99ms | p999ms | batches 148,266 | 0 | 8,361 | 663.86 | 767.56 | 817.89 | 30.91 Operation LOAD_20220823-182343-074618 completed successfully in 17 seconds. Last processed positions can be found in positions.txt \u2705 8c - Count Check than the data has been imported with cqlsh SH astra db cqlsh demo -e \"select * from demo.cities_by_country LIMIT 20;\" \ud83d\udda5\ufe0f Sample output Cqlsh is starting please wait for connection establishment... country_name | name | country_code | country_id | id | latitude | longitude | state_code | state_id | state_name | wikidataid --------------+---------------------+--------------+------------+------+----------+-----------+------------+----------+---------------------+------------ Bangladesh | Azimpur | BD | 19 | 8454 | 23.7298 | 90.3854 | 13 | 771 | Dhaka District | null Bangladesh | Badarganj | BD | 19 | 8455 | 25.67419 | 89.05377 | 55 | 759 | Rangpur District | null Bangladesh | Bagerhat | BD | 19 | 8456 | 22.4 | 89.75 | 27 | 811 | Khulna District | null Bangladesh | Bandarban | BD | 19 | 8457 | 22 | 92.33333 | B | 803 | Chittagong Division | null Bangladesh | Baniachang | BD | 19 | 8458 | 24.51863 | 91.35787 | 60 | 767 | Sylhet District | null Bangladesh | Barguna | BD | 19 | 8459 | 22.13333 | 90.13333 | 06 | 818 | Barisal District | null Bangladesh | Barisal | BD | 19 | 8460 | 22.8 | 90.5 | 06 | 818 | Barisal District | null Bangladesh | Bera | BD | 19 | 8462 | 24.07821 | 89.63262 | 54 | 813 | Rajshahi District | null Bangladesh | Bhairab B\u0101z\u0101r | BD | 19 | 8463 | 24.0524 | 90.9764 | 13 | 771 | Dhaka District | null Bangladesh | Bher\u0101m\u0101ra | BD | 19 | 8464 | 24.02452 | 88.99234 | 27 | 811 | Khulna District | null Bangladesh | Bhola | BD | 19 | 8465 | 22.36667 | 90.81667 | 06 | 818 | Barisal District | null Bangladesh | Bh\u0101nd\u0101ria | BD | 19 | 8466 | 22.48898 | 90.06273 | 06 | 818 | Barisal District | null Bangladesh | Bh\u0101tp\u0101ra Abhaynagar | BD | 19 | 8467 | 23.01472 | 89.43936 | 27 | 811 | Khulna District | null Bangladesh | Bibir Hat | BD | 19 | 8468 | 22.68347 | 91.79058 | B | 803 | Chittagong Division | null Bangladesh | Bogra | BD | 19 | 8469 | 24.78333 | 89.35 | 54 | 813 | Rajshahi District | null Bangladesh | Brahmanbaria | BD | 19 | 8470 | 23.98333 | 91.16667 | B | 803 | Chittagong Division | null Bangladesh | Burh\u0101nuddin | BD | 19 | 8471 | 22.49518 | 90.72391 | 06 | 818 | Barisal District | null Bangladesh | B\u0101jitpur | BD | 19 | 8472 | 24.21623 | 90.95002 | 13 | 771 | Dhaka District | null Bangladesh | Chandpur | BD | 19 | 8474 | 23.25 | 90.83333 | B | 803 | Chittagong Division | null Bangladesh | Chapai Nababganj | BD | 19 | 8475 | 24.68333 | 88.25 | 54 | 813 | Rajshahi District | null Count with ds bulkd astra db dsbulk demo count -k demo -t cities_by_country \ud83d\udda5\ufe0f Sample output DSBulk is starting please wait ... [INFO ] - RUNNING: /Users/cedricklunven/.astra/dsbulk-1.9.1/bin/dsbulk count -k demo -t cities_by_country -u token -p AstraCS:gdZaqzmFZszaBTOlLgeecuPs:edd25600df1c01506f5388340f138f277cece2c93cb70f4b5fa386490daa5d44 -b /Users/cedricklunven/.astra/scb/scb_071d7059-d55b-4cdb-90c6-41c26da1a029_us-east-1.zip Username and password provided but auth provider not specified, inferring PlainTextAuthProvider A cloud secure connect bundle was provided: ignoring all explicit contact points. Operation directory: /Users/cedricklunven/Downloads/logs/COUNT_20220823-182833-197954 total | failed | rows/s | p50ms | p99ms | p999ms 134,574 | 0 | 43,307 | 315.71 | 457.18 | 457.18 \u2705 8d - Unload Data astra db dsbulk demo unload -k demo -t cities_by_country -url /tmp/unload \ud83d\udda5\ufe0f Sample output DSBulk is starting please wait ... Username and password provided but auth provider not specified, inferring PlainTextAuthProvider A cloud secure connect bundle was provided: ignoring all explicit contact points. Operation directory: /Users/cedricklunven/Downloads/logs/UNLOAD_20220823-183054-208353 total | failed | rows/s | p50ms | p99ms | p999ms 134,574 | 0 | 14,103 | 927.51 | 1,853.88 | 1,853.88 Operation UNLOAD_20220823-183054-208353 completed successfully in 9 seconds.","title":"8. DSBulk"},{"location":"pages/astra/astra-cli/#9-download-secure-bundle","text":"\u2705 9a - Default values Download the different secure bundles (one per region) with the pattern scb_${dbid}-${dbregion}.zip in the current folder. mkdir db-demo cd db-demo astra db download-scb demo ls \u2705 9b - Download in target folder Download the different secure bundles (one per region) with the pattern scb_${dbid}-${dbregion}.zip in the folder provide with option -d ( --output-director ). astra db download-scb demo -d /tmp \u2705 9c - Download in target folder Provide the target filename with -f ( --output-file ). It will work only if you have a SINGLE REGION for your database (or you will have to use the flag -d ) astra db download-scb demo -f /tmp/demo.zip","title":"9. Download Secure bundle"},{"location":"pages/astra/astra-cli/#astra-streaming","text":"","title":"Astra STREAMING"},{"location":"pages/astra/astra-cli/#1-list-tenants","text":"\u2705 1a - list To get the list of tenant in your oganization use the command list in the group streaming . astra streaming list \ud83d\udda5\ufe0f Sample output +---------------------+-----------+----------------+----------------+ | name | cloud | region | Status | +---------------------+-----------+----------------+----------------+ | cedrick-20220910 | aws | useast2 | active | | trollsquad-2022 | aws | useast2 | active | +---------------------+-----------+----------------+----------------+ \u2705 1b - Change output as csv amd json astra streaming list -o csv \ud83d\udda5\ufe0f Sample output name,cloud,region,Status cedrick-20220910,aws,useast2,active trollsquad-2022,aws,useast2,active astra streaming list -o json \ud83d\udda5\ufe0f Sample output { \"code\" : 0 , \"message\" : \"astra streaming list -o json\" , \"data\" : [ { \"cloud\" : \"aws\" , \"Status\" : \"active\" , \"name\" : \"cedrick-20220910\" , \"region\" : \"useast2\" }, { \"cloud\" : \"aws\" , \"Status\" : \"active\" , \"name\" : \"trollsquad-2022\" , \"region\" : \"useast2\" } ] }","title":"1. List tenants"},{"location":"pages/astra/astra-cli/#2-create-tenant","text":"\u2705 2a - Check tenant existence with exist The tenant name needs to be unique for the cluster (Cloud provider / region). It may be useful to check if the name is already in use by somebody else. astra streaming exist new_tenant_from_cli \ud83d\udda5\ufe0f Sample output [ INFO ] - Tenant 'new_tenant_from_cli' does not exist. \u2705 2b - Create tenant To create a tenant with default cloud ( aws ), default region ( useast2 ), plan ( free ) and namespace ( default ): astra streaming create new_tenant_from_cli To know all supported option please use astra help streaming create","title":"2. Create tenant"},{"location":"pages/astra/astra-cli/#3-get-tenant-details","text":"\u2705 3a - To get i nformation or details on an entity use the command get . astra streaming get trollsquad-2022 The pulsar token is not displayed in this view as too loong, there are dedicated command to display it. \ud83d\udda5\ufe0f Sample output +------------------+-------------------------------------------------------------+ | Attribute | Value | +------------------+-------------------------------------------------------------+ | Name | trollsquad-2022 | | Status | active | | Cloud Provider | aws | | Cloud region | useast2 | | Cluster Name | pulsar-aws-useast2 | | Pulsar Version | 2.10 | | Jvm Version | JDK11 | | Plan | payg | | WebServiceUrl | https://pulsar-aws-useast2.api.streaming.datastax.com | | BrokerServiceUrl | pulsar+ssl://pulsar-aws-useast2.streaming.datastax.com:6651 | | WebSocketUrl | wss://pulsar-aws-useast2.streaming.datastax.com:8001/ws/v2 | +------------------+-------------------------------------------------------------+ \u2705 3b. To get a special property you can add the option --key . Multiple keys are available: status , cloud , pulsar_token . Notice that the output is raw. This command is expected to be used in scripts astra streaming get trollsquad-2022 --key cloud \ud83d\udda5\ufe0f Sample output aws \u2705 3c. To get tenant pulsar-token please use pulsar-token command astra streaming pulsar-token trollsquad-2022 \ud83d\udda5\ufe0f Sample output eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpYXQiOjE2NjI5NzcyNzksImlzcyI6ImRhdGFzdGF4Iiwic3ViIjoiY2xpZW50O2Y5NDYwZjE0LTk4NzktNGViZS04M2YyLTQ4ZDNmM2RjZTEzYztkSEp2Ykd4emNYVmhaQzB5TURJeTsxOTZlYjg0YTMzIiwidG9rZW5pZCI6IjE5NmViODRhMzMifQ.rjJYDG_nJu0YpgATfjeKeUUAqwJGyVlvzpA5iP-d5-bReQf1FPaDlGxo40ADHHn2kx2NOdgMsm-Ys4K... \u2705 3d. To get tenant status in a human readble for use status command astra streaming status trollsquad-2022 \ud83d\udda5\ufe0f Sample output [ INFO ] - Tenant 'trollsquad-2022' has status 'active'","title":"3. Get tenant details"},{"location":"pages/astra/astra-cli/#4-delete-tenant","text":"\u2705 4a. To delete a tenant simply use the command delete astra streaming delete trollsquad","title":"4. Delete Tenant"},{"location":"pages/astra/astra-cli/#5-pulsar-shell","text":"Pulsar-Shell is a standalone shell to work with Apache Pulsar. It is compliant with Astra but requires a few extra steps of configuration. The purpose of the CLI is to integrate with pulsar-shell and do the integration for you. Astra Cli will download , install , setup and wrap pulsar-shell for you to interact with Astra. \u2705 5a - Interactive mode If no option are provided, you enter pulsar-shell interactive mode astra streaming pulsar-shell trollsquad-2022 \ud83d\udda5\ufe0f Sample output /Users/cedricklunven/.astra/lunastreaming-shell-2.10.1.1/conf/client-aws-useast2-trollsquad-2022.conf Pulsar-shell is starting please wait for connection establishment... Using directory: /Users/cedricklunven/.pulsar-shell Welcome to Pulsar shell! Service URL: pulsar+ssl://pulsar-aws-useast2.streaming.datastax.com:6651 Admin URL: https://pulsar-aws-useast2.api.streaming.datastax.com Type help to get started or try the autocompletion (TAB button). Type exit or quit to end the shell session. default(pulsar-aws-useast2.streaming.datastax.com)> You can quit with exit. \u2705 5b - Execute Pulsar Shell command To execute command with pushar-shell use the flag -e . astra streaming pulsar-shell trollsquad-2022 -e \"admin namespaces list trollsquad-2022\" \ud83d\udda5\ufe0f Sample output /Users/cedricklunven/.astra/lunastreaming-shell-2.10.1.1/conf/client-aws-useast2-trollsquad-2022.conf Pulsar-shell is starting please wait for connection establishment... Using directory: /Users/cedricklunven/.pulsar-shell [1/1] Executing admin namespaces list trollsquad-2022 [1/1] \u2714 admin namespaces list trollsquad-2022 \u2705 5c - Execute Pulsar Shell files To execute CQL Files with pushar-shell use the flag -e . astra streaming pulsar-shell trollsquad-2022 -f create_topics.txt","title":"5. Pulsar-Shell"},{"location":"pages/astra/astra-cli/#6-pulsar-client-and-admin","text":"Pulsar client and admin are provided within pulsar-shell. This section simply provide some examples to write and read in a topic with client. \u2705 6a - Create a topic demo . First start the pulsar-shell on 2 different terminal astra streaming pulsar-shell trollsquad-2022 Then on first terminal create a topic demo in namespace default admin topics create persistent://trollsquad-2022/default/demo You can now list the different topics in namespace default admin topics list trollsquad-2022/default \ud83d\udda5\ufe0f Sample output persistent://trollsquad-2022/default/demo Start a consumer on this topic client consume persistent://trollsquad-2022/default/demo -s astra_cli_tuto -n 0 \ud83d\udda5\ufe0f Sample output .. init ... 83 - R:pulsar-aws-useast2.streaming.datastax.com/3.16.119.226:6651]] Connected to server 2022-09-12T12:28:34,869+0200 [pulsar-client-io-1-1] INFO org.apache.pulsar.client.impl.ClientCnx - [id: 0xc5ce3ec4, L:/192.168.82.1:53683 - R:pulsar-aws-useast2.streaming.datastax.com/3.16.119.226:6651] Connected through proxy to target broker at 192.168.7.141:6650 2022-09-12T12:28:35,460+0200 [pulsar-client-io-1-1] INFO org.apache.pulsar.client.impl.ConsumerImpl - [persistent://trollsquad-2022/default/demo][astra_cli_tuto] Subscribing to topic on cnx [id: 0xc5ce3ec4, L:/192.168.82.1:53683 - R:pulsar-aws-useast2.streaming.datastax.com/3.16.119.226:6651], consumerId 0 2022-09-12T12:28:35,645+0200 [pulsar-client-io-1-1] INFO org.apache.pulsar.client.impl.ConsumerImpl - [persistent://trollsquad-2022/default/demo][astra_cli_tuto] Subscribed to topic on pulsar-aws-useast2.streaming.datastax.com/3.16.119.226:6651 -- consumer: 0 On the second terminal you can now start a producer client produce persistent://trollsquad-2022/default/demo -m \"hello,world\" -n 20 \ud83d\udda5\ufe0f Sample output 2022-09-12T12:36:28,684+0200 [pulsar-client-io-14-1] INFO org.apache.pulsar.client.impl.ClientCnx - [id: 0x682890b5, L:/192.168.1.106:53796 ! R:pulsar-aws-useast2.streaming.datastax.com/3.138.177.230:6651] Disconnected 2022-09-12T12:36:30,756+0200 [main] INFO org.apache.pulsar.client.cli.PulsarClientTool - 40 messages successfully produced And on the client side key:[null], properties:[], content:world ----- got message ----- key:[null], properties:[], content:hello","title":"6. Pulsar-client and Admin"},{"location":"pages/astra/astra-cli/#user-and-roles","text":"","title":"User and Roles"},{"location":"pages/astra/astra-cli/#1-list-users","text":"astra user list \ud83d\udda5\ufe0f Sample output +--------------------------------------+-----------------------------+---------------------+ | User Id | User Email | Status | +--------------------------------------+-----------------------------+---------------------+ | b665658a-ae6a-4f30-a740-2342a7fb469c | cedrick.lunven@datastax.com | active | +--------------------------------------+-----------------------------+---------------------+","title":"1. List users"},{"location":"pages/astra/astra-cli/#2-invite-user","text":"astra user invite cedrick.lunven@gmail.com Check the list of user and notice the new user invited. astra user list \ud83d\udda5\ufe0f Sample output +--------------------------------------+-----------------------------+---------------------+ | User Id | User Email | Status | +--------------------------------------+-----------------------------+---------------------+ | 825bd3d3-82ae-404b-9aad-bbb4c53da315 | cedrick.lunven@gmail.com | invited | | b665658a-ae6a-4f30-a740-2342a7fb469c | cedrick.lunven@datastax.com | active | +--------------------------------------+-----------------------------+---------------------+","title":"2. Invite User"},{"location":"pages/astra/astra-cli/#3-revoke-user","text":"astra user delete cedrick.lunven@gmail.com \ud83d\udda5\ufe0f Sample output +--------------------------------------+-----------------------------+---------------------+ | User Id | User Email | Status | +--------------------------------------+-----------------------------+---------------------+ | b665658a-ae6a-4f30-a740-2342a7fb469c | cedrick.lunven@datastax.com | active | +--------------------------------------+-----------------------------+---------------------+","title":"3. Revoke User"},{"location":"pages/astra/astra-cli/#4-list-roles","text":"astra role list","title":"4. List roles"},{"location":"pages/astra/astra-cli/#6-get-role-infos","text":"astra role get \"Database Administrator\"","title":"6. Get role infos"},{"location":"pages/astra/astra-cli/#advanced-topics","text":"","title":"Advanced Topics"},{"location":"pages/astra/astra-cli/#1-config-management","text":"If you work with multiple organizations it could be useful to switch from configuration to another, one token to another. The Cli provides a configuration management solution to handle this use case. \u2705 1a - List available configuration astra config list \u2705 1b - Create a new section astra config create dev --token <token_of_org_2> \u2705 1c - Use your section config anywhere You can use any organization anytime with --config <onfig_name> . astra user list --config dev \u2705 1d - Select a section as defaul Change the current org astra config use dev See your new list astra config list \u2705 1e - Delete a section You can delete any organization. If you delete the selected organization you will have to pick a new one. Delete you config astra config delete dev See the new list astra config list","title":"1 Config Management"},{"location":"pages/astra/cdc-for-astra/","text":"\ud83d\udcd6 Reference Documentation and resources \ud83d\udcd6 Astra Docs - Reference documentation \ud83c\udfa5 Youtube Video - Astra Streaming demo \ud83c\udfa5 Pulsar Documentation - Getting Started \ud83c\udfa5 Apache Pulsar Documentation CDC for Astra DB \u00b6 CDC for Astra DB automatically captures changes in real time, de-duplicates the changes, and streams the clean set of changed data into Astra Streaming where it can be processed by client applications or sent to downstream systems. Astra Streaming processes data changes via a Pulsar topic. By design, the Change Data Capture (CDC) component is simple, with a 1:1 correspondence between the table and a single Pulsar topic. This doc will show you how to create a CDC connector for your Astra DB deployment and send change data to an Elasticsearch sink. Creating a tenant and topic \u00b6 In astra.datastax.com , select Create Streaming . Enter the name for your new streaming tenant and select a provider. Select Create Tenant . Use the default persistent and non-partitioned topic settings. Note Astra Streaming CDC can only be used in a region that supports both Astra Streaming and AstraDB. See Regions for more information. Creating a table \u00b6 In your Astra Database , create a table with a primary key column: CREATE TABLE IF NOT EXISTS <keyspacename>.tbl1 ( key text PRIMARY KEY, c1 text ) ; Confirm you created your table: select * from <mykeyspace>.tbl1 ; Results: Connecting to CDC for Astra DB \u00b6 Select the CDC tab in your database dashboard. Select Enable CDC . Complete the fields to connect CDC. Select Enable CDC . Once created, your CDC connector will appear: Enabling CDC creates a new astracdc namespace with two new topics, data- and log- . The log- topic consumes schema changes, processes them, and then writes clean data to the data- topic. The log- topic is for CDC functionality and should not be used. The data- topic can be used to consume CDC data in Astra Streaming. Connecting Elasticsearch sink \u00b6 After creating your CDC connector, connect an Elasticsearch sink to it. DataStax recommends using the default Astra Streaming settings. Select Add Elastic Search Sink from the database CDC console to enforce the default settings. Use your Elasticsearch deployment to complete the fields. To find your Elasticsearch URL , navigate to your deployment within the Elastic Common Schema (ECS). Copy the Elasticsearch endpoint to the Elastic Search URL field. Complete the remaining fields. Most values will auto-populate. These values are recommended: ignoreKey as false nullValueAction as DELETE enabled as true When the fields are completed, select Create . If creation is successful, <sink-name> created successfully appears at the top of the screen. You can confirm your new sink was created in the Sinks tab. Sending messages \u00b6 Let's process some changes with CDC. Go to the CQL console. Modify the table you created. INSERT INTO <keyspacename>.tbl1 (key,c1) VALUES ('32a','bob3123'); INSERT INTO <keyspacename>.tbl1 (key,c1) VALUES ('32b','bob3123b'); Confirm the changes you've made: select * from <keyspacename>.tbl1; Results: Confirming ECS is receiving data \u00b6 To confirm ECS is receiving your CDC changes, use a curl request to your ECS deployment. Get your index name from your ECS sink tab: Issue your curl request with your Elastic username , password , and index name : curl -u <username>:<password> \\ -XGET \"https://asdev.es.westus2.azure.elastic-cloud.com:9243/<index_name>.tbl1/_search?pretty=true\" \\ -H 'Content-Type: application/json' Note If you have a trial account, the username is elastic . You will receive a JSON response with your changes to the index, which confirms Astra Streaming is sending your CDC changes to your ECS sink. { \"_index\" : \"index.tbl1\", \"_type\" : \"_doc\", \"_id\" : \"32a\", \"_score\" : 1.0, \"_source\" : { \"c1\" : \"bob3123\" } } { \"_index\" : \"index.tbl1\", \"_type\" : \"_doc\", \"_id\" : \"32b\", \"_score\" : 1.0, \"_source\" : { \"c1\" : \"bob3123b\" } }","title":"\u2023 CDC for Astra Streaming"},{"location":"pages/astra/cdc-for-astra/#cdc-for-astra-db","text":"CDC for Astra DB automatically captures changes in real time, de-duplicates the changes, and streams the clean set of changed data into Astra Streaming where it can be processed by client applications or sent to downstream systems. Astra Streaming processes data changes via a Pulsar topic. By design, the Change Data Capture (CDC) component is simple, with a 1:1 correspondence between the table and a single Pulsar topic. This doc will show you how to create a CDC connector for your Astra DB deployment and send change data to an Elasticsearch sink.","title":"CDC for Astra DB"},{"location":"pages/astra/cdc-for-astra/#creating-a-tenant-and-topic","text":"In astra.datastax.com , select Create Streaming . Enter the name for your new streaming tenant and select a provider. Select Create Tenant . Use the default persistent and non-partitioned topic settings. Note Astra Streaming CDC can only be used in a region that supports both Astra Streaming and AstraDB. See Regions for more information.","title":"Creating a tenant and topic"},{"location":"pages/astra/cdc-for-astra/#creating-a-table","text":"In your Astra Database , create a table with a primary key column: CREATE TABLE IF NOT EXISTS <keyspacename>.tbl1 ( key text PRIMARY KEY, c1 text ) ; Confirm you created your table: select * from <mykeyspace>.tbl1 ; Results:","title":"Creating a table"},{"location":"pages/astra/cdc-for-astra/#connecting-to-cdc-for-astra-db","text":"Select the CDC tab in your database dashboard. Select Enable CDC . Complete the fields to connect CDC. Select Enable CDC . Once created, your CDC connector will appear: Enabling CDC creates a new astracdc namespace with two new topics, data- and log- . The log- topic consumes schema changes, processes them, and then writes clean data to the data- topic. The log- topic is for CDC functionality and should not be used. The data- topic can be used to consume CDC data in Astra Streaming.","title":"Connecting to CDC for Astra DB"},{"location":"pages/astra/cdc-for-astra/#connecting-elasticsearch-sink","text":"After creating your CDC connector, connect an Elasticsearch sink to it. DataStax recommends using the default Astra Streaming settings. Select Add Elastic Search Sink from the database CDC console to enforce the default settings. Use your Elasticsearch deployment to complete the fields. To find your Elasticsearch URL , navigate to your deployment within the Elastic Common Schema (ECS). Copy the Elasticsearch endpoint to the Elastic Search URL field. Complete the remaining fields. Most values will auto-populate. These values are recommended: ignoreKey as false nullValueAction as DELETE enabled as true When the fields are completed, select Create . If creation is successful, <sink-name> created successfully appears at the top of the screen. You can confirm your new sink was created in the Sinks tab.","title":"Connecting Elasticsearch sink"},{"location":"pages/astra/cdc-for-astra/#sending-messages","text":"Let's process some changes with CDC. Go to the CQL console. Modify the table you created. INSERT INTO <keyspacename>.tbl1 (key,c1) VALUES ('32a','bob3123'); INSERT INTO <keyspacename>.tbl1 (key,c1) VALUES ('32b','bob3123b'); Confirm the changes you've made: select * from <keyspacename>.tbl1; Results:","title":"Sending messages"},{"location":"pages/astra/cdc-for-astra/#confirming-ecs-is-receiving-data","text":"To confirm ECS is receiving your CDC changes, use a curl request to your ECS deployment. Get your index name from your ECS sink tab: Issue your curl request with your Elastic username , password , and index name : curl -u <username>:<password> \\ -XGET \"https://asdev.es.westus2.azure.elastic-cloud.com:9243/<index_name>.tbl1/_search?pretty=true\" \\ -H 'Content-Type: application/json' Note If you have a trial account, the username is elastic . You will receive a JSON response with your changes to the index, which confirms Astra Streaming is sending your CDC changes to your ECS sink. { \"_index\" : \"index.tbl1\", \"_type\" : \"_doc\", \"_id\" : \"32a\", \"_score\" : 1.0, \"_source\" : { \"c1\" : \"bob3123\" } } { \"_index\" : \"index.tbl1\", \"_type\" : \"_doc\", \"_id\" : \"32b\", \"_score\" : 1.0, \"_source\" : { \"c1\" : \"bob3123b\" } }","title":"Confirming ECS is receiving data"},{"location":"pages/astra/cqlproxy/","text":"This page is a copy This page is a copy of CQL PROXY Reference Documentation . if you encounter some discrepancies please open a JIRA in our repository. What is cql-proxy ? \u00b6 cql-proxy is designed to forward your application's CQL traffic to an appropriate database service. It listens on a local address and securely forwards that traffic. Note : cql-proxy was made as a genrally available release on 2/16/2022 . See this blog for additional details. Please give it a try and let us know what you think! When to use cql-proxy \u00b6 The cql-proxy sidecar enables unsupported CQL drivers to work with DataStax Astra . These drivers include both legacy DataStax drivers and community-maintained CQL drivers, such as the gocql driver and the rust-driver . cql-proxy also enables applications that are currently using Apache Cassandra or DataStax Enterprise (DSE) to use Astra without requiring any code changes. Your application just needs to be configured to use the proxy. If you're building a new application using DataStax drivers , cql-proxy is not required, as the drivers can communicate directly with Astra. DataStax drivers have excellent support for Astra out-of-the-box, and are well-documented in the driver-guide guide. Configuration \u00b6 Use the -h or --help flag to display a listing all flags and their corresponding descriptions and environment variables (shown below as items starting with $ ): $ ./cql-proxy -h Usage: cql-proxy Flags: -h, --help Show context-sensitive help. -b, --astra-bundle = STRING Path to secure connect bundle for an Astra database. Requires '--username' and '--password' . Ignored if using the token or contact points option ( $ASTRA_BUNDLE ) . -t, --astra-token = STRING Token used to authenticate to an Astra database. Requires '--astra-database-id' . Ignored if using the bundle path or contact points option ( $ASTRA_TOKEN ) . -i, --astra-database-id = STRING Database ID of the Astra database. Requires '--astra-token' ( $ASTRA_DATABASE_ID ) --astra-api-url = \"https://api.astra.datastax.com\" URL for the Astra API ( $ASTRA_API_URL ) -c, --contact-points = CONTACT-POINTS,... Contact points for cluster. Ignored if using the bundle path or token option ( $CONTACT_POINTS ) . -u, --username = STRING Username to use for authentication ( $USERNAME ) -p, --password = STRING Password to use for authentication ( $PASSWORD ) -r, --port = 9042 Default port to use when connecting to cluster ( $PORT ) -n, --protocol-version = \"v4\" Initial protocol version to use when connecting to the backend cluster ( default: v4, options: v3, v4, v5, DSEv1, DSEv2 ) ( $PROTOCOL_VERSION ) -m, --max-protocol-version = \"v4\" Max protocol version supported by the backend cluster ( default: v4, options: v3, v4, v5, DSEv1, DSEv2 ) ( $MAX_PROTOCOL_VERSION ) -a, --bind = \":9042\" Address to use to bind server ( $BIND ) -f, --config = CONFIG YAML configuration file ( $CONFIG_FILE ) --debug Show debug logging ( $DEBUG ) --health-check Enable liveness and readiness checks ( $HEALTH_CHECK ) --http-bind = \":8000\" Address to use to bind HTTP server used for health checks ( $HTTP_BIND ) --heartbeat-interval = 30s Interval between performing heartbeats to the cluster ( $HEARTBEAT_INTERVAL ) --idle-timeout = 60s Duration between successful heartbeats before a connection to the cluster is considered unresponsive and closed ( $IDLE_TIMEOUT ) --readiness-timeout = 30s Duration the proxy is unable to connect to the backend cluster before it is considered not ready ( $READINESS_TIMEOUT ) --num-conns = 1 Number of connection to create to each node of the backend cluster ( $NUM_CONNS ) --rpc-address = STRING Address to advertise in the 'system.local' table for 'rpc_address' . It must be set if configuring peer proxies ( $RPC_ADDRESS ) --data-center = STRING Data center to use in system tables ( $DATA_CENTER ) --tokens = TOKENS,... Tokens to use in the system tables. It ' s not recommended ( $TOKENS ) To pass configuration to cql-proxy , either command-line flags, environment variables, or a configuration file can be used. Using the docker method as an example, the following samples show how the token and database ID are defined with each method. Using flags \u00b6 docker run -p 9042 :9042 \\ --rm datastax/cql-proxy:v0.1.2 \\ --astra-token <astra-token> --astra-database-id <astra-datbase-id> Using environment variables \u00b6 docker run -p 9042 :9042 \\ --rm datastax/cql-proxy:v0.1.2 \\ -e ASTRA_TOKEN = <astra-token> -e ASTRA_DATABASE_ID = <astra-datbase-id> Using a configuration file \u00b6 Proxy settings can also be passed using a configuration file with the --config /path/to/proxy.yaml flag. This can be mixed and matched with command-line flags and environment variables. Here are some example configuration files: contact-points : - 127.0.0.1 username : cassandra password : cassandra port : 9042 bind : 127.0.0.1:9042 # ... or with a Astra token: astra-token : <astra-token> astra-database-id : <astra-database-id> bind : 127.0.0.1:9042 # ... All configuration keys match their command-line flag counterpart, e.g. --astra-bundle is astra-bundle: , --contact-points is contact-points: etc. Setting up peer proxies \u00b6 Multi-region failover with DC-aware load balancing policy is the most useful case for a multiple proxy setup. When configuring peers: it is required to set --rpc-address (or rpc-address: in the yaml) for each proxy and it must match is corresponding peers: entry. Also, peers: is only available in the configuration file and cannot be set using a command-line flag. Multi-region setup \u00b6 Here's an example of configuring multi-region failover with two proxies. A proxy is started for each region of the cluster connecting to it using that region's bundle. They all share a common configuration file that contains the full list of proxies. Note: Only bundles are supported for multi-region setups. cql-proxy --astra-bundle astra-region1-bundle.zip --username token --passowrd <astra-token> \\ --bind 127 .0.0.1:9042 --rpc-address 127 .0.0.1 --data-center dc-1 --config proxy.yaml cql-proxy ---astra-bundle astra-region2-bundle.zip --username token --passowrd <astra-token> \\ --bind 127 .0.0.2:9042 --rpc-address 127 .0.0.2 --data-center dc-2 --config proxy.yaml The peers settings are configured using a yaml file. It's a good idea to explicitly provide the --data-center flag, otherwise; these values are pulled from the backend cluster and would need to be pulled from the system.local and system.peers table to properly setup the peers data-center: values. Here's an example proxy.yaml : peers : - rpc-address : 127.0.0.1 data-center : dc-1 - rpc-address : 127.0.0.2 data-center : dc-2 Note: It's okay for the peers: to contain entries for the current proxy itself because they'll just be omitted. Getting started \u00b6 There are three methods for using cql-proxy : Locally build and run cql-proxy Run a docker image that has cql-proxy installed Install locally on a Mac with Homebrew Use a Kubernetes container to run cql-proxy Locally build and run \u00b6 Build cql-proxy . go build Run with your desired database. DataStax Astra cluster: ./cql-proxy --astra-token <astra-token> --astra-database-id <astra-database-id> The <astra-token> can be generated using these instructions . The proxy also supports using the Astra Secure Connect Bundle along with a client ID and secret generated using these instructions : ./cql-proxy --astra-bundle <your-secure-connect-zip> \\ --username <astra-client-id> --password <astra-client-secret> Apache Cassandra cluster: ./cql-proxy --contact-points <cluster node IPs or DNS names> [ --username <username> ] [ --password <password> ] Run a cql-proxy docker image \u00b6 Run with your desired database. DataStax Astra cluster: docker run -p 9042 :9042 \\ datastax/cql-proxy:v0.1.2 \\ --astra-token <astra-token> --astra-database-id <astra-database-id> The <astra-token> can be generated using these instructions . The proxy also supports using the Astra Secure Connect Bundle , but it requires mounting the bundle to a volume in the container: docker run -v <your-secure-connect-bundle.zip>:/tmp/scb.zip -p 9042 :9042 \\ --rm datastax/cql-proxy:v0.1.2 \\ --astra-bundle /tmp/scb.zip --username <astra-client-id> --password <astra-client-secret> Apache Cassandra cluster: docker run -p 9042 :9042 \\ datastax/cql-proxy:v0.1.2 \\ --contact-points <cluster node IPs or DNS names> [ --username <username> ] [ --password <password> ] If you wish to have the docker image removed after you are done with it, add --rm before the image name datastax/cql-proxy:v0.1.2 . Homebrew on a Mac \u00b6 Install with one simple command: brew install cql-proxy Use Kubernetes \u00b6 Using Kubernetes with cql-proxy requires a number of steps: Generate a token following the Astra instructions . This step will display your Client ID, Client Secret, and Token; make sure you download the information for the next steps. Store the secure bundle in /tmp/scb.zip to match the example below. Create cql-proxy.yaml . You'll need to add three sets of information: arguments, volume mounts, and volumes. Argument: Modify the local bundle location, username and password, using the client ID and client secret obtained in the last step to the container argument. command: [\"./cql-proxy\"] args: [\"--astra-bundle=/tmp/scb.zip\",\"--username=Client ID\",\"--password=Client Secret\"] Volume mounts: Modify /tmp/ as a volume mount as required. volumeMounts: - name: my-cm-vol mountPath: /tmp/ Volume: Modify the configMap filename as required. In this example, it is named cql-proxy-configmap . Use the same name for the volumes that you used for the volumeMounts . volumes: - name: my-cm-vol configMap: name: cql-proxy-configmap Create a configmap. Use the same secure bundle that was specified in the cql-proxy.yaml . kubectl create configmap cql-proxy-configmap --from-file /tmp/scb.zip Check the configmap that was created. kubectl describe configmap config Name: config Namespace: default Labels: <none> Annotations: <none> Data ==== BinaryData ==== scb.zip: 12311 bytes Create a Kubernetes deployment with the YAML file you created: kubectl create -f cql-proxy.yaml Check the logs: kubectl logs <deployment-name> Known issues \u00b6 Token-aware load balancing \u00b6 Drivers that use token-aware load balancing may print a warning or may not work when using cql-proxy. Because cql-proxy abstracts the backend cluster as a single endpoint this doesn't always work well with token-aware drivers that expect there to be at least \"replication factor\" number of nodes in the cluster. Many drivers print a warning (which can be ignored) and fallback to something like round-robin, but other drivers might fail with an error. For the drivers that fail with an error it is required that they disable token-aware or configure the round-robin load balancing policy.","title":"\u2023 CQL Proxy"},{"location":"pages/astra/cqlproxy/#what-is-cql-proxy","text":"cql-proxy is designed to forward your application's CQL traffic to an appropriate database service. It listens on a local address and securely forwards that traffic. Note : cql-proxy was made as a genrally available release on 2/16/2022 . See this blog for additional details. Please give it a try and let us know what you think!","title":"What is cql-proxy?"},{"location":"pages/astra/cqlproxy/#when-to-use-cql-proxy","text":"The cql-proxy sidecar enables unsupported CQL drivers to work with DataStax Astra . These drivers include both legacy DataStax drivers and community-maintained CQL drivers, such as the gocql driver and the rust-driver . cql-proxy also enables applications that are currently using Apache Cassandra or DataStax Enterprise (DSE) to use Astra without requiring any code changes. Your application just needs to be configured to use the proxy. If you're building a new application using DataStax drivers , cql-proxy is not required, as the drivers can communicate directly with Astra. DataStax drivers have excellent support for Astra out-of-the-box, and are well-documented in the driver-guide guide.","title":"When to use cql-proxy"},{"location":"pages/astra/cqlproxy/#configuration","text":"Use the -h or --help flag to display a listing all flags and their corresponding descriptions and environment variables (shown below as items starting with $ ): $ ./cql-proxy -h Usage: cql-proxy Flags: -h, --help Show context-sensitive help. -b, --astra-bundle = STRING Path to secure connect bundle for an Astra database. Requires '--username' and '--password' . Ignored if using the token or contact points option ( $ASTRA_BUNDLE ) . -t, --astra-token = STRING Token used to authenticate to an Astra database. Requires '--astra-database-id' . Ignored if using the bundle path or contact points option ( $ASTRA_TOKEN ) . -i, --astra-database-id = STRING Database ID of the Astra database. Requires '--astra-token' ( $ASTRA_DATABASE_ID ) --astra-api-url = \"https://api.astra.datastax.com\" URL for the Astra API ( $ASTRA_API_URL ) -c, --contact-points = CONTACT-POINTS,... Contact points for cluster. Ignored if using the bundle path or token option ( $CONTACT_POINTS ) . -u, --username = STRING Username to use for authentication ( $USERNAME ) -p, --password = STRING Password to use for authentication ( $PASSWORD ) -r, --port = 9042 Default port to use when connecting to cluster ( $PORT ) -n, --protocol-version = \"v4\" Initial protocol version to use when connecting to the backend cluster ( default: v4, options: v3, v4, v5, DSEv1, DSEv2 ) ( $PROTOCOL_VERSION ) -m, --max-protocol-version = \"v4\" Max protocol version supported by the backend cluster ( default: v4, options: v3, v4, v5, DSEv1, DSEv2 ) ( $MAX_PROTOCOL_VERSION ) -a, --bind = \":9042\" Address to use to bind server ( $BIND ) -f, --config = CONFIG YAML configuration file ( $CONFIG_FILE ) --debug Show debug logging ( $DEBUG ) --health-check Enable liveness and readiness checks ( $HEALTH_CHECK ) --http-bind = \":8000\" Address to use to bind HTTP server used for health checks ( $HTTP_BIND ) --heartbeat-interval = 30s Interval between performing heartbeats to the cluster ( $HEARTBEAT_INTERVAL ) --idle-timeout = 60s Duration between successful heartbeats before a connection to the cluster is considered unresponsive and closed ( $IDLE_TIMEOUT ) --readiness-timeout = 30s Duration the proxy is unable to connect to the backend cluster before it is considered not ready ( $READINESS_TIMEOUT ) --num-conns = 1 Number of connection to create to each node of the backend cluster ( $NUM_CONNS ) --rpc-address = STRING Address to advertise in the 'system.local' table for 'rpc_address' . It must be set if configuring peer proxies ( $RPC_ADDRESS ) --data-center = STRING Data center to use in system tables ( $DATA_CENTER ) --tokens = TOKENS,... Tokens to use in the system tables. It ' s not recommended ( $TOKENS ) To pass configuration to cql-proxy , either command-line flags, environment variables, or a configuration file can be used. Using the docker method as an example, the following samples show how the token and database ID are defined with each method.","title":"Configuration"},{"location":"pages/astra/cqlproxy/#using-flags","text":"docker run -p 9042 :9042 \\ --rm datastax/cql-proxy:v0.1.2 \\ --astra-token <astra-token> --astra-database-id <astra-datbase-id>","title":"Using flags"},{"location":"pages/astra/cqlproxy/#using-environment-variables","text":"docker run -p 9042 :9042 \\ --rm datastax/cql-proxy:v0.1.2 \\ -e ASTRA_TOKEN = <astra-token> -e ASTRA_DATABASE_ID = <astra-datbase-id>","title":"Using environment variables"},{"location":"pages/astra/cqlproxy/#using-a-configuration-file","text":"Proxy settings can also be passed using a configuration file with the --config /path/to/proxy.yaml flag. This can be mixed and matched with command-line flags and environment variables. Here are some example configuration files: contact-points : - 127.0.0.1 username : cassandra password : cassandra port : 9042 bind : 127.0.0.1:9042 # ... or with a Astra token: astra-token : <astra-token> astra-database-id : <astra-database-id> bind : 127.0.0.1:9042 # ... All configuration keys match their command-line flag counterpart, e.g. --astra-bundle is astra-bundle: , --contact-points is contact-points: etc.","title":"Using a configuration file"},{"location":"pages/astra/cqlproxy/#setting-up-peer-proxies","text":"Multi-region failover with DC-aware load balancing policy is the most useful case for a multiple proxy setup. When configuring peers: it is required to set --rpc-address (or rpc-address: in the yaml) for each proxy and it must match is corresponding peers: entry. Also, peers: is only available in the configuration file and cannot be set using a command-line flag.","title":"Setting up peer proxies"},{"location":"pages/astra/cqlproxy/#multi-region-setup","text":"Here's an example of configuring multi-region failover with two proxies. A proxy is started for each region of the cluster connecting to it using that region's bundle. They all share a common configuration file that contains the full list of proxies. Note: Only bundles are supported for multi-region setups. cql-proxy --astra-bundle astra-region1-bundle.zip --username token --passowrd <astra-token> \\ --bind 127 .0.0.1:9042 --rpc-address 127 .0.0.1 --data-center dc-1 --config proxy.yaml cql-proxy ---astra-bundle astra-region2-bundle.zip --username token --passowrd <astra-token> \\ --bind 127 .0.0.2:9042 --rpc-address 127 .0.0.2 --data-center dc-2 --config proxy.yaml The peers settings are configured using a yaml file. It's a good idea to explicitly provide the --data-center flag, otherwise; these values are pulled from the backend cluster and would need to be pulled from the system.local and system.peers table to properly setup the peers data-center: values. Here's an example proxy.yaml : peers : - rpc-address : 127.0.0.1 data-center : dc-1 - rpc-address : 127.0.0.2 data-center : dc-2 Note: It's okay for the peers: to contain entries for the current proxy itself because they'll just be omitted.","title":"Multi-region setup"},{"location":"pages/astra/cqlproxy/#getting-started","text":"There are three methods for using cql-proxy : Locally build and run cql-proxy Run a docker image that has cql-proxy installed Install locally on a Mac with Homebrew Use a Kubernetes container to run cql-proxy","title":"Getting started"},{"location":"pages/astra/cqlproxy/#locally-build-and-run","text":"Build cql-proxy . go build Run with your desired database. DataStax Astra cluster: ./cql-proxy --astra-token <astra-token> --astra-database-id <astra-database-id> The <astra-token> can be generated using these instructions . The proxy also supports using the Astra Secure Connect Bundle along with a client ID and secret generated using these instructions : ./cql-proxy --astra-bundle <your-secure-connect-zip> \\ --username <astra-client-id> --password <astra-client-secret> Apache Cassandra cluster: ./cql-proxy --contact-points <cluster node IPs or DNS names> [ --username <username> ] [ --password <password> ]","title":"Locally build and run"},{"location":"pages/astra/cqlproxy/#run-a-cql-proxy-docker-image","text":"Run with your desired database. DataStax Astra cluster: docker run -p 9042 :9042 \\ datastax/cql-proxy:v0.1.2 \\ --astra-token <astra-token> --astra-database-id <astra-database-id> The <astra-token> can be generated using these instructions . The proxy also supports using the Astra Secure Connect Bundle , but it requires mounting the bundle to a volume in the container: docker run -v <your-secure-connect-bundle.zip>:/tmp/scb.zip -p 9042 :9042 \\ --rm datastax/cql-proxy:v0.1.2 \\ --astra-bundle /tmp/scb.zip --username <astra-client-id> --password <astra-client-secret> Apache Cassandra cluster: docker run -p 9042 :9042 \\ datastax/cql-proxy:v0.1.2 \\ --contact-points <cluster node IPs or DNS names> [ --username <username> ] [ --password <password> ] If you wish to have the docker image removed after you are done with it, add --rm before the image name datastax/cql-proxy:v0.1.2 .","title":"Run a cql-proxy docker image"},{"location":"pages/astra/cqlproxy/#homebrew-on-a-mac","text":"Install with one simple command: brew install cql-proxy","title":"Homebrew on a Mac"},{"location":"pages/astra/cqlproxy/#use-kubernetes","text":"Using Kubernetes with cql-proxy requires a number of steps: Generate a token following the Astra instructions . This step will display your Client ID, Client Secret, and Token; make sure you download the information for the next steps. Store the secure bundle in /tmp/scb.zip to match the example below. Create cql-proxy.yaml . You'll need to add three sets of information: arguments, volume mounts, and volumes. Argument: Modify the local bundle location, username and password, using the client ID and client secret obtained in the last step to the container argument. command: [\"./cql-proxy\"] args: [\"--astra-bundle=/tmp/scb.zip\",\"--username=Client ID\",\"--password=Client Secret\"] Volume mounts: Modify /tmp/ as a volume mount as required. volumeMounts: - name: my-cm-vol mountPath: /tmp/ Volume: Modify the configMap filename as required. In this example, it is named cql-proxy-configmap . Use the same name for the volumes that you used for the volumeMounts . volumes: - name: my-cm-vol configMap: name: cql-proxy-configmap Create a configmap. Use the same secure bundle that was specified in the cql-proxy.yaml . kubectl create configmap cql-proxy-configmap --from-file /tmp/scb.zip Check the configmap that was created. kubectl describe configmap config Name: config Namespace: default Labels: <none> Annotations: <none> Data ==== BinaryData ==== scb.zip: 12311 bytes Create a Kubernetes deployment with the YAML file you created: kubectl create -f cql-proxy.yaml Check the logs: kubectl logs <deployment-name>","title":"Use Kubernetes"},{"location":"pages/astra/cqlproxy/#known-issues","text":"","title":"Known issues"},{"location":"pages/astra/cqlproxy/#token-aware-load-balancing","text":"Drivers that use token-aware load balancing may print a warning or may not work when using cql-proxy. Because cql-proxy abstracts the backend cluster as a single endpoint this doesn't always work well with token-aware drivers that expect there to be at least \"replication factor\" number of nodes in the cluster. Many drivers print a warning (which can be ignored) and fallback to something like round-robin, but other drivers might fail with an error. For the drivers that fail with an error it is required that they disable token-aware or configure the round-robin load balancing policy.","title":"Token-aware load balancing"},{"location":"pages/astra/create-account/","text":"A - Overview \u00b6 ASTRA DB is the simplest way to run Cassandra with zero operations. No credit card required and $25.00 USD credit every month ( roughly 20M reads/writes, 80GB storage monthly ) which is sufficient to run small production workloads. https://astra.datastax.com is the URL create an account and get started with the solution. B - Sign Up \u00b6 You can use your Github , Google accounts or register with an email . 1. Sign In with Github \u00b6 Click the [Sign In with Github] button 1\ufe0f\u20e3 Click Continue on the OAuth claims delegation The OAuth2 delegation screen from github is asking for permissions. 2\ufe0f\u20e3 You are redirected to the homepage 2. Sign In with Google \u00b6 1\ufe0f\u20e3 Click the [Sign In with Google] button 2\ufe0f\u20e3 You are redirected to the homepage 3. Sign Up \u00b6 1\ufe0f\u20e3 Click the Sign up on the bottom of the page 2\ufe0f\u20e3 Provide your information and validate the captcha 3\ufe0f\u20e3 Accept terms and policies Astra is now looking for you to validate your email adress 4\ufe0f\u20e3 Open the mail in your inbox and validate with the Verify my email link Astra will show a validation message, select Click Here to proceed . Select back to application 5\ufe0f\u20e3 You are redirected to the homepage C - Account and Organization \u00b6 1. Overview \u00b6 When you create an account your personal Organization is created, this is your tenant : The name of the organization is your email address, (1) in the picture below The unique identifier (GUID) is present in the URL on the dashboard. (2) in the picture below 2. Organization Objects \u00b6 Databases , Tenants and Security Tokens objects are created within the organization, as shown on the Organization Dashboard. graph TD User(User) -->|1..n| ORG(Organization) ORG(Organization) -->|0..n| DB(Databases) ORG(Organization) -->|0..n| ST(Streaming Tenants) ORG(Organization) -->|0..n| ROLE(Roles) ORG(Organization) -->|0..n| TOK(Security Tokens) TOK(Security Tokens) -->|1..1| ROLE DB(Databases) -->|1..n| KEY(Keyspaces) KEY(Keyspaces) -->|0..n| TABLE(Tables) ST(Streaming Tenants) -->|0..n| TOPIC(Topics) 3. Multiple Organizations \u00b6 You can create multiple organizations through the Manage Organizations menu option and invite other users to join as well. It is useful when the same database could be accessed by multiple users with different emails. As a consequence a user can be part of multiple organizations; the personal organization created during registration, new user-defined organizations, and shared organizations. graph TD USER(User) -->|1..n| PORG(Personal Organization - registration) USER -->|0..n| CORG(Organizations he created) USER -->|0...n| IORG(Organizations he was invited to)","title":"\u2023 Create Account"},{"location":"pages/astra/create-account/#a-overview","text":"ASTRA DB is the simplest way to run Cassandra with zero operations. No credit card required and $25.00 USD credit every month ( roughly 20M reads/writes, 80GB storage monthly ) which is sufficient to run small production workloads. https://astra.datastax.com is the URL create an account and get started with the solution.","title":"A - Overview"},{"location":"pages/astra/create-account/#b-sign-up","text":"You can use your Github , Google accounts or register with an email .","title":"B - Sign Up"},{"location":"pages/astra/create-account/#1-sign-in-with-github","text":"Click the [Sign In with Github] button 1\ufe0f\u20e3 Click Continue on the OAuth claims delegation The OAuth2 delegation screen from github is asking for permissions. 2\ufe0f\u20e3 You are redirected to the homepage","title":"1. Sign In with Github"},{"location":"pages/astra/create-account/#2-sign-in-with-google","text":"1\ufe0f\u20e3 Click the [Sign In with Google] button 2\ufe0f\u20e3 You are redirected to the homepage","title":"2. Sign In with Google"},{"location":"pages/astra/create-account/#3-sign-up","text":"1\ufe0f\u20e3 Click the Sign up on the bottom of the page 2\ufe0f\u20e3 Provide your information and validate the captcha 3\ufe0f\u20e3 Accept terms and policies Astra is now looking for you to validate your email adress 4\ufe0f\u20e3 Open the mail in your inbox and validate with the Verify my email link Astra will show a validation message, select Click Here to proceed . Select back to application 5\ufe0f\u20e3 You are redirected to the homepage","title":"3. Sign Up"},{"location":"pages/astra/create-account/#c-account-and-organization","text":"","title":"C - Account and Organization"},{"location":"pages/astra/create-account/#1-overview","text":"When you create an account your personal Organization is created, this is your tenant : The name of the organization is your email address, (1) in the picture below The unique identifier (GUID) is present in the URL on the dashboard. (2) in the picture below","title":"1. Overview"},{"location":"pages/astra/create-account/#2-organization-objects","text":"Databases , Tenants and Security Tokens objects are created within the organization, as shown on the Organization Dashboard. graph TD User(User) -->|1..n| ORG(Organization) ORG(Organization) -->|0..n| DB(Databases) ORG(Organization) -->|0..n| ST(Streaming Tenants) ORG(Organization) -->|0..n| ROLE(Roles) ORG(Organization) -->|0..n| TOK(Security Tokens) TOK(Security Tokens) -->|1..1| ROLE DB(Databases) -->|1..n| KEY(Keyspaces) KEY(Keyspaces) -->|0..n| TABLE(Tables) ST(Streaming Tenants) -->|0..n| TOPIC(Topics)","title":"2. Organization Objects"},{"location":"pages/astra/create-account/#3-multiple-organizations","text":"You can create multiple organizations through the Manage Organizations menu option and invite other users to join as well. It is useful when the same database could be accessed by multiple users with different emails. As a consequence a user can be part of multiple organizations; the personal organization created during registration, new user-defined organizations, and shared organizations. graph TD USER(User) -->|1..n| PORG(Personal Organization - registration) USER -->|0..n| CORG(Organizations he created) USER -->|0...n| IORG(Organizations he was invited to)","title":"3. Multiple Organizations"},{"location":"pages/astra/create-instance/","text":"\ud83d\udcd6 Reference Documentation and resources \ud83d\udcd6 Astra Docs - The Astra database creation procedure \ud83c\udfa5 Youtube Video - Walk through instance creation A - Overview \u00b6 ASTRA DB is the simplest way to run Cassandra with zero operations - just push the button and get your cluster. No credit card required and $25.00 USD credit every month ( roughly 20M reads/writes, 80GB storage monthly ) which is sufficient to run small production workloads. B - Prerequisites \u00b6 You should have an Astra account . If you don't have one yet, keep reading and we'll show you how to create it. C - Procedure \u00b6 \u2705 Step 1:Click the sign-in button to login or register. You can use your Github , Google accounts or register with an email . Make sure to chose a password with a minimum of 8 characters, containing upper and lowercase letters, and at least one number and special character. If you already have an Astra account, you can skip this step. Locate and click the \"Create Database\" button on the left-side navigation bar of your Astra UI, and read the next step. \u2705 Step 2: Complete the creation form As you create a new account, you will be prompted to create a database; you will see the same form if you simply hit the \"Create database\" button in your existing Astra account. \u2139\ufe0f Fields Description Field Description database name It does not need to be unique, is not used to initialize a connection, and is only a label (Between 2 and 50 characters). It is recommended to have a database for each of your applications. The free tier is limited to 5 databases. keyspace It is a logical grouping of your tables (Between 2 and 48 characters). Please use lower case and snake_case . Cloud Provider Use the one you like. Click a cloud provider logo, pick an Area in the list and finally pick a region. We recommend choosing a region that is closest to you to reduce latency. In free tier, there is very little difference. \u2139\ufe0f Create Database button becomes enabled only when all fields are filled in properly. Please use only lower case and no spaces for a keyspace name. \u2139\ufe0f You will see your new database pending in the Dashboard. The status will change to Active when the database is ready, which will only take 2-3 minutes. You will also receive an email when it is ready. \u2705 Step 3: Download your token The database will take about a minute to inialize. Progression can be seen on the right panel ( Pending - Deployed to region... , in the screenshot below ). Download your token file it will be useful later to use the DB. \u2705 Step 4: You are all set When the status is changed to Your Database is now active you are good to go Click [Go To Database] .","title":"\u2023 Create Database"},{"location":"pages/astra/create-instance/#a-overview","text":"ASTRA DB is the simplest way to run Cassandra with zero operations - just push the button and get your cluster. No credit card required and $25.00 USD credit every month ( roughly 20M reads/writes, 80GB storage monthly ) which is sufficient to run small production workloads.","title":"A - Overview"},{"location":"pages/astra/create-instance/#b-prerequisites","text":"You should have an Astra account . If you don't have one yet, keep reading and we'll show you how to create it.","title":"B - Prerequisites"},{"location":"pages/astra/create-instance/#c-procedure","text":"\u2705 Step 1:Click the sign-in button to login or register. You can use your Github , Google accounts or register with an email . Make sure to chose a password with a minimum of 8 characters, containing upper and lowercase letters, and at least one number and special character. If you already have an Astra account, you can skip this step. Locate and click the \"Create Database\" button on the left-side navigation bar of your Astra UI, and read the next step. \u2705 Step 2: Complete the creation form As you create a new account, you will be prompted to create a database; you will see the same form if you simply hit the \"Create database\" button in your existing Astra account. \u2139\ufe0f Fields Description Field Description database name It does not need to be unique, is not used to initialize a connection, and is only a label (Between 2 and 50 characters). It is recommended to have a database for each of your applications. The free tier is limited to 5 databases. keyspace It is a logical grouping of your tables (Between 2 and 48 characters). Please use lower case and snake_case . Cloud Provider Use the one you like. Click a cloud provider logo, pick an Area in the list and finally pick a region. We recommend choosing a region that is closest to you to reduce latency. In free tier, there is very little difference. \u2139\ufe0f Create Database button becomes enabled only when all fields are filled in properly. Please use only lower case and no spaces for a keyspace name. \u2139\ufe0f You will see your new database pending in the Dashboard. The status will change to Active when the database is ready, which will only take 2-3 minutes. You will also receive an email when it is ready. \u2705 Step 3: Download your token The database will take about a minute to inialize. Progression can be seen on the right panel ( Pending - Deployed to region... , in the screenshot below ). Download your token file it will be useful later to use the DB. \u2705 Step 4: You are all set When the status is changed to Your Database is now active you are good to go Click [Go To Database] .","title":"C - Procedure"},{"location":"pages/astra/create-token/","text":"Reference Documentation and resources Astra Docs - The Astra token creation procedure Youtube Video - Walk through token creation Youtube Video - More about token and roles in Astra A - Overview \u00b6 As stated in the Create Account page the security token is associated to one and only one organization and only one role. graph TD USER(Users) -->|n...m|ORG(Organizations) ORG -->|0..n|DB(Dabatases) ORG -->|0..n|TOKEN(Tokens) ORG -->|0..n|STR(Streaming Tenants) TOKEN-->|1:1|ROLE(role) ROLE-->|1..n|PERMISSIONS(permissions) There are a set of predefined roles within an organization which are associated with some default permissions. The full list of permissions and roles is available in Astra Documentation Figure 1: Default Roles Figure 2: Permissions for a role here `Database Administrator` It is also possible to create custom roles and associate fined grained permissions. ( Organizations Settings / Role Managennt ) Figure 3: Custom Roles screen B - Prerequisites \u00b6 To create a new token: You should have an Astra account C - Procedure \u00b6 1\ufe0f\u20e3 Open the Organization settings panel On the top left hand corner locate the panel Current Organization with your email address. Use the chevron down \u2304 to open the menu and pick Organization Settings 2\ufe0f\u20e3 Open the token management page On the new page, select Token Management in the menu. Then use the Select Roles combo to select the Organization Administrator role. This is the administrator of your tenant with all permissions. The same page can be accessed from the dashboard You can reach the Token Management page directly from the ellipsis menu next to your database in the main Astra dashboard. Expand to see how 3\ufe0f\u20e3 Save the token as a CSV. The values of clientSecret and token will not be shown to you later for security reasons. Do not share those values and never commit them on github. 4\ufe0f\u20e3 Walkthrough. Copy values in the clipboard You can use the clipboard icons next to each parameter to copy and paste them elsewhere.","title":"\u2023 Create Token"},{"location":"pages/astra/create-token/#a-overview","text":"As stated in the Create Account page the security token is associated to one and only one organization and only one role. graph TD USER(Users) -->|n...m|ORG(Organizations) ORG -->|0..n|DB(Dabatases) ORG -->|0..n|TOKEN(Tokens) ORG -->|0..n|STR(Streaming Tenants) TOKEN-->|1:1|ROLE(role) ROLE-->|1..n|PERMISSIONS(permissions) There are a set of predefined roles within an organization which are associated with some default permissions. The full list of permissions and roles is available in Astra Documentation Figure 1: Default Roles Figure 2: Permissions for a role here `Database Administrator` It is also possible to create custom roles and associate fined grained permissions. ( Organizations Settings / Role Managennt ) Figure 3: Custom Roles screen","title":"A - Overview"},{"location":"pages/astra/create-token/#b-prerequisites","text":"To create a new token: You should have an Astra account","title":"B - Prerequisites"},{"location":"pages/astra/create-token/#c-procedure","text":"1\ufe0f\u20e3 Open the Organization settings panel On the top left hand corner locate the panel Current Organization with your email address. Use the chevron down \u2304 to open the menu and pick Organization Settings 2\ufe0f\u20e3 Open the token management page On the new page, select Token Management in the menu. Then use the Select Roles combo to select the Organization Administrator role. This is the administrator of your tenant with all permissions. The same page can be accessed from the dashboard You can reach the Token Management page directly from the ellipsis menu next to your database in the main Astra dashboard. Expand to see how 3\ufe0f\u20e3 Save the token as a CSV. The values of clientSecret and token will not be shown to you later for security reasons. Do not share those values and never commit them on github. 4\ufe0f\u20e3 Walkthrough. Copy values in the clipboard You can use the clipboard icons next to each parameter to copy and paste them elsewhere.","title":"C - Procedure"},{"location":"pages/astra/create-topic/","text":"\ud83d\udcd6 Reference Documentation and resources \ud83d\udcd6 Astra Docs - Reference documentation \ud83c\udfa5 Youtube Video - Astra Streaming demo \ud83c\udfa5 Pulsar Documentation - Getting Started Apache Pulsar documentation A - Overview \u00b6 ASTRA STREAMING is the simplest way to use the Apache Pulsar messaging/streaming service with zero operations - just push the button and get your messages flowing. No credit card required, $25.00 USD credit every month, and all of thethe strength and features of Apache Pulsar managed for you in the cloud. This page explains how to create a new tenant in Astra Streaming, a new namespace in the tenant (if desired) and a new topic in the namespace. Also instructions are given to retrieve the connection parameters to later connect to the topic and start messaging from your application. B - Prerequisites \u00b6 You should have an Astra account . Have a tenant_name , optionally a namespace (if not using \"default\"), and a topic_name ready to create the topic. C - Procedure \u00b6 Make sure you are logged in to your Astra account before proceeding. \u2705 Step 1: Create a tenant Go to your Astra console, click the \"Create Stream\" button next to the Streaming section. Set up a new Tenant (remember Pulsar has a multi-tenant architecture): you have to find a globally unique name for it . Pick the provider and region ( try to have it close to you for reduced latency ) and finally hit \"Create Tenant\". You'll shortly see the dashboard for your newly-created Tenant. \u2705 Step 2: Create a namespace A default namespace is created for you with the tenant and you can use it as is. However, you may want to create a separate namespace to host your topic(s). Go to the \"Namespaces\" tab of your Tenant dashboard and click on the \"Create namespace\" button on the right. Choose a name and hit \"Create\". You should see it listed among the available namespaces in a moment. \u2705 Step 3: Create a topic Switch to the \"Topics\" tab and click the \"Add Topic\" button next to the namespace that you want to use. Choose a topic name, review and/or modify the topic settings (such as persistent=yes, partitioned=no ), and click \"Save\". It takes no more than a couple of minutes to create your new topic. It will then be ready to receive and dispatch messages. \ud83d\udc41\ufe0f Walkthrough for topic creation \u2705 Step 4: retrieve the Broker URL All that is left is to make sure you have the connection parameters needed to reach the topic programmatically. If you click the \"Connect\" tab you will see a list of \"Tenant Details\", along with links to look at code examples in various languages. There are several ways to connect to the topic. If you plan to use the Pulsar drivers from your application, the important bits are the \"Broker Service URL\" and the \"Streaming Token\" secret. The \"Broker Service URL\" is shown right in the \"Connect\" tab and looks like pulsar+ssl://pulsar-[...].streaming.datastax.com:6651 . You can click on the clipboard icon to copy it. \u2705 Step 5: Manage secrets and retrieve the Streaming Token You will also need a Token, a long secret string providing authentication info when the driver will connect to the topic. The token must be treated as a secret, which means do not post it publicly and do not check it in to version control. Note : Streaming Tokens are completely separate from Astra DB Tokens. Navigate to the \"Token Manager\" by clicking on the link in the \"Tenant Details\" list: there you will be able to create, copy and revoke streaming tokens for your tenant. Note that a default token has already been created for you, so you don't necessarily need to create a new token. Click on the clipboard icon to copy it. The token is a long random-looking string, such as eyJhbGci [...] cpNpX_qN68Q (about 500 chars long). \ud83d\udc41\ufe0f Screenshot for the connection parameters","title":"\u2023 Create Topic"},{"location":"pages/astra/create-topic/#a-overview","text":"ASTRA STREAMING is the simplest way to use the Apache Pulsar messaging/streaming service with zero operations - just push the button and get your messages flowing. No credit card required, $25.00 USD credit every month, and all of thethe strength and features of Apache Pulsar managed for you in the cloud. This page explains how to create a new tenant in Astra Streaming, a new namespace in the tenant (if desired) and a new topic in the namespace. Also instructions are given to retrieve the connection parameters to later connect to the topic and start messaging from your application.","title":"A - Overview"},{"location":"pages/astra/create-topic/#b-prerequisites","text":"You should have an Astra account . Have a tenant_name , optionally a namespace (if not using \"default\"), and a topic_name ready to create the topic.","title":"B - Prerequisites"},{"location":"pages/astra/create-topic/#c-procedure","text":"Make sure you are logged in to your Astra account before proceeding. \u2705 Step 1: Create a tenant Go to your Astra console, click the \"Create Stream\" button next to the Streaming section. Set up a new Tenant (remember Pulsar has a multi-tenant architecture): you have to find a globally unique name for it . Pick the provider and region ( try to have it close to you for reduced latency ) and finally hit \"Create Tenant\". You'll shortly see the dashboard for your newly-created Tenant. \u2705 Step 2: Create a namespace A default namespace is created for you with the tenant and you can use it as is. However, you may want to create a separate namespace to host your topic(s). Go to the \"Namespaces\" tab of your Tenant dashboard and click on the \"Create namespace\" button on the right. Choose a name and hit \"Create\". You should see it listed among the available namespaces in a moment. \u2705 Step 3: Create a topic Switch to the \"Topics\" tab and click the \"Add Topic\" button next to the namespace that you want to use. Choose a topic name, review and/or modify the topic settings (such as persistent=yes, partitioned=no ), and click \"Save\". It takes no more than a couple of minutes to create your new topic. It will then be ready to receive and dispatch messages. \ud83d\udc41\ufe0f Walkthrough for topic creation \u2705 Step 4: retrieve the Broker URL All that is left is to make sure you have the connection parameters needed to reach the topic programmatically. If you click the \"Connect\" tab you will see a list of \"Tenant Details\", along with links to look at code examples in various languages. There are several ways to connect to the topic. If you plan to use the Pulsar drivers from your application, the important bits are the \"Broker Service URL\" and the \"Streaming Token\" secret. The \"Broker Service URL\" is shown right in the \"Connect\" tab and looks like pulsar+ssl://pulsar-[...].streaming.datastax.com:6651 . You can click on the clipboard icon to copy it. \u2705 Step 5: Manage secrets and retrieve the Streaming Token You will also need a Token, a long secret string providing authentication info when the driver will connect to the topic. The token must be treated as a secret, which means do not post it publicly and do not check it in to version control. Note : Streaming Tokens are completely separate from Astra DB Tokens. Navigate to the \"Token Manager\" by clicking on the link in the \"Tenant Details\" list: there you will be able to create, copy and revoke streaming tokens for your tenant. Note that a default token has already been created for you, so you don't necessarily need to create a new token. Click on the clipboard icon to copy it. The token is a long random-looking string, such as eyJhbGci [...] cpNpX_qN68Q (about 500 chars long). \ud83d\udc41\ufe0f Screenshot for the connection parameters","title":"C - Procedure"},{"location":"pages/astra/download-scb/","text":"\ud83d\udcd6 Reference Documentation and resources \ud83d\udcd6 Astra Docs - Download Cloud Secure Bundle \ud83c\udfa5 Youtube Video - Walk through secure A - Overview \u00b6 To initialize a secured 2-way TLS connection between clients and Astra x509 certificates are needed. The strong authentication is key for maximum security and still benefits from robust driver features (health-check, load-balancing, fail-over). Under the hood the protocol SNI over TCP is used to contact each node independently. The configuration and required certificates are provided to the user through a zip file called the secure connect bundle which can be downloaded for each DATABASE REGION . This means that a database deployed across multiple regions will have one secure connect bundle per region. (1 region = 1 underlying Apache Cassandra\u2122 datacenter) B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database C - Procedure \u00b6 \u2705 Step 1 : Select your database Once signed in Astra the list of available databases is located in the menu on the left. Select the database that you want to work with. Click on the CONNECT tab or the big blue button [CONNECT] in the upper right hand corner. \u2705 Step 2 : Download the ZIP On this screen look for Connect using a driver . You can pick any of the folowing options; Node.js ( javascript ), Python or Java . It will be the same file download. Click on Download Bundle and select the region that you want to use. Click on target region to copy the link locally. Remarks \u00b6 Most browsers will give you the option to open the zip file directly. Do not do that, save it locally instead : the bundle zipfile will be passed to the drivers as is! The generated link to the bundle zipfile will expire a few minutes after it is generated. If you wait too long on the \"Connect\" page, you might end up with a faulty bundle. As a check, make sure the zipfile you downloaded is around 12-13 KB in size.","title":"\u2023 Secure Connect Bundle"},{"location":"pages/astra/download-scb/#a-overview","text":"To initialize a secured 2-way TLS connection between clients and Astra x509 certificates are needed. The strong authentication is key for maximum security and still benefits from robust driver features (health-check, load-balancing, fail-over). Under the hood the protocol SNI over TCP is used to contact each node independently. The configuration and required certificates are provided to the user through a zip file called the secure connect bundle which can be downloaded for each DATABASE REGION . This means that a database deployed across multiple regions will have one secure connect bundle per region. (1 region = 1 underlying Apache Cassandra\u2122 datacenter)","title":"A - Overview"},{"location":"pages/astra/download-scb/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database","title":"B - Prerequisites"},{"location":"pages/astra/download-scb/#c-procedure","text":"\u2705 Step 1 : Select your database Once signed in Astra the list of available databases is located in the menu on the left. Select the database that you want to work with. Click on the CONNECT tab or the big blue button [CONNECT] in the upper right hand corner. \u2705 Step 2 : Download the ZIP On this screen look for Connect using a driver . You can pick any of the folowing options; Node.js ( javascript ), Python or Java . It will be the same file download. Click on Download Bundle and select the region that you want to use. Click on target region to copy the link locally.","title":"C - Procedure"},{"location":"pages/astra/download-scb/#remarks","text":"Most browsers will give you the option to open the zip file directly. Do not do that, save it locally instead : the bundle zipfile will be passed to the drivers as is! The generated link to the bundle zipfile will expire a few minutes after it is generated. If you wait too long on the \"Connect\" page, you might end up with a faulty bundle. As a check, make sure the zipfile you downloaded is around 12-13 KB in size.","title":"Remarks"},{"location":"pages/astra/faq/","text":"Questions List \u00b6 Where should I find a database identifier ? Where should I find a database region name ? How to create a keyspace or a namespace ? Where should I find a database identifier ? \u00b6 The database id is a unique identifier ( GUID ) for your database. You can find it on the main dashboard of AstraDB. You can copy it to the clipboard by clicking the small icon \ud83d\udccb \u2139\ufe0f Note that, unlike the database identifier, the database name is not unique in an organization. ( project in the screenshot above) Where should I find a database region name ? \u00b6 A database can have one or multiple regions. Each region will have a datacenter Id and a region name. The region name is the one used in the Api Urls. Access your database dashboard by clicking its name either in the menu on the left or on the main panel. Locate the region name in the Regions table. In the screenshot below we do have two; europe-west1 and us-east1 . How do I create a namespace or a keyspace ? \u00b6 Namespaces and keyspaces are synonyms. There are two ways to create them. You can create them when you create a database You can create them on the Database Dashboard in the Keyspaces section by clicking the button Add Keyspace . Access your database dashboard by clicking its name either in the menu on the left or on the main panel. Locate the Add Keyspace button in the botton right-hand corner Create Keyspace from its name. The database will switch to MAINTENANCE mode for a few seconds but do not worry the application can still request the other keyspaces.","title":"\u2023 FAQ"},{"location":"pages/astra/faq/#questions-list","text":"Where should I find a database identifier ? Where should I find a database region name ? How to create a keyspace or a namespace ?","title":"Questions List"},{"location":"pages/astra/faq/#where-should-i-find-a-database-identifier","text":"The database id is a unique identifier ( GUID ) for your database. You can find it on the main dashboard of AstraDB. You can copy it to the clipboard by clicking the small icon \ud83d\udccb \u2139\ufe0f Note that, unlike the database identifier, the database name is not unique in an organization. ( project in the screenshot above)","title":"Where should I find a database identifier ?"},{"location":"pages/astra/faq/#where-should-i-find-a-database-region-name","text":"A database can have one or multiple regions. Each region will have a datacenter Id and a region name. The region name is the one used in the Api Urls. Access your database dashboard by clicking its name either in the menu on the left or on the main panel. Locate the region name in the Regions table. In the screenshot below we do have two; europe-west1 and us-east1 .","title":"Where should I find a database region name ?"},{"location":"pages/astra/faq/#how-do-i-create-a-namespace-or-a-keyspace","text":"Namespaces and keyspaces are synonyms. There are two ways to create them. You can create them when you create a database You can create them on the Database Dashboard in the Keyspaces section by clicking the button Add Keyspace . Access your database dashboard by clicking its name either in the menu on the left or on the main panel. Locate the Add Keyspace button in the botton right-hand corner Create Keyspace from its name. The database will switch to MAINTENANCE mode for a few seconds but do not worry the application can still request the other keyspaces.","title":"How do I create a namespace or a keyspace ?"},{"location":"pages/astra/multi-regions/","text":"\ud83d\udcd6 Reference Documentation and resources \ud83d\udcd6 Astra Docs - Reference documentation \ud83c\udfa5 Youtube Video - Walk through instance creation A - Overview \u00b6 AstraDB allows you to replicate data across multiple regions to maintain data availability for multi-region application architectures. Configuring multiple regions can also satisfy data locality requirements and save money. \ud83d\udd04 Eventual Consistency \u00b6 Apache Cassandra\u00ae and DataStax Astra DB follow the eventual consistency model. As a result, data written to one datacenter/region may not be immediately accessible in other datacenters/regions in the same database cluster. It normally only takes a few minutes to fully replicate the data. However, it could take longer, and possibly span one or more days. There are several contributing factors to the latter scenario; such as the workload volume, the number of regions, the process that runs data repair operations, and network resources. \u2696\ufe0f Data sovereignty \u00b6 Astra DB serverless replicates all data in the database to all of a database\u2019s regions. By contrast, multiple keyspaces in Apache Cassandra\u00ae and DataStax Enterprise (DSE) allow a database to replicate some tables to a subset of regions. To achieve the same behavior as Cassandra or DSE, create a separate Astra DB instance that adheres to the necessary region restrictions. The database client will need to add a separate connection for the additional database and send queries to the appropriate connection depending on the table being queried. \u26a0\ufe0f Limitations \u00b6 Lightweight transactions only work for a single-region datacenter. If your original region is disconnected, schema changes are suspended and repairs do not run. If any regions are disconnected, the writes to those regions will not be forwarded. While adding a new region, you cannot drop a table or keyspace and you cannot truncate a table. If any region is not online, you cannot truncate a table. B - Prerequisites \u00b6 You should have an Astra account You should have a CREDIT CARD in the system AND/OR have MORE THAN 25$ in Astra Credits. C - Create a new Region \u00b6 \u2705 Step 1: Click the Add Region Button Select the database to show the Dashboard and select Add Region. \u2705 Step 2: Select your region Select your desired region from the dropdown menu. Your selected region and the costs appear below the dropdown menu. You can add only a single region at a time. \u2705 Step 3: Validate your region Select Add Region to add the region to your database. The database with switch to Maintenance status. Do not worry, the existing regions will remain active and available for operations. There is no downtime. After you add the new region, your new region will show up in the list of regions on your database Dashboard. After the initialization, you will get: D - Delete a new Region \u00b6 \u2705 Step 1: Select Region to delete From your database Dashboard, select the overflow menu for the database that you want to delete and select Delete. You will notice that you CANNOT delete the original main region. \u2705 Step 2: Validate your action Removing a region is not reversible so proceed with caution. A pop-up will ask you to validate this operation by entering the delete word. The database will switch to Maintenance mode. After a few seconds, you will see the status of the deleted regions change from Active to Offline . Finally the region will not be visible in the Regions list.","title":"\u2023 Multi Regions"},{"location":"pages/astra/multi-regions/#a-overview","text":"AstraDB allows you to replicate data across multiple regions to maintain data availability for multi-region application architectures. Configuring multiple regions can also satisfy data locality requirements and save money.","title":"A - Overview"},{"location":"pages/astra/multi-regions/#eventual-consistency","text":"Apache Cassandra\u00ae and DataStax Astra DB follow the eventual consistency model. As a result, data written to one datacenter/region may not be immediately accessible in other datacenters/regions in the same database cluster. It normally only takes a few minutes to fully replicate the data. However, it could take longer, and possibly span one or more days. There are several contributing factors to the latter scenario; such as the workload volume, the number of regions, the process that runs data repair operations, and network resources.","title":"\ud83d\udd04 Eventual Consistency"},{"location":"pages/astra/multi-regions/#data-sovereignty","text":"Astra DB serverless replicates all data in the database to all of a database\u2019s regions. By contrast, multiple keyspaces in Apache Cassandra\u00ae and DataStax Enterprise (DSE) allow a database to replicate some tables to a subset of regions. To achieve the same behavior as Cassandra or DSE, create a separate Astra DB instance that adheres to the necessary region restrictions. The database client will need to add a separate connection for the additional database and send queries to the appropriate connection depending on the table being queried.","title":"\u2696\ufe0f Data sovereignty"},{"location":"pages/astra/multi-regions/#limitations","text":"Lightweight transactions only work for a single-region datacenter. If your original region is disconnected, schema changes are suspended and repairs do not run. If any regions are disconnected, the writes to those regions will not be forwarded. While adding a new region, you cannot drop a table or keyspace and you cannot truncate a table. If any region is not online, you cannot truncate a table.","title":"\u26a0\ufe0f Limitations"},{"location":"pages/astra/multi-regions/#b-prerequisites","text":"You should have an Astra account You should have a CREDIT CARD in the system AND/OR have MORE THAN 25$ in Astra Credits.","title":"B - Prerequisites"},{"location":"pages/astra/multi-regions/#c-create-a-new-region","text":"\u2705 Step 1: Click the Add Region Button Select the database to show the Dashboard and select Add Region. \u2705 Step 2: Select your region Select your desired region from the dropdown menu. Your selected region and the costs appear below the dropdown menu. You can add only a single region at a time. \u2705 Step 3: Validate your region Select Add Region to add the region to your database. The database with switch to Maintenance status. Do not worry, the existing regions will remain active and available for operations. There is no downtime. After you add the new region, your new region will show up in the list of regions on your database Dashboard. After the initialization, you will get:","title":"C - Create a new Region"},{"location":"pages/astra/multi-regions/#d-delete-a-new-region","text":"\u2705 Step 1: Select Region to delete From your database Dashboard, select the overflow menu for the database that you want to delete and select Delete. You will notice that you CANNOT delete the original main region. \u2705 Step 2: Validate your action Removing a region is not reversible so proceed with caution. A pop-up will ask you to validate this operation by entering the delete word. The database will switch to Maintenance mode. After a few seconds, you will see the status of the deleted regions change from Active to Offline . Finally the region will not be visible in the Regions list.","title":"D - Delete a new Region"},{"location":"pages/astra/resume-db/","text":"A - Overview \u00b6 In the free tier (serverless) , after 23 hours , your database will be hibernated and the status will change to Hibernated . From there it needs to be resumed, there are multiple ways to do it. B - Prerequisites \u00b6 You should have an Astra account C - Procedure \u00b6 \u2705 Option 1: Resume with button in the User interface Access the database by clicking its name in the menu on the left. Once the database is selected, on any tab you will get the Resume Database button available at top. \u2705 Option 2: Resume with a first request to the database Invoking and Stargate endpoints associated with your database will also trigger resuming. You would have to replace the dbId , dbRegion and token below with values for your environment. curl --location \\ --request GET 'https://{dbId}-{dbRegion}.apps.astra.datastax.com/api/rest/v2/schemas/keyspaces/' \\ --header 'X-Cassandra-Token: {token}' You will get a 503 error with the following payload. { \"message\" : \"Resuming your database, please try again shortly.\" } In the user interface the status changes to resuming... After a few seconds the database will be active.","title":"\u2023 Resume a database"},{"location":"pages/astra/resume-db/#a-overview","text":"In the free tier (serverless) , after 23 hours , your database will be hibernated and the status will change to Hibernated . From there it needs to be resumed, there are multiple ways to do it.","title":"A - Overview"},{"location":"pages/astra/resume-db/#b-prerequisites","text":"You should have an Astra account","title":"B - Prerequisites"},{"location":"pages/astra/resume-db/#c-procedure","text":"\u2705 Option 1: Resume with button in the User interface Access the database by clicking its name in the menu on the left. Once the database is selected, on any tab you will get the Resume Database button available at top. \u2705 Option 2: Resume with a first request to the database Invoking and Stargate endpoints associated with your database will also trigger resuming. You would have to replace the dbId , dbRegion and token below with values for your environment. curl --location \\ --request GET 'https://{dbId}-{dbRegion}.apps.astra.datastax.com/api/rest/v2/schemas/keyspaces/' \\ --header 'X-Cassandra-Token: {token}' You will get a 503 error with the following payload. { \"message\" : \"Resuming your database, please try again shortly.\" } In the user interface the status changes to resuming... After a few seconds the database will be active.","title":"C - Procedure"},{"location":"pages/contact/","text":"","title":"\u2022 Contact Us"},{"location":"pages/data/","text":"\ud83d\udce5 Load and Export \u00b6 In this section are listed third party tools that will help you import or export data from your databases. They are designed for bulk operations. \ud83d\udd0d Browse \u00b6 In this section are listed third party tools that will help you browse your data. You will find listed keyspaces, tables and will be able to edit the values. \ud83d\udccb Data Modelling \u00b6","title":"Home"},{"location":"pages/data/#load-and-export","text":"In this section are listed third party tools that will help you import or export data from your databases. They are designed for bulk operations.","title":"\ud83d\udce5 Load and Export"},{"location":"pages/data/#browse","text":"In this section are listed third party tools that will help you browse your data. You will find listed keyspaces, tables and will be able to edit the values.","title":"\ud83d\udd0d Browse"},{"location":"pages/data/#data-modelling","text":"","title":"\ud83d\udccb Data Modelling"},{"location":"pages/data/explore/cqlsh/","text":"\ud83d\udcd6 Reference Documentations and resources \ud83d\udcd6 Astra Docs - Reference documentation \ud83d\udcd6 Cql Tool Docs - Reference Documentation A - Overview \u00b6 CqlSH is a command-line interface for interacting with Cassandra using CQL (the Cassandra Query Language). It is shipped with every Cassandra package, and can be found in the bin/ directory alongside the cassandra executable. cqlsh is implemented with the Python native protocol driver, and connects to the single specified node. You can setup the software by providing options in the command line and/OR provide the settings in a file called cqlshrc located in ~/.cassandra > cqlsh --help Usage: cqlsh [options] [host [port]] CQL Shell Options: --version show program's version number and exit -h, --help show this help message and exit -C, --color Always use color output --no-color Never use color output --browser=BROWSER The browser to use to display CQL help, where BROWSER can be: - one of the supported browsers in https://docs.python.org/2/library/webbrowser.html. - browser path followed by %s, example: /usr/bin /google-chrome-stable %s --ssl Use SSL -u USERNAME, --username=USERNAME Authenticate as user. -p PASSWORD, --password=PASSWORD Authenticate using password. -k KEYSPACE, --keyspace=KEYSPACE Authenticate to the given keyspace. -b SECURE_CONNECT_BUNDLE, --secure-connect-bundle=SECURE_CONNECT_BUNDLE Connect using secure connect bundle. If this option is specified host, port settings are ignored -f FILE, --file=FILE Execute commands from FILE, then exit --debug Show additional debugging information --coverage Collect coverage data --encoding=ENCODING Specify a non-default encoding for output. (Default: utf-8) --cqlshrc=CQLSHRC Specify an alternative cqlshrc file location. --cqlversion=CQLVERSION Specify a particular CQL version, by default the highest version supported by the server will be used. Examples: \"3.0.3\", \"3.1.0\" --protocol-version=PROTOCOL_VERSION Specify a specific protocol version; otherwise the client will default and downgrade as necessary. Mutually exclusive with --dse-protocol-version. -e EXECUTE, --execute=EXECUTE Execute the statement and quit. --connect-timeout=CONNECT_TIMEOUT Specify the connection timeout in seconds (default: 5 seconds). --request-timeout=REQUEST_TIMEOUT Specify the default request timeout in seconds (default: 10 seconds). --consistency-level=CONSISTENCY_LEVEL Specify the initial consistency level. --serial-consistency-level=SERIAL_CONSISTENCY_LEVEL Specify the initial serial consistency level. -t, --tty Force tty mode (command prompt). --no-file-io Disable cqlsh commands that perform file I/O. --disable-history Disable saving of history Connects to 127.0.0.1:9042 by default. These defaults can be changed by setting $CQLSH_HOST and/or $CQLSH_PORT. When a host (and optional port number) are given on the command line, they take precedence over any defaults. B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should download the Cqlsh Version for Astra DB You should NOT use a WINDOWS machine, as of today cqlsh is not supported on Windows C - Installation \u00b6 \u2705 Step 1: Download and extract the archive To download the archive you can go on the download page , check the box and download the file: You can also use the command line: wget https://downloads.datastax.com/enterprise/cqlsh-astra.tar.gz \\ && tar xvzf cqlsh-astra.tar.gz \\ && rm -f cqlsh-astra.tar.gz The archive should look like: \u2705 Step 2: Start cqlsh providing parameters in the command line: From the directory where you extracted the CQLSH tarball, run the cqlsh script from the command line: $ cd /cqlsh-astra/bin $ ./cqlsh -u ${ CLIENT_ID } -p ${ CLIENT_SECRET } -b ${ PATH_TO_SECURE_BUNDLE .zip } -u (username) - Client ID as provided in the token generation page -p (password) - Client secret as provided in the token generation page -b (bundle) - location of the secure connect bundle that you downloaded for your database. \u2705 Step 3: Start Cqlsh providing parameters in cqlshrc Configure the cqlshrc file If you do not want to pass the secure connect bundle on the command line every time, set up the location in your cqlshrc file in ~/.cassandra [authentication] username = ${CLIENT_ID} password = ${CLIENT_SECRET} [connection] secure_connect_bundle = ${PATH_TO_SECURE_BUNDLE.zip} D - Tips and tricks \u00b6 If is a good idea to add cqlsh in your path to be able to use from everywhere If you want to work with multiple DB use some alias with the parameters alias cqlsh_db1='cqlsh -u user -p password -b secure-connect-db1.zip' alias cqlsh_db2='cqlsh --cqlshrc_db2'","title":"\u2023 Cqlsh"},{"location":"pages/data/explore/cqlsh/#a-overview","text":"CqlSH is a command-line interface for interacting with Cassandra using CQL (the Cassandra Query Language). It is shipped with every Cassandra package, and can be found in the bin/ directory alongside the cassandra executable. cqlsh is implemented with the Python native protocol driver, and connects to the single specified node. You can setup the software by providing options in the command line and/OR provide the settings in a file called cqlshrc located in ~/.cassandra > cqlsh --help Usage: cqlsh [options] [host [port]] CQL Shell Options: --version show program's version number and exit -h, --help show this help message and exit -C, --color Always use color output --no-color Never use color output --browser=BROWSER The browser to use to display CQL help, where BROWSER can be: - one of the supported browsers in https://docs.python.org/2/library/webbrowser.html. - browser path followed by %s, example: /usr/bin /google-chrome-stable %s --ssl Use SSL -u USERNAME, --username=USERNAME Authenticate as user. -p PASSWORD, --password=PASSWORD Authenticate using password. -k KEYSPACE, --keyspace=KEYSPACE Authenticate to the given keyspace. -b SECURE_CONNECT_BUNDLE, --secure-connect-bundle=SECURE_CONNECT_BUNDLE Connect using secure connect bundle. If this option is specified host, port settings are ignored -f FILE, --file=FILE Execute commands from FILE, then exit --debug Show additional debugging information --coverage Collect coverage data --encoding=ENCODING Specify a non-default encoding for output. (Default: utf-8) --cqlshrc=CQLSHRC Specify an alternative cqlshrc file location. --cqlversion=CQLVERSION Specify a particular CQL version, by default the highest version supported by the server will be used. Examples: \"3.0.3\", \"3.1.0\" --protocol-version=PROTOCOL_VERSION Specify a specific protocol version; otherwise the client will default and downgrade as necessary. Mutually exclusive with --dse-protocol-version. -e EXECUTE, --execute=EXECUTE Execute the statement and quit. --connect-timeout=CONNECT_TIMEOUT Specify the connection timeout in seconds (default: 5 seconds). --request-timeout=REQUEST_TIMEOUT Specify the default request timeout in seconds (default: 10 seconds). --consistency-level=CONSISTENCY_LEVEL Specify the initial consistency level. --serial-consistency-level=SERIAL_CONSISTENCY_LEVEL Specify the initial serial consistency level. -t, --tty Force tty mode (command prompt). --no-file-io Disable cqlsh commands that perform file I/O. --disable-history Disable saving of history Connects to 127.0.0.1:9042 by default. These defaults can be changed by setting $CQLSH_HOST and/or $CQLSH_PORT. When a host (and optional port number) are given on the command line, they take precedence over any defaults.","title":"A - Overview"},{"location":"pages/data/explore/cqlsh/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should download the Cqlsh Version for Astra DB You should NOT use a WINDOWS machine, as of today cqlsh is not supported on Windows","title":"B - Prerequisites"},{"location":"pages/data/explore/cqlsh/#c-installation","text":"\u2705 Step 1: Download and extract the archive To download the archive you can go on the download page , check the box and download the file: You can also use the command line: wget https://downloads.datastax.com/enterprise/cqlsh-astra.tar.gz \\ && tar xvzf cqlsh-astra.tar.gz \\ && rm -f cqlsh-astra.tar.gz The archive should look like: \u2705 Step 2: Start cqlsh providing parameters in the command line: From the directory where you extracted the CQLSH tarball, run the cqlsh script from the command line: $ cd /cqlsh-astra/bin $ ./cqlsh -u ${ CLIENT_ID } -p ${ CLIENT_SECRET } -b ${ PATH_TO_SECURE_BUNDLE .zip } -u (username) - Client ID as provided in the token generation page -p (password) - Client secret as provided in the token generation page -b (bundle) - location of the secure connect bundle that you downloaded for your database. \u2705 Step 3: Start Cqlsh providing parameters in cqlshrc Configure the cqlshrc file If you do not want to pass the secure connect bundle on the command line every time, set up the location in your cqlshrc file in ~/.cassandra [authentication] username = ${CLIENT_ID} password = ${CLIENT_SECRET} [connection] secure_connect_bundle = ${PATH_TO_SECURE_BUNDLE.zip}","title":"C - Installation"},{"location":"pages/data/explore/cqlsh/#d-tips-and-tricks","text":"If is a good idea to add cqlsh in your path to be able to use from everywhere If you want to work with multiple DB use some alias with the parameters alias cqlsh_db1='cqlsh -u user -p password -b secure-connect-db1.zip' alias cqlsh_db2='cqlsh --cqlshrc_db2'","title":"D - Tips and tricks"},{"location":"pages/data/explore/datagrip/","text":"A - Overview \u00b6 \u2139\ufe0f Astra Docs - Reference documentation \u2139\ufe0f Instructions on Sebastian Estevez's blog post \u2139\ufe0f Datagrip reference documentation DataGrip is a database management environment for developers. It is designed to query, create, and manage databases. Databases can work locally, on a server, or in the cloud. Supports MySQL, PostgreSQL, Microsoft SQL Server, Oracle, and more. If you have a JDBC driver, add it to DataGrip, connect to your DBMS, and start working. B - Prerequisites \u00b6 Create an Astra Database Create an Astra Token Download your secure connect bundle ZIP Download and install DataGrip C - Installation and Setup \u00b6 Step 1: Download JDBC Driver \u00b6 Download the JDBC driver from the DataStax website: Go to downloads.datastax.com/#odbc-jdbc-drivers . Select Simba JDBC Driver for Apache Cassandra. Select JDBC 4.2. Read the license terms and accept it (click the checkbox). Hit the blue Download button. Once the download completes, unzip the downloaded file. Step 2: Download Settings.zip \u00b6 Download the settings.zip locally If you are already a DataGrip user, back up your existing settings because downloading settings.zip might override your existing settings. Step 3: Import the settings.zip into DataGrip \u00b6 Selecting File \u2192 Manage IDE Settings \u2192 Import Settings in DataGrip. From the directory menu, select the settings.zip file from the directory where it is stored. Select Import and Restart . You will see a new database connection type called Astra: Simba Cassandra JDBC 4.2 driver shown. Go to the Advanced Settings to confirm the VM home path is set to Default. VM home path is set to a value named Default. Step 4: Establish the connection \u00b6 When you create your connection, the URL will look like this: jdbc:cassandra://;AuthMech=<2>;UID=token;PWD=<ApplicationToken>;SecureConnectionBundlePath=<PATH TO YOUR SECURE CONNECT BUNDLE>;TunableConsistency=<6> URL in the screenshot shows the format described in the previous sentence. AuthMech: Specifies whether the driver connects to a Cassandra or Astra DB database and whether the driver authenticates the connection. ApplicationToken: Generated from Astra DB console. SecureConnectionBundlePath: Path to where your downloaded Secure Connect Bundle is located. TunableConsistency: Specifies Cassandra replica or the number of Cassandra replicas that must process a query for the query to be considered successful.","title":"\u2023 Datagrip"},{"location":"pages/data/explore/datagrip/#a-overview","text":"\u2139\ufe0f Astra Docs - Reference documentation \u2139\ufe0f Instructions on Sebastian Estevez's blog post \u2139\ufe0f Datagrip reference documentation DataGrip is a database management environment for developers. It is designed to query, create, and manage databases. Databases can work locally, on a server, or in the cloud. Supports MySQL, PostgreSQL, Microsoft SQL Server, Oracle, and more. If you have a JDBC driver, add it to DataGrip, connect to your DBMS, and start working.","title":"A - Overview"},{"location":"pages/data/explore/datagrip/#b-prerequisites","text":"Create an Astra Database Create an Astra Token Download your secure connect bundle ZIP Download and install DataGrip","title":"B - Prerequisites"},{"location":"pages/data/explore/datagrip/#c-installation-and-setup","text":"","title":"C - Installation and Setup"},{"location":"pages/data/explore/datagrip/#step-1-download-jdbc-driver","text":"Download the JDBC driver from the DataStax website: Go to downloads.datastax.com/#odbc-jdbc-drivers . Select Simba JDBC Driver for Apache Cassandra. Select JDBC 4.2. Read the license terms and accept it (click the checkbox). Hit the blue Download button. Once the download completes, unzip the downloaded file.","title":"Step 1: Download JDBC Driver"},{"location":"pages/data/explore/datagrip/#step-2-download-settingszip","text":"Download the settings.zip locally If you are already a DataGrip user, back up your existing settings because downloading settings.zip might override your existing settings.","title":"Step 2: Download Settings.zip"},{"location":"pages/data/explore/datagrip/#step-3-import-the-settingszip-into-datagrip","text":"Selecting File \u2192 Manage IDE Settings \u2192 Import Settings in DataGrip. From the directory menu, select the settings.zip file from the directory where it is stored. Select Import and Restart . You will see a new database connection type called Astra: Simba Cassandra JDBC 4.2 driver shown. Go to the Advanced Settings to confirm the VM home path is set to Default. VM home path is set to a value named Default.","title":"Step 3: Import the settings.zip into DataGrip"},{"location":"pages/data/explore/datagrip/#step-4-establish-the-connection","text":"When you create your connection, the URL will look like this: jdbc:cassandra://;AuthMech=<2>;UID=token;PWD=<ApplicationToken>;SecureConnectionBundlePath=<PATH TO YOUR SECURE CONNECT BUNDLE>;TunableConsistency=<6> URL in the screenshot shows the format described in the previous sentence. AuthMech: Specifies whether the driver connects to a Cassandra or Astra DB database and whether the driver authenticates the connection. ApplicationToken: Generated from Astra DB console. SecureConnectionBundlePath: Path to where your downloaded Secure Connect Bundle is located. TunableConsistency: Specifies Cassandra replica or the number of Cassandra replicas that must process a query for the query to be considered successful.","title":"Step 4: Establish the connection"},{"location":"pages/data/explore/dbeaver/","text":"This article includes information that was originally written by Erick Ramirez on DataStax Community A - Overview \u00b6 DBeaver is a universal database management tool for everyone who needs to work with data in a professional way. With DBeaver you are able to manipulate with your data like in a regular spreadsheet, create analytical reports based on records from different data storages, export information in an appropriate format. \u2139\ufe0f Introduction to DBeaver \ud83d\udce5 DBeaver Download Link B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle This article assumes you have installed DBeaver Community Edition on your laptop or PC. It was written for version 21.2.0 on MacOS but it should also work for the Windows version. C - Installation and Setup \u00b6 \u2705 Step 1: JDBC Driver \u00b6 Download the JDBC driver from the DataStax website: Go to https://downloads.datastax.com/#odbc-jdbc-drivers. Select Simba JDBC Driver for Apache Cassandra. Select JDBC 4.2. Read the license terms and accept it (click the checkbox). Hit the blue Download button. Once the download completes, unzip the downloaded file. \u2705 Step 2: Import Driver \u00b6 Go to the Driver Manager. Click the New button. In the Libraries tab, click the Add File button. Locate the directory where you unzipped the driver download and add the CassandraJDBC42.jar file. Click the Find Class button which should identify the driver class as com.simba.cassandra.jdbc42.Driver . In the Settings tab, set the following: Driver Name: Astra DB Driver Type: Generic Class Name: com.simba.cassandra.jdbc42.Driver Click the OK button to save the driver At this point, you should see Astra DB as one of the drivers on the list: \u2705 Step 3: Create New Connection \u00b6 Connect to your Astra DB in DBeaver: Open the New Database Connection dialog box. Select Astra DB from the list of drivers. In the Main tab, set the JDBC URL to: jdbc:cassandra://;AuthMech=2;TunableConsistency=6;SecureConnectionBundlePath=/path/to/secure-connect-dbeaver.zip Note That you will need to specify the full path to your secure bundle. In the Username field, enter the string token In the Password field, paste the value of the token you created in the Prerequisites section above. The token looks like AstraCS:AbC...XYz:123...edf0 . Click on the Connection details button In Connection name field, give your DB connection a name: Click the Finish button Click on the Test Connection button to confirm that the driver configuration is working: \u2705 Step 4: Final Test \u00b6 Connect to your Astra DB. If the connection was successful, you should be able to explore the keyspaces and tables in your DB on the left-hand side of the UI. Here's an example output: \ud83c\udfe0 Back to home","title":"\u2023 DBeaver"},{"location":"pages/data/explore/dbeaver/#a-overview","text":"DBeaver is a universal database management tool for everyone who needs to work with data in a professional way. With DBeaver you are able to manipulate with your data like in a regular spreadsheet, create analytical reports based on records from different data storages, export information in an appropriate format. \u2139\ufe0f Introduction to DBeaver \ud83d\udce5 DBeaver Download Link","title":"A - Overview"},{"location":"pages/data/explore/dbeaver/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle This article assumes you have installed DBeaver Community Edition on your laptop or PC. It was written for version 21.2.0 on MacOS but it should also work for the Windows version.","title":"B - Prerequisites"},{"location":"pages/data/explore/dbeaver/#c-installation-and-setup","text":"","title":"C - Installation and Setup"},{"location":"pages/data/explore/dbeaver/#step-1-jdbc-driver","text":"Download the JDBC driver from the DataStax website: Go to https://downloads.datastax.com/#odbc-jdbc-drivers. Select Simba JDBC Driver for Apache Cassandra. Select JDBC 4.2. Read the license terms and accept it (click the checkbox). Hit the blue Download button. Once the download completes, unzip the downloaded file.","title":"\u2705 Step 1: JDBC Driver"},{"location":"pages/data/explore/dbeaver/#step-2-import-driver","text":"Go to the Driver Manager. Click the New button. In the Libraries tab, click the Add File button. Locate the directory where you unzipped the driver download and add the CassandraJDBC42.jar file. Click the Find Class button which should identify the driver class as com.simba.cassandra.jdbc42.Driver . In the Settings tab, set the following: Driver Name: Astra DB Driver Type: Generic Class Name: com.simba.cassandra.jdbc42.Driver Click the OK button to save the driver At this point, you should see Astra DB as one of the drivers on the list:","title":"\u2705 Step 2: Import Driver"},{"location":"pages/data/explore/dbeaver/#step-3-create-new-connection","text":"Connect to your Astra DB in DBeaver: Open the New Database Connection dialog box. Select Astra DB from the list of drivers. In the Main tab, set the JDBC URL to: jdbc:cassandra://;AuthMech=2;TunableConsistency=6;SecureConnectionBundlePath=/path/to/secure-connect-dbeaver.zip Note That you will need to specify the full path to your secure bundle. In the Username field, enter the string token In the Password field, paste the value of the token you created in the Prerequisites section above. The token looks like AstraCS:AbC...XYz:123...edf0 . Click on the Connection details button In Connection name field, give your DB connection a name: Click the Finish button Click on the Test Connection button to confirm that the driver configuration is working:","title":"\u2705 Step 3: Create New Connection"},{"location":"pages/data/explore/dbeaver/#step-4-final-test","text":"Connect to your Astra DB. If the connection was successful, you should be able to explore the keyspaces and tables in your DB on the left-hand side of the UI. Here's an example output: \ud83c\udfe0 Back to home","title":"\u2705 Step 4: Final Test"},{"location":"pages/data/explore/dbschema/","text":"\ud83d\udcd6 Reference Documentations and resources \ud83d\udcd6 Astra Docs - Reference documentation DBSchema Tutorials A - Overview \u00b6 DbSchema is a universal database designer for out-of-the-box schema management and documentation, sharing the schema in the team, and deploying on different databases. Visual tools can help developers, database administrators, and decision-makers to query, explore and manage the data. \u2139\ufe0f Introduction to DBSchema \ud83d\udce5 DBSchema Installation DBSchema uses the Simba JDBC driver to connect to Cassandra as the storage backend. The Java driver itself supports connections to Astra DB natively. B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle This article assumes you have installed the latest version of DBSchema on your laptop or PC. C - Installation and Setup \u00b6 \u2705 Step 1: JDBC Driver Download the JDBC driver from the DataStax website: Go to https://downloads.datastax.com/#odbc-jdbc-drivers. Select Simba JDBC Driver for Apache Cassandra. Select JDBC 4.2. Read the license terms and accept it (click the checkbox). Hit the blue Download button. Once the download completes, unzip the downloaded file. \u2705 Step 2: Establish the Connection Open DB Schema Select Connect to the Database Select Start In the Choose your database menu, select Cassandra. Select Next. Select JDBC Driver edit option. In the JDBC Driver Manager, select New . In the Add RDBMS window, enter Astra and select OK Select OK in the confirmation message. Upload the Simba JDBC Driver. Select Open Once you upload the Simba JDBC Driver, you will see Astra in the Choose your Database window. Select Next . In the Astra Connection Dialog, add JDBC URL as jdbc:cassandra:// ; AuthMech = < 2 > ; UID = token ; PWD = <ApplicationToken> ; SecureConnectionBundlePath = <PATH TO YOUR SECURE CONNECT BUNDLE> ; TunableConsistency = < 6 > with the following variables: AuthMech: Specifies whether the driver connects to a Cassandra or Astra DB database and whether the driver authenticates the connection. ApplicationToken: Generated from Astra DB console. See Manage application tokens. SecureConnectionBundlePath: Path to where your downloaded Secure Connect Bundle is located. See Get secure connect bundle. TunableConsistency: Specifies Cassandra replica or the number of Cassandra replicas that must process a query for the query to be considered successful. Select Connect In the Select Schemas/Catalogs , select the keyspace to which you want to connect. Select OK. \u2705 Step 3: Final Test Now that your connection is working, you can create tables, introspect your keyspaces, view your data in the DBSchema GUI, and more. To learn more about DBSchema, see Quick start with DBSchema \ud83c\udfe0 Back to HOME","title":"\u2023 DBSchema"},{"location":"pages/data/explore/dbschema/#a-overview","text":"DbSchema is a universal database designer for out-of-the-box schema management and documentation, sharing the schema in the team, and deploying on different databases. Visual tools can help developers, database administrators, and decision-makers to query, explore and manage the data. \u2139\ufe0f Introduction to DBSchema \ud83d\udce5 DBSchema Installation DBSchema uses the Simba JDBC driver to connect to Cassandra as the storage backend. The Java driver itself supports connections to Astra DB natively.","title":"A - Overview"},{"location":"pages/data/explore/dbschema/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle This article assumes you have installed the latest version of DBSchema on your laptop or PC.","title":"B - Prerequisites"},{"location":"pages/data/explore/dbschema/#c-installation-and-setup","text":"\u2705 Step 1: JDBC Driver Download the JDBC driver from the DataStax website: Go to https://downloads.datastax.com/#odbc-jdbc-drivers. Select Simba JDBC Driver for Apache Cassandra. Select JDBC 4.2. Read the license terms and accept it (click the checkbox). Hit the blue Download button. Once the download completes, unzip the downloaded file. \u2705 Step 2: Establish the Connection Open DB Schema Select Connect to the Database Select Start In the Choose your database menu, select Cassandra. Select Next. Select JDBC Driver edit option. In the JDBC Driver Manager, select New . In the Add RDBMS window, enter Astra and select OK Select OK in the confirmation message. Upload the Simba JDBC Driver. Select Open Once you upload the Simba JDBC Driver, you will see Astra in the Choose your Database window. Select Next . In the Astra Connection Dialog, add JDBC URL as jdbc:cassandra:// ; AuthMech = < 2 > ; UID = token ; PWD = <ApplicationToken> ; SecureConnectionBundlePath = <PATH TO YOUR SECURE CONNECT BUNDLE> ; TunableConsistency = < 6 > with the following variables: AuthMech: Specifies whether the driver connects to a Cassandra or Astra DB database and whether the driver authenticates the connection. ApplicationToken: Generated from Astra DB console. See Manage application tokens. SecureConnectionBundlePath: Path to where your downloaded Secure Connect Bundle is located. See Get secure connect bundle. TunableConsistency: Specifies Cassandra replica or the number of Cassandra replicas that must process a query for the query to be considered successful. Select Connect In the Select Schemas/Catalogs , select the keyspace to which you want to connect. Select OK. \u2705 Step 3: Final Test Now that your connection is working, you can create tables, introspect your keyspaces, view your data in the DBSchema GUI, and more. To learn more about DBSchema, see Quick start with DBSchema \ud83c\udfe0 Back to HOME","title":"C - Installation and Setup"},{"location":"pages/data/explore/mindsdb/","text":"This article was originally written by Steven Matison on Datastax JIRA This page will go into details about what I had to do to build the project, modify the cassandra.py and scylla_ds.py , and get mindsdb GUI connected to Astra. Source Repo: GitHub - mindsdb/mindsdb My Fork: ds-steven-matison/mindsdb Docs: https://docs.mindsdb.com/ Files Changed \u00b6 /root/mindsdb/lib64/python3.6/site-packages/mindsdb_datasources/datasources/scylla_ds.py Path here is correct. \u201clib64\u201d not part of repo, so I put scylla_ds.py in repo so you can see source code here: github diff /root/mindsdb/lib64/python3.6/site-packages/mindsdb_datasources/cassandra.py Changes diff in repo here: github diff Git Status \u00b6 Untracked files: ( use \"git add <file>...\" to include in what will be committed ) LICENSE.txt MindsDB.egg-info/ bin/ config.json include/ lib/ lib64 mindsdb/integrations/cassandra/cassandra.py.bk mindsdb/integrations/cassandra/tmp.py pip-selfcheck.json share/ Terminal History \u00b6 503 python -v 504 python -version 505 ls 506 python3 -v 507 python3 -m venv mindsdb 508 source mindsdb/bin/activate 509 pip3 install mindsdb 510 pip3 install Cython 511 pip3 install mindsdb 512 pip3 install sentencepiece 513 pip3 freeze 514 pip install --upgrade pip 515 pip3 install sentencepiece 516 pip3 install mindsdb 517 pip3 freeze 518 python3 -m mindsdb 533 pip3 install mindsdb 548 pip3 install cassandra-driver 549 python3 -c 'import cassandra; print (cassandra.__version__)' 570 python3 mindsdb_cassandra.py 571 nano mindsdb_cassandra.py 572 python3 mindsdb_cassandra.py 573 python3 -m mindsdb --api = mysql --config = config.json 574 pip3 uninstall mindsdb 577 git clone https://github.com/ds-steven-matison/mindsdb.git 578 cd mindsdb/mindsdb/integrations/cassandra/ 579 ls 580 nano cassandra.py 581 cp cassandra.py cassandra.py.bk 582 nano cassandra.py 588 pip3 install -r requirements.txt 592 pip3 install cassandra-driver 636 cp secure-connect-mindsdb.zip /tmp 637 chmod 755 /tmp/secure-connect-mindsdb.zip 654 pip3 freeze 658 pip3 install cassandra-driver 659 python3 -c 'import cassandra; print (cassandra.__version__)' 662 nano /root/mindsdb/lib64/python3.6/site-packages/mindsdb_datasources/cassandra.py 669 nano ./mindsdb/lib/python3.6/site-packages/cassandra/cluster.py 670 nano /root/mindsdb/lib64/python3.6/site-packages/mindsdb_datasources/datasources/scylla_ds.py 672 python3 -m venv mindsdb 673 source mindsdb/bin/activate 674 cd mindsdb && pip3 install -r requirements.txt 675 pip install --upgrade pip 676 pip3 install --upgrade pip 677 pip3 install -r requirements.txt 678 python setup.py develop 683 mkdir /storage 695 install mindsdb_native [ cassandra ] 696 pip3 install mindsdb-sdk 721 nano config.json 722 python3 -m mindsdb --config = config.json","title":"\u2023 MindsDB"},{"location":"pages/data/explore/mindsdb/#files-changed","text":"/root/mindsdb/lib64/python3.6/site-packages/mindsdb_datasources/datasources/scylla_ds.py Path here is correct. \u201clib64\u201d not part of repo, so I put scylla_ds.py in repo so you can see source code here: github diff /root/mindsdb/lib64/python3.6/site-packages/mindsdb_datasources/cassandra.py Changes diff in repo here: github diff","title":"Files Changed"},{"location":"pages/data/explore/mindsdb/#git-status","text":"Untracked files: ( use \"git add <file>...\" to include in what will be committed ) LICENSE.txt MindsDB.egg-info/ bin/ config.json include/ lib/ lib64 mindsdb/integrations/cassandra/cassandra.py.bk mindsdb/integrations/cassandra/tmp.py pip-selfcheck.json share/","title":"Git Status"},{"location":"pages/data/explore/mindsdb/#terminal-history","text":"503 python -v 504 python -version 505 ls 506 python3 -v 507 python3 -m venv mindsdb 508 source mindsdb/bin/activate 509 pip3 install mindsdb 510 pip3 install Cython 511 pip3 install mindsdb 512 pip3 install sentencepiece 513 pip3 freeze 514 pip install --upgrade pip 515 pip3 install sentencepiece 516 pip3 install mindsdb 517 pip3 freeze 518 python3 -m mindsdb 533 pip3 install mindsdb 548 pip3 install cassandra-driver 549 python3 -c 'import cassandra; print (cassandra.__version__)' 570 python3 mindsdb_cassandra.py 571 nano mindsdb_cassandra.py 572 python3 mindsdb_cassandra.py 573 python3 -m mindsdb --api = mysql --config = config.json 574 pip3 uninstall mindsdb 577 git clone https://github.com/ds-steven-matison/mindsdb.git 578 cd mindsdb/mindsdb/integrations/cassandra/ 579 ls 580 nano cassandra.py 581 cp cassandra.py cassandra.py.bk 582 nano cassandra.py 588 pip3 install -r requirements.txt 592 pip3 install cassandra-driver 636 cp secure-connect-mindsdb.zip /tmp 637 chmod 755 /tmp/secure-connect-mindsdb.zip 654 pip3 freeze 658 pip3 install cassandra-driver 659 python3 -c 'import cassandra; print (cassandra.__version__)' 662 nano /root/mindsdb/lib64/python3.6/site-packages/mindsdb_datasources/cassandra.py 669 nano ./mindsdb/lib/python3.6/site-packages/cassandra/cluster.py 670 nano /root/mindsdb/lib64/python3.6/site-packages/mindsdb_datasources/datasources/scylla_ds.py 672 python3 -m venv mindsdb 673 source mindsdb/bin/activate 674 cd mindsdb && pip3 install -r requirements.txt 675 pip install --upgrade pip 676 pip3 install --upgrade pip 677 pip3 install -r requirements.txt 678 python setup.py develop 683 mkdir /storage 695 install mindsdb_native [ cassandra ] 696 pip3 install mindsdb-sdk 721 nano config.json 722 python3 -m mindsdb --config = config.json","title":"Terminal History"},{"location":"pages/data/explore/netflix-data-explorer/","text":"\ud83d\udcd6 Reference Documentations and resources \ud83d\udcd6 Netlix Blog - Introduction of the tool by Netflix Github Repository - Core project Github Repository - Fork for Astra A - Overview \u00b6 The Data Explorer by netflix is a web-based tools that will help you navigate and edit your data. It supports both Cassandra and Dynomite but here we will focus on Astra . There a few killer features Multi Cluster Access Multi-cluster access provides easy access to all of the clusters in your environment. The cluster selector in the top nav allows you to switch to any of your discovered clusters quickly. Explore your data The Explore view provides a simple way to explore your data quickly. You can query by partition and clustering keys, insert and edit records, and easily export the results or download them as CQL statements. Schema Designer Creating a new Keyspace and Table by hand can be error-prone Our schema designer UI streamlines creating a new Table with improved validation and enforcement of best practices. Query IDE The Query Mode provides a powerful IDE-like experience for writing free-form CQL queries. B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle C - Procedure \u00b6 C.1 Run Locally \u00b6 Prerequisites: You need node , npm and yarn Install Yarn on MAC brew install yarn Clone the repository git clone https://github.com/DataStax-Examples/nf-data-explorer.git cd nf-data-explorer Install the dependencies ( expect a 2min build it will download quite some packages ) yarn && yarn build Start the applications yarn start Import your bundle Here are the instructions to get your cloud secure bundle C.2 Execute with Gitpod \u00b6 Click the button Open the application Import your bundle \ud83c\udfe0 Back to home","title":"\u2023 Netflix Data Explorer"},{"location":"pages/data/explore/netflix-data-explorer/#a-overview","text":"The Data Explorer by netflix is a web-based tools that will help you navigate and edit your data. It supports both Cassandra and Dynomite but here we will focus on Astra . There a few killer features Multi Cluster Access Multi-cluster access provides easy access to all of the clusters in your environment. The cluster selector in the top nav allows you to switch to any of your discovered clusters quickly. Explore your data The Explore view provides a simple way to explore your data quickly. You can query by partition and clustering keys, insert and edit records, and easily export the results or download them as CQL statements. Schema Designer Creating a new Keyspace and Table by hand can be error-prone Our schema designer UI streamlines creating a new Table with improved validation and enforcement of best practices. Query IDE The Query Mode provides a powerful IDE-like experience for writing free-form CQL queries.","title":"A - Overview"},{"location":"pages/data/explore/netflix-data-explorer/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle","title":"B - Prerequisites"},{"location":"pages/data/explore/netflix-data-explorer/#c-procedure","text":"","title":"C - Procedure"},{"location":"pages/data/explore/netflix-data-explorer/#c1-run-locally","text":"Prerequisites: You need node , npm and yarn Install Yarn on MAC brew install yarn Clone the repository git clone https://github.com/DataStax-Examples/nf-data-explorer.git cd nf-data-explorer Install the dependencies ( expect a 2min build it will download quite some packages ) yarn && yarn build Start the applications yarn start Import your bundle Here are the instructions to get your cloud secure bundle","title":"C.1 Run Locally"},{"location":"pages/data/explore/netflix-data-explorer/#c2-execute-with-gitpod","text":"Click the button Open the application Import your bundle \ud83c\udfe0 Back to home","title":"C.2 Execute with Gitpod"},{"location":"pages/data/explore/presto/","text":"A - Overview \u00b6 Presto is a distributed SQL query engine for big data analytics. Presto can query data from over 30 different data sources, including Cassandra, MongoDB, MySQL, PostgresSQL, and Redis. Common Presto use cases include: interactive data analytics, SQL-based analytics over object storage systems, data access and analytics across multiple data sources with query federation, batch ETL processing across disparate systems. In this tutorial, we show how to use Presto to explore and query data in Astra DB with SQL . The overall architecture of this solution is depicted below. Presto CLI Client sends SQL queries to Presto Server . Presto Server retrieves data from Astra DB via CQL Proxy , computes the query results and returns them to the client. B - Prerequisites \u00b6 Create an Astra Database Create an Astra Token C - Setup Astra DB \u00b6 \u2705 1. Sign in Connect to your Astra account and create a new Astra database or select an existing one. Add a new keyspace with name banking_db or use an existing one. \u2705 2. Create the following tables using the CQL Console USE banking_db ; CREATE TABLE customer ( id UUID , name TEXT , email TEXT , PRIMARY KEY ( id ) ); CREATE TABLE accounts_by_customer ( customer_id UUID , account_number TEXT , account_type TEXT , account_balance DECIMAL , customer_name TEXT STATIC , PRIMARY KEY (( customer_id ), account_number ) ); \u2705 3. Insert the rows using the CQL Console INSERT INTO customer ( id , name , email ) VALUES ( 8 d6c1271 - 16 b6 - 479 d - 8 ea9 - 546 c37381ab3 , 'Alice' , 'alice@example.org' ); INSERT INTO customer ( id , name , email ) VALUES ( 0 e5d9e8c - 2 e3b - 4576 - 8515 - 58 b491cb859e , 'Bob' , 'bob@example.org' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 8 d6c1271 - 16 b6 - 479 d - 8 ea9 - 546 c37381ab3 , 'A-101' , 'Checking' , 100 . 01 , 'Alice' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 8 d6c1271 - 16 b6 - 479 d - 8 ea9 - 546 c37381ab3 , 'A-102' , 'Savings' , 200 . 02 , 'Alice' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 0 e5d9e8c - 2 e3b - 4576 - 8515 - 58 b491cb859e , 'B-101' , 'Checking' , 300 . 03 , 'Bob' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 0 e5d9e8c - 2 e3b - 4576 - 8515 - 58 b491cb859e , 'B-102' , 'Savings' , 400 . 04 , 'Bob' ); D - Deploy CQL Proxy \u00b6 \u2705 4. Installation Follow the instructions to deploy a CQL Proxy as close to a Presto Server as possible, preferrably deploying both components on the same server. The simplest way to start cql-proxy is to use an <astra-token> and <astra-database-id> : ./cql-proxy \\ --astra-token <astra-token> \\ --astra-database-id <astra-database-id> An example command with a sample, invalid token and database id: ./cql-proxy \\ --astra-token AstraCS:NoBhcuwCrIhZxqzjEMCSuGos:8a85142b47a588472a1f3b1314e2141f098785895411dee9db11f2a7ade457ce \\ --astra-database-id e5e4e925-289a-8231-83fd-25918093257b E - Setup Presto Server \u00b6 \u2705 5. Presto intallation Follow the instructions to download, install and configure a Presto Server or use an existing deployment. The minimal configuration requirements for a local single-machine deployment are: Node properties in file etc/node.properties node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data JVM config in file etc/jvm.config -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:+ExitOnOutOfMemoryError -Djdk.attach.allowAttachSelf=true Config properties in file etc/config.properties coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=5GB query.max-memory-per-node=1GB query.max-total-memory-per-node=2GB discovery-server.enabled=true discovery.uri=http://localhost:8080 Catalog properties in file etc/catalog/cassandra.properties connector.name=cassandra cassandra.contact-points=localhost cassandra.consistency-level=QUORUM The above configuration uses the Cassandra connector to interact with cql-proxy . \u2705 6. Start the Presto Server: bin/launcher run Wait for message ======== SERVER STARTED ======== to confirm a successful start. F - SQL Queries with Presto Client \u00b6 In this section you will execute SQL Queries against Astra DB using Presto CLI Client. \u2705 7. Install Presto Client Follow the instructions to download and install a CLI Presto Client . \u2705 8. Start the CLI Presto Client : ./presto --server http://localhost:8080 --catalog cassandra The server option specifies the HTTP(S) address and port of the Presto coordinator, and the catalog option sets the default catalog. \u2705 9. Execute the SQL query to find the total number of customers: SELECT COUNT ( * ) AS customer_count FROM banking_db . customer ; Output: customer_count ---------------- 2 (1 row) \u2705 10. Execute the SQL query to find emails of customers with account balances of 300.00 or higher: SELECT DISTINCT email AS customer_email FROM banking_db . customer INNER JOIN banking_db . accounts_by_customer ON ( id = customer_id ) WHERE account_balance >= 300 . 00 ; Output: customer_email ----------------- bob@example.org (1 row) \u2705 11. Execute the SQL query to find customers and sums of their account balances: SELECT id AS customer_id , name AS customer_name , email AS customer_email , SUM ( CAST ( COALESCE ( account_balance , 0 ) AS DECIMAL ( 12 , 2 ) ) ) AS customer_funds FROM banking_db . customer LEFT OUTER JOIN banking_db . accounts_by_customer ON ( id = customer_id ) GROUP BY id , name , email ; Output: customer_id | customer_name | customer_email | customer_funds --------------------------------------+---------------+-------------------+---------------- 8d6c1271-16b6-479d-8ea9-546c37381ab3 | Alice | alice@example.org | 300.03 0e5d9e8c-2e3b-4576-8515-58b491cb859e | Bob | bob@example.org | 700.07 (2 rows)","title":"\u2023 Presto"},{"location":"pages/data/explore/presto/#a-overview","text":"Presto is a distributed SQL query engine for big data analytics. Presto can query data from over 30 different data sources, including Cassandra, MongoDB, MySQL, PostgresSQL, and Redis. Common Presto use cases include: interactive data analytics, SQL-based analytics over object storage systems, data access and analytics across multiple data sources with query federation, batch ETL processing across disparate systems. In this tutorial, we show how to use Presto to explore and query data in Astra DB with SQL . The overall architecture of this solution is depicted below. Presto CLI Client sends SQL queries to Presto Server . Presto Server retrieves data from Astra DB via CQL Proxy , computes the query results and returns them to the client.","title":"A - Overview"},{"location":"pages/data/explore/presto/#b-prerequisites","text":"Create an Astra Database Create an Astra Token","title":"B - Prerequisites"},{"location":"pages/data/explore/presto/#c-setup-astra-db","text":"\u2705 1. Sign in Connect to your Astra account and create a new Astra database or select an existing one. Add a new keyspace with name banking_db or use an existing one. \u2705 2. Create the following tables using the CQL Console USE banking_db ; CREATE TABLE customer ( id UUID , name TEXT , email TEXT , PRIMARY KEY ( id ) ); CREATE TABLE accounts_by_customer ( customer_id UUID , account_number TEXT , account_type TEXT , account_balance DECIMAL , customer_name TEXT STATIC , PRIMARY KEY (( customer_id ), account_number ) ); \u2705 3. Insert the rows using the CQL Console INSERT INTO customer ( id , name , email ) VALUES ( 8 d6c1271 - 16 b6 - 479 d - 8 ea9 - 546 c37381ab3 , 'Alice' , 'alice@example.org' ); INSERT INTO customer ( id , name , email ) VALUES ( 0 e5d9e8c - 2 e3b - 4576 - 8515 - 58 b491cb859e , 'Bob' , 'bob@example.org' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 8 d6c1271 - 16 b6 - 479 d - 8 ea9 - 546 c37381ab3 , 'A-101' , 'Checking' , 100 . 01 , 'Alice' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 8 d6c1271 - 16 b6 - 479 d - 8 ea9 - 546 c37381ab3 , 'A-102' , 'Savings' , 200 . 02 , 'Alice' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 0 e5d9e8c - 2 e3b - 4576 - 8515 - 58 b491cb859e , 'B-101' , 'Checking' , 300 . 03 , 'Bob' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 0 e5d9e8c - 2 e3b - 4576 - 8515 - 58 b491cb859e , 'B-102' , 'Savings' , 400 . 04 , 'Bob' );","title":"C - Setup Astra DB"},{"location":"pages/data/explore/presto/#d-deploy-cql-proxy","text":"\u2705 4. Installation Follow the instructions to deploy a CQL Proxy as close to a Presto Server as possible, preferrably deploying both components on the same server. The simplest way to start cql-proxy is to use an <astra-token> and <astra-database-id> : ./cql-proxy \\ --astra-token <astra-token> \\ --astra-database-id <astra-database-id> An example command with a sample, invalid token and database id: ./cql-proxy \\ --astra-token AstraCS:NoBhcuwCrIhZxqzjEMCSuGos:8a85142b47a588472a1f3b1314e2141f098785895411dee9db11f2a7ade457ce \\ --astra-database-id e5e4e925-289a-8231-83fd-25918093257b","title":"D - Deploy CQL Proxy"},{"location":"pages/data/explore/presto/#e-setup-presto-server","text":"\u2705 5. Presto intallation Follow the instructions to download, install and configure a Presto Server or use an existing deployment. The minimal configuration requirements for a local single-machine deployment are: Node properties in file etc/node.properties node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data JVM config in file etc/jvm.config -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:+ExitOnOutOfMemoryError -Djdk.attach.allowAttachSelf=true Config properties in file etc/config.properties coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=5GB query.max-memory-per-node=1GB query.max-total-memory-per-node=2GB discovery-server.enabled=true discovery.uri=http://localhost:8080 Catalog properties in file etc/catalog/cassandra.properties connector.name=cassandra cassandra.contact-points=localhost cassandra.consistency-level=QUORUM The above configuration uses the Cassandra connector to interact with cql-proxy . \u2705 6. Start the Presto Server: bin/launcher run Wait for message ======== SERVER STARTED ======== to confirm a successful start.","title":"E - Setup Presto Server"},{"location":"pages/data/explore/presto/#f-sql-queries-with-presto-client","text":"In this section you will execute SQL Queries against Astra DB using Presto CLI Client. \u2705 7. Install Presto Client Follow the instructions to download and install a CLI Presto Client . \u2705 8. Start the CLI Presto Client : ./presto --server http://localhost:8080 --catalog cassandra The server option specifies the HTTP(S) address and port of the Presto coordinator, and the catalog option sets the default catalog. \u2705 9. Execute the SQL query to find the total number of customers: SELECT COUNT ( * ) AS customer_count FROM banking_db . customer ; Output: customer_count ---------------- 2 (1 row) \u2705 10. Execute the SQL query to find emails of customers with account balances of 300.00 or higher: SELECT DISTINCT email AS customer_email FROM banking_db . customer INNER JOIN banking_db . accounts_by_customer ON ( id = customer_id ) WHERE account_balance >= 300 . 00 ; Output: customer_email ----------------- bob@example.org (1 row) \u2705 11. Execute the SQL query to find customers and sums of their account balances: SELECT id AS customer_id , name AS customer_name , email AS customer_email , SUM ( CAST ( COALESCE ( account_balance , 0 ) AS DECIMAL ( 12 , 2 ) ) ) AS customer_funds FROM banking_db . customer LEFT OUTER JOIN banking_db . accounts_by_customer ON ( id = customer_id ) GROUP BY id , name , email ; Output: customer_id | customer_name | customer_email | customer_funds --------------------------------------+---------------+-------------------+---------------- 8d6c1271-16b6-479d-8ea9-546c37381ab3 | Alice | alice@example.org | 300.03 0e5d9e8c-2e3b-4576-8515-58b491cb859e | Bob | bob@example.org | 700.07 (2 rows)","title":"F - SQL Queries with Presto Client"},{"location":"pages/data/explore/tableplus/","text":"This article includes information that was originally written by Erick Ramirez on DataStax Community A - Overview \u00b6 TablePlus is a modern, native tool with elegant UI that allows you to simultaneously manage multiple databases such as MySQL, PostgreSQL, SQLite, Microsoft SQL Server and more. \u2139\ufe0f Introduction to TablePlus \ud83d\udce5 TablePlus Download Link B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle This article assumes you have a running installation of Tableplus on your laptop or PC. It was written for the MacOS version but it should also work for the Windows version. C - Installation and Setup \u00b6 Note: For simplicity, the secure connect bundle has been placed in /path/to/scb \u2705 Step 1: DB Information \u00b6 On your laptop or PC where Tableplus is installed, unpack your secure bundle. For example: $ cd /path/to/scb $ unzip secure-connect-getvaxxed.zip Here is an example file listing after unpacking the bundle: / path/ to/ scb/ ca.crt cert cert.pfx config.json cqlshrc identity.jks key trustStore.jks Obtain information about your database from the config.json file. Here is an example: { \"host\" : \"<YOUR_ENDPOINT>.db.astra.datastax.com\" , \"port\" : 98765 , \"cql_port\" : 34567 , \"keyspace\" : \"<KEYSPACE_NAME>\" , \"localDC\" : \"us-west-2\" , \"caCertLocation\" : \"./ca.crt\" , \"keyLocation\" : \"./key\" , \"certLocation\" : \"./cert\" , ... } We will use this information to configure Astra DB as the data source in Tableplus. \u2705 Step 2: New Connection \u00b6 In Tableplus, create a new connection and select Cassandra as the target database. In the Host and Port fields, use the host and cql_port values in the config.json above. In the User and Password fields, use the client ID and client secret from the token you created in the Prerequisites section of this article. In the Keyspace field, use the keyspace values in the config.json above. Choose SSL VERIFY NONE for the SSL mode. For SSL keys, select the secure bundle files: Secure Bundle Files key for Private Key (leave the password blank when prompted) cert for Cert ca.crt for Trusted Cert Here's an example of what the Cassandra Connection dialog box should look like: \u2705 Step 3: Final Test \u00b6 Connect to your Astra DB. If the connection was successful, you should be able to see all the tables on the left-hand side of the UI. Here's an example output:","title":"\u2023 TablePlus"},{"location":"pages/data/explore/tableplus/#a-overview","text":"TablePlus is a modern, native tool with elegant UI that allows you to simultaneously manage multiple databases such as MySQL, PostgreSQL, SQLite, Microsoft SQL Server and more. \u2139\ufe0f Introduction to TablePlus \ud83d\udce5 TablePlus Download Link","title":"A - Overview"},{"location":"pages/data/explore/tableplus/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle This article assumes you have a running installation of Tableplus on your laptop or PC. It was written for the MacOS version but it should also work for the Windows version.","title":"B - Prerequisites"},{"location":"pages/data/explore/tableplus/#c-installation-and-setup","text":"Note: For simplicity, the secure connect bundle has been placed in /path/to/scb","title":"C - Installation and Setup"},{"location":"pages/data/explore/tableplus/#step-1-db-information","text":"On your laptop or PC where Tableplus is installed, unpack your secure bundle. For example: $ cd /path/to/scb $ unzip secure-connect-getvaxxed.zip Here is an example file listing after unpacking the bundle: / path/ to/ scb/ ca.crt cert cert.pfx config.json cqlshrc identity.jks key trustStore.jks Obtain information about your database from the config.json file. Here is an example: { \"host\" : \"<YOUR_ENDPOINT>.db.astra.datastax.com\" , \"port\" : 98765 , \"cql_port\" : 34567 , \"keyspace\" : \"<KEYSPACE_NAME>\" , \"localDC\" : \"us-west-2\" , \"caCertLocation\" : \"./ca.crt\" , \"keyLocation\" : \"./key\" , \"certLocation\" : \"./cert\" , ... } We will use this information to configure Astra DB as the data source in Tableplus.","title":"\u2705 Step 1: DB Information"},{"location":"pages/data/explore/tableplus/#step-2-new-connection","text":"In Tableplus, create a new connection and select Cassandra as the target database. In the Host and Port fields, use the host and cql_port values in the config.json above. In the User and Password fields, use the client ID and client secret from the token you created in the Prerequisites section of this article. In the Keyspace field, use the keyspace values in the config.json above. Choose SSL VERIFY NONE for the SSL mode. For SSL keys, select the secure bundle files: Secure Bundle Files key for Private Key (leave the password blank when prompted) cert for Cert ca.crt for Trusted Cert Here's an example of what the Cassandra Connection dialog box should look like:","title":"\u2705 Step 2: New Connection"},{"location":"pages/data/explore/tableplus/#step-3-final-test","text":"Connect to your Astra DB. If the connection was successful, you should be able to see all the tables on the left-hand side of the UI. Here's an example output:","title":"\u2705 Step 3: Final Test"},{"location":"pages/data/explore/trino/","text":"A - Overview \u00b6 Trino is a distributed SQL query engine for big data analytics. Trino can query data from over 30 different data sources, including Cassandra, MongoDB, MySQL, PostgresSQL, and Redis. Common Trino use cases include: interactive data analytics, SQL-based analytics over object storage systems, data access and analytics across multiple data sources with query federation, batch ETL processing across disparate systems. In this tutorial, we show how to use Trino to explore and query data in Astra DB with SQL . The overall architecture of this solution is depicted below. Trino CLI Client sends SQL queries to Trino Server . Trino Server retrieves data from Astra DB via CQL Proxy , computes the query results and returns them to the client. B - Prerequisites \u00b6 Create an Astra Database Create an Astra Token C - Setup Astra DB \u00b6 \u2705 1. Sign in Connect to your Astra account and create a new Astra database or select an existing one. Add a new keyspace with name banking_db or use an existing one. \u2705 2. Create the following tables using the CQL Console USE banking_db ; CREATE TABLE customer ( id UUID , name TEXT , email TEXT , PRIMARY KEY ( id ) ); CREATE TABLE accounts_by_customer ( customer_id UUID , account_number TEXT , account_type TEXT , account_balance DECIMAL , customer_name TEXT STATIC , PRIMARY KEY (( customer_id ), account_number ) ); \u2705 3. Insert the rows using the CQL Console INSERT INTO customer ( id , name , email ) VALUES ( 8 d6c1271 - 16 b6 - 479 d - 8 ea9 - 546 c37381ab3 , 'Alice' , 'alice@example.org' ); INSERT INTO customer ( id , name , email ) VALUES ( 0 e5d9e8c - 2 e3b - 4576 - 8515 - 58 b491cb859e , 'Bob' , 'bob@example.org' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 8 d6c1271 - 16 b6 - 479 d - 8 ea9 - 546 c37381ab3 , 'A-101' , 'Checking' , 100 . 01 , 'Alice' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 8 d6c1271 - 16 b6 - 479 d - 8 ea9 - 546 c37381ab3 , 'A-102' , 'Savings' , 200 . 02 , 'Alice' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 0 e5d9e8c - 2 e3b - 4576 - 8515 - 58 b491cb859e , 'B-101' , 'Checking' , 300 . 03 , 'Bob' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 0 e5d9e8c - 2 e3b - 4576 - 8515 - 58 b491cb859e , 'B-102' , 'Savings' , 400 . 04 , 'Bob' ); D - Deploy CQL Proxy \u00b6 \u2705 4. Installation Follow the instructions to deploy a CQL Proxy as close to a Trino Server as possible, preferrably deploying both components on the same server. The simplest way to start cql-proxy is to use an <astra-token> and <astra-database-id> : ./cql-proxy \\ --astra-token <astra-token> \\ --astra-database-id <astra-database-id> An example command with a sample, invalid token and database id: ./cql-proxy \\ --astra-token AstraCS:NoBhcuwCrIhZxqzjEMCSuGos:8a85142b47a588472a1f3b1314e2141f098785895411dee9db11f2a7ade457ce \\ --astra-database-id e5e4e925-289a-8231-83fd-25918093257b E - Setup Trino Server \u00b6 \u2705 5. Trino intallation Follow the instructions to download, install and configure a Trino Server or use an existing deployment. The minimal configuration requirements for a local single-machine deployment are: Node properties in file etc/node.properties node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/trino/data JVM config in file etc/jvm.config -server -Xmx16G -XX:-UseBiasedLocking -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+ExplicitGCInvokesConcurrent -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError -XX:-OmitStackTraceInFastThrow -XX:ReservedCodeCacheSize=512M -XX:PerMethodRecompilationCutoff=10000 -XX:PerBytecodeRecompilationCutoff=10000 -Djdk.attach.allowAttachSelf=true -Djdk.nio.maxCachedBufferSize=2000000 Config properties in file etc/config.properties coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=5GB query.max-memory-per-node=1GB discovery.uri=http://localhost:8080 Catalog properties in file etc/catalog/cassandra.properties connector.name=cassandra cassandra.contact-points=localhost cassandra.consistency-level=QUORUM The above configuration uses the Cassandra connector to interact with cql-proxy . \u2705 6. Start the Trino Server: bin/launcher run Wait for message ======== SERVER STARTED ======== to confirm a successful start. F - SQL Queries with Trino Client \u00b6 In this section you will execute SQL Queries against Astra DB using Trino CLI Client. \u2705 7. Install Trino Client Follow the instructions to download and install a CLI Trino Client . \u2705 8. Start the CLI Trino Client : ./trino --server http://localhost:8080 --catalog cassandra The server option specifies the HTTP(S) address and port of the Trino coordinator, and the catalog option sets the default catalog. \u2705 9. Insert a new customer into table customer : INSERT INTO banking_db . customer ( id , name , email ) VALUES ( uuid (), 'Luis' , 'luis@example.org' ); \u2705 10. Execute the SQL query to find the total number of customers: SELECT COUNT ( * ) AS customer_count FROM banking_db . customer ; Output: customer_count ---------------- 3 (1 row) \u2705 11. Execute the SQL query to find emails of customers with account balances of 300.00 or higher: SELECT DISTINCT email AS customer_email FROM banking_db . customer INNER JOIN banking_db . accounts_by_customer ON ( id = customer_id ) WHERE account_balance >= 300 . 00 ; Output: customer_email ----------------- bob@example.org (1 row) \u2705 12. Execute the SQL query to find customers and sums of their account balances: SELECT id AS customer_id , name AS customer_name , email AS customer_email , SUM ( CAST ( COALESCE ( account_balance , 0 ) AS DECIMAL ( 12 , 2 ) ) ) AS customer_funds FROM banking_db . customer LEFT OUTER JOIN banking_db . accounts_by_customer ON ( id = customer_id ) GROUP BY id , name , email ; Output: customer_id | customer_name | customer_email | customer_funds --------------------------------------+---------------+-------------------+---------------- 0e5d9e8c-2e3b-4576-8515-58b491cb859e | Bob | bob@example.org | 700.07 c628dca6-a8a6-4f37-ac29-44975af069fb | Luis | luis@example.org | 0.00 8d6c1271-16b6-479d-8ea9-546c37381ab3 | Alice | alice@example.org | 300.03 (3 rows)","title":"\u2023 Trino"},{"location":"pages/data/explore/trino/#a-overview","text":"Trino is a distributed SQL query engine for big data analytics. Trino can query data from over 30 different data sources, including Cassandra, MongoDB, MySQL, PostgresSQL, and Redis. Common Trino use cases include: interactive data analytics, SQL-based analytics over object storage systems, data access and analytics across multiple data sources with query federation, batch ETL processing across disparate systems. In this tutorial, we show how to use Trino to explore and query data in Astra DB with SQL . The overall architecture of this solution is depicted below. Trino CLI Client sends SQL queries to Trino Server . Trino Server retrieves data from Astra DB via CQL Proxy , computes the query results and returns them to the client.","title":"A - Overview"},{"location":"pages/data/explore/trino/#b-prerequisites","text":"Create an Astra Database Create an Astra Token","title":"B - Prerequisites"},{"location":"pages/data/explore/trino/#c-setup-astra-db","text":"\u2705 1. Sign in Connect to your Astra account and create a new Astra database or select an existing one. Add a new keyspace with name banking_db or use an existing one. \u2705 2. Create the following tables using the CQL Console USE banking_db ; CREATE TABLE customer ( id UUID , name TEXT , email TEXT , PRIMARY KEY ( id ) ); CREATE TABLE accounts_by_customer ( customer_id UUID , account_number TEXT , account_type TEXT , account_balance DECIMAL , customer_name TEXT STATIC , PRIMARY KEY (( customer_id ), account_number ) ); \u2705 3. Insert the rows using the CQL Console INSERT INTO customer ( id , name , email ) VALUES ( 8 d6c1271 - 16 b6 - 479 d - 8 ea9 - 546 c37381ab3 , 'Alice' , 'alice@example.org' ); INSERT INTO customer ( id , name , email ) VALUES ( 0 e5d9e8c - 2 e3b - 4576 - 8515 - 58 b491cb859e , 'Bob' , 'bob@example.org' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 8 d6c1271 - 16 b6 - 479 d - 8 ea9 - 546 c37381ab3 , 'A-101' , 'Checking' , 100 . 01 , 'Alice' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 8 d6c1271 - 16 b6 - 479 d - 8 ea9 - 546 c37381ab3 , 'A-102' , 'Savings' , 200 . 02 , 'Alice' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 0 e5d9e8c - 2 e3b - 4576 - 8515 - 58 b491cb859e , 'B-101' , 'Checking' , 300 . 03 , 'Bob' ); INSERT INTO accounts_by_customer ( customer_id , account_number , account_type , account_balance , customer_name ) VALUES ( 0 e5d9e8c - 2 e3b - 4576 - 8515 - 58 b491cb859e , 'B-102' , 'Savings' , 400 . 04 , 'Bob' );","title":"C - Setup Astra DB"},{"location":"pages/data/explore/trino/#d-deploy-cql-proxy","text":"\u2705 4. Installation Follow the instructions to deploy a CQL Proxy as close to a Trino Server as possible, preferrably deploying both components on the same server. The simplest way to start cql-proxy is to use an <astra-token> and <astra-database-id> : ./cql-proxy \\ --astra-token <astra-token> \\ --astra-database-id <astra-database-id> An example command with a sample, invalid token and database id: ./cql-proxy \\ --astra-token AstraCS:NoBhcuwCrIhZxqzjEMCSuGos:8a85142b47a588472a1f3b1314e2141f098785895411dee9db11f2a7ade457ce \\ --astra-database-id e5e4e925-289a-8231-83fd-25918093257b","title":"D - Deploy CQL Proxy"},{"location":"pages/data/explore/trino/#e-setup-trino-server","text":"\u2705 5. Trino intallation Follow the instructions to download, install and configure a Trino Server or use an existing deployment. The minimal configuration requirements for a local single-machine deployment are: Node properties in file etc/node.properties node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/trino/data JVM config in file etc/jvm.config -server -Xmx16G -XX:-UseBiasedLocking -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+ExplicitGCInvokesConcurrent -XX:+ExitOnOutOfMemoryError -XX:+HeapDumpOnOutOfMemoryError -XX:-OmitStackTraceInFastThrow -XX:ReservedCodeCacheSize=512M -XX:PerMethodRecompilationCutoff=10000 -XX:PerBytecodeRecompilationCutoff=10000 -Djdk.attach.allowAttachSelf=true -Djdk.nio.maxCachedBufferSize=2000000 Config properties in file etc/config.properties coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=5GB query.max-memory-per-node=1GB discovery.uri=http://localhost:8080 Catalog properties in file etc/catalog/cassandra.properties connector.name=cassandra cassandra.contact-points=localhost cassandra.consistency-level=QUORUM The above configuration uses the Cassandra connector to interact with cql-proxy . \u2705 6. Start the Trino Server: bin/launcher run Wait for message ======== SERVER STARTED ======== to confirm a successful start.","title":"E - Setup Trino Server"},{"location":"pages/data/explore/trino/#f-sql-queries-with-trino-client","text":"In this section you will execute SQL Queries against Astra DB using Trino CLI Client. \u2705 7. Install Trino Client Follow the instructions to download and install a CLI Trino Client . \u2705 8. Start the CLI Trino Client : ./trino --server http://localhost:8080 --catalog cassandra The server option specifies the HTTP(S) address and port of the Trino coordinator, and the catalog option sets the default catalog. \u2705 9. Insert a new customer into table customer : INSERT INTO banking_db . customer ( id , name , email ) VALUES ( uuid (), 'Luis' , 'luis@example.org' ); \u2705 10. Execute the SQL query to find the total number of customers: SELECT COUNT ( * ) AS customer_count FROM banking_db . customer ; Output: customer_count ---------------- 3 (1 row) \u2705 11. Execute the SQL query to find emails of customers with account balances of 300.00 or higher: SELECT DISTINCT email AS customer_email FROM banking_db . customer INNER JOIN banking_db . accounts_by_customer ON ( id = customer_id ) WHERE account_balance >= 300 . 00 ; Output: customer_email ----------------- bob@example.org (1 row) \u2705 12. Execute the SQL query to find customers and sums of their account balances: SELECT id AS customer_id , name AS customer_name , email AS customer_email , SUM ( CAST ( COALESCE ( account_balance , 0 ) AS DECIMAL ( 12 , 2 ) ) ) AS customer_funds FROM banking_db . customer LEFT OUTER JOIN banking_db . accounts_by_customer ON ( id = customer_id ) GROUP BY id , name , email ; Output: customer_id | customer_name | customer_email | customer_funds --------------------------------------+---------------+-------------------+---------------- 0e5d9e8c-2e3b-4576-8515-58b491cb859e | Bob | bob@example.org | 700.07 c628dca6-a8a6-4f37-ac29-44975af069fb | Luis | luis@example.org | 0.00 8d6c1271-16b6-479d-8ea9-546c37381ab3 | Alice | alice@example.org | 300.03 (3 rows)","title":"F - SQL Queries with Trino Client"},{"location":"pages/data/load/astra-data-loader/","text":"\ud83d\udcd6 Reference Documentations and Resources \ud83d\udcd6 Data Loader - Astra Reference documentation \ud83c\udfa5 Youtube Video - Walk through data loader usage A - Overview \u00b6 Astra DB conveniently has its own data loader built in to the user interface. Use this DataStax Astra DB Data Loader to load your own data into your database or try one of our sample datasets. B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database C - Procedure \u00b6 \u2705 Step 1 : From your Astra DB Dashboard, select Load Data for the database where you want to load data. The Astra DB Data Loader launches. \u2705 Step 2 : Load your data using one of the options: Upload your dataset \u00b6 Drag and drop your own .csv file into the Astra DB Data Loader. CSV files must be less than 40 MB . You will see a status bar to show how much data has uploaded. Ensure the column names in your .csv do not include spaces. Underscores are accepted. For example, ShoeSize , ShirtColor , Shoe_Size , and Shirt_Color are accepted column names. Load example dataset \u00b6 Select one of the two examples given to use as a sample dataset. Load DynamoDB from S3 \u00b6 First, export your DynamoDB data to S3 as described here. Then in AWS console, grant read access to the following ARN: arn:aws:iam::445559476293:role/astra-loader Your bucket policy should use: { \"Statement\" : [ { \"Action\" : [ \"s3:ListBucket\" , \"s3:GetBucketLocation\" ], \"Principal\" : { \"AWS\" : \"arn:aws:iam::445559476293:role/astra-loader\" }, \"Effect\" : \"Allow\" , \"Resource\" : \"arn:aws:s3:::YOUR_BUCKET_NAME\" }, { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::445559476293:role/astra-loader\" }, \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : \"arn:aws:s3:::YOUR_BUCKET_NAME/*\" } ] } This bucket policy allows Astra DB automation to pull data from your identified shared S3 bucket, and load the data into Astra DB. You can remove the permission after the data load finishes. In the Option 3 prompts, enter your S3 Bucket name, and enter the Key value. To find the Key, navigate in AWS console to the S3 subdirectory that contains your exported DynamoDB data. Look for the Key on its Properties tab. Here\u2019s a sample screen with the Key shown near the lower-left corner: S3 Properties with Key value for exported DynamoDB data file. Once you configure your option, select Next. Import Procedure \u00b6 Give your table for this dataset a name. Your dataset will be included in the Data Preview and Types. Select the data type for each column. :information: The Astra DB Data Loader automatically selects data types for your dataset. If needed, you can change this to your own selection. Select your partition key and clustering column for your data. Select Next. Select your database from the dropdown menu. Select your keyspace from the available keyspaces. Select Next. You will see a confirmation that your data is being imported. Within a few minutes, your dataset will begin uploading to your database. You will receive an email when the job has started and when the dataset has been loaded.","title":"\u2023 Astra Data Loader"},{"location":"pages/data/load/astra-data-loader/#a-overview","text":"Astra DB conveniently has its own data loader built in to the user interface. Use this DataStax Astra DB Data Loader to load your own data into your database or try one of our sample datasets.","title":"A - Overview"},{"location":"pages/data/load/astra-data-loader/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database","title":"B - Prerequisites"},{"location":"pages/data/load/astra-data-loader/#c-procedure","text":"\u2705 Step 1 : From your Astra DB Dashboard, select Load Data for the database where you want to load data. The Astra DB Data Loader launches. \u2705 Step 2 : Load your data using one of the options:","title":"C - Procedure"},{"location":"pages/data/load/astra-data-loader/#upload-your-dataset","text":"Drag and drop your own .csv file into the Astra DB Data Loader. CSV files must be less than 40 MB . You will see a status bar to show how much data has uploaded. Ensure the column names in your .csv do not include spaces. Underscores are accepted. For example, ShoeSize , ShirtColor , Shoe_Size , and Shirt_Color are accepted column names.","title":"Upload your dataset"},{"location":"pages/data/load/astra-data-loader/#load-example-dataset","text":"Select one of the two examples given to use as a sample dataset.","title":"Load example dataset"},{"location":"pages/data/load/astra-data-loader/#load-dynamodb-from-s3","text":"First, export your DynamoDB data to S3 as described here. Then in AWS console, grant read access to the following ARN: arn:aws:iam::445559476293:role/astra-loader Your bucket policy should use: { \"Statement\" : [ { \"Action\" : [ \"s3:ListBucket\" , \"s3:GetBucketLocation\" ], \"Principal\" : { \"AWS\" : \"arn:aws:iam::445559476293:role/astra-loader\" }, \"Effect\" : \"Allow\" , \"Resource\" : \"arn:aws:s3:::YOUR_BUCKET_NAME\" }, { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::445559476293:role/astra-loader\" }, \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : \"arn:aws:s3:::YOUR_BUCKET_NAME/*\" } ] } This bucket policy allows Astra DB automation to pull data from your identified shared S3 bucket, and load the data into Astra DB. You can remove the permission after the data load finishes. In the Option 3 prompts, enter your S3 Bucket name, and enter the Key value. To find the Key, navigate in AWS console to the S3 subdirectory that contains your exported DynamoDB data. Look for the Key on its Properties tab. Here\u2019s a sample screen with the Key shown near the lower-left corner: S3 Properties with Key value for exported DynamoDB data file. Once you configure your option, select Next.","title":"Load DynamoDB from S3"},{"location":"pages/data/load/astra-data-loader/#import-procedure","text":"Give your table for this dataset a name. Your dataset will be included in the Data Preview and Types. Select the data type for each column. :information: The Astra DB Data Loader automatically selects data types for your dataset. If needed, you can change this to your own selection. Select your partition key and clustering column for your data. Select Next. Select your database from the dropdown menu. Select your keyspace from the available keyspaces. Select Next. You will see a confirmation that your data is being imported. Within a few minutes, your dataset will begin uploading to your database. You will receive an email when the job has started and when the dataset has been loaded.","title":"Import Procedure"},{"location":"pages/data/load/dsbulk/","text":"\ud83d\udcd6 Reference Documentations and resources \ud83d\udcd6 DSBulks Docs - Reference documentation \ud83d\udcd6 Datastax Docs - Reference Documentation A - Overview \u00b6 \ud83d\udcd8 What is DSBulk ? \u00b6 The DataStax Bulk Loader tool (DSBulk) is a unified tool for loading into and unloading from Cassandra-compatible storage engines, such as OSS Apache Cassandra\u00ae, DataStax Astra and DataStax Enterprise (DSE). Out of the box, DSBulk provides the ability to: Load (import) large amounts of data into the database efficiently and reliably; Unload (export) large amounts of data from the database efficiently and reliably; Count elements in a database table: how many rows in total, how many rows per replica and per token range, and how many rows in the top N largest partitions. # Load data dsbulk load <options> # Unload data dsbulk unload <options> # Count rows dsbulk count <options> Currently, CSV and Json formats are supported for both loading and unloading data. \ud83d\udcd8 Datastax Bulk Loader with Astra \u00b6 Use DataStax Bulk Loader (dsbulk) to load and unload data in CSV or JSON format with your DataStax Astra DB database efficiently and reliably. You can use dsbulk as a standalone tool to remotely connect to a cluster. The tool is not required to run locally on an instances, but can be used in this configuration. B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle This article was written for Datastax Bulk Loader version 1.9.1 . Starting with version 1.9 , dsbulk can detect and respect server-side rate limiting. This is very useful when working with Astra DB, which by default has some throughput guardrails in place. C - Installation \u00b6 \u2705 Step 1 : Download the archive and unzip locally curl -OL https://downloads.datastax.com/dsbulk/dsbulk-1.9.1.tar.gz \\ && tar xvzf dsbulk-1.9.1.tar.gz \\ && rm -f dsbulk-1.9.1.tar.gz it will take a few seconds (file is about 30M)... % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 49 30.0M 49 14.8M 0 0 343k 0 0:01:29 0:00:44 0:00:45 244k D - Usage \u00b6 \ud83d\udcd8 Load Data \u00b6 Given a table CREATE TABLE better_reads . book_by_id ( id text PRIMARY KEY , author_id list < text > , author_names list < text > , book_description text , book_name text , cover_ids list < text > , published_date date ) A sample CSV could be: id|author_id|author_names|book_description|book_name|cover_ids|published_date 1234|[\"id1\",\"id2\",\"id3\"]|[\"name1\",\"name2\",\"name3\"]|this is a dsecription|Book name|[\"cover1\",\"cover2\"]|2022-02-02 Loaded with the following command: dsbulk load \\ -url book_by_id.csv \\ -c csv \\ -delim '|' \\ -k better_reads \\ -t book_by_id \\ --schema.allowMissingFields true \\ -u clientId \\ -p clientSecret \\ -b secureBundle.zip \ud83d\udcd8 Export Data \u00b6 Unloaded the same table with the following command: dsbulk unload \\ -k better_reads \\ -t book_by_id \\ -c csv \\ -u clientId \\ -p clientSecret \\ -b secureBundle.zip \\ > book_by_id_export.csv \ud83d\udcd8 Count Table Records \u00b6 Counted the rows in the table with the following command: dsbulk count \\ -k better_reads \\ -t book_by_id \\ -u clientId \\ -p clientSecret \\ -b secureBundle.zip Produces the following output: Operation directory: /local/dsbulk-1.9.1/logs/COUNT_20220223-213637-046128 total | failed | rows/s | p50ms | p99ms | p999ms 143,475 | 0 | 87,509 | 155.34 | 511.71 | 511.71 Operation COUNT_20220223-213637-046128 completed successfully in 1 second. 143475","title":"\u2023 DSBulk"},{"location":"pages/data/load/dsbulk/#a-overview","text":"","title":"A - Overview"},{"location":"pages/data/load/dsbulk/#what-is-dsbulk","text":"The DataStax Bulk Loader tool (DSBulk) is a unified tool for loading into and unloading from Cassandra-compatible storage engines, such as OSS Apache Cassandra\u00ae, DataStax Astra and DataStax Enterprise (DSE). Out of the box, DSBulk provides the ability to: Load (import) large amounts of data into the database efficiently and reliably; Unload (export) large amounts of data from the database efficiently and reliably; Count elements in a database table: how many rows in total, how many rows per replica and per token range, and how many rows in the top N largest partitions. # Load data dsbulk load <options> # Unload data dsbulk unload <options> # Count rows dsbulk count <options> Currently, CSV and Json formats are supported for both loading and unloading data.","title":"\ud83d\udcd8 What is DSBulk ?"},{"location":"pages/data/load/dsbulk/#datastax-bulk-loader-with-astra","text":"Use DataStax Bulk Loader (dsbulk) to load and unload data in CSV or JSON format with your DataStax Astra DB database efficiently and reliably. You can use dsbulk as a standalone tool to remotely connect to a cluster. The tool is not required to run locally on an instances, but can be used in this configuration.","title":"\ud83d\udcd8 Datastax Bulk Loader with Astra"},{"location":"pages/data/load/dsbulk/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle This article was written for Datastax Bulk Loader version 1.9.1 . Starting with version 1.9 , dsbulk can detect and respect server-side rate limiting. This is very useful when working with Astra DB, which by default has some throughput guardrails in place.","title":"B - Prerequisites"},{"location":"pages/data/load/dsbulk/#c-installation","text":"\u2705 Step 1 : Download the archive and unzip locally curl -OL https://downloads.datastax.com/dsbulk/dsbulk-1.9.1.tar.gz \\ && tar xvzf dsbulk-1.9.1.tar.gz \\ && rm -f dsbulk-1.9.1.tar.gz it will take a few seconds (file is about 30M)... % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 49 30.0M 49 14.8M 0 0 343k 0 0:01:29 0:00:44 0:00:45 244k","title":"C - Installation"},{"location":"pages/data/load/dsbulk/#d-usage","text":"","title":"D - Usage"},{"location":"pages/data/load/dsbulk/#load-data","text":"Given a table CREATE TABLE better_reads . book_by_id ( id text PRIMARY KEY , author_id list < text > , author_names list < text > , book_description text , book_name text , cover_ids list < text > , published_date date ) A sample CSV could be: id|author_id|author_names|book_description|book_name|cover_ids|published_date 1234|[\"id1\",\"id2\",\"id3\"]|[\"name1\",\"name2\",\"name3\"]|this is a dsecription|Book name|[\"cover1\",\"cover2\"]|2022-02-02 Loaded with the following command: dsbulk load \\ -url book_by_id.csv \\ -c csv \\ -delim '|' \\ -k better_reads \\ -t book_by_id \\ --schema.allowMissingFields true \\ -u clientId \\ -p clientSecret \\ -b secureBundle.zip","title":"\ud83d\udcd8 Load Data"},{"location":"pages/data/load/dsbulk/#export-data","text":"Unloaded the same table with the following command: dsbulk unload \\ -k better_reads \\ -t book_by_id \\ -c csv \\ -u clientId \\ -p clientSecret \\ -b secureBundle.zip \\ > book_by_id_export.csv","title":"\ud83d\udcd8 Export Data"},{"location":"pages/data/load/dsbulk/#count-table-records","text":"Counted the rows in the table with the following command: dsbulk count \\ -k better_reads \\ -t book_by_id \\ -u clientId \\ -p clientSecret \\ -b secureBundle.zip Produces the following output: Operation directory: /local/dsbulk-1.9.1/logs/COUNT_20220223-213637-046128 total | failed | rows/s | p50ms | p99ms | p999ms 143,475 | 0 | 87,509 | 155.34 | 511.71 | 511.71 Operation COUNT_20220223-213637-046128 completed successfully in 1 second. 143475","title":"\ud83d\udcd8 Count Table Records"},{"location":"pages/data/load/nosqlbench/","text":"A - Overview \u00b6 \u2139\ufe0f NoSQLBench documentation \u2139\ufe0f Astra Docs on NoSQLBench What is NoSQLBench ? \u00b6 NoSQLBench is a powerful, state-of-the-art tool for emulating real application workloads and direct them to actual target data stores for reliable, reproducible benchmarking. NoSQLBench is extremely customizable, yet comes with many pre-defined workloads, ready for several types of distributed, NoSQL data systems. One of the target databases is Cassandra/Astra DB, supported out-of-the-box by NoSQLBench and complemented by some ready-made realistic workloads for benchmarking. At the heart of NoSQLBench are a few principles: ease-of-use, meaning that one can start using it without learning all layers of customizability; modularity in the design: building a new driver is comparatively easy; workloads are reproducible down to the individual statement (no \"actual randomness\" involved); reliable performance timing, i.e. care is taken on the client side to avoid unexpected JVM pauses. NoSQLBench and Astra DB \u00b6 NoSQLBench uses the (CQL-based) Cassandra Java Drivers, which means that it supports Astra DB natively with its drivers. The only care is in providing access to an Astra DB instance, which is done via command-line parameters. The only care is that, while running a benchmark against a generic Cassandra installation usually entails creation of a keyspace if it does not exist, when benchmarking Astra DB you should make sure the keyspace exists already (the keyspace name can be passed as a command-line parameter when launching NoSQLBench). B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle C - Installation \u00b6 The following installation instructions are taken from the official NoSQLBench documentation . Please refer to it for more details and updates. Step 1 : Download the binaries \u00b6 Go to the releases page and download the latest version. The suggested option is to download the Linux binary ( nb ), but as an alternative the nb.jar version can also be used: here we assume the Linux binary is used, please see the NoSQLBench documentation for more on using the JAR. Step 2 : Make executable and put in search path \u00b6 Once the file is downloaded, make it executable with chmod +x nb and put it (or make a symlink) somewhere in your system's search path, such as /home/${USER}/.local/bin/ . As a quick test, try the command nb --version . D - Usage \u00b6 Command \u00b6 If you already use NoSQLBench... then all you need to know is that invocations should include the following parameters to locate an Astra DB instance and authenticate to it: nb \\ [ ... ] \\ username = CLIENT_ID \\ password = CLIENT_SECRET \\ secureconnectbundle = /PATH/TO/SECURE-CONNECT-DB.zip \\ keyspace = KEYSPACE_NAME \\ [ ... ] In the above, you should pass your Client ID and Client Secret as found in the Astra DB Token, and the path to the Secure Bundle zipfile you obtained earlier (see Prerequisites ). Please prepend ./ to the bundle path if it is a relative path. You may want to specify a keyspace, as seen in the sample command quoted here, because, as Astra DB does not support the CREATE KEYSPACE CQL command, it would be your responsibility to match the keyspace used in the benchmark with the name of an existing one. Please inspect the contents of your workload yaml file more closely for more details on the keyspace name used by default. Quick-start \u00b6 There is a comprehensive Getting Started page on NoSQLBench documentation, so here only a couple of sample full commands will be given. Please consult the full documentation for more options and configurations. Some of the ready-made workloads included with NoSQLBench are specific for benchmarking realistic usage patterns of Cassandra/Astra DB. A quick way to get started is to launch those workloads with the \"workload scenario\" syntax (where the 'scenario' specifies that you are targeting an Astra DB instance). \"cql-keyvalue\" workload The following will run the \"cql-keyvalue\" workload, specifically its \"astra\" scenario, on an Astra DB instance: nb cql-keyvalue astra \\ username = CLIENT_ID \\ password = CLIENT_SECRET \\ secureconnectbundle = /PATH/TO/SECURE-CONNECT-DB.zip \\ keyspace = KEYSPACE_NAME Alternatively, if you are using the jar version of the tool, java -jar nb.jar cql-keyvalue astra \\ username = CLIENT_ID \\ password = CLIENT_SECRET \\ secureconnectbundle = /PATH/TO/SECURE-CONNECT-DB.zip \\ keyspace = KEYSPACE_NAME This workload emulates usage of Astra DB/Cassandra as a simple key-value store, and does so by alternating \"random\" reads and writes to a single table. A preliminar \"rampup\" phase is run, consisting only of writes, and then the \"main\" phase takes place (this structure is a rather universal features of these benchmarks). (Note: most likely you may want to add further options, such as cyclerate , rampup-cycles , main-cycles or --progress console . See the NoSQLBench docs and inspect the workload yaml for more). The above command, as things progress, will produce an output similar to: cqlkeyvalue_astra_schema: 100.00%/Stopped (details: min=0 cycle=1 max=1) (last report) cqlkeyvalue_astra_rampup: 15.60%/Running (details: min=0 cycle=234 max=1500) cqlkeyvalue_astra_rampup: 32.40%/Running (details: min=0 cycle=486 max=1500) [...] cqlkeyvalue_astra_main: 96.67%/Running (details: min=0 cycle=1450 max=1500) cqlkeyvalue_astra_main: 100.00%/Running (details: min=0 cycle=1500 max=1500) (last report) followed by a final summary - reflected also in files in the logs/ directory, similar to: -- Gauges ---------------------------------------------------------------------- cqlkeyvalue_astra_main.cycles.config.burstrate value = 55.00000000000001 [...] cqlkeyvalue_astra_schema.tokenfiller count = 57086 mean rate = 941.29 calls/second 1-minute rate = 940.67 calls/second 5-minute rate = 939.91 calls/second 15-minute rate = 939.71 calls/second You may want to check that at this point a new table has been created, if it did not exist yet, in the keyspace. \"cql-iot\" workload Similar to the above for the \"cql-iot\" workload, aimed at emulating time-series-based reads and writes for a hypothetical IoT system: nb cql-iot astra \\ username = CLIENT_ID \\ password = CLIENT_SECRET \\ secureconnectbundle = /PATH/TO/SECURE-CONNECT-DB.zip \\ keyspace = KEYSPACE_NAME Other workloads You can inspect all available workloads with: nb --list-scenarios and look for astra in the output example scenario invocations there. Moreover, you can design your own workload .","title":"\u2023 NoSQL Bench"},{"location":"pages/data/load/nosqlbench/#a-overview","text":"\u2139\ufe0f NoSQLBench documentation \u2139\ufe0f Astra Docs on NoSQLBench","title":"A - Overview"},{"location":"pages/data/load/nosqlbench/#what-is-nosqlbench","text":"NoSQLBench is a powerful, state-of-the-art tool for emulating real application workloads and direct them to actual target data stores for reliable, reproducible benchmarking. NoSQLBench is extremely customizable, yet comes with many pre-defined workloads, ready for several types of distributed, NoSQL data systems. One of the target databases is Cassandra/Astra DB, supported out-of-the-box by NoSQLBench and complemented by some ready-made realistic workloads for benchmarking. At the heart of NoSQLBench are a few principles: ease-of-use, meaning that one can start using it without learning all layers of customizability; modularity in the design: building a new driver is comparatively easy; workloads are reproducible down to the individual statement (no \"actual randomness\" involved); reliable performance timing, i.e. care is taken on the client side to avoid unexpected JVM pauses.","title":"What is NoSQLBench ?"},{"location":"pages/data/load/nosqlbench/#nosqlbench-and-astra-db","text":"NoSQLBench uses the (CQL-based) Cassandra Java Drivers, which means that it supports Astra DB natively with its drivers. The only care is in providing access to an Astra DB instance, which is done via command-line parameters. The only care is that, while running a benchmark against a generic Cassandra installation usually entails creation of a keyspace if it does not exist, when benchmarking Astra DB you should make sure the keyspace exists already (the keyspace name can be passed as a command-line parameter when launching NoSQLBench).","title":"NoSQLBench and Astra DB"},{"location":"pages/data/load/nosqlbench/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle","title":"B - Prerequisites"},{"location":"pages/data/load/nosqlbench/#c-installation","text":"The following installation instructions are taken from the official NoSQLBench documentation . Please refer to it for more details and updates.","title":"C - Installation"},{"location":"pages/data/load/nosqlbench/#step-1-download-the-binaries","text":"Go to the releases page and download the latest version. The suggested option is to download the Linux binary ( nb ), but as an alternative the nb.jar version can also be used: here we assume the Linux binary is used, please see the NoSQLBench documentation for more on using the JAR.","title":"Step 1 : Download the binaries"},{"location":"pages/data/load/nosqlbench/#step-2-make-executable-and-put-in-search-path","text":"Once the file is downloaded, make it executable with chmod +x nb and put it (or make a symlink) somewhere in your system's search path, such as /home/${USER}/.local/bin/ . As a quick test, try the command nb --version .","title":"Step 2 : Make executable and put in search path"},{"location":"pages/data/load/nosqlbench/#d-usage","text":"","title":"D - Usage"},{"location":"pages/data/load/nosqlbench/#command","text":"If you already use NoSQLBench... then all you need to know is that invocations should include the following parameters to locate an Astra DB instance and authenticate to it: nb \\ [ ... ] \\ username = CLIENT_ID \\ password = CLIENT_SECRET \\ secureconnectbundle = /PATH/TO/SECURE-CONNECT-DB.zip \\ keyspace = KEYSPACE_NAME \\ [ ... ] In the above, you should pass your Client ID and Client Secret as found in the Astra DB Token, and the path to the Secure Bundle zipfile you obtained earlier (see Prerequisites ). Please prepend ./ to the bundle path if it is a relative path. You may want to specify a keyspace, as seen in the sample command quoted here, because, as Astra DB does not support the CREATE KEYSPACE CQL command, it would be your responsibility to match the keyspace used in the benchmark with the name of an existing one. Please inspect the contents of your workload yaml file more closely for more details on the keyspace name used by default.","title":"Command"},{"location":"pages/data/load/nosqlbench/#quick-start","text":"There is a comprehensive Getting Started page on NoSQLBench documentation, so here only a couple of sample full commands will be given. Please consult the full documentation for more options and configurations. Some of the ready-made workloads included with NoSQLBench are specific for benchmarking realistic usage patterns of Cassandra/Astra DB. A quick way to get started is to launch those workloads with the \"workload scenario\" syntax (where the 'scenario' specifies that you are targeting an Astra DB instance). \"cql-keyvalue\" workload The following will run the \"cql-keyvalue\" workload, specifically its \"astra\" scenario, on an Astra DB instance: nb cql-keyvalue astra \\ username = CLIENT_ID \\ password = CLIENT_SECRET \\ secureconnectbundle = /PATH/TO/SECURE-CONNECT-DB.zip \\ keyspace = KEYSPACE_NAME Alternatively, if you are using the jar version of the tool, java -jar nb.jar cql-keyvalue astra \\ username = CLIENT_ID \\ password = CLIENT_SECRET \\ secureconnectbundle = /PATH/TO/SECURE-CONNECT-DB.zip \\ keyspace = KEYSPACE_NAME This workload emulates usage of Astra DB/Cassandra as a simple key-value store, and does so by alternating \"random\" reads and writes to a single table. A preliminar \"rampup\" phase is run, consisting only of writes, and then the \"main\" phase takes place (this structure is a rather universal features of these benchmarks). (Note: most likely you may want to add further options, such as cyclerate , rampup-cycles , main-cycles or --progress console . See the NoSQLBench docs and inspect the workload yaml for more). The above command, as things progress, will produce an output similar to: cqlkeyvalue_astra_schema: 100.00%/Stopped (details: min=0 cycle=1 max=1) (last report) cqlkeyvalue_astra_rampup: 15.60%/Running (details: min=0 cycle=234 max=1500) cqlkeyvalue_astra_rampup: 32.40%/Running (details: min=0 cycle=486 max=1500) [...] cqlkeyvalue_astra_main: 96.67%/Running (details: min=0 cycle=1450 max=1500) cqlkeyvalue_astra_main: 100.00%/Running (details: min=0 cycle=1500 max=1500) (last report) followed by a final summary - reflected also in files in the logs/ directory, similar to: -- Gauges ---------------------------------------------------------------------- cqlkeyvalue_astra_main.cycles.config.burstrate value = 55.00000000000001 [...] cqlkeyvalue_astra_schema.tokenfiller count = 57086 mean rate = 941.29 calls/second 1-minute rate = 940.67 calls/second 5-minute rate = 939.91 calls/second 15-minute rate = 939.71 calls/second You may want to check that at this point a new table has been created, if it did not exist yet, in the keyspace. \"cql-iot\" workload Similar to the above for the \"cql-iot\" workload, aimed at emulating time-series-based reads and writes for a hypothetical IoT system: nb cql-iot astra \\ username = CLIENT_ID \\ password = CLIENT_SECRET \\ secureconnectbundle = /PATH/TO/SECURE-CONNECT-DB.zip \\ keyspace = KEYSPACE_NAME Other workloads You can inspect all available workloads with: nb --list-scenarios and look for astra in the output example scenario invocations there. Moreover, you can design your own workload .","title":"Quick-start"},{"location":"pages/develop/","text":"Dear Developers , so you have a running database ... now what ? In this page we provide you with the minimum amount of code needed to use Astra with the language or framework of your choice. \ud83d\udee0\ufe0f Pick an API \u00b6 Astra offers different Apis and interfaces. The choice of one against another will be driven by your use cases. \ud83d\udee0\ufe0f Pick a language \u00b6 Click the tile and learn how to interact with each language-specific interface exposed in Astra. \ud83d\udee0\ufe0f Pick a framework \u00b6","title":"List"},{"location":"pages/develop/#pick-an-api","text":"Astra offers different Apis and interfaces. The choice of one against another will be driven by your use cases.","title":"\ud83d\udee0\ufe0f Pick an API"},{"location":"pages/develop/#pick-a-language","text":"Click the tile and learn how to interact with each language-specific interface exposed in Astra.","title":"\ud83d\udee0\ufe0f Pick a language"},{"location":"pages/develop/#pick-a-framework","text":"","title":"\ud83d\udee0\ufe0f Pick a framework"},{"location":"pages/develop/api/document/","text":"1. Overview \u00b6 The Document API is an HTTP REST API and part of the open source Stargate.io . The idea is to provide an abstraction on top of Apache Cassandra\u2122 to allow document-oriented access patterns. A namespace (replacement for keyspace) will hold multiple collections (not tables) to store Documents You interact with the database through JSON documents and no validation (sometimes called _schemaless_ but a better term would be validationless). Each documents has a unique identifier within the collection. Each insert is an upsert. You can query on any field ( thanks to out of the box support for the secondary index SAI ) graph LR DB(Database) -->|1...n|NS(Namespaces) NS -->|1..n|COL(Collections) COL -->|1..n|DOC(Documents) DOC -->|1..49 Nested docs|DOC How is the data stored in Cassandra? The JSON documents are stored using an internal data model. The table schema is generic as is each collection. The algorithm used to transform the document is called document shredding . The schema is optimized for searches but also to limit tombstones on edits and deletes. create table < collection_name > ( key text , p0 text , ... p [ N ] text , bool_value boolean , txt_value text , dbl_value double , leaf text ) A JSON like {\"a\": { \"b\": 1 }, \"c\": 2} will be stored like key p0 p1 dbl*value {docid} a b 1 {docid} c _null* 2 This also works with arrays {\"a\": { \"b\": 1 }, \"c\": [{\"d\": 2}]} key p0 p1 p2 dbl_value {docid} a b null 1 {docid} c [0] d 2 Known Limitations As of today there is no aggregation or sorting available in the Document Api. Queries are paged with a pagesize of 3 records by default and you can increase up to a maximum of 20 records. Otherwise, the payload would be too large. 2. Prerequesites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token 3. Browse Api with Swagger \u00b6 3.1 Provide Database Details \u00b6 Astra DB Setup Authentication token * Database identifier * (Where find it ?) Database Region * (Where find it ?) Pick your region (GCP) asia-south1 (GCP) europe-west1 (GCP) europe-west2 (GCP) northamerica-northeast1 (GCP) southamerica-east1 (GCP) us-central1 (GCP) us-east1 (GCP) us-east4 (GCP) us-west1 (AWS) ap-southeast-1 (AWS) eu-central-1 (AWS) eu-west-1 (AWS) us-east-1 (AWS) us-east-2 (AWS) us-west-2 (Azure) northeurope (Azure) westeurope (Azure) eastus (Azure) eastus2 (Azure) southcentralus (Azure) westus2 (Azure) canadacentral (Azure) brazilsouth (Azure) centralindia (Azure) australiaeast 3.2 Use Swagger \u00b6 The swagger client below will have fields pre-populated with your database details. function setupSwagger() { window.ui = SwaggerUIBundle({ url: \"../swagger-api-document.json\", dom_id: '#swagger-ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ], plugins: [ UrlMutatorPlugin ], layout: \"StandaloneLayout\", onComplete: () => { dbSelectorBuildStargateEndpoint('ASTRA_DB_ID', 'ASTRA_DB_REGION') } }); document.querySelector(\".topbar\").hidden=true; // Add the populate field function Hook. setTimeout(hookSwagger, 100); } window.onload = setupSwagger; 4. Browse Api with Postman \u00b6 4.1 Installation \u00b6 Install Postman to import the sample collections that we have provided. You can also import the collection in Hoppscotch.io not to install anything. Postman Collection 4.2 Postman Setup \u00b6 Import the configuration File Astra_Document_Api_Configuration.json in postman. In the menu locate File > Import and drag the file in the box. Postman Configuration Edit the values for your db: Parameter Name parameter value Description token AstraCS:.... When you generate a new token it is the third field. Make sure you add enough privileges to use the APis, Database Administrator is a good choice to develop db 00000000-0000-0000-0000-00000000000 Unique identifier of your DB, you find on the main dashboard region us-east1 region name, you find on the datanase dashboard namespace demo Namespaces are the same as keyspaces. They are created with the database or added from the database dashboard: How to create a keyspace ] collection person Collection name (like table) to store one type of documents. this is what it is looks like Import the Document Api Collection Astra_Document_Api.json in postman. Same as before File > Menu That's it! You have now access to a few dozens operations for namespace , collections and documents 5. Api Sandbox with Curl \u00b6 Provide the parameters asked at the beginning and see a first set of commands in action. 6. Extra Resources \u00b6 Reference Documentation Document API reference Blogpost Design Improvements in 2021 QuickStart","title":"\u2022 Document"},{"location":"pages/develop/api/document/#1-overview","text":"The Document API is an HTTP REST API and part of the open source Stargate.io . The idea is to provide an abstraction on top of Apache Cassandra\u2122 to allow document-oriented access patterns. A namespace (replacement for keyspace) will hold multiple collections (not tables) to store Documents You interact with the database through JSON documents and no validation (sometimes called _schemaless_ but a better term would be validationless). Each documents has a unique identifier within the collection. Each insert is an upsert. You can query on any field ( thanks to out of the box support for the secondary index SAI ) graph LR DB(Database) -->|1...n|NS(Namespaces) NS -->|1..n|COL(Collections) COL -->|1..n|DOC(Documents) DOC -->|1..49 Nested docs|DOC How is the data stored in Cassandra? The JSON documents are stored using an internal data model. The table schema is generic as is each collection. The algorithm used to transform the document is called document shredding . The schema is optimized for searches but also to limit tombstones on edits and deletes. create table < collection_name > ( key text , p0 text , ... p [ N ] text , bool_value boolean , txt_value text , dbl_value double , leaf text ) A JSON like {\"a\": { \"b\": 1 }, \"c\": 2} will be stored like key p0 p1 dbl*value {docid} a b 1 {docid} c _null* 2 This also works with arrays {\"a\": { \"b\": 1 }, \"c\": [{\"d\": 2}]} key p0 p1 p2 dbl_value {docid} a b null 1 {docid} c [0] d 2 Known Limitations As of today there is no aggregation or sorting available in the Document Api. Queries are paged with a pagesize of 3 records by default and you can increase up to a maximum of 20 records. Otherwise, the payload would be too large.","title":"1. Overview"},{"location":"pages/develop/api/document/#2-prerequesites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token","title":"2. Prerequesites"},{"location":"pages/develop/api/document/#3-browse-api-with-swagger","text":"","title":"3. Browse Api with Swagger"},{"location":"pages/develop/api/document/#31-provide-database-details","text":"Astra DB Setup Authentication token * Database identifier * (Where find it ?) Database Region * (Where find it ?) Pick your region (GCP) asia-south1 (GCP) europe-west1 (GCP) europe-west2 (GCP) northamerica-northeast1 (GCP) southamerica-east1 (GCP) us-central1 (GCP) us-east1 (GCP) us-east4 (GCP) us-west1 (AWS) ap-southeast-1 (AWS) eu-central-1 (AWS) eu-west-1 (AWS) us-east-1 (AWS) us-east-2 (AWS) us-west-2 (Azure) northeurope (Azure) westeurope (Azure) eastus (Azure) eastus2 (Azure) southcentralus (Azure) westus2 (Azure) canadacentral (Azure) brazilsouth (Azure) centralindia (Azure) australiaeast","title":"3.1 Provide Database Details"},{"location":"pages/develop/api/document/#32-use-swagger","text":"The swagger client below will have fields pre-populated with your database details. function setupSwagger() { window.ui = SwaggerUIBundle({ url: \"../swagger-api-document.json\", dom_id: '#swagger-ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset ], plugins: [ UrlMutatorPlugin ], layout: \"StandaloneLayout\", onComplete: () => { dbSelectorBuildStargateEndpoint('ASTRA_DB_ID', 'ASTRA_DB_REGION') } }); document.querySelector(\".topbar\").hidden=true; // Add the populate field function Hook. setTimeout(hookSwagger, 100); } window.onload = setupSwagger;","title":"3.2 Use Swagger"},{"location":"pages/develop/api/document/#4-browse-api-with-postman","text":"","title":"4. Browse Api with Postman"},{"location":"pages/develop/api/document/#41-installation","text":"Install Postman to import the sample collections that we have provided. You can also import the collection in Hoppscotch.io not to install anything. Postman Collection","title":"4.1 Installation"},{"location":"pages/develop/api/document/#42-postman-setup","text":"Import the configuration File Astra_Document_Api_Configuration.json in postman. In the menu locate File > Import and drag the file in the box. Postman Configuration Edit the values for your db: Parameter Name parameter value Description token AstraCS:.... When you generate a new token it is the third field. Make sure you add enough privileges to use the APis, Database Administrator is a good choice to develop db 00000000-0000-0000-0000-00000000000 Unique identifier of your DB, you find on the main dashboard region us-east1 region name, you find on the datanase dashboard namespace demo Namespaces are the same as keyspaces. They are created with the database or added from the database dashboard: How to create a keyspace ] collection person Collection name (like table) to store one type of documents. this is what it is looks like Import the Document Api Collection Astra_Document_Api.json in postman. Same as before File > Menu That's it! You have now access to a few dozens operations for namespace , collections and documents","title":"4.2 Postman Setup"},{"location":"pages/develop/api/document/#5-api-sandbox-with-curl","text":"Provide the parameters asked at the beginning and see a first set of commands in action.","title":"5. Api Sandbox with Curl"},{"location":"pages/develop/api/document/#6-extra-resources","text":"Reference Documentation Document API reference Blogpost Design Improvements in 2021 QuickStart","title":"6. Extra Resources"},{"location":"pages/develop/api/graphql/","text":"Overview \u00b6 GraphQL is a query language for APIs and a runtime for fulfilling those queries with existing data. Stargate.io provides a graphQL interface which allows you to easily modify and query your table data using GraphQL types, mutations, and queries. Stargate GraphQL API supports two modes of interaction: schema-first which allows you to create idiomatic GraphQL types, mutations, and queries in a manner familiar to GraphQL developers. The schema is deployed and can be updated by deploying a new schema without recreating the tables and columns directly. cql-first which translates CQL tables into GraphQL types, mutations, and queries. The GraphQL schema is automatically generated from the keyspace, tables, and columns defined, but no customization is allowed. Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token Exploring the GraphQL API with the GraphQL playground \u00b6 A simple way to get started with GraphQL is to use the built-in GraphQL playground. The playground allows you to create new schema and interact with the GraphQL APIs. The server paths are structured to provide access to creating and querying your schemas, as well as querying and modifying your data. \u2705 Open the GraphQL Playground Open the playground from the Connect tab in the APIs section. Remember to add your token to the HTTP HEADERS at the bottom of the screen. \u2705 Creating a keyspace : Before you can start using the GraphQL API, you must first create a keyspace and at least one table in your database. If you are connecting to a database with an existing schema, you can skip this step. For this example, we will use a keyspace called library : \u2705 Creating a Table : There are three Stargate GraphQL API endpoints, one for creating schema in cql-first, one for deploying a schema in the schema-first, and the third for querying or mutating a keyspace. Schema https:// \\(ASTRA_CLUSTER_ID-\\) ASTRA_REGION.apps.astra.datastax.com:8080/api/graphql-schema Admin https:// \\(ASTRA_CLUSTER_ID-\\) ASTRA_REGION.apps.astra.datastax.com:8080/api/graphql-admin Querying https:// \\(ASTRA_CLUSTER_ID-\\) ASTRA_REGION.apps.astra.datastax.com:8080/api/graphql/{keyspace} In the graphql-schema endpoint, use this query to create a new table mutation { books: createTable( keyspaceName:\"library\", tableName:\"books\", partitionKeys: [ # The keys required to access your data { name: \"title\", type: {basic: TEXT} } ] values: [ # The values associated with the keys { name: \"author\", type: {basic: TEXT} } ] ) authors: createTable( keyspaceName:\"library\", tableName:\"authors\", partitionKeys: [ { name: \"name\", type: {basic: TEXT} } ] clusteringKeys: [ # Secondary key used to access values within the partition { name: \"title\", type: {basic: TEXT}, order: \"ASC\" } ] ) } You should see the following confirmation once the command executes. \u2705 Inserting Data : Any of the created APIs can be used to interact with the GraphQL data, to write or read data. First, let\u2019s navigate to your new keyspace library inside the playground. Switch to graphql tab and pick the url /graphql/library . Use this query mutation insert2Books { moby: insertbooks(value: {title:\"Moby Dick\", author:\"Herman Melville\"}) { value { title } } catch22: insertbooks(value: {title:\"Catch-22\", author:\"Joseph Heller\"}) { value { title } } } Don't forget to update the header again with your token details { \"x-cassandra-token\":\"your token\" } You should see that two books have been added to the table. \u2705 Querying Data : To query the data, switch to the graphql/library endpoint and execute the following query oneBook { books (value: {title:\"Moby Dick\"}) { values { title author } } } The query results will look like the following Extra Resources \u00b6 Developing with GraphQL Introduction to GraphQL Workshop","title":"\u2022 graphQL"},{"location":"pages/develop/api/graphql/#overview","text":"GraphQL is a query language for APIs and a runtime for fulfilling those queries with existing data. Stargate.io provides a graphQL interface which allows you to easily modify and query your table data using GraphQL types, mutations, and queries. Stargate GraphQL API supports two modes of interaction: schema-first which allows you to create idiomatic GraphQL types, mutations, and queries in a manner familiar to GraphQL developers. The schema is deployed and can be updated by deploying a new schema without recreating the tables and columns directly. cql-first which translates CQL tables into GraphQL types, mutations, and queries. The GraphQL schema is automatically generated from the keyspace, tables, and columns defined, but no customization is allowed.","title":"Overview"},{"location":"pages/develop/api/graphql/#prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token","title":"Prerequisites"},{"location":"pages/develop/api/graphql/#exploring-the-graphql-api-with-the-graphql-playground","text":"A simple way to get started with GraphQL is to use the built-in GraphQL playground. The playground allows you to create new schema and interact with the GraphQL APIs. The server paths are structured to provide access to creating and querying your schemas, as well as querying and modifying your data. \u2705 Open the GraphQL Playground Open the playground from the Connect tab in the APIs section. Remember to add your token to the HTTP HEADERS at the bottom of the screen. \u2705 Creating a keyspace : Before you can start using the GraphQL API, you must first create a keyspace and at least one table in your database. If you are connecting to a database with an existing schema, you can skip this step. For this example, we will use a keyspace called library : \u2705 Creating a Table : There are three Stargate GraphQL API endpoints, one for creating schema in cql-first, one for deploying a schema in the schema-first, and the third for querying or mutating a keyspace. Schema https:// \\(ASTRA_CLUSTER_ID-\\) ASTRA_REGION.apps.astra.datastax.com:8080/api/graphql-schema Admin https:// \\(ASTRA_CLUSTER_ID-\\) ASTRA_REGION.apps.astra.datastax.com:8080/api/graphql-admin Querying https:// \\(ASTRA_CLUSTER_ID-\\) ASTRA_REGION.apps.astra.datastax.com:8080/api/graphql/{keyspace} In the graphql-schema endpoint, use this query to create a new table mutation { books: createTable( keyspaceName:\"library\", tableName:\"books\", partitionKeys: [ # The keys required to access your data { name: \"title\", type: {basic: TEXT} } ] values: [ # The values associated with the keys { name: \"author\", type: {basic: TEXT} } ] ) authors: createTable( keyspaceName:\"library\", tableName:\"authors\", partitionKeys: [ { name: \"name\", type: {basic: TEXT} } ] clusteringKeys: [ # Secondary key used to access values within the partition { name: \"title\", type: {basic: TEXT}, order: \"ASC\" } ] ) } You should see the following confirmation once the command executes. \u2705 Inserting Data : Any of the created APIs can be used to interact with the GraphQL data, to write or read data. First, let\u2019s navigate to your new keyspace library inside the playground. Switch to graphql tab and pick the url /graphql/library . Use this query mutation insert2Books { moby: insertbooks(value: {title:\"Moby Dick\", author:\"Herman Melville\"}) { value { title } } catch22: insertbooks(value: {title:\"Catch-22\", author:\"Joseph Heller\"}) { value { title } } } Don't forget to update the header again with your token details { \"x-cassandra-token\":\"your token\" } You should see that two books have been added to the table. \u2705 Querying Data : To query the data, switch to the graphql/library endpoint and execute the following query oneBook { books (value: {title:\"Moby Dick\"}) { values { title author } } } The query results will look like the following","title":"Exploring the GraphQL API with the GraphQL playground"},{"location":"pages/develop/api/graphql/#extra-resources","text":"Developing with GraphQL Introduction to GraphQL Workshop","title":"Extra Resources"},{"location":"pages/develop/api/grpc/","text":"This page has not been written yet or the content has not be backported from a previous location","title":"\u2022 gRPC"},{"location":"pages/develop/api/rest/","text":"Overview \u00b6 Stargate is a data gateway (Proxy) on top of Apache Cassandra which exposes new interfaces to simplify integration in your applications. It is a way to create stateless components and ease the integration through one of four different HTTP Apis (rest, doc, graphQL, gRPC). In this chapter we will cover integration with REST Apis also called DATA in the swagger specifications. To know more regarding this interface specially you can have a look to dedicated section of the wiki or reference Stargate Rest Api Quick Start Guide . \u26a0\ufe0f We recommend using version V2 ( with V2 in the URL ) as it covers more features and V1 will be deprecated eventually. Design \u00b6 Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token Swagger Sandbox \u00b6 todo Operations \u00b6 List keyspaces private static void listKeyspaces ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { // Build Request HttpGet listKeyspacesReq = new HttpGet ( apiRestEndpoint + \"/v2/schemas/keyspaces\" ); listKeyspacesReq . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( listKeyspacesReq )) { if ( 200 == res . getCode ()) { logger . info ( \"[OK] Keyspaces list retrieved\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } Create a Table Query used is createTableJson here: { \"name\" : \"users\" , \"columnDefinitions\" : [ { \"name\" : \"firstname\" , \"typeDefinition\" : \"text\" }, { \"name\" : \"lastname\" , \"typeDefinition\" : \"text\" }, { \"name\" : \"email\" , \"typeDefinition\" : \"text\" }, { \"name\" : \"color\" , \"typeDefinition\" : \"text\" } ], \"primaryKey\" : { \"partitionKey\" : [ \"firstname\" ], \"clusteringKey\" : [ \"lastname\" ] }, \"tableOptions\" : { \"defaultTimeToLive\" : 0 , \"clusteringExpression\" : [{ \"column\" : \"lastname\" , \"order\" : \"ASC\" }] } } Create Table code private static void createTable ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { HttpPost createTableReq = new HttpPost ( apiRestEndpoint + \"/v2/schemas/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/tables\" ); createTableReq . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); String createTableJson = \"{...JSON.....}\" ; createTableReq . setEntity ( new StringEntity ( createTableJson , ContentType . APPLICATION_JSON )); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( createTableReq )) { if ( 201 == res . getCode ()) { logger . info ( \"[OK] Table Created (if needed)\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } Insert a Row private static void insertRow ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { HttpPost insertCedrick = new HttpPost ( apiRestEndpoint + \"/v2/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/users\" ); insertCedrick . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); insertCedrick . setEntity ( new StringEntity ( \"{\" + \" \\\"firstname\\\": \\\"Cedrick\\\",\" + \" \\\"lastname\\\" : \\\"Lunven\\\",\" + \" \\\"email\\\" : \\\"c.lunven@gmail.com\\\",\" + \" \\\"color\\\" : \\\"blue\\\" }\" , ContentType . APPLICATION_JSON )); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( insertCedrick )) { if ( 201 == res . getCode ()) { logger . info ( \"[OK] Row inserted\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } Retrieve a row private static void retrieveRow ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { // Build Request HttpGet rowReq = new HttpGet ( apiRestEndpoint + \"/v2/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/users/Cedrick/Lunven\" ); rowReq . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( rowReq )) { if ( 200 == res . getCode ()) { String payload = EntityUtils . toString ( res . getEntity ()); logger . info ( \"[OK] Row retrieved\" ); logger . info ( \"Row retrieved : {}\" , payload ); } } }","title":"\u2022 REST"},{"location":"pages/develop/api/rest/#overview","text":"Stargate is a data gateway (Proxy) on top of Apache Cassandra which exposes new interfaces to simplify integration in your applications. It is a way to create stateless components and ease the integration through one of four different HTTP Apis (rest, doc, graphQL, gRPC). In this chapter we will cover integration with REST Apis also called DATA in the swagger specifications. To know more regarding this interface specially you can have a look to dedicated section of the wiki or reference Stargate Rest Api Quick Start Guide . \u26a0\ufe0f We recommend using version V2 ( with V2 in the URL ) as it covers more features and V1 will be deprecated eventually.","title":"Overview"},{"location":"pages/develop/api/rest/#design","text":"","title":"Design"},{"location":"pages/develop/api/rest/#prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token","title":"Prerequisites"},{"location":"pages/develop/api/rest/#swagger-sandbox","text":"todo","title":"Swagger Sandbox"},{"location":"pages/develop/api/rest/#operations","text":"List keyspaces private static void listKeyspaces ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { // Build Request HttpGet listKeyspacesReq = new HttpGet ( apiRestEndpoint + \"/v2/schemas/keyspaces\" ); listKeyspacesReq . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( listKeyspacesReq )) { if ( 200 == res . getCode ()) { logger . info ( \"[OK] Keyspaces list retrieved\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } Create a Table Query used is createTableJson here: { \"name\" : \"users\" , \"columnDefinitions\" : [ { \"name\" : \"firstname\" , \"typeDefinition\" : \"text\" }, { \"name\" : \"lastname\" , \"typeDefinition\" : \"text\" }, { \"name\" : \"email\" , \"typeDefinition\" : \"text\" }, { \"name\" : \"color\" , \"typeDefinition\" : \"text\" } ], \"primaryKey\" : { \"partitionKey\" : [ \"firstname\" ], \"clusteringKey\" : [ \"lastname\" ] }, \"tableOptions\" : { \"defaultTimeToLive\" : 0 , \"clusteringExpression\" : [{ \"column\" : \"lastname\" , \"order\" : \"ASC\" }] } } Create Table code private static void createTable ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { HttpPost createTableReq = new HttpPost ( apiRestEndpoint + \"/v2/schemas/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/tables\" ); createTableReq . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); String createTableJson = \"{...JSON.....}\" ; createTableReq . setEntity ( new StringEntity ( createTableJson , ContentType . APPLICATION_JSON )); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( createTableReq )) { if ( 201 == res . getCode ()) { logger . info ( \"[OK] Table Created (if needed)\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } Insert a Row private static void insertRow ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { HttpPost insertCedrick = new HttpPost ( apiRestEndpoint + \"/v2/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/users\" ); insertCedrick . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); insertCedrick . setEntity ( new StringEntity ( \"{\" + \" \\\"firstname\\\": \\\"Cedrick\\\",\" + \" \\\"lastname\\\" : \\\"Lunven\\\",\" + \" \\\"email\\\" : \\\"c.lunven@gmail.com\\\",\" + \" \\\"color\\\" : \\\"blue\\\" }\" , ContentType . APPLICATION_JSON )); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( insertCedrick )) { if ( 201 == res . getCode ()) { logger . info ( \"[OK] Row inserted\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } Retrieve a row private static void retrieveRow ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { // Build Request HttpGet rowReq = new HttpGet ( apiRestEndpoint + \"/v2/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/users/Cedrick/Lunven\" ); rowReq . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( rowReq )) { if ( 200 == res . getCode ()) { String payload = EntityUtils . toString ( res . getEntity ()); logger . info ( \"[OK] Row retrieved\" ); logger . info ( \"Row retrieved : {}\" , payload ); } } }","title":"Operations"},{"location":"pages/develop/frameworks/fastapi/","text":"FastAPI framework, high performance, easy to learn, fast to code, ready for production. FastAPI is a modern, fast web framework for building APIs with Python 3.6+ based on standard Python type hints. FastAPI strives to minimize boilerplate and maximize performance. To get more information regarding the framework visit the reference website fastapi.tiangolo.com . 1. Overview \u00b6 This guide, and the accompanying sample code, highlight the practices and the patterns to best integrate FastAPI with Astra DB to use the latter as backing storage. Two important choices are made in the following: Astra DB is accessed with the Python driver; no Object Mappers are used, just plain simple CQL statements. 2. FastAPI and Astra DB \u00b6 The goal is to provide access to one or more tables stored in Astra DB to the FastAPI endpoint functions, so that the API can write data to them and read from them. This should be done keeping in mind the best practices for using the Cassandra drivers, and in as flexible and concise way as possible. Session \u00b6 Virtually every endpoint needs access to the database. On the other hand, the driver's cassandra.cluster.Session is a stateful, resource-intensive object that should be created once and re-used throughout the life cycle of the Python process. For this reason (file storage/db_connect.py in the sample app) there is a get_session() function that keeps a globally-cached session object and returns it any time it is called, in a singleton fashion. On its first invocation, of course, the session is created in the idiomatic way, looking for the necessary connection parameters from a .env file. This file contains secrets, so it should never be checked in to version control. Note . If the application runs on regular Cassandra (as opposed to Astra DB), this is the only part of the code that would change: the parameters for instantiating the Cluster and the Session would differ slightly. Since it is a good practice to explicitly free resources once we're done, in this module there's also a shutdown hook that takes care of cleanup by closing the session and shutting down the Cluster object. (which for a FastAPI application that runs indefinitely is a bit of a moot point, but still illustrates the point). Endpoint dependencies \u00b6 Next comes the task of making the session object available to the function endpoints: these will need to retrieve rows and/or write them, after all. Taking advantage of FastAPI's advanced dependency injection facilities, one can add a Depends parameter to the endpoint functions , which will be automatically resolved when the function gets executed. This makes the session available to the function: @app . get ( '/animal/ {genus} ' ) async def get_animals ( genus , session = Depends ( g_get_session )): # etc, etc ... The argument of Depends is a function itself, more precisely an async generator, which must yield the session object. For this reason there is a thin wrapper function that, in practice, promotes the ordinary function get_session to a generator with the desired signature: async def g_get_session (): yield get_session () At this point, FastAPI takes care of the wiring. What is still missing is the business logic itself, i.e. what happens within the endpoint functions. Prepared statements \u00b6 It is a good practice to keep the code in the endpoint function short and not to embed much logic into it, except for the handling of the request-response cycle itself. For this reason, each endpoint function in turn invokes a function in the storage/db_io.py module, which is where the actual database operations are executed. In this module another important observation is in order: since it is expected that the API endpoints will be called many times, the corrsponding CQL statements are made into \"prepared statements\" once and then re-used over and over. To achieve that, the db_io.py module holds a cache of prepared statements, one per different type of database query. This cache ( prepared_cache ) is filled on the first invocation of each endpoint, but after that there is a sizable gain in performance and reduction of overhead for all subsequent calls. Streaming a large response from DB \u00b6 In some cases, an API endpoint may return a large response (such as a GET returning a long list of items). It might be unwieldy, and suboptimal, to retrieve the full list at API level and then prepare a whole response string to return to the caller. Ideally, one would like to start sending out the response as the data keeps coming in (to the API) from the database. This is exactly what StreamingResponse makes possible . The Cassandra driver handles pagination of large result sets transparently: regardless of the grouping of rows into pages, at the Python-code level all you see is a homogeneous iterable over all rows. This means that one can simply make the corresponding data-retrieval function a generator almost with no changes in the code. Things get slightly more tricky on the other side, that is, between the endpoint function and the caller. Fortunately, FastAPI offers the StreamingResponse construct that makes it possible to \"consume\" a generator and return its components as a \"Chunked\" type of response. The client will still receive a full response (and will be able to start processing it once it is there in full), but never throughout the live of the request will there be \"the full thing\" on the API side. But beware: in this case, the endpoint function will need to manually construct \"pieces of a syntactically valid JSON\". In the sample app, this is achieved by a format_streaming_response function which takes care of the opening/closing square brackets for a list and of the correct placement of the commas. In practice, this function makes a generator over homogeneous items into a generator returning something like (row-by-row; note the commas): 1. [ 2. {\"a\": 1, \"b\": 100} 3. ,{\"a\": 2, \"b\": 200} 4. ,{\"a\": 3, \"b\": 300} 5. ,{\"a\": 4, \"b\": 400} 6. ] 3. Reference application \u00b6 You can clone the reference application coming with this page and run it in minutes, provided you have an Astra DB instance (click here to create one). The setup instructions are outlined below: for more details, refer to the repo's README . Setup \u00b6 An Astra DB instance, with corresponding Token and Secure connect bundle, are required to run this app: make sure you have them at your disposal. Once you cloned the repository, create the .env file with the required secrets and (preferrably in a Python virtual environment) install all dependencies with pip install -r requirements.txt . In order to populate the database (table creation and insertion of sample rows), you should run once the script python storage/db_initialize.py . You are now ready to run the API. Run sample app \u00b6 Running the API is as simple as uvicorn api:app You can now issue requests to it. Look in the repo's README for example requests, testing all provided endpoints, as curl commands (of course you can use any tool you like, such as Postman, to achieve the same effect).","title":"\u2022 FastAPI"},{"location":"pages/develop/frameworks/fastapi/#1-overview","text":"This guide, and the accompanying sample code, highlight the practices and the patterns to best integrate FastAPI with Astra DB to use the latter as backing storage. Two important choices are made in the following: Astra DB is accessed with the Python driver; no Object Mappers are used, just plain simple CQL statements.","title":"1. Overview"},{"location":"pages/develop/frameworks/fastapi/#2-fastapi-and-astra-db","text":"The goal is to provide access to one or more tables stored in Astra DB to the FastAPI endpoint functions, so that the API can write data to them and read from them. This should be done keeping in mind the best practices for using the Cassandra drivers, and in as flexible and concise way as possible.","title":"2. FastAPI and Astra DB"},{"location":"pages/develop/frameworks/fastapi/#session","text":"Virtually every endpoint needs access to the database. On the other hand, the driver's cassandra.cluster.Session is a stateful, resource-intensive object that should be created once and re-used throughout the life cycle of the Python process. For this reason (file storage/db_connect.py in the sample app) there is a get_session() function that keeps a globally-cached session object and returns it any time it is called, in a singleton fashion. On its first invocation, of course, the session is created in the idiomatic way, looking for the necessary connection parameters from a .env file. This file contains secrets, so it should never be checked in to version control. Note . If the application runs on regular Cassandra (as opposed to Astra DB), this is the only part of the code that would change: the parameters for instantiating the Cluster and the Session would differ slightly. Since it is a good practice to explicitly free resources once we're done, in this module there's also a shutdown hook that takes care of cleanup by closing the session and shutting down the Cluster object. (which for a FastAPI application that runs indefinitely is a bit of a moot point, but still illustrates the point).","title":"Session"},{"location":"pages/develop/frameworks/fastapi/#endpoint-dependencies","text":"Next comes the task of making the session object available to the function endpoints: these will need to retrieve rows and/or write them, after all. Taking advantage of FastAPI's advanced dependency injection facilities, one can add a Depends parameter to the endpoint functions , which will be automatically resolved when the function gets executed. This makes the session available to the function: @app . get ( '/animal/ {genus} ' ) async def get_animals ( genus , session = Depends ( g_get_session )): # etc, etc ... The argument of Depends is a function itself, more precisely an async generator, which must yield the session object. For this reason there is a thin wrapper function that, in practice, promotes the ordinary function get_session to a generator with the desired signature: async def g_get_session (): yield get_session () At this point, FastAPI takes care of the wiring. What is still missing is the business logic itself, i.e. what happens within the endpoint functions.","title":"Endpoint dependencies"},{"location":"pages/develop/frameworks/fastapi/#prepared-statements","text":"It is a good practice to keep the code in the endpoint function short and not to embed much logic into it, except for the handling of the request-response cycle itself. For this reason, each endpoint function in turn invokes a function in the storage/db_io.py module, which is where the actual database operations are executed. In this module another important observation is in order: since it is expected that the API endpoints will be called many times, the corrsponding CQL statements are made into \"prepared statements\" once and then re-used over and over. To achieve that, the db_io.py module holds a cache of prepared statements, one per different type of database query. This cache ( prepared_cache ) is filled on the first invocation of each endpoint, but after that there is a sizable gain in performance and reduction of overhead for all subsequent calls.","title":"Prepared statements"},{"location":"pages/develop/frameworks/fastapi/#streaming-a-large-response-from-db","text":"In some cases, an API endpoint may return a large response (such as a GET returning a long list of items). It might be unwieldy, and suboptimal, to retrieve the full list at API level and then prepare a whole response string to return to the caller. Ideally, one would like to start sending out the response as the data keeps coming in (to the API) from the database. This is exactly what StreamingResponse makes possible . The Cassandra driver handles pagination of large result sets transparently: regardless of the grouping of rows into pages, at the Python-code level all you see is a homogeneous iterable over all rows. This means that one can simply make the corresponding data-retrieval function a generator almost with no changes in the code. Things get slightly more tricky on the other side, that is, between the endpoint function and the caller. Fortunately, FastAPI offers the StreamingResponse construct that makes it possible to \"consume\" a generator and return its components as a \"Chunked\" type of response. The client will still receive a full response (and will be able to start processing it once it is there in full), but never throughout the live of the request will there be \"the full thing\" on the API side. But beware: in this case, the endpoint function will need to manually construct \"pieces of a syntactically valid JSON\". In the sample app, this is achieved by a format_streaming_response function which takes care of the opening/closing square brackets for a list and of the correct placement of the commas. In practice, this function makes a generator over homogeneous items into a generator returning something like (row-by-row; note the commas): 1. [ 2. {\"a\": 1, \"b\": 100} 3. ,{\"a\": 2, \"b\": 200} 4. ,{\"a\": 3, \"b\": 300} 5. ,{\"a\": 4, \"b\": 400} 6. ]","title":"Streaming a large response from DB"},{"location":"pages/develop/frameworks/fastapi/#3-reference-application","text":"You can clone the reference application coming with this page and run it in minutes, provided you have an Astra DB instance (click here to create one). The setup instructions are outlined below: for more details, refer to the repo's README .","title":"3. Reference application"},{"location":"pages/develop/frameworks/fastapi/#setup","text":"An Astra DB instance, with corresponding Token and Secure connect bundle, are required to run this app: make sure you have them at your disposal. Once you cloned the repository, create the .env file with the required secrets and (preferrably in a Python virtual environment) install all dependencies with pip install -r requirements.txt . In order to populate the database (table creation and insertion of sample rows), you should run once the script python storage/db_initialize.py . You are now ready to run the API.","title":"Setup"},{"location":"pages/develop/frameworks/fastapi/#run-sample-app","text":"Running the API is as simple as uvicorn api:app You can now issue requests to it. Look in the repo's README for example requests, testing all provided endpoints, as curl commands (of course you can use any tool you like, such as Postman, to achieve the same effect).","title":"Run sample app"},{"location":"pages/develop/frameworks/flask/","text":"Flask - Web development, one drop at a time. Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. It began as a simple wrapper around Werkzeug and Jinja and has become one of the most popular Python web application frameworks. To get more information regarding the framework visit the reference website flask.palletsprojects.com . 1. Overview \u00b6 This guide, and the accompanying sample code, highlight the practices and the patterns to best integrate Flask with Astra DB to use the latter as backing storage. Two important choices are made in the following: Astra DB is accessed with the Python driver; no Object Mappers are used, just plain simple CQL statements. 2. Flask and Astra DB \u00b6 The goal is to provide access to one or more tables stored in Astra DB to the Flask endpoint functions, so that the API can write data to them and read from them. This should be done keeping in mind the best practices for using the Cassandra drivers, and in as flexible and concise way as possible. Session \u00b6 Virtually every endpoint needs access to the database. On the other hand, the driver's cassandra.cluster.Session is a stateful, resource-intensive object that should be created once and re-used throughout the life cycle of the Python process. For this reason (file storage/db_connect.py in the sample app) there is a get_session() function that keeps a globally-cached session object and returns it any time it is called, in a singleton fashion. On its first invocation, of course, the session is created in the idiomatic way, looking for the necessary connection parameters from a .env file. This file contains secrets, so it should never be checked in to version control. Note . If the application runs on regular Cassandra (as opposed to Astra DB), this is the only part of the code that would change: the parameters for instantiating the Cluster and the Session would differ slightly. Since it is a good practice to explicitly free resources once we're done, in this module there's also a shutdown hook that takes care of cleanup by closing the session and shutting down the Cluster object. (which for a FastAPI application that runs indefinitely is a bit of a moot point, but still illustrates the point). Endpoint dependencies \u00b6 Next comes the task of making the session object available to the function endpoints: these will need to retrieve rows and/or write them, after all. Typically all API endpoints need to access the database: in this case, the easiest way is to use Flask's application-context g object and the before_request hook to make sure each request will find a reference to the session. Once the pre-request hook is set up with @app . before_request def get_db_session (): g . session = get_session () all endpoints will be able to read and use g.session as they need. Remember that get_session() does not create a new Cassandra Session object at each invocation! @app . route ( '/animal/<genus>' ) def get_animals ( genus ): animals = retrieve_animals_by_genus ( g . session , genus ) return jsonify ([ animal . dict () for animal in animals ]) Prepared statements \u00b6 It is a good practice to keep the code in the endpoint function short and not to embed much logic into it, except for the handling of the request-response cycle itself. For this reason, each endpoint function in turn invokes a function in the storage/db_io.py module, which is where the actual database operations are executed. In this module another important observation is in order: since it is expected that the API endpoints will be called many times, the corrsponding CQL statements are made into \"prepared statements\" once and then re-used over and over. To achieve that, the db_io.py module holds a cache of prepared statements, one per different type of database query. This cache ( prepared_cache ) is filled on the first invocation of each endpoint, but after that there is a sizable gain in performance and reduction of overhead for all subsequent calls. Streaming a large response from DB \u00b6 In some cases, an API endpoint may return a large response (such as a GET returning a long list of items). It might be unwieldy, and suboptimal, to retrieve the full list at API level and then prepare a whole response string to return to the caller. Ideally, one would like to start sending out the response as the data keeps coming in (to the API) from the database. This is exactly what Flask's \"streaming responses\" make possible . The Cassandra driver handles pagination of large result sets transparently: regardless of the grouping of rows into pages, at the Python-code level all you see is a homogeneous iterable over all rows. This means that one can simply make the corresponding data-retrieval function a generator almost with no changes in the code. Things get slightly more tricky on the other side, that is, between the endpoint function and the caller. Fortunately, Flask endpoint functions may return a (generator, headers) pair and will construct, from this, a \"Chunked\" response which will be sent to the caller piecewise, as the generator gets consumed. The client will still receive a full response (and will be able to start processing it once it is there in full), but never throughout the live of the request will there be \"the full thing\" on the API side. But beware: in this case, the endpoint function will need to manually construct \"pieces of a syntactically valid JSON\". In the sample app, this is achieved by a format_streaming_response function which takes care of the opening/closing square brackets for a list and of the correct placement of the commas. In practice, this function makes a generator over homogeneous items into a generator returning something like (row-by-row; note the commas): 1. [ 2. {\"a\": 1, \"b\": 100} 3. ,{\"a\": 2, \"b\": 200} 4. ,{\"a\": 3, \"b\": 300} 5. ,{\"a\": 4, \"b\": 400} 6. ] Pydantic usage \u00b6 The reference API uses Pydantic for validation and handling of request/response data types (this is not strictly necessary, but very handy). However, this requires some manual plumbing, as can be seen in the code in two ways: First, to validate/cast the POST request payload, a animal = Animal(**request.get_json()) is wrapped in a try/except construct , in order to return a meaningful error (from Pydantic) and a status 422 (\"unprocessable entity\") if anything is off. Note : request is a Flask abstraction Moreover, when returning responses, one must explicitly jsonify not directly the Pydantic object, rather its .dict() representation. So, for example, return jsonify(animal.dict()) . Note : jsonify is a Flask primitive, whole .dict() is a built-in method for Pydantic models. 3. Reference application \u00b6 You can clone the reference application coming with this page and run it in minutes, provided you have an Astra DB instance (click here to create one). The setup instructions are outlined below: for more details, refer to the repo's README . Setup \u00b6 An Astra DB instance, with corresponding Token and Secure connect bundle, are required to run this app: make sure you have them at your disposal. Once you cloned the repository, create the .env file with the required secrets and (preferrably in a Python virtual environment) install all dependencies with pip install -r requirements.txt . In order to populate the database (table creation and insertion of sample rows), you should run once the script python storage/db_initialize.py . You are now ready to run the API. Run sample app \u00b6 Running the API is as simple as flask --app api run --reload You can now issue requests to it. Look in the repo's README for example requests, testing all provided endpoints, as curl commands (of course you can use any tool you like, such as Postman, to achieve the same effect).","title":"\u2022 Flask"},{"location":"pages/develop/frameworks/flask/#1-overview","text":"This guide, and the accompanying sample code, highlight the practices and the patterns to best integrate Flask with Astra DB to use the latter as backing storage. Two important choices are made in the following: Astra DB is accessed with the Python driver; no Object Mappers are used, just plain simple CQL statements.","title":"1. Overview"},{"location":"pages/develop/frameworks/flask/#2-flask-and-astra-db","text":"The goal is to provide access to one or more tables stored in Astra DB to the Flask endpoint functions, so that the API can write data to them and read from them. This should be done keeping in mind the best practices for using the Cassandra drivers, and in as flexible and concise way as possible.","title":"2. Flask and Astra DB"},{"location":"pages/develop/frameworks/flask/#session","text":"Virtually every endpoint needs access to the database. On the other hand, the driver's cassandra.cluster.Session is a stateful, resource-intensive object that should be created once and re-used throughout the life cycle of the Python process. For this reason (file storage/db_connect.py in the sample app) there is a get_session() function that keeps a globally-cached session object and returns it any time it is called, in a singleton fashion. On its first invocation, of course, the session is created in the idiomatic way, looking for the necessary connection parameters from a .env file. This file contains secrets, so it should never be checked in to version control. Note . If the application runs on regular Cassandra (as opposed to Astra DB), this is the only part of the code that would change: the parameters for instantiating the Cluster and the Session would differ slightly. Since it is a good practice to explicitly free resources once we're done, in this module there's also a shutdown hook that takes care of cleanup by closing the session and shutting down the Cluster object. (which for a FastAPI application that runs indefinitely is a bit of a moot point, but still illustrates the point).","title":"Session"},{"location":"pages/develop/frameworks/flask/#endpoint-dependencies","text":"Next comes the task of making the session object available to the function endpoints: these will need to retrieve rows and/or write them, after all. Typically all API endpoints need to access the database: in this case, the easiest way is to use Flask's application-context g object and the before_request hook to make sure each request will find a reference to the session. Once the pre-request hook is set up with @app . before_request def get_db_session (): g . session = get_session () all endpoints will be able to read and use g.session as they need. Remember that get_session() does not create a new Cassandra Session object at each invocation! @app . route ( '/animal/<genus>' ) def get_animals ( genus ): animals = retrieve_animals_by_genus ( g . session , genus ) return jsonify ([ animal . dict () for animal in animals ])","title":"Endpoint dependencies"},{"location":"pages/develop/frameworks/flask/#prepared-statements","text":"It is a good practice to keep the code in the endpoint function short and not to embed much logic into it, except for the handling of the request-response cycle itself. For this reason, each endpoint function in turn invokes a function in the storage/db_io.py module, which is where the actual database operations are executed. In this module another important observation is in order: since it is expected that the API endpoints will be called many times, the corrsponding CQL statements are made into \"prepared statements\" once and then re-used over and over. To achieve that, the db_io.py module holds a cache of prepared statements, one per different type of database query. This cache ( prepared_cache ) is filled on the first invocation of each endpoint, but after that there is a sizable gain in performance and reduction of overhead for all subsequent calls.","title":"Prepared statements"},{"location":"pages/develop/frameworks/flask/#streaming-a-large-response-from-db","text":"In some cases, an API endpoint may return a large response (such as a GET returning a long list of items). It might be unwieldy, and suboptimal, to retrieve the full list at API level and then prepare a whole response string to return to the caller. Ideally, one would like to start sending out the response as the data keeps coming in (to the API) from the database. This is exactly what Flask's \"streaming responses\" make possible . The Cassandra driver handles pagination of large result sets transparently: regardless of the grouping of rows into pages, at the Python-code level all you see is a homogeneous iterable over all rows. This means that one can simply make the corresponding data-retrieval function a generator almost with no changes in the code. Things get slightly more tricky on the other side, that is, between the endpoint function and the caller. Fortunately, Flask endpoint functions may return a (generator, headers) pair and will construct, from this, a \"Chunked\" response which will be sent to the caller piecewise, as the generator gets consumed. The client will still receive a full response (and will be able to start processing it once it is there in full), but never throughout the live of the request will there be \"the full thing\" on the API side. But beware: in this case, the endpoint function will need to manually construct \"pieces of a syntactically valid JSON\". In the sample app, this is achieved by a format_streaming_response function which takes care of the opening/closing square brackets for a list and of the correct placement of the commas. In practice, this function makes a generator over homogeneous items into a generator returning something like (row-by-row; note the commas): 1. [ 2. {\"a\": 1, \"b\": 100} 3. ,{\"a\": 2, \"b\": 200} 4. ,{\"a\": 3, \"b\": 300} 5. ,{\"a\": 4, \"b\": 400} 6. ]","title":"Streaming a large response from DB"},{"location":"pages/develop/frameworks/flask/#pydantic-usage","text":"The reference API uses Pydantic for validation and handling of request/response data types (this is not strictly necessary, but very handy). However, this requires some manual plumbing, as can be seen in the code in two ways: First, to validate/cast the POST request payload, a animal = Animal(**request.get_json()) is wrapped in a try/except construct , in order to return a meaningful error (from Pydantic) and a status 422 (\"unprocessable entity\") if anything is off. Note : request is a Flask abstraction Moreover, when returning responses, one must explicitly jsonify not directly the Pydantic object, rather its .dict() representation. So, for example, return jsonify(animal.dict()) . Note : jsonify is a Flask primitive, whole .dict() is a built-in method for Pydantic models.","title":"Pydantic usage"},{"location":"pages/develop/frameworks/flask/#3-reference-application","text":"You can clone the reference application coming with this page and run it in minutes, provided you have an Astra DB instance (click here to create one). The setup instructions are outlined below: for more details, refer to the repo's README .","title":"3. Reference application"},{"location":"pages/develop/frameworks/flask/#setup","text":"An Astra DB instance, with corresponding Token and Secure connect bundle, are required to run this app: make sure you have them at your disposal. Once you cloned the repository, create the .env file with the required secrets and (preferrably in a Python virtual environment) install all dependencies with pip install -r requirements.txt . In order to populate the database (table creation and insertion of sample rows), you should run once the script python storage/db_initialize.py . You are now ready to run the API.","title":"Setup"},{"location":"pages/develop/frameworks/flask/#run-sample-app","text":"Running the API is as simple as flask --app api run --reload You can now issue requests to it. Look in the repo's README for example requests, testing all provided endpoints, as curl commands (of course you can use any tool you like, such as Postman, to achieve the same effect).","title":"Run sample app"},{"location":"pages/develop/frameworks/lagom/","text":"Working with Lagom \u00b6 Lagom is an open source framework for building out Reactive microservices. Lagom essentially wires-up your services, freeing you from having to spend time writing lots of \"boiler-plate\" code. To get more information regarding the framework visit its website @ lagomframework.com . \ud83d\udce6. Prerequisites [ASTRA DB] \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token \ud83d\udce6. Prerequisites [Development Environment] \u00b6 You should install the Java Development Kit (JDK) , of at least version 8: Use the reference documentation to install a Java Development Kit . Java 8 Java 11 Java 17 Validate your installation with: java --version You should install Apache Maven : Use the reference documentation and validate your installation with: mvn -version Building a sample Lagom project \u00b6 Create a new Maven project in your IDE which uses the maven-archetype-lagom-java archetype. You can also do this from the command line: mvn archetype:generate -DarchetypeGroupId = com.lightbend.lagom \\ -DarchetypeArtifactId = maven-archetype-lagom-java -DarchetypeVersion = 1 .2.0 This will create a sample Lagom project with two services: Hello and Stream. The project can be built and run with this command: mvn lagom:runAll Note that by default, Lagom will start using embedded Cassandra as its data store, running on localhost:4000 . Port 4000 was chosen (instead of 9042) so as not to collide with another instance of Cassandra running locally. Using with Apache Cassandra \u00b6 To get Lagom to connect to Cassandra (for local development) there are two places which need changes: Maven's pom.xml and the services' application.conf files. Inside the pom.xml file, locate the lagom-maven-plugin and make the following adjustments: <plugin> <groupId> com.lightbend.lagom </groupId> <artifactId> lagom-maven-plugin </artifactId> <version> ${lagom.version} </version> <configuration> <unmanagedServices> <cas_native> localhost:9042 </cas_native> </unmanagedServices> <cassandraEnabled> false </cassandraEnabled> </configuration> </plugin> That these settings perform two functions: - Disables Lagom's embedded Cassandra, causing it not to start. - Informs Lagom designate Cassandra as an \"unmanaged\" service, and provides it with the contact point for the cluster/instance. If your local Cassandra does not use SSL or authentication, then you are finished. But if your local Cassandra does have security enabled, you'll want to make these changes to each of your services' application.conf files: your-service.cassandra { authentication { username = \"yourUserName\" password = \"yourPassword\" } ssl { truststore.path = \"/Users/youruser/cassandra/truststore.jks\" truststore.password = \"yourTrustStorePassword\" keystore.path = \"/Users/youruser/cassandra/keystore.jks\" keystore.password = \"yourKeyStorePassword\" } keyspace = your_service } cassandra-journal { keyspace = ${your-service.cassandra.keyspace} authentication = ${your-service.cassandra.authentication} ssl = ${your-service.cassandra.ssl} } cassandra-snapshot-store { keyspace = ${your-service.cassandra.keyspace} authentication = ${your-service.cassandra.authentication} ssl = ${your-service.cassandra.ssl} } lagom.persistence.read-side.cassandra { keyspace = ${your-service.cassandra.keyspace} authentication = ${your-service.cassandra.authentication} ssl = ${your-service.cassandra.ssl} } These settings will allow Lagom to connect to your local Cassandra with authentication and client-to-node SSL. If you're only using auth, simply remove the config lines containing ssl . Using with DataStax Astra DB \u00b6 For connecting to DataStax Astra DB, it is similar. You will need to set both authentication and SSL to connect with Astra DB, as well as a few additional properties. The pom.xml is largely the same, except that you'll need to add your Astra host name here. Note that in \"production mode,\" you should not need to modify this file. But if you're connecting to an Astra DB cluster in \"development mode,\" you'll still need to disable embedded Cassandra and designate the Cassandra service as \"unmanaged\" with a contact point: <configuration> <unmanagedServices> <cas_native> https://ASTRA_DB_ID-ASTRA_DB_REGION.db.astra.datastax.com:29042 </cas_native> </unmanagedServices> <cassandraEnabled> false </cassandraEnabled> </configuration> The application.conf service files will require similar modifications: stream.cassandra { contact-points = [\"ASTRA_DB_ID-ASTRA_DB_REGION.db.astra.datastax.com:29042\"] authentication { username = \"token\" password = \"AstraCS:yourAstraT0ken\" } ssl { truststore.path = \"/Users/youruser/astradb/trustStore.jks\" truststore.password = \"yourTrustStorePassword\" keystore.path = \"/Users/youruser/stackoverflow/identity.jks\" keystore.password = \"Tte3jRy07ocEf6Z8h\" } session-provider = akka.persistence.cassandra.ConfigSessionProvider keyspace = \"stream\" } cassandra-journal { contact-points = ${stream.cassandra.contact-points} keyspace = ${stream.cassandra.keyspace} authentication = ${stream.cassandra.authentication} ssl = ${stream.cassandra.ssl} session-provider = ${stream.cassandra.session-provider} keyspace-autocreate = false tables-autocreate = true } cassandra-snapshot-store { contact-points = ${stream.cassandra.contact-points} keyspace = ${stream.cassandra.keyspace} authentication = ${stream.cassandra.authentication} ssl = ${stream.cassandra.ssl} session-provider = ${stream.cassandra.session-provider} keyspace-autocreate = false tables-autocreate = true } lagom.persistence.read-side.cassandra { contact-points = ${stream.cassandra.contact-points} keyspace = ${stream.cassandra.keyspace} authentication = ${stream.cassandra.authentication} ssl = ${stream.cassandra.ssl} session-provider = ${stream.cassandra.session-provider} keyspace-autocreate = false tables-autocreate = true } Note the options for keyspace-autocreate and tables-autocreate are shown set here. By default, these are both set to true . However, Astra DB only permits keyspace creation to happen via the Astra Dashboard. This means that: Keyspaces must be created before connecting a Lagom microservice to Astra DB. Lagom's attempts to create keyspaces will fail (due to a permissions error). Table creation should function appropriately, assuming the required keyspaces already exist. \ud83c\udfe0 Back to home","title":"\u2022 Lagom"},{"location":"pages/develop/frameworks/lagom/#working-with-lagom","text":"Lagom is an open source framework for building out Reactive microservices. Lagom essentially wires-up your services, freeing you from having to spend time writing lots of \"boiler-plate\" code. To get more information regarding the framework visit its website @ lagomframework.com .","title":"Working with Lagom"},{"location":"pages/develop/frameworks/lagom/#prerequisites-astra-db","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token","title":"\ud83d\udce6. Prerequisites [ASTRA DB]"},{"location":"pages/develop/frameworks/lagom/#prerequisites-development-environment","text":"You should install the Java Development Kit (JDK) , of at least version 8: Use the reference documentation to install a Java Development Kit . Java 8 Java 11 Java 17 Validate your installation with: java --version You should install Apache Maven : Use the reference documentation and validate your installation with: mvn -version","title":"\ud83d\udce6. Prerequisites [Development Environment]"},{"location":"pages/develop/frameworks/lagom/#building-a-sample-lagom-project","text":"Create a new Maven project in your IDE which uses the maven-archetype-lagom-java archetype. You can also do this from the command line: mvn archetype:generate -DarchetypeGroupId = com.lightbend.lagom \\ -DarchetypeArtifactId = maven-archetype-lagom-java -DarchetypeVersion = 1 .2.0 This will create a sample Lagom project with two services: Hello and Stream. The project can be built and run with this command: mvn lagom:runAll Note that by default, Lagom will start using embedded Cassandra as its data store, running on localhost:4000 . Port 4000 was chosen (instead of 9042) so as not to collide with another instance of Cassandra running locally.","title":"Building a sample Lagom project"},{"location":"pages/develop/frameworks/lagom/#using-with-apache-cassandra","text":"To get Lagom to connect to Cassandra (for local development) there are two places which need changes: Maven's pom.xml and the services' application.conf files. Inside the pom.xml file, locate the lagom-maven-plugin and make the following adjustments: <plugin> <groupId> com.lightbend.lagom </groupId> <artifactId> lagom-maven-plugin </artifactId> <version> ${lagom.version} </version> <configuration> <unmanagedServices> <cas_native> localhost:9042 </cas_native> </unmanagedServices> <cassandraEnabled> false </cassandraEnabled> </configuration> </plugin> That these settings perform two functions: - Disables Lagom's embedded Cassandra, causing it not to start. - Informs Lagom designate Cassandra as an \"unmanaged\" service, and provides it with the contact point for the cluster/instance. If your local Cassandra does not use SSL or authentication, then you are finished. But if your local Cassandra does have security enabled, you'll want to make these changes to each of your services' application.conf files: your-service.cassandra { authentication { username = \"yourUserName\" password = \"yourPassword\" } ssl { truststore.path = \"/Users/youruser/cassandra/truststore.jks\" truststore.password = \"yourTrustStorePassword\" keystore.path = \"/Users/youruser/cassandra/keystore.jks\" keystore.password = \"yourKeyStorePassword\" } keyspace = your_service } cassandra-journal { keyspace = ${your-service.cassandra.keyspace} authentication = ${your-service.cassandra.authentication} ssl = ${your-service.cassandra.ssl} } cassandra-snapshot-store { keyspace = ${your-service.cassandra.keyspace} authentication = ${your-service.cassandra.authentication} ssl = ${your-service.cassandra.ssl} } lagom.persistence.read-side.cassandra { keyspace = ${your-service.cassandra.keyspace} authentication = ${your-service.cassandra.authentication} ssl = ${your-service.cassandra.ssl} } These settings will allow Lagom to connect to your local Cassandra with authentication and client-to-node SSL. If you're only using auth, simply remove the config lines containing ssl .","title":"Using with Apache Cassandra"},{"location":"pages/develop/frameworks/lagom/#using-with-datastax-astra-db","text":"For connecting to DataStax Astra DB, it is similar. You will need to set both authentication and SSL to connect with Astra DB, as well as a few additional properties. The pom.xml is largely the same, except that you'll need to add your Astra host name here. Note that in \"production mode,\" you should not need to modify this file. But if you're connecting to an Astra DB cluster in \"development mode,\" you'll still need to disable embedded Cassandra and designate the Cassandra service as \"unmanaged\" with a contact point: <configuration> <unmanagedServices> <cas_native> https://ASTRA_DB_ID-ASTRA_DB_REGION.db.astra.datastax.com:29042 </cas_native> </unmanagedServices> <cassandraEnabled> false </cassandraEnabled> </configuration> The application.conf service files will require similar modifications: stream.cassandra { contact-points = [\"ASTRA_DB_ID-ASTRA_DB_REGION.db.astra.datastax.com:29042\"] authentication { username = \"token\" password = \"AstraCS:yourAstraT0ken\" } ssl { truststore.path = \"/Users/youruser/astradb/trustStore.jks\" truststore.password = \"yourTrustStorePassword\" keystore.path = \"/Users/youruser/stackoverflow/identity.jks\" keystore.password = \"Tte3jRy07ocEf6Z8h\" } session-provider = akka.persistence.cassandra.ConfigSessionProvider keyspace = \"stream\" } cassandra-journal { contact-points = ${stream.cassandra.contact-points} keyspace = ${stream.cassandra.keyspace} authentication = ${stream.cassandra.authentication} ssl = ${stream.cassandra.ssl} session-provider = ${stream.cassandra.session-provider} keyspace-autocreate = false tables-autocreate = true } cassandra-snapshot-store { contact-points = ${stream.cassandra.contact-points} keyspace = ${stream.cassandra.keyspace} authentication = ${stream.cassandra.authentication} ssl = ${stream.cassandra.ssl} session-provider = ${stream.cassandra.session-provider} keyspace-autocreate = false tables-autocreate = true } lagom.persistence.read-side.cassandra { contact-points = ${stream.cassandra.contact-points} keyspace = ${stream.cassandra.keyspace} authentication = ${stream.cassandra.authentication} ssl = ${stream.cassandra.ssl} session-provider = ${stream.cassandra.session-provider} keyspace-autocreate = false tables-autocreate = true } Note the options for keyspace-autocreate and tables-autocreate are shown set here. By default, these are both set to true . However, Astra DB only permits keyspace creation to happen via the Astra Dashboard. This means that: Keyspaces must be created before connecting a Lagom microservice to Astra DB. Lagom's attempts to create keyspaces will fail (due to a permissions error). Table creation should function appropriately, assuming the required keyspaces already exist. \ud83c\udfe0 Back to home","title":"Using with DataStax Astra DB"},{"location":"pages/develop/frameworks/micronaut/","text":"This guide was built based on the Micronaut Cassandra Guide A - Overview \u00b6 Micronaut is a modern, JVM-based, full stack Java framework designed for building modular, easily testable JVM applications with support for Java, Kotlin, and Groovy. Micronaut is developed by the creators of the Grails framework and takes inspiration from lessons learnt over the years building real-world applications from monoliths to microservices using Spring, Spring Boot and Grails. For more information refer to the user guide The micronaut-cassandra module includes support for integrating Micronaut services with Cassandra. B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle You should install `Java JDK 1.8+` and Apache Maven C - Configuration \u00b6 You can find a working sample here \u2705 Step 1: Create your project To create a micronaut project and CLI mn is provided. You can install it using sdkman as describe in the doc #Download SDKMan curl -s https://get.sdkman.io | bash #Setup SDKMan source \" $HOME /.sdkman/bin/sdkman-init.sh\" #Download Micronaut sdk install micronaut To generate a new project use mn create-app adding the feature cassandra mn create-app astra-todo-micronaut --features cassandra Notice that in your pom.xml you now have the following <dependency> <groupId> io.micronaut.cassandra </groupId> <artifactId> micronaut-cassandra </artifactId> <scope> compile </scope> </dependency> \u2705 Step 2: Setup your project All configuration of your project will be defined in application.yaml in src/main/resources . The module is clever enough to load all properties as if it was the driver configuration file . You can defined multiple profiles and each profile will be identified with key cassandra.${profile_name} . There is a default profile. In the following sample file we provide 2 profiles one for local and one for Astra. There is no extra code needed, simply configuration. cassandra : default : basic : session-keyspace : micronaut contact-points : - \"localhost:9042\" load-balancing-policy : local-datacenter : datacenter1 astra : basic : request : timeout : 5 seconds consistency : LOCAL_QUORUM page-size : 5000 session-keyspace : micronaut cloud : secure-connect-bundle : /Users/cedricklunven/Downloads/secure-connect-workshops.zip advanced : auth-provider : class : PlainTextAuthProvider username : token password : \"AstraCS:blahblahblah\" connection : init-query-timeout : 10 seconds set-keyspace-timeout : 10 seconds control-connection.timeout : 10 seconds \u2705 Step 3: Application Startup At startup you may want create the different tables needed for you application. In Astra you can only create keyspaces from the devops API or the user interface. . To enable content at startup simple implement ApplicationEventListener<ServiceReadyEvent> as shown below @Singleton public class TodoApplicationStartup implements ApplicationEventListener < ServiceReadyEvent > { /** Logger for the class. */ private static final Logger LOGGER = LoggerFactory . getLogger ( TodoApplicationStartup . class ); @Property ( name = \"todo.cassandra.create_schema\" , defaultValue = \"false\" ) private boolean createTable ; @Inject private CqlSession cqlSession ; /** {@inheritDoc} */ @Override public void onApplicationEvent ( final ServiceReadyEvent event ) { LOGGER . info ( \"Startup Initialization\" ); if ( createTable ) { TodoServiceCassandraCql . createTableTodo ( cqlSession ); LOGGER . info ( \"+ Table TodoItems created if needed.\" ); } LOGGER . info ( \"[OK]\" ); } } \u2705 Step 4: Use Cassandra To use Cassandra you will reuse the CqlSession from the Datastax drivers. You can simply inject it where you needed as shown in this sample code @Validated @Controller ( \"/api/v1\" ) public class TodoRestController { /** Logger for our Client. */ private static final Logger LOGGER = LoggerFactory . getLogger ( TodoRestController . class ); /** CqlSession initialized from application.yaml */ @Inject private CqlSession cqlSession ; Happy coding. \ud83c\udfe0 Back to home","title":"\u2022 Micronaut"},{"location":"pages/develop/frameworks/micronaut/#a-overview","text":"Micronaut is a modern, JVM-based, full stack Java framework designed for building modular, easily testable JVM applications with support for Java, Kotlin, and Groovy. Micronaut is developed by the creators of the Grails framework and takes inspiration from lessons learnt over the years building real-world applications from monoliths to microservices using Spring, Spring Boot and Grails. For more information refer to the user guide The micronaut-cassandra module includes support for integrating Micronaut services with Cassandra.","title":"A - Overview"},{"location":"pages/develop/frameworks/micronaut/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle You should install `Java JDK 1.8+` and Apache Maven","title":"B - Prerequisites"},{"location":"pages/develop/frameworks/micronaut/#c-configuration","text":"You can find a working sample here \u2705 Step 1: Create your project To create a micronaut project and CLI mn is provided. You can install it using sdkman as describe in the doc #Download SDKMan curl -s https://get.sdkman.io | bash #Setup SDKMan source \" $HOME /.sdkman/bin/sdkman-init.sh\" #Download Micronaut sdk install micronaut To generate a new project use mn create-app adding the feature cassandra mn create-app astra-todo-micronaut --features cassandra Notice that in your pom.xml you now have the following <dependency> <groupId> io.micronaut.cassandra </groupId> <artifactId> micronaut-cassandra </artifactId> <scope> compile </scope> </dependency> \u2705 Step 2: Setup your project All configuration of your project will be defined in application.yaml in src/main/resources . The module is clever enough to load all properties as if it was the driver configuration file . You can defined multiple profiles and each profile will be identified with key cassandra.${profile_name} . There is a default profile. In the following sample file we provide 2 profiles one for local and one for Astra. There is no extra code needed, simply configuration. cassandra : default : basic : session-keyspace : micronaut contact-points : - \"localhost:9042\" load-balancing-policy : local-datacenter : datacenter1 astra : basic : request : timeout : 5 seconds consistency : LOCAL_QUORUM page-size : 5000 session-keyspace : micronaut cloud : secure-connect-bundle : /Users/cedricklunven/Downloads/secure-connect-workshops.zip advanced : auth-provider : class : PlainTextAuthProvider username : token password : \"AstraCS:blahblahblah\" connection : init-query-timeout : 10 seconds set-keyspace-timeout : 10 seconds control-connection.timeout : 10 seconds \u2705 Step 3: Application Startup At startup you may want create the different tables needed for you application. In Astra you can only create keyspaces from the devops API or the user interface. . To enable content at startup simple implement ApplicationEventListener<ServiceReadyEvent> as shown below @Singleton public class TodoApplicationStartup implements ApplicationEventListener < ServiceReadyEvent > { /** Logger for the class. */ private static final Logger LOGGER = LoggerFactory . getLogger ( TodoApplicationStartup . class ); @Property ( name = \"todo.cassandra.create_schema\" , defaultValue = \"false\" ) private boolean createTable ; @Inject private CqlSession cqlSession ; /** {@inheritDoc} */ @Override public void onApplicationEvent ( final ServiceReadyEvent event ) { LOGGER . info ( \"Startup Initialization\" ); if ( createTable ) { TodoServiceCassandraCql . createTableTodo ( cqlSession ); LOGGER . info ( \"+ Table TodoItems created if needed.\" ); } LOGGER . info ( \"[OK]\" ); } } \u2705 Step 4: Use Cassandra To use Cassandra you will reuse the CqlSession from the Datastax drivers. You can simply inject it where you needed as shown in this sample code @Validated @Controller ( \"/api/v1\" ) public class TodoRestController { /** Logger for our Client. */ private static final Logger LOGGER = LoggerFactory . getLogger ( TodoRestController . class ); /** CqlSession initialized from application.yaml */ @Inject private CqlSession cqlSession ; Happy coding. \ud83c\udfe0 Back to home","title":"C - Configuration"},{"location":"pages/develop/frameworks/quarkus/","text":"Quarkus Documentation Quarkus Workshop Todo Application Quarkus","title":"\u2022 Quarkus"},{"location":"pages/develop/frameworks/spring/","text":"Spring makes programming Java quicker, easier, and safer for everybody. Spring\u2019s focus on speed, simplicity, and productivity has made it the world's most popular Java framework. To get more information regarding the framework visit the reference website Spring.io . Spring-Data is the module use to interact with Databases whereas Spring Boot is the runtime for microservices. In this page we detail how to setup both modules to interact with Astra. 1. Overview \u00b6 1.1 Modules dependencies \u00b6 Spring is an ecosystem with dozens of modules. The component used to connect a Spring application to Astra (Cassandra) is Spring Data and especially Spring Data Cassandra . It relies on the Datastax native java cassandra drivers and only provides an abstraction with Spring concepts (templates, repository, Entities...) The stateful object CqlSession is instantiated and injected in spring CassandraTemplate (aka CassandraOperations ). From there, it is used either directly or injected in different CassandraRepository (specialization of Spring Data CrudRepository for Apache Cassandra\u2122). The configuration of spring-data-cassandra in Spring-Boot applications is simplified with the usage of starters . One is associated to the standard web stack and called spring-boot-starter-data-cassandra and the other is named spring-boot-starter-data-cassandra-reactive for the reactive stack. 1.2 Compatibility Matrix \u00b6 In January 2019, the native Cassandra Drivers got an important, not backward compatible, upgrade. To get informations regarding Apache Cassandra\u2122 support here is the Cassandra compatibility matrix . Spring Data copes with the new generation of drivers starting with Spring data 3.x. Support of Astra was introduced in 2020 for all native versions (4.x and 3.x). This leads to the following table for minimal library versions for Astra Support : Drivers Release Drivers Version Spring-Data Spring Boot Unified 4.x 4.6.0 3.0.0.RELEASE 2.3.0.RELEASE OSS 3.x 3.8.0 Setup below table 2.2.13.RELEASE DSE 2.x 2.3.0 3.0.0.RELEASE 2.3.0.RELEASE DSE 1.x 1.9.0 Setup below table 2.2 Setup Spring Data 2.2.x (and before) to work with Astra As stated in the matrix, even the latest Spring Data 2.2.13.RELEASE rely on cassandra-driver version 3.7.2 that where not yet compatible to Astra. To work with Astra you have to override the cassandra-drivers version as below. <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-data-cassandra </artifactId> <version> 2.2.13.RELEASE </version> </dependency> <dependency> <groupId> com.datastax.cassandra </groupId> <artifactId> cassandra-driver-core </artifactId> <version> 3.11.2 </version> </dependency> You can find here a sample project that uses Spring Boot version as old as 1.5.4 . Setup Spring Data 2.2 (and before) to work with DataStax Enterprise (DSE) Before 4.x and the unified drivers you have to use dse-java-driver-core to have access to enterprise features but also the be elligible for the support. To enable it you need to exclude cassandra-driver-core and import dse-java-driver-core as show below <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-data-cassandra </artifactId> <version> 2.2.13.RELEASE </version> <exclusions> <exclusion> <groupId> com.datastax.cassandra </groupId> <artifactId> cassandra-driver-core </artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId> com.datastax.dse </groupId> <artifactId> dse-java-driver-core </artifactId> <version> 1.9.0 </version> </dependency> 1.3 Rules and Pitfalls \u00b6 Define your own CqlSession bean (spring-data will find it !) Spring Data Cassandra starters provide some dedicated keys in the configuration file application.yaml ( spring.data.cassandra.* ) but you do not get the complete list of options of the drivers. In the same way some super classes like AbstractCassandraConfiguration are provided where you can specify a few configuration properties but a limited set of keys are available. Do not use findAll() It can be tempting to use this method to test new repositories as no parameter is required - but this is dangerous. The default paging mechanism is skipped and this method will retrieve every single record of the table. As such, it would perform a full scan of the cluster (pick data for each node) that (1) would be slow and (2) could lead to OutOfMemoryException as Cassandra Tables are expected to store billions of records. Do not use @AllowFiltering This annotation (some for associated CQL Statement) is limited for the use cases where (1) you provide the partition key AND (2) you know your partition size is fairly small. In 99% of the cases the need of this annotation (or ALLOW FILTERING in the CQL ) is a sign of a wrong data model: your primary key is invalid and you need another table to store the same data (or eventually to create a secondary index). Do not rely (only) on Spring Data to create your schema SDC provide a configuration key spring.data.cassandra.schema-action: CREATE_IF_NOT_EXISTS that proposes to create the Cassandra Tables based on your annotated beans. It is NOT a good idea. Indeed, it could lead to wrong data model (cf next point) but also it does not give access to fine grained properties like COMPACTION and TTL that might be different in development and production. Let a Cassandra Administrator reviews your DDL scripts and updates them for production. Data Model First, Entities second With the JPA (entity, repository) methodology, you are tempting to reuse the same entities and repositories to perform multiple queries against the same table. Most new requests will be not valid as you will not request using the primary key. You can be tempting to create a secondary index or use allow filtering; WRONG ! . The good practice is to CREATE ANOTHER TABLE, ANOTHER ENTITY and ANOTHER REPOSITORY - and even if data stored is the same. With Cassandra 1 query = 1 table (mostly). CassandraRepository probably cannot implement it all With real-life applications you might probably need to go back to the CqlSession and execute custom fine-grained queries ( Batches , TTL , LWT ...). The interfaces and CassandraRepostiory would not be enough. The class SimpleCassandraRepository is an abstract class (not interface0 you can inherit from that give you access to the CqlSession and execute your queries as you like, it is a good trade off. 2. Astra Spring Boot Starter \u00b6 2.1 Introduction \u00b6 The Astra Spring Boot Starter will configure both Astra SDK and Spring Data Cassandra to work with AstraDB. Configuration keys are provided in application.yaml like any spring applications with a dedicated prefix astra.* . The starter will initialize any beans you would need ( AstraClient , CqlSession , StargateClient ) to use every interfaces exposes by Astra. Not all are activated by default though, you want to initialize only what you need. 2.2 Project Setup \u00b6 Prerequisites [ASTRA] \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token Prerequisites [Development Environment] \u00b6 You should install Java Development Kit (JDK) 8 : Use the reference documentation to install a Java Development Kit , Validate your installation with java --version You should install Apache Maven : Use the reference documentation and validate your installation with mvn -version Setup Project \u00b6 Create your project with Spring Initializr . Dependencies needed are web and data-cassandra but we did the work for you if you click the template link Property Value Property Value groupId com.datastax.tutorial package com.datastax.tutorial artifactId sdk-quickstart-spring description Sample Spring App name sdk-quickstart-spring dependencies Spring Web and Spring Data for Cassandra packaging JAR Java Version 8 or 11 Import the application in your favorite IDE but do not start the application immediately. Add the latest version of starter as a dependency in pom.xml of astra-spring-boot-starter in the project. <dependency> <groupId> com.datastax.astra </groupId> <artifactId> astra-spring-boot-starter </artifactId> <version> 0.3.0 </version> </dependency> 2.3 Code and Configuration \u00b6 Change the main class with the following code, we are leveraging on the unique AstraClient to interact with multiple interfaces. @RestController @SpringBootApplication public class SdkQuickstartSpringApplication { public static void main ( String [] args ) { SpringApplication . run ( SdkQuickstartSpringApplication . class , args ); } // Provided by the Starter @Autowired private AstraClient astraClient ; // Spring Data using the CqlSession initialized by the starter @Autowired private CassandraTemplate cassandraTemplate ; @GetMapping ( \"/api/devops/organizationid\" ) public String showOrganizationId () { return astraClient . apiDevopsOrganizations (). organizationId (); } @GetMapping ( \"/api/spring-data/datacenter\" ) public String showDatacenterNameWithSpringData () { return cassandraTemplate . getCqlOperations () . queryForObject ( \"SELECT data_center FROM system.local\" , String . class ); } @GetMapping ( \"/api/cql/datacenter\" ) public String showDatacenterNameWithSpringData () { return astraClient . cqlSession () . execute ( \"SELECT data_center FROM system.local\" ) . one (). getString ( \"data_center\" ); } } Rename src/main/resources/application.properties to src/main/resources/application.yaml . This step eases the configuration with hierarchical keys. Populate application.yaml with the following content and replace the values with expected values (how to retrieve the values are explained in the Quickstart Astra astra : # Allow usage of devops and Stargate apis api : application-token : <your_token> database-id : <your_database_id> database-region : <your_database_region> # Connectivity to Cassandra cql : enabled : true download-scb : enabled : true driver-config : basic : session-keyspace : <your_keyspace> Start the application mvn clean install spring-boot:run Access the resources we created Get your Organization ID: http://localhost:8080/api/devops/organizationid Get your Datacenter Name (Spring-data): http://localhost:8080/api/spring-data/datacenter Get your Datacenter Name (cql): http://localhost:8080/api/cql/datacenter 3. Spring Data Cassandra \u00b6 3.1 Project Setup \u00b6 Prerequisites [ASTRA] \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Have downloaded your Cloud Secure Bundle Prerequisites [Development Environment] \u00b6 You should install Java Development Kit (JDK) 8 : Use the reference documentation to install a Java Development Kit , Validate your installation with java --version You should install Apache Maven : Use the reference documentation and validate your installation with mvn -version Setup Project \u00b6 Create a Spring Boot application from the initializer and add the spring-boot-starter-data-cassandra <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-data-cassandra </artifactId> </dependency> 3.2 Code and Configuration \u00b6 Setup the configuration file application.yaml spring.data.cassandra : keyspace-name : myks username : myClientId password : myClientSecret schema-action : CREATE_IF_NOT_EXISTS # for dev purpose request : timeout : 10s connection : connect-timeout : 10s init-query-timeout : 10s datastax.astra : # You must download it before secure-connect-bundle : /tmp/secure-connect-bundle.zip Create a dedicated configuration bean to parse datastax.astra @ConfigurationProperties ( prefix = \"datastax.astra\" ) public class DataStaxAstraProperties { private File secureConnectBundle ; // Getter and Setter omitted } Define a bean of CqlSessionBuilderCustomizer to add this CloudSecureBundle @SpringBootApplication @EnableConfigurationProperties ( DataStaxAstraProperties . class ) public class SpringDataCassandraApplication { public static void main ( String [] args ) { SpringApplication . run ( SpringDataCassandraApplication . class , args ); } @Bean public CqlSessionBuilderCustomizer sessionBuilderCustomizer ( DataStaxAstraProperties astraProperties ) { Path bundle = astraProperties . getSecureConnectBundle (). toPath (); return builder -> builder . withCloudSecureConnectBundle ( bundle ); } }","title":"\u2022 Spring"},{"location":"pages/develop/frameworks/spring/#1-overview","text":"","title":"1. Overview"},{"location":"pages/develop/frameworks/spring/#11-modules-dependencies","text":"Spring is an ecosystem with dozens of modules. The component used to connect a Spring application to Astra (Cassandra) is Spring Data and especially Spring Data Cassandra . It relies on the Datastax native java cassandra drivers and only provides an abstraction with Spring concepts (templates, repository, Entities...) The stateful object CqlSession is instantiated and injected in spring CassandraTemplate (aka CassandraOperations ). From there, it is used either directly or injected in different CassandraRepository (specialization of Spring Data CrudRepository for Apache Cassandra\u2122). The configuration of spring-data-cassandra in Spring-Boot applications is simplified with the usage of starters . One is associated to the standard web stack and called spring-boot-starter-data-cassandra and the other is named spring-boot-starter-data-cassandra-reactive for the reactive stack.","title":"1.1 Modules dependencies"},{"location":"pages/develop/frameworks/spring/#12-compatibility-matrix","text":"In January 2019, the native Cassandra Drivers got an important, not backward compatible, upgrade. To get informations regarding Apache Cassandra\u2122 support here is the Cassandra compatibility matrix . Spring Data copes with the new generation of drivers starting with Spring data 3.x. Support of Astra was introduced in 2020 for all native versions (4.x and 3.x). This leads to the following table for minimal library versions for Astra Support : Drivers Release Drivers Version Spring-Data Spring Boot Unified 4.x 4.6.0 3.0.0.RELEASE 2.3.0.RELEASE OSS 3.x 3.8.0 Setup below table 2.2.13.RELEASE DSE 2.x 2.3.0 3.0.0.RELEASE 2.3.0.RELEASE DSE 1.x 1.9.0 Setup below table 2.2 Setup Spring Data 2.2.x (and before) to work with Astra As stated in the matrix, even the latest Spring Data 2.2.13.RELEASE rely on cassandra-driver version 3.7.2 that where not yet compatible to Astra. To work with Astra you have to override the cassandra-drivers version as below. <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-data-cassandra </artifactId> <version> 2.2.13.RELEASE </version> </dependency> <dependency> <groupId> com.datastax.cassandra </groupId> <artifactId> cassandra-driver-core </artifactId> <version> 3.11.2 </version> </dependency> You can find here a sample project that uses Spring Boot version as old as 1.5.4 . Setup Spring Data 2.2 (and before) to work with DataStax Enterprise (DSE) Before 4.x and the unified drivers you have to use dse-java-driver-core to have access to enterprise features but also the be elligible for the support. To enable it you need to exclude cassandra-driver-core and import dse-java-driver-core as show below <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-data-cassandra </artifactId> <version> 2.2.13.RELEASE </version> <exclusions> <exclusion> <groupId> com.datastax.cassandra </groupId> <artifactId> cassandra-driver-core </artifactId> </exclusion> </exclusions> </dependency> <dependency> <groupId> com.datastax.dse </groupId> <artifactId> dse-java-driver-core </artifactId> <version> 1.9.0 </version> </dependency>","title":"1.2 Compatibility Matrix"},{"location":"pages/develop/frameworks/spring/#13-rules-and-pitfalls","text":"Define your own CqlSession bean (spring-data will find it !) Spring Data Cassandra starters provide some dedicated keys in the configuration file application.yaml ( spring.data.cassandra.* ) but you do not get the complete list of options of the drivers. In the same way some super classes like AbstractCassandraConfiguration are provided where you can specify a few configuration properties but a limited set of keys are available. Do not use findAll() It can be tempting to use this method to test new repositories as no parameter is required - but this is dangerous. The default paging mechanism is skipped and this method will retrieve every single record of the table. As such, it would perform a full scan of the cluster (pick data for each node) that (1) would be slow and (2) could lead to OutOfMemoryException as Cassandra Tables are expected to store billions of records. Do not use @AllowFiltering This annotation (some for associated CQL Statement) is limited for the use cases where (1) you provide the partition key AND (2) you know your partition size is fairly small. In 99% of the cases the need of this annotation (or ALLOW FILTERING in the CQL ) is a sign of a wrong data model: your primary key is invalid and you need another table to store the same data (or eventually to create a secondary index). Do not rely (only) on Spring Data to create your schema SDC provide a configuration key spring.data.cassandra.schema-action: CREATE_IF_NOT_EXISTS that proposes to create the Cassandra Tables based on your annotated beans. It is NOT a good idea. Indeed, it could lead to wrong data model (cf next point) but also it does not give access to fine grained properties like COMPACTION and TTL that might be different in development and production. Let a Cassandra Administrator reviews your DDL scripts and updates them for production. Data Model First, Entities second With the JPA (entity, repository) methodology, you are tempting to reuse the same entities and repositories to perform multiple queries against the same table. Most new requests will be not valid as you will not request using the primary key. You can be tempting to create a secondary index or use allow filtering; WRONG ! . The good practice is to CREATE ANOTHER TABLE, ANOTHER ENTITY and ANOTHER REPOSITORY - and even if data stored is the same. With Cassandra 1 query = 1 table (mostly). CassandraRepository probably cannot implement it all With real-life applications you might probably need to go back to the CqlSession and execute custom fine-grained queries ( Batches , TTL , LWT ...). The interfaces and CassandraRepostiory would not be enough. The class SimpleCassandraRepository is an abstract class (not interface0 you can inherit from that give you access to the CqlSession and execute your queries as you like, it is a good trade off.","title":"1.3 Rules and Pitfalls"},{"location":"pages/develop/frameworks/spring/#2-astra-spring-boot-starter","text":"","title":"2. Astra Spring Boot Starter"},{"location":"pages/develop/frameworks/spring/#21-introduction","text":"The Astra Spring Boot Starter will configure both Astra SDK and Spring Data Cassandra to work with AstraDB. Configuration keys are provided in application.yaml like any spring applications with a dedicated prefix astra.* . The starter will initialize any beans you would need ( AstraClient , CqlSession , StargateClient ) to use every interfaces exposes by Astra. Not all are activated by default though, you want to initialize only what you need.","title":"2.1 Introduction"},{"location":"pages/develop/frameworks/spring/#22-project-setup","text":"","title":"2.2 Project Setup"},{"location":"pages/develop/frameworks/spring/#prerequisites-astra","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token","title":"Prerequisites [ASTRA]"},{"location":"pages/develop/frameworks/spring/#prerequisites-development-environment","text":"You should install Java Development Kit (JDK) 8 : Use the reference documentation to install a Java Development Kit , Validate your installation with java --version You should install Apache Maven : Use the reference documentation and validate your installation with mvn -version","title":"Prerequisites [Development Environment]"},{"location":"pages/develop/frameworks/spring/#setup-project","text":"Create your project with Spring Initializr . Dependencies needed are web and data-cassandra but we did the work for you if you click the template link Property Value Property Value groupId com.datastax.tutorial package com.datastax.tutorial artifactId sdk-quickstart-spring description Sample Spring App name sdk-quickstart-spring dependencies Spring Web and Spring Data for Cassandra packaging JAR Java Version 8 or 11 Import the application in your favorite IDE but do not start the application immediately. Add the latest version of starter as a dependency in pom.xml of astra-spring-boot-starter in the project. <dependency> <groupId> com.datastax.astra </groupId> <artifactId> astra-spring-boot-starter </artifactId> <version> 0.3.0 </version> </dependency>","title":"Setup Project"},{"location":"pages/develop/frameworks/spring/#23-code-and-configuration","text":"Change the main class with the following code, we are leveraging on the unique AstraClient to interact with multiple interfaces. @RestController @SpringBootApplication public class SdkQuickstartSpringApplication { public static void main ( String [] args ) { SpringApplication . run ( SdkQuickstartSpringApplication . class , args ); } // Provided by the Starter @Autowired private AstraClient astraClient ; // Spring Data using the CqlSession initialized by the starter @Autowired private CassandraTemplate cassandraTemplate ; @GetMapping ( \"/api/devops/organizationid\" ) public String showOrganizationId () { return astraClient . apiDevopsOrganizations (). organizationId (); } @GetMapping ( \"/api/spring-data/datacenter\" ) public String showDatacenterNameWithSpringData () { return cassandraTemplate . getCqlOperations () . queryForObject ( \"SELECT data_center FROM system.local\" , String . class ); } @GetMapping ( \"/api/cql/datacenter\" ) public String showDatacenterNameWithSpringData () { return astraClient . cqlSession () . execute ( \"SELECT data_center FROM system.local\" ) . one (). getString ( \"data_center\" ); } } Rename src/main/resources/application.properties to src/main/resources/application.yaml . This step eases the configuration with hierarchical keys. Populate application.yaml with the following content and replace the values with expected values (how to retrieve the values are explained in the Quickstart Astra astra : # Allow usage of devops and Stargate apis api : application-token : <your_token> database-id : <your_database_id> database-region : <your_database_region> # Connectivity to Cassandra cql : enabled : true download-scb : enabled : true driver-config : basic : session-keyspace : <your_keyspace> Start the application mvn clean install spring-boot:run Access the resources we created Get your Organization ID: http://localhost:8080/api/devops/organizationid Get your Datacenter Name (Spring-data): http://localhost:8080/api/spring-data/datacenter Get your Datacenter Name (cql): http://localhost:8080/api/cql/datacenter","title":"2.3 Code and Configuration"},{"location":"pages/develop/frameworks/spring/#3-spring-data-cassandra","text":"","title":"3. Spring Data Cassandra"},{"location":"pages/develop/frameworks/spring/#31-project-setup","text":"","title":"3.1 Project Setup"},{"location":"pages/develop/frameworks/spring/#prerequisites-astra_1","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Have downloaded your Cloud Secure Bundle","title":"Prerequisites [ASTRA]"},{"location":"pages/develop/frameworks/spring/#prerequisites-development-environment_1","text":"You should install Java Development Kit (JDK) 8 : Use the reference documentation to install a Java Development Kit , Validate your installation with java --version You should install Apache Maven : Use the reference documentation and validate your installation with mvn -version","title":"Prerequisites [Development Environment]"},{"location":"pages/develop/frameworks/spring/#setup-project_1","text":"Create a Spring Boot application from the initializer and add the spring-boot-starter-data-cassandra <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-data-cassandra </artifactId> </dependency>","title":"Setup Project"},{"location":"pages/develop/frameworks/spring/#32-code-and-configuration","text":"Setup the configuration file application.yaml spring.data.cassandra : keyspace-name : myks username : myClientId password : myClientSecret schema-action : CREATE_IF_NOT_EXISTS # for dev purpose request : timeout : 10s connection : connect-timeout : 10s init-query-timeout : 10s datastax.astra : # You must download it before secure-connect-bundle : /tmp/secure-connect-bundle.zip Create a dedicated configuration bean to parse datastax.astra @ConfigurationProperties ( prefix = \"datastax.astra\" ) public class DataStaxAstraProperties { private File secureConnectBundle ; // Getter and Setter omitted } Define a bean of CqlSessionBuilderCustomizer to add this CloudSecureBundle @SpringBootApplication @EnableConfigurationProperties ( DataStaxAstraProperties . class ) public class SpringDataCassandraApplication { public static void main ( String [] args ) { SpringApplication . run ( SpringDataCassandraApplication . class , args ); } @Bean public CqlSessionBuilderCustomizer sessionBuilderCustomizer ( DataStaxAstraProperties astraProperties ) { Path bundle = astraProperties . getSecureConnectBundle (). toPath (); return builder -> builder . withCloudSecureConnectBundle ( bundle ); } }","title":"3.2 Code and Configuration"},{"location":"pages/develop/languages/csharp/","text":"1. Overview \u00b6 Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces . There are different frameworks and tools to connect to Astra depending on the Api interface you choose. Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here . 2. Interfaces List \u00b6 Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming 3. CQL \u00b6 3.1 Cassandra Drivers \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 3.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 4. Stargate REST Api \u00b6 4.1 Axios \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 4.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 5. Stargate Document Api \u00b6 5.1 Axios \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 5.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 6 Stargate GraphQL \u00b6 6.1 CQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 6.2 GraphQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7. Stargate gRPC \u00b6 7.1 Stargate Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8. Pulsar Client \u00b6 8.1 Pulsar Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 9. Pulsar Admin \u00b6 10 Devops API Database \u00b6 11 Devops API Organization \u00b6 12 Devops API Streaming \u00b6","title":"\u2022 CSharp"},{"location":"pages/develop/languages/csharp/#1-overview","text":"Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces . There are different frameworks and tools to connect to Astra depending on the Api interface you choose. Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here .","title":"1. Overview"},{"location":"pages/develop/languages/csharp/#2-interfaces-list","text":"Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming","title":"2. Interfaces List"},{"location":"pages/develop/languages/csharp/#3-cql","text":"","title":"3. CQL"},{"location":"pages/develop/languages/csharp/#31-cassandra-drivers","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"3.1 Cassandra Drivers"},{"location":"pages/develop/languages/csharp/#32-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"3.2 Astra SDK"},{"location":"pages/develop/languages/csharp/#4-stargate-rest-api","text":"","title":"4. Stargate REST Api"},{"location":"pages/develop/languages/csharp/#41-axios","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"4.1 Axios"},{"location":"pages/develop/languages/csharp/#42-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"4.2 Astra SDK"},{"location":"pages/develop/languages/csharp/#5-stargate-document-api","text":"","title":"5. Stargate Document Api"},{"location":"pages/develop/languages/csharp/#51-axios","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"5.1 Axios"},{"location":"pages/develop/languages/csharp/#52-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"5.2 Astra SDK"},{"location":"pages/develop/languages/csharp/#6-stargate-graphql","text":"","title":"6 Stargate GraphQL"},{"location":"pages/develop/languages/csharp/#61-cql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.1 CQL First"},{"location":"pages/develop/languages/csharp/#62-graphql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.2 GraphQL First"},{"location":"pages/develop/languages/csharp/#7-stargate-grpc","text":"","title":"7. Stargate gRPC"},{"location":"pages/develop/languages/csharp/#71-stargate-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.1 Stargate Client"},{"location":"pages/develop/languages/csharp/#72-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.2 Astra SDK"},{"location":"pages/develop/languages/csharp/#8-pulsar-client","text":"","title":"8. Pulsar Client"},{"location":"pages/develop/languages/csharp/#81-pulsar-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.1 Pulsar Client"},{"location":"pages/develop/languages/csharp/#82-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.2 Astra SDK"},{"location":"pages/develop/languages/csharp/#9-pulsar-admin","text":"","title":"9. Pulsar Admin"},{"location":"pages/develop/languages/csharp/#10-devops-api-database","text":"","title":"10 Devops API Database"},{"location":"pages/develop/languages/csharp/#11-devops-api-organization","text":"","title":"11 Devops API Organization"},{"location":"pages/develop/languages/csharp/#12-devops-api-streaming","text":"","title":"12 Devops API Streaming"},{"location":"pages/develop/languages/go/","text":"1. Overview \u00b6 Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces . There are different frameworks and tools to connect to Astra depending on the Api interface you choose. Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) for Go is forthcoming, and will be available in the near future. 2. Interfaces List \u00b6 Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming 3. CQL \u00b6 3.1 The Gocql Cassandra Driver \u00b6 \u2139\ufe0f Overview These instructions are aimed at helping people connect to Astra DB programmatically using the community-driven Gocql driver. This driver does not have an option to process the Astra secure connect bundle, so part of connecting is completing that process manually, as is shown below. \ud83d\udce6 Prerequisites [ASTRA] You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle \ud83d\udce6 Prerequisites [Development Environment] You will need to have a recent (1.17+) version of Go. Visit the official download page , and select the appropriate version for your machine architecture. To verify that Go is installed, run the following command: go version With Go installed locally, you can now use the Go package manager ( go get ) to install the Gocql driver. go get github.com/gocql/gocql \ud83d\udda5\ufe0f Sample Code To connect to an Astra DB cluster, you will need a secure token generated specifically for use with your Astra DB cluster. You will also need to unzip your secure bundle, to ensure that you can access the files contained within. mkdir mySecureBundleDir cd mySecureBundleDir mv ~/Downloads/secure-connect-bundle.zip . unzip secure-connect-bundle.zip Inside your editor/IDE, create a new code file with a .go extension, and import several libraries. import ( \"crypto/tls\" \"crypto/x509\" \"context\" \"fmt\" \"io/ioutil\" \"github.com/gocql/gocql\" \"os\" \"path/filepath\" \"strconv\" ) Next, create a func main() method. func main () { // set default port var port int = 29042 var err error As seen above, we'll define Astra DB's default CQL port to 29042 as well as an error variable (which we'll use later). Next we will inject the connection parameters into the code. This can be done either by reading them as environment variables or passing them as command line arguments. This example will be done using command line arguments: hostname := os . Args [ 1 ] username := os . Args [ 2 ] password := os . Args [ 3 ] caPath , _ := filepath . Abs ( os . Args [ 4 ]) certPath , _ := filepath . Abs ( os . Args [ 5 ]) keyPath , _ := filepath . Abs ( os . Args [ 6 ]) As seen above, we are going to read in six arguments. First, we'll take the hostname and port to establish our connection endpoint. With Astra DB, you should only use a single endpoint to connect, as that Astra endpoint itself resolves to multiple nodes. cluster := gocql . NewCluster ( hostname ) cluster . Port = port Next, we'll define our connection authenticator and pass our credentials to it. cluster . Authenticator = gocql . PasswordAuthenticator { Username : username , Password : password , } Finally, we'll need to process the filepaths of our TLS/X509 certificate, key, and certificate authority files. cert , _ := tls . LoadX509KeyPair ( certPath , keyPath ) caCert , err := ioutil . ReadFile ( caPath ) caCertPool := x509 . NewCertPool () caCertPool . AppendCertsFromPEM ( caCert ) tlsConfig := & tls . Config { Certificates : [] tls . Certificate { cert }, RootCAs : caCertPool , } We'll them pass our tlsConfig to the SslOpts property on the cluster object. cluster . SslOpts = & gocql . SslOptions { Config : tlsConfig , EnableHostVerification : false , } With all of that defined, we can open a connection to our cluster: session , err := cluster . CreateSession () if err != nil { fmt . Println ( err ) } defer session . Close () ctx := context . Background () If you get an error concerning a mismatch of the CQL protocol version at this point, try forcing protocol version 4 before the session code block above. cluster . ProtoVersion = 4 With a connection made, we can run a simple query to return the name of the cluster from the system.local table: var strClusterName string err2 := session . Query ( `SELECT cluster_name FROM system.local` ). WithContext ( ctx ). Scan ( & strClusterName ) if err2 != nil { fmt . Println ( err ) } else { fmt . Println ( \"cluster_name:\" , strClusterName ) } Running this code with arguments in the proper order should yield output similar to this: go run testCassandraSSL.go ce111111-1111-1111-1111-d11b1d4bc111-us-east1.db.astra.datastax.com token \"AstraCS:ASjPlHbTYourSecureTokenGoesHered3cdab53b\" /Users/aaronploetz/mySecureBundleDir/ca.crt /Users/aaronploetz/mySecureBundleDir/cert /Users/aaronploetz/mySecureBundleDir/key cluster_name: cndb The complete code to this example can be found here . 3.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 4. Stargate REST Api \u00b6 4.1 Axios \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 4.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 5. Stargate Document Api \u00b6 5.1 Axios \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 5.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 6 Stargate GraphQL \u00b6 6.1 CQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 6.2 GraphQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7. Stargate gRPC \u00b6 7.1 Stargate Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8. Pulsar Client \u00b6 8.1 Pulsar Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 9. Pulsar Admin \u00b6 10 Devops API Database \u00b6 11 Devops API Organization \u00b6 12 Devops API Streaming \u00b6 package main import ( \"archive/zip\" \"context\" \"crypto/tls\" \"crypto/x509\" \"encoding/json\" \"fmt\" \"io\" \"io/ioutil\" \"os\" \"path/filepath\" \"strconv\" \"strings\" \"github.com/gocql/gocql\" ) type Config struct { Host string `json:\"host\"` Port int `json:\"cql_port\"` } func main () { var clientID = os . Getenv ( \"ASTRA_CLIENT_ID\" ) var clientSecret = os . Getenv ( \"ASTRA_CLIENT_SECRET\" ) var secureConnectBundle = os . Getenv ( \"SECURE_CONNECT_BUNDLE\" ) if clientID == \"\" || clientSecret == \"\" || secureConnectBundle == \"\" { panic ( \"missing required environment variables\" ) } secureBundleDir := os . TempDir () fmt . Printf ( \"extracting secure connect bundle [%s] to [%s]\\n\" , secureConnectBundle , secureBundleDir ) if err := Unzip ( secureConnectBundle , secureBundleDir ); err != nil { panic ( err ) } configPath , _ := filepath . Abs ( secureBundleDir + \"/config.json\" ) fmt . Println ( \"config: \" + configPath ) configData , _ := ioutil . ReadFile ( configPath ) var cfg Config json . Unmarshal ( configData , & cfg ) cluster := gocql . NewCluster ( cfg . Host ) cluster . Authenticator = gocql . PasswordAuthenticator { Username : clientID , Password : clientSecret , } host := cfg . Host + \":\" + strconv . Itoa ( cfg . Port ) cluster . Hosts = [] string { host } fmt . Println ( \"connecting to: \" + host ) certPath , _ := filepath . Abs ( secureBundleDir + \"/cert\" ) keyPath , _ := filepath . Abs ( secureBundleDir + \"/key\" ) caPath , _ := filepath . Abs ( secureBundleDir + \"/ca.crt\" ) cert , _ := tls . LoadX509KeyPair ( certPath , keyPath ) caCert , _ := ioutil . ReadFile ( caPath ) caCertPool := x509 . NewCertPool () caCertPool . AppendCertsFromPEM ( caCert ) cluster . SslOpts = & gocql . SslOptions { Config : & tls . Config { Certificates : [] tls . Certificate { cert }, ServerName : cfg . Host , RootCAs : caCertPool , }, } session , err := cluster . CreateSession () if err != nil { panic ( err ) } fmt . Printf ( \"session established: %v\\n\" , session ) var releaseVersion string if err := session . Query ( \"select release_version from system.local\" ). WithContext ( context . Background ()). Consistency ( gocql . One ). Scan ( & releaseVersion ); err != nil { panic ( err ) } fmt . Printf ( \"release version: %s\\n\" , releaseVersion ) } func Unzip ( src , dest string ) error { r , err := zip . OpenReader ( src ) if err != nil { return err } defer func () { if err := r . Close (); err != nil { panic ( err ) } }() os . MkdirAll ( dest , 0755 ) // Closure to address file descriptors issue with all the deferred .Close() methods extractAndWriteFile := func ( f * zip . File ) error { rc , err := f . Open () if err != nil { return err } defer func () { if err := rc . Close (); err != nil { panic ( err ) } }() path := filepath . Join ( dest , f . Name ) // Check for ZipSlip (Directory traversal) if ! strings . HasPrefix ( path , filepath . Clean ( dest ) + string ( os . PathSeparator )) { return fmt . Errorf ( \"illegal file path: %s\" , path ) } if f . FileInfo (). IsDir () { os . MkdirAll ( path , f . Mode ()) } else { os . MkdirAll ( filepath . Dir ( path ), f . Mode ()) f , err := os . OpenFile ( path , os . O_WRONLY | os . O_CREATE | os . O_TRUNC , f . Mode ()) if err != nil { return err } defer func () { if err := f . Close (); err != nil { panic ( err ) } }() _ , err = io . Copy ( f , rc ) if err != nil { return err } } return nil } for _ , f := range r . File { err := extractAndWriteFile ( f ) if err != nil { return err } } return nil }","title":"\u2022 GoLang"},{"location":"pages/develop/languages/go/#1-overview","text":"Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces . There are different frameworks and tools to connect to Astra depending on the Api interface you choose. Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) for Go is forthcoming, and will be available in the near future.","title":"1. Overview"},{"location":"pages/develop/languages/go/#2-interfaces-list","text":"Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming","title":"2. Interfaces List"},{"location":"pages/develop/languages/go/#3-cql","text":"","title":"3. CQL"},{"location":"pages/develop/languages/go/#31-the-gocql-cassandra-driver","text":"\u2139\ufe0f Overview These instructions are aimed at helping people connect to Astra DB programmatically using the community-driven Gocql driver. This driver does not have an option to process the Astra secure connect bundle, so part of connecting is completing that process manually, as is shown below. \ud83d\udce6 Prerequisites [ASTRA] You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle \ud83d\udce6 Prerequisites [Development Environment] You will need to have a recent (1.17+) version of Go. Visit the official download page , and select the appropriate version for your machine architecture. To verify that Go is installed, run the following command: go version With Go installed locally, you can now use the Go package manager ( go get ) to install the Gocql driver. go get github.com/gocql/gocql \ud83d\udda5\ufe0f Sample Code To connect to an Astra DB cluster, you will need a secure token generated specifically for use with your Astra DB cluster. You will also need to unzip your secure bundle, to ensure that you can access the files contained within. mkdir mySecureBundleDir cd mySecureBundleDir mv ~/Downloads/secure-connect-bundle.zip . unzip secure-connect-bundle.zip Inside your editor/IDE, create a new code file with a .go extension, and import several libraries. import ( \"crypto/tls\" \"crypto/x509\" \"context\" \"fmt\" \"io/ioutil\" \"github.com/gocql/gocql\" \"os\" \"path/filepath\" \"strconv\" ) Next, create a func main() method. func main () { // set default port var port int = 29042 var err error As seen above, we'll define Astra DB's default CQL port to 29042 as well as an error variable (which we'll use later). Next we will inject the connection parameters into the code. This can be done either by reading them as environment variables or passing them as command line arguments. This example will be done using command line arguments: hostname := os . Args [ 1 ] username := os . Args [ 2 ] password := os . Args [ 3 ] caPath , _ := filepath . Abs ( os . Args [ 4 ]) certPath , _ := filepath . Abs ( os . Args [ 5 ]) keyPath , _ := filepath . Abs ( os . Args [ 6 ]) As seen above, we are going to read in six arguments. First, we'll take the hostname and port to establish our connection endpoint. With Astra DB, you should only use a single endpoint to connect, as that Astra endpoint itself resolves to multiple nodes. cluster := gocql . NewCluster ( hostname ) cluster . Port = port Next, we'll define our connection authenticator and pass our credentials to it. cluster . Authenticator = gocql . PasswordAuthenticator { Username : username , Password : password , } Finally, we'll need to process the filepaths of our TLS/X509 certificate, key, and certificate authority files. cert , _ := tls . LoadX509KeyPair ( certPath , keyPath ) caCert , err := ioutil . ReadFile ( caPath ) caCertPool := x509 . NewCertPool () caCertPool . AppendCertsFromPEM ( caCert ) tlsConfig := & tls . Config { Certificates : [] tls . Certificate { cert }, RootCAs : caCertPool , } We'll them pass our tlsConfig to the SslOpts property on the cluster object. cluster . SslOpts = & gocql . SslOptions { Config : tlsConfig , EnableHostVerification : false , } With all of that defined, we can open a connection to our cluster: session , err := cluster . CreateSession () if err != nil { fmt . Println ( err ) } defer session . Close () ctx := context . Background () If you get an error concerning a mismatch of the CQL protocol version at this point, try forcing protocol version 4 before the session code block above. cluster . ProtoVersion = 4 With a connection made, we can run a simple query to return the name of the cluster from the system.local table: var strClusterName string err2 := session . Query ( `SELECT cluster_name FROM system.local` ). WithContext ( ctx ). Scan ( & strClusterName ) if err2 != nil { fmt . Println ( err ) } else { fmt . Println ( \"cluster_name:\" , strClusterName ) } Running this code with arguments in the proper order should yield output similar to this: go run testCassandraSSL.go ce111111-1111-1111-1111-d11b1d4bc111-us-east1.db.astra.datastax.com token \"AstraCS:ASjPlHbTYourSecureTokenGoesHered3cdab53b\" /Users/aaronploetz/mySecureBundleDir/ca.crt /Users/aaronploetz/mySecureBundleDir/cert /Users/aaronploetz/mySecureBundleDir/key cluster_name: cndb The complete code to this example can be found here .","title":"3.1 The Gocql Cassandra Driver"},{"location":"pages/develop/languages/go/#32-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"3.2 Astra SDK"},{"location":"pages/develop/languages/go/#4-stargate-rest-api","text":"","title":"4. Stargate REST Api"},{"location":"pages/develop/languages/go/#41-axios","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"4.1 Axios"},{"location":"pages/develop/languages/go/#42-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"4.2 Astra SDK"},{"location":"pages/develop/languages/go/#5-stargate-document-api","text":"","title":"5. Stargate Document Api"},{"location":"pages/develop/languages/go/#51-axios","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"5.1 Axios"},{"location":"pages/develop/languages/go/#52-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"5.2 Astra SDK"},{"location":"pages/develop/languages/go/#6-stargate-graphql","text":"","title":"6 Stargate GraphQL"},{"location":"pages/develop/languages/go/#61-cql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.1 CQL First"},{"location":"pages/develop/languages/go/#62-graphql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.2 GraphQL First"},{"location":"pages/develop/languages/go/#7-stargate-grpc","text":"","title":"7. Stargate gRPC"},{"location":"pages/develop/languages/go/#71-stargate-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.1 Stargate Client"},{"location":"pages/develop/languages/go/#72-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.2 Astra SDK"},{"location":"pages/develop/languages/go/#8-pulsar-client","text":"","title":"8. Pulsar Client"},{"location":"pages/develop/languages/go/#81-pulsar-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.1 Pulsar Client"},{"location":"pages/develop/languages/go/#82-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.2 Astra SDK"},{"location":"pages/develop/languages/go/#9-pulsar-admin","text":"","title":"9. Pulsar Admin"},{"location":"pages/develop/languages/go/#10-devops-api-database","text":"","title":"10 Devops API Database"},{"location":"pages/develop/languages/go/#11-devops-api-organization","text":"","title":"11 Devops API Organization"},{"location":"pages/develop/languages/go/#12-devops-api-streaming","text":"package main import ( \"archive/zip\" \"context\" \"crypto/tls\" \"crypto/x509\" \"encoding/json\" \"fmt\" \"io\" \"io/ioutil\" \"os\" \"path/filepath\" \"strconv\" \"strings\" \"github.com/gocql/gocql\" ) type Config struct { Host string `json:\"host\"` Port int `json:\"cql_port\"` } func main () { var clientID = os . Getenv ( \"ASTRA_CLIENT_ID\" ) var clientSecret = os . Getenv ( \"ASTRA_CLIENT_SECRET\" ) var secureConnectBundle = os . Getenv ( \"SECURE_CONNECT_BUNDLE\" ) if clientID == \"\" || clientSecret == \"\" || secureConnectBundle == \"\" { panic ( \"missing required environment variables\" ) } secureBundleDir := os . TempDir () fmt . Printf ( \"extracting secure connect bundle [%s] to [%s]\\n\" , secureConnectBundle , secureBundleDir ) if err := Unzip ( secureConnectBundle , secureBundleDir ); err != nil { panic ( err ) } configPath , _ := filepath . Abs ( secureBundleDir + \"/config.json\" ) fmt . Println ( \"config: \" + configPath ) configData , _ := ioutil . ReadFile ( configPath ) var cfg Config json . Unmarshal ( configData , & cfg ) cluster := gocql . NewCluster ( cfg . Host ) cluster . Authenticator = gocql . PasswordAuthenticator { Username : clientID , Password : clientSecret , } host := cfg . Host + \":\" + strconv . Itoa ( cfg . Port ) cluster . Hosts = [] string { host } fmt . Println ( \"connecting to: \" + host ) certPath , _ := filepath . Abs ( secureBundleDir + \"/cert\" ) keyPath , _ := filepath . Abs ( secureBundleDir + \"/key\" ) caPath , _ := filepath . Abs ( secureBundleDir + \"/ca.crt\" ) cert , _ := tls . LoadX509KeyPair ( certPath , keyPath ) caCert , _ := ioutil . ReadFile ( caPath ) caCertPool := x509 . NewCertPool () caCertPool . AppendCertsFromPEM ( caCert ) cluster . SslOpts = & gocql . SslOptions { Config : & tls . Config { Certificates : [] tls . Certificate { cert }, ServerName : cfg . Host , RootCAs : caCertPool , }, } session , err := cluster . CreateSession () if err != nil { panic ( err ) } fmt . Printf ( \"session established: %v\\n\" , session ) var releaseVersion string if err := session . Query ( \"select release_version from system.local\" ). WithContext ( context . Background ()). Consistency ( gocql . One ). Scan ( & releaseVersion ); err != nil { panic ( err ) } fmt . Printf ( \"release version: %s\\n\" , releaseVersion ) } func Unzip ( src , dest string ) error { r , err := zip . OpenReader ( src ) if err != nil { return err } defer func () { if err := r . Close (); err != nil { panic ( err ) } }() os . MkdirAll ( dest , 0755 ) // Closure to address file descriptors issue with all the deferred .Close() methods extractAndWriteFile := func ( f * zip . File ) error { rc , err := f . Open () if err != nil { return err } defer func () { if err := rc . Close (); err != nil { panic ( err ) } }() path := filepath . Join ( dest , f . Name ) // Check for ZipSlip (Directory traversal) if ! strings . HasPrefix ( path , filepath . Clean ( dest ) + string ( os . PathSeparator )) { return fmt . Errorf ( \"illegal file path: %s\" , path ) } if f . FileInfo (). IsDir () { os . MkdirAll ( path , f . Mode ()) } else { os . MkdirAll ( filepath . Dir ( path ), f . Mode ()) f , err := os . OpenFile ( path , os . O_WRONLY | os . O_CREATE | os . O_TRUNC , f . Mode ()) if err != nil { return err } defer func () { if err := f . Close (); err != nil { panic ( err ) } }() _ , err = io . Copy ( f , rc ) if err != nil { return err } } return nil } for _ , f := range r . File { err := extractAndWriteFile ( f ) if err != nil { return err } } return nil }","title":"12 Devops API Streaming"},{"location":"pages/develop/languages/java/","text":"1. Overview \u00b6 The Astra platform provides multiple services such as; Databases and Streaming . For each service there are multiple Apis and interfaces available. In this page we will explain how to use each interface with JAVA. For each interface we list minimal dependencies and show you the minimal code . Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here . 2. Interfaces list \u00b6 Pick the interface you want to use from the list: Component Interface Description Astra DB Cassandra Native Drivers Use Cassandra with java native drivers and CQL language. Astra DB Rest CQL operations exposed as stateless rest resources. You will interact with this interface through an HttpClient . Astra DB Document Use Cassandra as a Document-Oriented database. You will interact with this interface through an HttpClient . Astra DB GraphQL CQL operations exposed as a GraqphQL interface. Each query and mutation definitions are generated from the table schema. You will interact with this interface through an HttpClient or GraphQL client like DGS . Astra DB gRPC CQL operations exposed as a gRPC API. You will interact with this interface through a grpc Client generated from the specification proto files Astra Streaming Pulsar Client Create Producer, Consumers, Subscriptions.. Astra Streaming Pulsar Admin Administrate your Pulsar cluster Astra Core Devops Apis Administration operations for the platform exposed as stateless HTTP Apis. You will interact with this interface through an HttpClient . 3. CQL Cassandra Drivers \u00b6 Driver reference documentation can be found HERE , this page is focused on connectivity with Astra DB only. Prerequisites Astra You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle Development environment You should install Java Development Kit (JDK) 8 : Use the reference documentation to install a Java Development Kit , Validate your installation with java --version You should install Apache Maven : Use the reference documentation and validate your installation with mvn -version 3.1 Drivers 4.x \u00b6 Version 4.x is recommended Version 4 is major redesign of the internal architecture. As such, it is not binary compatible with previous versions. However, most of the concepts remain unchanged, and the new API will look very familiar to 2.x and 3.x users. Setup pom.xml Any version 4.x should be compatible with Astra. Update your pom.xml file with the latest version of the 4.x libraries: <!-- (REQUIRED) --> <dependency> <groupId> com.datastax.oss </groupId> <artifactId> java-driver-core </artifactId> <version> ${latest4x} </version> </dependency> <!-- OPTIONAL --> <dependency> <groupId> com.datastax.oss </groupId> <artifactId> java-driver-query-builder </artifactId> <version> ${latest4x} </version> </dependency> <dependency> <groupId> com.datastax.oss </groupId> <artifactId> java-driver-mapper-runtime </artifactId> <version> ${latest4x} </version> </dependency> Sample Code import java.nio.file.Paths ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import com.datastax.oss.driver.api.core.CqlSession ; public class AstraDriver4x { static final String ASTRA_ZIP_FILE = \"<path_to_secureConnectBundle.zip>\" ; static final String ASTRA_USERNAME = \"<provide_a_clientId>\" ; static final String ASTRA_PASSWORD = \"<provide_a_clientSecret>\" ; static final String ASTRA_KEYSPACE = \"<provide_your_keyspace>\" ; public static void main ( String [] args ) { Logger logger = LoggerFactory . getLogger ( AstraDriver4x . class ); try ( CqlSession cqlSession = CqlSession . builder () . withCloudSecureConnectBundle ( Paths . get ( ASTRA_ZIP_FILE )) . withAuthCredentials ( ASTRA_USERNAME , ASTRA_PASSWORD ) . withKeyspace ( ASTRA_KEYSPACE ) . build ()) { logger . info ( \"[OK] Welcome to ASTRA. Connected to Keyspace {}\" , cqlSession . getKeyspace (). get ()); } logger . info ( \"[OK] Success\" ); System . exit ( 0 ); } } Extra Resources To learn more about the history of the 4.x Java driver, check out this blogpost . If you are still using 3.x and want to migrate you can have a look the upgrade guide but you can also keep using 3.x as described below Multiple Standalone Classes using driver 4.x Spring PetClinic in Reactive and especially the mapper Download Driver 4x Sample 3.2 Drivers 3.x \u00b6 Please note that version 3.8+ is required to connect to Astra. Setup pom.xml Update your pom.xml file with the latest version of the 3.x libraries: <dependency> <groupId> com.datastax.cassandra </groupId> <artifactId> cassandra-driver-core </artifactId> <version> ${latest3x} </version> </dependency> Sample Code import java.io.File ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import com.datastax.driver.core.Cluster ; import com.datastax.driver.core.Session ; public class AstraDriver3x { // Define inputs static final String ASTRA_ZIP_FILE = \"<path_to_secureConnectBundle.zip>\" ; static final String ASTRA_USERNAME = \"<provide_a_clientId>\" ; static final String ASTRA_PASSWORD = \"<provide_a_clientSecret>\" ; static final String ASTRA_KEYSPACE = \"<provide_your_keyspace>\" ; public static void main ( String [] args ) { Logger logger = LoggerFactory . getLogger ( AstraDriver3x . class ); try ( Cluster cluster = Cluster . builder () . withCloudSecureConnectBundle ( new File ( ASTRA_ZIP_FILE )) . withCredentials ( ASTRA_USERNAME , ASTRA_PASSWORD ) . build () ) { Session session = cluster . connect ( ASTRA_KEYSPACE ); logger . info ( \"[OK] Welcome to ASTRA. Connected to Keyspace {}\" , session . getLoggedKeyspace ()); } logger . info ( \"[OK] Success\" ); System . exit ( 0 ); } } Extra Resources for Cassandra Drivers 3.x Multiple Standalone Classes using driver 3.x Download Driver 3x Sample 3.3 Astra SDK \u00b6 This SDK (Software Development Kit) makes it easy to call Stargate and/or Astra services using idiomatic Java APIs. The Astra SDK sets up the connection to work with the AstraDB cloud-based service. You will work with the class AstraClient (which configures the StargateClient for you). As you can see on the figure below the AstraClient handles not only Stargate Apis but also the Astra Devops Api and Apache Pulsar. Reference documentation . Setup pom.xml Update your pom.xml file with the latest version of the SDK <dependencies> <dependency> <groupId> com.datastax.astra </groupId> <artifactId> astra-sdk </artifactId> <version> 0.3.0 </version> </dependency> </dependencies> Sample Code // Init Astra Client AstraClient cli = AstraClient . builder () . withToken ( ASTRA_DB_TOKEN ) . withDatabaseId ( ASTRA_DB_ID ) . withDatabaseRegion ( ASTRA_DB_REGION ) . withCqlKeyspace ( ASTRA_DB_KEYSPACE ) . enableCql () . build (); // Using Cassandra Interface CqlSession cqlSession = astraClient . cqlSession (); String cqlVersion = cqlSession . execute ( \"SELECT cql_version from system.local\" ) . one (). getString ( \"cql_version\" )); Extra Resources To get the full fledged information regarding the SDK check the github repository Download SDK Sample 4. Stargate REST Api \u00b6 \u2139\ufe0f Overview Stargate is a data gateway (Proxy) on top of Apache Cassandra which exposes new interfaces to simplify application integration. It is a way to create stateless components and ease the integration with 4 different HTTP Apis (rest, doc, graphQL, gRPC). In this chapter we will cover integration with REST Apis also called DATA in the swagger specifications. To know more regarding this interface specially you can have a look to dedicated section of the wiki or reference Stargate Rest Api Quick Start Guide . \u26a0\ufe0f We recommend to use version V2 ( with V2 in the URL ) as it covers more features and the V1 would be deprecated sooner. \ud83d\udce6 Prerequisites [ASTRA] You should have an Astra account You should Create an Astra Database You should Have an Astra Token \ud83d\udce6 Prerequisites [Development Environment] You should install Java Development Kit (JDK) 8 : Use the reference documentation to install a Java Development Kit . You should install Apache Maven : Use the reference documentation and validate your installation with mvn -version \ud83d\udce6 Setup Project You simply need an HTTP Client to use the Rest API. There are a lot of clients in the Java languages like HttpURLConnection , HttpClient introduced in Java 11 , Apache HTTPClient , OkHttpClient , Jetty HttpClient . A comparison is provided is this blogpost to make your choice. In this tutorial, we will use the Apache HttpClient , which is included in the SDK. You should adapt the code depending on the framework you have chosen. Import relevant dependencies for Apache Http Client in your pom.xml <dependency> <groupId> org.apache.httpcomponents.client5 </groupId> <artifactId> httpclient5 </artifactId> <version> 5.1.3 </version> </dependency> \ud83d\udda5\ufe0f Sample Code (project astra-httpclient-restapi ) View Code public class AstraRestApiHttpClient { static final String ASTRA_TOKEN = \"<change_with_your_token>\" ; static final String ASTRA_DB_ID = \"<change_with_your_database_identifier>\" ; static final String ASTRA_DB_REGION = \"<change_with_your_database_region>\" ; static final String ASTRA_DB_KEYSPACE = \"<change_with_your_keyspace>\" ; static Logger logger = LoggerFactory . getLogger ( AstraRestApiHttpClient . class ); public static void main ( String [] args ) throws Exception { String apiRestEndpoint = new StringBuilder ( \"https://\" ) . append ( ASTRA_DB_ID ). append ( \"-\" ) . append ( ASTRA_DB_REGION ) . append ( \".apps.astra.datastax.com/api/rest\" ) . toString (); logger . info ( \"Rest Endpoint is {}\" , apiRestEndpoint ); try ( CloseableHttpClient httpClient = HttpClients . createDefault ()) { listKeyspaces ( httpClient , apiRestEndpoint ); createTable ( httpClient , apiRestEndpoint ); insertRow ( httpClient , apiRestEndpoint ); retrieveRow ( httpClient , apiRestEndpoint ); } logger . info ( \"[OK] Success\" ); System . exit ( 0 ); } List keyspaces private static void listKeyspaces ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { // Build Request HttpGet listKeyspacesReq = new HttpGet ( apiRestEndpoint + \"/v2/schemas/keyspaces\" ); listKeyspacesReq . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( listKeyspacesReq )) { if ( 200 == res . getCode ()) { logger . info ( \"[OK] Keyspaces list retrieved\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } Create a Table Query used is createTableJson here: { \"name\" : \"users\" , \"columnDefinitions\" : [ { \"name\" : \"firstname\" , \"typeDefinition\" : \"text\" }, { \"name\" : \"lastname\" , \"typeDefinition\" : \"text\" }, { \"name\" : \"email\" , \"typeDefinition\" : \"text\" }, { \"name\" : \"color\" , \"typeDefinition\" : \"text\" } ], \"primaryKey\" : { \"partitionKey\" : [ \"firstname\" ], \"clusteringKey\" : [ \"lastname\" ] }, \"tableOptions\" : { \"defaultTimeToLive\" : 0 , \"clusteringExpression\" : [{ \"column\" : \"lastname\" , \"order\" : \"ASC\" }] } } Create Table code private static void createTable ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { HttpPost createTableReq = new HttpPost ( apiRestEndpoint + \"/v2/schemas/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/tables\" ); createTableReq . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); String createTableJson = \"{...JSON.....}\" ; createTableReq . setEntity ( new StringEntity ( createTableJson , ContentType . APPLICATION_JSON )); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( createTableReq )) { if ( 201 == res . getCode ()) { logger . info ( \"[OK] Table Created (if needed)\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } Insert a Row private static void insertRow ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { HttpPost insertCedrick = new HttpPost ( apiRestEndpoint + \"/v2/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/users\" ); insertCedrick . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); insertCedrick . setEntity ( new StringEntity ( \"{\" + \" \\\"firstname\\\": \\\"Cedrick\\\",\" + \" \\\"lastname\\\" : \\\"Lunven\\\",\" + \" \\\"email\\\" : \\\"c.lunven@gmail.com\\\",\" + \" \\\"color\\\" : \\\"blue\\\" }\" , ContentType . APPLICATION_JSON )); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( insertCedrick )) { if ( 201 == res . getCode ()) { logger . info ( \"[OK] Row inserted\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } Retrieve a row private static void retrieveRow ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { // Build Request HttpGet rowReq = new HttpGet ( apiRestEndpoint + \"/v2/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/users/Cedrick/Lunven\" ); rowReq . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( rowReq )) { if ( 200 == res . getCode ()) { String payload = EntityUtils . toString ( res . getEntity ()); logger . info ( \"[OK] Row retrieved\" ); logger . info ( \"Row retrieved : {}\" , payload ); } } } 5. Stargate Document Api \u00b6 \u2139\ufe0f Overview The Document API is an HTTP REST API and part of the open source Stargate.io. The idea is to provide an abstraction on top of Apache Cassandra\u2122 to allow document-oriented access patterns. To get familiar with it you can access documentation and sandbox here 5.1 Http Client \u00b6 \ud83d\udce6 Prerequisites You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should install Java Development Kit (JDK) 8 : Use the reference documentation to install a Java Development Kit . You should install Apache Maven : Use the reference documentation and validate your installation with mvn -version \ud83d\udce6 Setup Project Import relevant dependencies for Apache Http Client in your pom.xml . Jackon is also helpful to serialize or unserialized Java Objects as JSON. <dependency> <groupId> org.apache.httpcomponents.client5 </groupId> <artifactId> httpclient5 </artifactId> <version> 5.1.3 </version> </dependency> View Code Execute Code Download Project static final String ASTRA_TOKEN = \"change_me\" ; static final String ASTRA_DB_ID = \"change_me\" ; static final String ASTRA_DB_REGION = \"change_me\" ; static final String ASTRA_DB_KEYSPACE = \"change_me\" ; static Logger logger = LoggerFactory . getLogger ( AstraDocApiHttpClient . class ); public static void main ( String [] args ) throws Exception { try ( CloseableHttpClient httpClient = HttpClients . createDefault ()) { // Build Request String apiRestEndpoint = new StringBuilder ( \"https://\" ) . append ( ASTRA_DB_ID ). append ( \"-\" ) . append ( ASTRA_DB_REGION ) . append ( \".apps.astra.datastax.com/api/rest\" ) . toString (); HttpGet req = new HttpGet ( apiRestEndpoint + \"/v2/schemas/namespaces\" ); req . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( req )) { if ( 200 == res . getCode ()) { logger . info ( \"[OK] Namespaces list retrieved\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } } Download Document Api Code 5.2 Astra SDK \u00b6 \ud83d\udce6 Prerequisites You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should install Java Development Kit (JDK) 8 : Use the reference documentation to install a Java Development Kit . You should install Apache Maven : Use the reference documentation and validate your installation with mvn -version \ud83d\udce6 Setup Project Import relevant dependencies for Astra SDK in your pom.xml . <dependency> <groupId> com.datastax.astra </groupId> <artifactId> astra-sdk </artifactId> <version> 0.3.0 </version> </dependency> View Code Execute Code Download Project public static final String ASTRA_DB_TOKEN = \"CHANGE_ME\" ; public static final String ASTRA_DB_ID = \"CHANGE_ME\" ; public static final String ASTRA_DB_REGION = \"CHANGE_ME\" ; public static void main ( String [] args ) { try ( AstraClient astraClient = AstraClient . builder () . withToken ( ASTRA_DB_TOKEN ) . withDatabaseId ( ASTRA_DB_ID ) . withDatabaseRegion ( ASTRA_DB_REGION ) . build ()) { System . out . println ( \"+ Namespaces (doc) : \" + astraClient . apiStargateDocument () . namespaceNames () . collect ( Collectors . toList ())); } } Download Document Api Code 6 Stargate GraphQL \u00b6 6.1 CQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 6.2 GraphQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7. Stargate gRPC \u00b6 7.1 Stargate Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8. Pulsar Client \u00b6 8.1 Pulsar Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 9. Pulsar Admin \u00b6 10 Devops API \u00b6","title":"\u2022 Java"},{"location":"pages/develop/languages/java/#1-overview","text":"The Astra platform provides multiple services such as; Databases and Streaming . For each service there are multiple Apis and interfaces available. In this page we will explain how to use each interface with JAVA. For each interface we list minimal dependencies and show you the minimal code . Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here .","title":"1. Overview"},{"location":"pages/develop/languages/java/#2-interfaces-list","text":"Pick the interface you want to use from the list: Component Interface Description Astra DB Cassandra Native Drivers Use Cassandra with java native drivers and CQL language. Astra DB Rest CQL operations exposed as stateless rest resources. You will interact with this interface through an HttpClient . Astra DB Document Use Cassandra as a Document-Oriented database. You will interact with this interface through an HttpClient . Astra DB GraphQL CQL operations exposed as a GraqphQL interface. Each query and mutation definitions are generated from the table schema. You will interact with this interface through an HttpClient or GraphQL client like DGS . Astra DB gRPC CQL operations exposed as a gRPC API. You will interact with this interface through a grpc Client generated from the specification proto files Astra Streaming Pulsar Client Create Producer, Consumers, Subscriptions.. Astra Streaming Pulsar Admin Administrate your Pulsar cluster Astra Core Devops Apis Administration operations for the platform exposed as stateless HTTP Apis. You will interact with this interface through an HttpClient .","title":"2. Interfaces list"},{"location":"pages/develop/languages/java/#3-cql-cassandra-drivers","text":"Driver reference documentation can be found HERE , this page is focused on connectivity with Astra DB only. Prerequisites Astra You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle Development environment You should install Java Development Kit (JDK) 8 : Use the reference documentation to install a Java Development Kit , Validate your installation with java --version You should install Apache Maven : Use the reference documentation and validate your installation with mvn -version","title":"3. CQL Cassandra Drivers"},{"location":"pages/develop/languages/java/#31-drivers-4x","text":"Version 4.x is recommended Version 4 is major redesign of the internal architecture. As such, it is not binary compatible with previous versions. However, most of the concepts remain unchanged, and the new API will look very familiar to 2.x and 3.x users. Setup pom.xml Any version 4.x should be compatible with Astra. Update your pom.xml file with the latest version of the 4.x libraries: <!-- (REQUIRED) --> <dependency> <groupId> com.datastax.oss </groupId> <artifactId> java-driver-core </artifactId> <version> ${latest4x} </version> </dependency> <!-- OPTIONAL --> <dependency> <groupId> com.datastax.oss </groupId> <artifactId> java-driver-query-builder </artifactId> <version> ${latest4x} </version> </dependency> <dependency> <groupId> com.datastax.oss </groupId> <artifactId> java-driver-mapper-runtime </artifactId> <version> ${latest4x} </version> </dependency> Sample Code import java.nio.file.Paths ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import com.datastax.oss.driver.api.core.CqlSession ; public class AstraDriver4x { static final String ASTRA_ZIP_FILE = \"<path_to_secureConnectBundle.zip>\" ; static final String ASTRA_USERNAME = \"<provide_a_clientId>\" ; static final String ASTRA_PASSWORD = \"<provide_a_clientSecret>\" ; static final String ASTRA_KEYSPACE = \"<provide_your_keyspace>\" ; public static void main ( String [] args ) { Logger logger = LoggerFactory . getLogger ( AstraDriver4x . class ); try ( CqlSession cqlSession = CqlSession . builder () . withCloudSecureConnectBundle ( Paths . get ( ASTRA_ZIP_FILE )) . withAuthCredentials ( ASTRA_USERNAME , ASTRA_PASSWORD ) . withKeyspace ( ASTRA_KEYSPACE ) . build ()) { logger . info ( \"[OK] Welcome to ASTRA. Connected to Keyspace {}\" , cqlSession . getKeyspace (). get ()); } logger . info ( \"[OK] Success\" ); System . exit ( 0 ); } } Extra Resources To learn more about the history of the 4.x Java driver, check out this blogpost . If you are still using 3.x and want to migrate you can have a look the upgrade guide but you can also keep using 3.x as described below Multiple Standalone Classes using driver 4.x Spring PetClinic in Reactive and especially the mapper Download Driver 4x Sample","title":"3.1 Drivers 4.x"},{"location":"pages/develop/languages/java/#32-drivers-3x","text":"Please note that version 3.8+ is required to connect to Astra. Setup pom.xml Update your pom.xml file with the latest version of the 3.x libraries: <dependency> <groupId> com.datastax.cassandra </groupId> <artifactId> cassandra-driver-core </artifactId> <version> ${latest3x} </version> </dependency> Sample Code import java.io.File ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import com.datastax.driver.core.Cluster ; import com.datastax.driver.core.Session ; public class AstraDriver3x { // Define inputs static final String ASTRA_ZIP_FILE = \"<path_to_secureConnectBundle.zip>\" ; static final String ASTRA_USERNAME = \"<provide_a_clientId>\" ; static final String ASTRA_PASSWORD = \"<provide_a_clientSecret>\" ; static final String ASTRA_KEYSPACE = \"<provide_your_keyspace>\" ; public static void main ( String [] args ) { Logger logger = LoggerFactory . getLogger ( AstraDriver3x . class ); try ( Cluster cluster = Cluster . builder () . withCloudSecureConnectBundle ( new File ( ASTRA_ZIP_FILE )) . withCredentials ( ASTRA_USERNAME , ASTRA_PASSWORD ) . build () ) { Session session = cluster . connect ( ASTRA_KEYSPACE ); logger . info ( \"[OK] Welcome to ASTRA. Connected to Keyspace {}\" , session . getLoggedKeyspace ()); } logger . info ( \"[OK] Success\" ); System . exit ( 0 ); } } Extra Resources for Cassandra Drivers 3.x Multiple Standalone Classes using driver 3.x Download Driver 3x Sample","title":"3.2 Drivers 3.x"},{"location":"pages/develop/languages/java/#33-astra-sdk","text":"This SDK (Software Development Kit) makes it easy to call Stargate and/or Astra services using idiomatic Java APIs. The Astra SDK sets up the connection to work with the AstraDB cloud-based service. You will work with the class AstraClient (which configures the StargateClient for you). As you can see on the figure below the AstraClient handles not only Stargate Apis but also the Astra Devops Api and Apache Pulsar. Reference documentation . Setup pom.xml Update your pom.xml file with the latest version of the SDK <dependencies> <dependency> <groupId> com.datastax.astra </groupId> <artifactId> astra-sdk </artifactId> <version> 0.3.0 </version> </dependency> </dependencies> Sample Code // Init Astra Client AstraClient cli = AstraClient . builder () . withToken ( ASTRA_DB_TOKEN ) . withDatabaseId ( ASTRA_DB_ID ) . withDatabaseRegion ( ASTRA_DB_REGION ) . withCqlKeyspace ( ASTRA_DB_KEYSPACE ) . enableCql () . build (); // Using Cassandra Interface CqlSession cqlSession = astraClient . cqlSession (); String cqlVersion = cqlSession . execute ( \"SELECT cql_version from system.local\" ) . one (). getString ( \"cql_version\" )); Extra Resources To get the full fledged information regarding the SDK check the github repository Download SDK Sample","title":"3.3 Astra SDK"},{"location":"pages/develop/languages/java/#4-stargate-rest-api","text":"\u2139\ufe0f Overview Stargate is a data gateway (Proxy) on top of Apache Cassandra which exposes new interfaces to simplify application integration. It is a way to create stateless components and ease the integration with 4 different HTTP Apis (rest, doc, graphQL, gRPC). In this chapter we will cover integration with REST Apis also called DATA in the swagger specifications. To know more regarding this interface specially you can have a look to dedicated section of the wiki or reference Stargate Rest Api Quick Start Guide . \u26a0\ufe0f We recommend to use version V2 ( with V2 in the URL ) as it covers more features and the V1 would be deprecated sooner. \ud83d\udce6 Prerequisites [ASTRA] You should have an Astra account You should Create an Astra Database You should Have an Astra Token \ud83d\udce6 Prerequisites [Development Environment] You should install Java Development Kit (JDK) 8 : Use the reference documentation to install a Java Development Kit . You should install Apache Maven : Use the reference documentation and validate your installation with mvn -version \ud83d\udce6 Setup Project You simply need an HTTP Client to use the Rest API. There are a lot of clients in the Java languages like HttpURLConnection , HttpClient introduced in Java 11 , Apache HTTPClient , OkHttpClient , Jetty HttpClient . A comparison is provided is this blogpost to make your choice. In this tutorial, we will use the Apache HttpClient , which is included in the SDK. You should adapt the code depending on the framework you have chosen. Import relevant dependencies for Apache Http Client in your pom.xml <dependency> <groupId> org.apache.httpcomponents.client5 </groupId> <artifactId> httpclient5 </artifactId> <version> 5.1.3 </version> </dependency> \ud83d\udda5\ufe0f Sample Code (project astra-httpclient-restapi ) View Code public class AstraRestApiHttpClient { static final String ASTRA_TOKEN = \"<change_with_your_token>\" ; static final String ASTRA_DB_ID = \"<change_with_your_database_identifier>\" ; static final String ASTRA_DB_REGION = \"<change_with_your_database_region>\" ; static final String ASTRA_DB_KEYSPACE = \"<change_with_your_keyspace>\" ; static Logger logger = LoggerFactory . getLogger ( AstraRestApiHttpClient . class ); public static void main ( String [] args ) throws Exception { String apiRestEndpoint = new StringBuilder ( \"https://\" ) . append ( ASTRA_DB_ID ). append ( \"-\" ) . append ( ASTRA_DB_REGION ) . append ( \".apps.astra.datastax.com/api/rest\" ) . toString (); logger . info ( \"Rest Endpoint is {}\" , apiRestEndpoint ); try ( CloseableHttpClient httpClient = HttpClients . createDefault ()) { listKeyspaces ( httpClient , apiRestEndpoint ); createTable ( httpClient , apiRestEndpoint ); insertRow ( httpClient , apiRestEndpoint ); retrieveRow ( httpClient , apiRestEndpoint ); } logger . info ( \"[OK] Success\" ); System . exit ( 0 ); } List keyspaces private static void listKeyspaces ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { // Build Request HttpGet listKeyspacesReq = new HttpGet ( apiRestEndpoint + \"/v2/schemas/keyspaces\" ); listKeyspacesReq . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( listKeyspacesReq )) { if ( 200 == res . getCode ()) { logger . info ( \"[OK] Keyspaces list retrieved\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } Create a Table Query used is createTableJson here: { \"name\" : \"users\" , \"columnDefinitions\" : [ { \"name\" : \"firstname\" , \"typeDefinition\" : \"text\" }, { \"name\" : \"lastname\" , \"typeDefinition\" : \"text\" }, { \"name\" : \"email\" , \"typeDefinition\" : \"text\" }, { \"name\" : \"color\" , \"typeDefinition\" : \"text\" } ], \"primaryKey\" : { \"partitionKey\" : [ \"firstname\" ], \"clusteringKey\" : [ \"lastname\" ] }, \"tableOptions\" : { \"defaultTimeToLive\" : 0 , \"clusteringExpression\" : [{ \"column\" : \"lastname\" , \"order\" : \"ASC\" }] } } Create Table code private static void createTable ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { HttpPost createTableReq = new HttpPost ( apiRestEndpoint + \"/v2/schemas/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/tables\" ); createTableReq . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); String createTableJson = \"{...JSON.....}\" ; createTableReq . setEntity ( new StringEntity ( createTableJson , ContentType . APPLICATION_JSON )); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( createTableReq )) { if ( 201 == res . getCode ()) { logger . info ( \"[OK] Table Created (if needed)\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } Insert a Row private static void insertRow ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { HttpPost insertCedrick = new HttpPost ( apiRestEndpoint + \"/v2/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/users\" ); insertCedrick . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); insertCedrick . setEntity ( new StringEntity ( \"{\" + \" \\\"firstname\\\": \\\"Cedrick\\\",\" + \" \\\"lastname\\\" : \\\"Lunven\\\",\" + \" \\\"email\\\" : \\\"c.lunven@gmail.com\\\",\" + \" \\\"color\\\" : \\\"blue\\\" }\" , ContentType . APPLICATION_JSON )); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( insertCedrick )) { if ( 201 == res . getCode ()) { logger . info ( \"[OK] Row inserted\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } Retrieve a row private static void retrieveRow ( CloseableHttpClient httpClient , String apiRestEndpoint ) throws Exception { // Build Request HttpGet rowReq = new HttpGet ( apiRestEndpoint + \"/v2/keyspaces/\" + ASTRA_DB_KEYSPACE + \"/users/Cedrick/Lunven\" ); rowReq . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( rowReq )) { if ( 200 == res . getCode ()) { String payload = EntityUtils . toString ( res . getEntity ()); logger . info ( \"[OK] Row retrieved\" ); logger . info ( \"Row retrieved : {}\" , payload ); } } }","title":"4. Stargate REST Api"},{"location":"pages/develop/languages/java/#5-stargate-document-api","text":"\u2139\ufe0f Overview The Document API is an HTTP REST API and part of the open source Stargate.io. The idea is to provide an abstraction on top of Apache Cassandra\u2122 to allow document-oriented access patterns. To get familiar with it you can access documentation and sandbox here","title":"5. Stargate Document Api"},{"location":"pages/develop/languages/java/#51-http-client","text":"\ud83d\udce6 Prerequisites You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should install Java Development Kit (JDK) 8 : Use the reference documentation to install a Java Development Kit . You should install Apache Maven : Use the reference documentation and validate your installation with mvn -version \ud83d\udce6 Setup Project Import relevant dependencies for Apache Http Client in your pom.xml . Jackon is also helpful to serialize or unserialized Java Objects as JSON. <dependency> <groupId> org.apache.httpcomponents.client5 </groupId> <artifactId> httpclient5 </artifactId> <version> 5.1.3 </version> </dependency> View Code Execute Code Download Project static final String ASTRA_TOKEN = \"change_me\" ; static final String ASTRA_DB_ID = \"change_me\" ; static final String ASTRA_DB_REGION = \"change_me\" ; static final String ASTRA_DB_KEYSPACE = \"change_me\" ; static Logger logger = LoggerFactory . getLogger ( AstraDocApiHttpClient . class ); public static void main ( String [] args ) throws Exception { try ( CloseableHttpClient httpClient = HttpClients . createDefault ()) { // Build Request String apiRestEndpoint = new StringBuilder ( \"https://\" ) . append ( ASTRA_DB_ID ). append ( \"-\" ) . append ( ASTRA_DB_REGION ) . append ( \".apps.astra.datastax.com/api/rest\" ) . toString (); HttpGet req = new HttpGet ( apiRestEndpoint + \"/v2/schemas/namespaces\" ); req . addHeader ( \"X-Cassandra-Token\" , ASTRA_TOKEN ); // Execute Request try ( CloseableHttpResponse res = httpClient . execute ( req )) { if ( 200 == res . getCode ()) { logger . info ( \"[OK] Namespaces list retrieved\" ); logger . info ( \"Returned message: {}\" , EntityUtils . toString ( res . getEntity ())); } } } } Download Document Api Code","title":"5.1 Http Client"},{"location":"pages/develop/languages/java/#52-astra-sdk","text":"\ud83d\udce6 Prerequisites You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should install Java Development Kit (JDK) 8 : Use the reference documentation to install a Java Development Kit . You should install Apache Maven : Use the reference documentation and validate your installation with mvn -version \ud83d\udce6 Setup Project Import relevant dependencies for Astra SDK in your pom.xml . <dependency> <groupId> com.datastax.astra </groupId> <artifactId> astra-sdk </artifactId> <version> 0.3.0 </version> </dependency> View Code Execute Code Download Project public static final String ASTRA_DB_TOKEN = \"CHANGE_ME\" ; public static final String ASTRA_DB_ID = \"CHANGE_ME\" ; public static final String ASTRA_DB_REGION = \"CHANGE_ME\" ; public static void main ( String [] args ) { try ( AstraClient astraClient = AstraClient . builder () . withToken ( ASTRA_DB_TOKEN ) . withDatabaseId ( ASTRA_DB_ID ) . withDatabaseRegion ( ASTRA_DB_REGION ) . build ()) { System . out . println ( \"+ Namespaces (doc) : \" + astraClient . apiStargateDocument () . namespaceNames () . collect ( Collectors . toList ())); } } Download Document Api Code","title":"5.2 Astra SDK"},{"location":"pages/develop/languages/java/#6-stargate-graphql","text":"","title":"6 Stargate GraphQL"},{"location":"pages/develop/languages/java/#61-cql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.1 CQL First"},{"location":"pages/develop/languages/java/#62-graphql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.2 GraphQL First"},{"location":"pages/develop/languages/java/#7-stargate-grpc","text":"","title":"7. Stargate gRPC"},{"location":"pages/develop/languages/java/#71-stargate-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.1 Stargate Client"},{"location":"pages/develop/languages/java/#72-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.2 Astra SDK"},{"location":"pages/develop/languages/java/#8-pulsar-client","text":"","title":"8. Pulsar Client"},{"location":"pages/develop/languages/java/#81-pulsar-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.1 Pulsar Client"},{"location":"pages/develop/languages/java/#82-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.2 Astra SDK"},{"location":"pages/develop/languages/java/#9-pulsar-admin","text":"","title":"9. Pulsar Admin"},{"location":"pages/develop/languages/java/#10-devops-api","text":"","title":"10 Devops API"},{"location":"pages/develop/languages/javascript/","text":"1. Overview \u00b6 Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces . There are different frameworks and tools to connect to Astra depending on the Api interface you choose. Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information here . 2. Interfaces List \u00b6 Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming 3. CQL \u00b6 3.1 Cassandra Drivers \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 3.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 4. Stargate REST Api \u00b6 4.1 Axios \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 4.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 5. Stargate Document Api \u00b6 5.1 Axios \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 5.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 6 Stargate GraphQL \u00b6 6.1 CQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 6.2 GraphQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7. Stargate gRPC \u00b6 7.1 Stargate Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8. Pulsar Client \u00b6 8.1 Pulsar Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 9. Pulsar Admin \u00b6 10 Devops API Database \u00b6 11 Devops API Organization \u00b6 12 Devops API Streaming \u00b6","title":"\u2022 Javascript"},{"location":"pages/develop/languages/javascript/#1-overview","text":"Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces . There are different frameworks and tools to connect to Astra depending on the Api interface you choose. Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information here .","title":"1. Overview"},{"location":"pages/develop/languages/javascript/#2-interfaces-list","text":"Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming","title":"2. Interfaces List"},{"location":"pages/develop/languages/javascript/#3-cql","text":"","title":"3. CQL"},{"location":"pages/develop/languages/javascript/#31-cassandra-drivers","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"3.1 Cassandra Drivers"},{"location":"pages/develop/languages/javascript/#32-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"3.2 Astra SDK"},{"location":"pages/develop/languages/javascript/#4-stargate-rest-api","text":"","title":"4. Stargate REST Api"},{"location":"pages/develop/languages/javascript/#41-axios","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"4.1 Axios"},{"location":"pages/develop/languages/javascript/#42-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"4.2 Astra SDK"},{"location":"pages/develop/languages/javascript/#5-stargate-document-api","text":"","title":"5. Stargate Document Api"},{"location":"pages/develop/languages/javascript/#51-axios","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"5.1 Axios"},{"location":"pages/develop/languages/javascript/#52-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"5.2 Astra SDK"},{"location":"pages/develop/languages/javascript/#6-stargate-graphql","text":"","title":"6 Stargate GraphQL"},{"location":"pages/develop/languages/javascript/#61-cql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.1 CQL First"},{"location":"pages/develop/languages/javascript/#62-graphql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.2 GraphQL First"},{"location":"pages/develop/languages/javascript/#7-stargate-grpc","text":"","title":"7. Stargate gRPC"},{"location":"pages/develop/languages/javascript/#71-stargate-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.1 Stargate Client"},{"location":"pages/develop/languages/javascript/#72-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.2 Astra SDK"},{"location":"pages/develop/languages/javascript/#8-pulsar-client","text":"","title":"8. Pulsar Client"},{"location":"pages/develop/languages/javascript/#81-pulsar-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.1 Pulsar Client"},{"location":"pages/develop/languages/javascript/#82-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.2 Astra SDK"},{"location":"pages/develop/languages/javascript/#9-pulsar-admin","text":"","title":"9. Pulsar Admin"},{"location":"pages/develop/languages/javascript/#10-devops-api-database","text":"","title":"10 Devops API Database"},{"location":"pages/develop/languages/javascript/#11-devops-api-organization","text":"","title":"11 Devops API Organization"},{"location":"pages/develop/languages/javascript/#12-devops-api-streaming","text":"","title":"12 Devops API Streaming"},{"location":"pages/develop/languages/python/","text":"1. Overview \u00b6 Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces . There are different frameworks and tools to connect to Astra depending on the Api interface you choose. Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here . 2. Interfaces List \u00b6 Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming 3. CQL \u00b6 3.1 Cassandra Drivers \u00b6 \u2139\ufe0f Overview These instructions are aimed at helping people connect to Astra DB programmatically using the DataStax Python driver. \ud83d\udce6 Prerequisites [ASTRA] You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle \ud83d\udce6 Prerequisites [Development Environment] You will need a recent version of Python 3. Visit https://www.python.org/downloads/ for more information on downloads and installation instructions for your machine architecture. To verify your Python install, run the following command: python -V With Python installed locally, you can now use Pip (Python's package manager) to install the DataStax Python driver. pip install cassandra-driver You can verify that the DataStax Python driver was installed successfully with this command: python -c 'import cassandra; print (cassandra.__version__)' \ud83d\udce6 Setup Project Create a new file and/or directory for your Python program. mkdir python_project cd python_project touch testAstra.py \ud83d\udda5\ufe0f Sample Code To connect to an Astra DB cluster, you will need a secure token generated specifically for use with your Astra DB cluster. mkdir ~/mySecureBundleDir cd ~/mySecureBundleDir mv ~/Downloads/secure-connect-bundle.zip . Open up your favorite editor or IDE, and add 3 imports: from cassandra.cluster import Cluster from cassandra.auth import PlainTextAuthProvider import sys Next we will inject the connection parameters into the code. This can be done either by reading them as environment variables or passing them as command line arguments. This example will be done using command line arguments: clientID = sys . argv [ 1 ] secret = sys . argv [ 2 ] secureBundleLocation = sys . argv [ 3 ] We'll also define the location of our secure connect bundle, and set that as a property in our cloud_config : cloud_config = { 'secure_connect_bundle' : secureBundleLocation } Next, we'll define our authenticator and pass our credentials to it. auth_provider = PlainTextAuthProvider ( clientID , secret ) With all of that defined, we can build a cluster object and a connection: cluster = Cluster ( cloud = cloud_config , auth_provider = auth_provider ) session = cluster . connect () With a connection made, we can run a simple query to return the name of the cluster from the system.local table: row = session . execute ( \"select cluster_name from system.local\" ) . one () if row : print ( row [ 0 ]) else : print ( \"An error occurred.\" ) Running this code with arguments in the proper order should yield output similar to this: python testAstra.py token \"AstraCS:ASjPlHbTYourSecureTokenGoesHered3cdab53b\" /Users/aaronploetz/mySecureBundleDir/secure-connect-bundle.zip cndb The complete code to this example can be found here . 3.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 4. Stargate REST Api \u00b6 4.1 Axios \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 4.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 5. Stargate Document Api \u00b6 5.1 Axios \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 5.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 6 Stargate GraphQL \u00b6 6.1 CQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 6.2 GraphQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7. Stargate gRPC \u00b6 7.1 Stargate Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8. Pulsar Client \u00b6 8.1 Pulsar Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 9. Pulsar Admin \u00b6 10 Devops API Database \u00b6 11 Devops API Organization \u00b6 12 Devops API Streaming \u00b6","title":"\u2022 Python"},{"location":"pages/develop/languages/python/#1-overview","text":"Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces . There are different frameworks and tools to connect to Astra depending on the Api interface you choose. Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here .","title":"1. Overview"},{"location":"pages/develop/languages/python/#2-interfaces-list","text":"Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming","title":"2. Interfaces List"},{"location":"pages/develop/languages/python/#3-cql","text":"","title":"3. CQL"},{"location":"pages/develop/languages/python/#31-cassandra-drivers","text":"\u2139\ufe0f Overview These instructions are aimed at helping people connect to Astra DB programmatically using the DataStax Python driver. \ud83d\udce6 Prerequisites [ASTRA] You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle \ud83d\udce6 Prerequisites [Development Environment] You will need a recent version of Python 3. Visit https://www.python.org/downloads/ for more information on downloads and installation instructions for your machine architecture. To verify your Python install, run the following command: python -V With Python installed locally, you can now use Pip (Python's package manager) to install the DataStax Python driver. pip install cassandra-driver You can verify that the DataStax Python driver was installed successfully with this command: python -c 'import cassandra; print (cassandra.__version__)' \ud83d\udce6 Setup Project Create a new file and/or directory for your Python program. mkdir python_project cd python_project touch testAstra.py \ud83d\udda5\ufe0f Sample Code To connect to an Astra DB cluster, you will need a secure token generated specifically for use with your Astra DB cluster. mkdir ~/mySecureBundleDir cd ~/mySecureBundleDir mv ~/Downloads/secure-connect-bundle.zip . Open up your favorite editor or IDE, and add 3 imports: from cassandra.cluster import Cluster from cassandra.auth import PlainTextAuthProvider import sys Next we will inject the connection parameters into the code. This can be done either by reading them as environment variables or passing them as command line arguments. This example will be done using command line arguments: clientID = sys . argv [ 1 ] secret = sys . argv [ 2 ] secureBundleLocation = sys . argv [ 3 ] We'll also define the location of our secure connect bundle, and set that as a property in our cloud_config : cloud_config = { 'secure_connect_bundle' : secureBundleLocation } Next, we'll define our authenticator and pass our credentials to it. auth_provider = PlainTextAuthProvider ( clientID , secret ) With all of that defined, we can build a cluster object and a connection: cluster = Cluster ( cloud = cloud_config , auth_provider = auth_provider ) session = cluster . connect () With a connection made, we can run a simple query to return the name of the cluster from the system.local table: row = session . execute ( \"select cluster_name from system.local\" ) . one () if row : print ( row [ 0 ]) else : print ( \"An error occurred.\" ) Running this code with arguments in the proper order should yield output similar to this: python testAstra.py token \"AstraCS:ASjPlHbTYourSecureTokenGoesHered3cdab53b\" /Users/aaronploetz/mySecureBundleDir/secure-connect-bundle.zip cndb The complete code to this example can be found here .","title":"3.1 Cassandra Drivers"},{"location":"pages/develop/languages/python/#32-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"3.2 Astra SDK"},{"location":"pages/develop/languages/python/#4-stargate-rest-api","text":"","title":"4. Stargate REST Api"},{"location":"pages/develop/languages/python/#41-axios","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"4.1 Axios"},{"location":"pages/develop/languages/python/#42-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"4.2 Astra SDK"},{"location":"pages/develop/languages/python/#5-stargate-document-api","text":"","title":"5. Stargate Document Api"},{"location":"pages/develop/languages/python/#51-axios","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"5.1 Axios"},{"location":"pages/develop/languages/python/#52-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"5.2 Astra SDK"},{"location":"pages/develop/languages/python/#6-stargate-graphql","text":"","title":"6 Stargate GraphQL"},{"location":"pages/develop/languages/python/#61-cql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.1 CQL First"},{"location":"pages/develop/languages/python/#62-graphql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.2 GraphQL First"},{"location":"pages/develop/languages/python/#7-stargate-grpc","text":"","title":"7. Stargate gRPC"},{"location":"pages/develop/languages/python/#71-stargate-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.1 Stargate Client"},{"location":"pages/develop/languages/python/#72-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.2 Astra SDK"},{"location":"pages/develop/languages/python/#8-pulsar-client","text":"","title":"8. Pulsar Client"},{"location":"pages/develop/languages/python/#81-pulsar-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.1 Pulsar Client"},{"location":"pages/develop/languages/python/#82-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.2 Astra SDK"},{"location":"pages/develop/languages/python/#9-pulsar-admin","text":"","title":"9. Pulsar Admin"},{"location":"pages/develop/languages/python/#10-devops-api-database","text":"","title":"10 Devops API Database"},{"location":"pages/develop/languages/python/#11-devops-api-organization","text":"","title":"11 Devops API Organization"},{"location":"pages/develop/languages/python/#12-devops-api-streaming","text":"","title":"12 Devops API Streaming"},{"location":"pages/develop/languages/rust/","text":"1. Overview \u00b6 Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces . There are different frameworks and tools to connect to Astra depending on the Api interface you choose. Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here . 2. Interfaces List \u00b6 Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming 3. CQL \u00b6 3.1 Cassandra Drivers \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 3.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 4. Stargate REST Api \u00b6 4.1 Axios \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 4.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 5. Stargate Document Api \u00b6 5.1 Axios \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 5.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 6 Stargate GraphQL \u00b6 6.1 CQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 6.2 GraphQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7. Stargate gRPC \u00b6 7.1 Stargate Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8. Pulsar Client \u00b6 8.1 Pulsar Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 9. Pulsar Admin \u00b6 10 Devops API Database \u00b6 11 Devops API Organization \u00b6 12 Devops API Streaming \u00b6","title":"\u2022 Rust"},{"location":"pages/develop/languages/rust/#1-overview","text":"Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces . There are different frameworks and tools to connect to Astra depending on the Api interface you choose. Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here .","title":"1. Overview"},{"location":"pages/develop/languages/rust/#2-interfaces-list","text":"Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming","title":"2. Interfaces List"},{"location":"pages/develop/languages/rust/#3-cql","text":"","title":"3. CQL"},{"location":"pages/develop/languages/rust/#31-cassandra-drivers","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"3.1 Cassandra Drivers"},{"location":"pages/develop/languages/rust/#32-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"3.2 Astra SDK"},{"location":"pages/develop/languages/rust/#4-stargate-rest-api","text":"","title":"4. Stargate REST Api"},{"location":"pages/develop/languages/rust/#41-axios","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"4.1 Axios"},{"location":"pages/develop/languages/rust/#42-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"4.2 Astra SDK"},{"location":"pages/develop/languages/rust/#5-stargate-document-api","text":"","title":"5. Stargate Document Api"},{"location":"pages/develop/languages/rust/#51-axios","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"5.1 Axios"},{"location":"pages/develop/languages/rust/#52-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"5.2 Astra SDK"},{"location":"pages/develop/languages/rust/#6-stargate-graphql","text":"","title":"6 Stargate GraphQL"},{"location":"pages/develop/languages/rust/#61-cql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.1 CQL First"},{"location":"pages/develop/languages/rust/#62-graphql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.2 GraphQL First"},{"location":"pages/develop/languages/rust/#7-stargate-grpc","text":"","title":"7. Stargate gRPC"},{"location":"pages/develop/languages/rust/#71-stargate-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.1 Stargate Client"},{"location":"pages/develop/languages/rust/#72-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.2 Astra SDK"},{"location":"pages/develop/languages/rust/#8-pulsar-client","text":"","title":"8. Pulsar Client"},{"location":"pages/develop/languages/rust/#81-pulsar-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.1 Pulsar Client"},{"location":"pages/develop/languages/rust/#82-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.2 Astra SDK"},{"location":"pages/develop/languages/rust/#9-pulsar-admin","text":"","title":"9. Pulsar Admin"},{"location":"pages/develop/languages/rust/#10-devops-api-database","text":"","title":"10 Devops API Database"},{"location":"pages/develop/languages/rust/#11-devops-api-organization","text":"","title":"11 Devops API Organization"},{"location":"pages/develop/languages/rust/#12-devops-api-streaming","text":"","title":"12 Devops API Streaming"},{"location":"pages/develop/languages/scala/","text":"1. Overview \u00b6 Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces . There are different frameworks and tools to connect to Astra depending on the Api interface you choose. Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here . 2. Interfaces List \u00b6 Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming 3. CQL \u00b6 3.1 Cassandra Drivers \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 3.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 4. Stargate REST Api \u00b6 4.1 Axios \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 4.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 5. Stargate Document Api \u00b6 5.1 Axios \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 5.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 6 Stargate GraphQL \u00b6 6.1 CQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 6.2 GraphQL First \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7. Stargate gRPC \u00b6 7.1 Stargate Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 7.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8. Pulsar Client \u00b6 8.1 Pulsar Client \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 8.2 Astra SDK \u00b6 \u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO 9. Pulsar Admin \u00b6 10 Devops API Database \u00b6 11 Devops API Organization \u00b6 12 Devops API Streaming \u00b6","title":"\u2022 Scala"},{"location":"pages/develop/languages/scala/#1-overview","text":"Astra provides multiple services such as; Database and Streaming, with multiple Apis and interfaces . There are different frameworks and tools to connect to Astra depending on the Api interface you choose. Pick the interface in the table below to get relevant instructions. In most cases, you will download a working sample. There are standalone examples designed to be as simple as possible. Please note that a Software developement KIT (SDK) is also available for you to reduce the amount of boilerplate code needed to get started. More information is here .","title":"1. Overview"},{"location":"pages/develop/languages/scala/#2-interfaces-list","text":"Component Interface Description Astra DB Main connection to Cassandra Astra DB CQL exposes as stateless rest resources Astra DB Use Cassandra as a Document DB Astra DB Create tables and use generated CRUD Astra DB CQL exposes through serialized protobuf Astra Streaming Create Producer, Consumers, Subscriptions.. Astra Streaming Administrate your Pulsar cluster Astra Core Manage Databases Astra Core Manage users and roles Astra Core Manage Streaming","title":"2. Interfaces List"},{"location":"pages/develop/languages/scala/#3-cql","text":"","title":"3. CQL"},{"location":"pages/develop/languages/scala/#31-cassandra-drivers","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"3.1 Cassandra Drivers"},{"location":"pages/develop/languages/scala/#32-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"3.2 Astra SDK"},{"location":"pages/develop/languages/scala/#4-stargate-rest-api","text":"","title":"4. Stargate REST Api"},{"location":"pages/develop/languages/scala/#41-axios","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"4.1 Axios"},{"location":"pages/develop/languages/scala/#42-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"4.2 Astra SDK"},{"location":"pages/develop/languages/scala/#5-stargate-document-api","text":"","title":"5. Stargate Document Api"},{"location":"pages/develop/languages/scala/#51-axios","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"5.1 Axios"},{"location":"pages/develop/languages/scala/#52-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"5.2 Astra SDK"},{"location":"pages/develop/languages/scala/#6-stargate-graphql","text":"","title":"6 Stargate GraphQL"},{"location":"pages/develop/languages/scala/#61-cql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.1 CQL First"},{"location":"pages/develop/languages/scala/#62-graphql-first","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"6.2 GraphQL First"},{"location":"pages/develop/languages/scala/#7-stargate-grpc","text":"","title":"7. Stargate gRPC"},{"location":"pages/develop/languages/scala/#71-stargate-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.1 Stargate Client"},{"location":"pages/develop/languages/scala/#72-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"7.2 Astra SDK"},{"location":"pages/develop/languages/scala/#8-pulsar-client","text":"","title":"8. Pulsar Client"},{"location":"pages/develop/languages/scala/#81-pulsar-client","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.1 Pulsar Client"},{"location":"pages/develop/languages/scala/#82-astra-sdk","text":"\u2139\ufe0f Overview TODO \ud83d\udce6 Prerequisites [ASTRA] TODO \ud83d\udce6 Prerequisites [Development Environment] TODO \ud83d\udce6 Setup Project TODO \ud83d\udda5\ufe0f Sample Code TODO","title":"8.2 Astra SDK"},{"location":"pages/develop/languages/scala/#9-pulsar-admin","text":"","title":"9. Pulsar Admin"},{"location":"pages/develop/languages/scala/#10-devops-api-database","text":"","title":"10 Devops API Database"},{"location":"pages/develop/languages/scala/#11-devops-api-organization","text":"","title":"11 Devops API Organization"},{"location":"pages/develop/languages/scala/#12-devops-api-streaming","text":"","title":"12 Devops API Streaming"},{"location":"pages/develop/platform/aws-lambda-function/","text":"AWS Lambda Functions \u00b6 A - Overview \u00b6 AWS Lambda is AWS' function-as-a-service offering that provides a serverless execution environment for your code. AWS Lambda functions are commonly used to: Extend Astra DB with additional data processing capabilities, such as aggregating, summarizing and validating data periodically; Connect Astra DB with other cloud services into data pipelines that move, process and analyze data. B - Prerequisites \u00b6 Create an Astra Database Create an Astra Token Download a Secure Connect Bundle Optionally, if you are new to AWS Lambda, practice creating a simpler function first C - Using Python Driver \u00b6 \u2705 1. Create a deployment package. \u00b6 A deployment package is a .zip file with a function source code and dependencies. To access Astra DB from a function using Python Driver, we must add cassandra-driver , a Python client library for Apache Cassandra, DataStax Astra DB and DataStax Enterprise, as a dependency. In addition, as part of the deployment package, we need to include a secure connect bundle for a database in Astra DB that we want to query. Open a command prompt and create a project directory: mkdir lambda-astra-db-project cd lambda-astra-db-project Create file lambda_function.py with the function source code: from cassandra.cluster import Cluster from cassandra.auth import PlainTextAuthProvider import os ASTRA_DB_CLIENT_ID = os . environ . get ( 'ASTRA_DB_CLIENT_ID' ) ASTRA_DB_CLIENT_SECRET = os . environ . get ( 'ASTRA_DB_CLIENT_SECRET' ) cloud_config = { 'secure_connect_bundle' : 'secure-connect-bundle-for-your-database.zip' , 'use_default_tempdir' : True } auth_provider = PlainTextAuthProvider ( ASTRA_DB_CLIENT_ID , ASTRA_DB_CLIENT_SECRET ) cluster = Cluster ( cloud = cloud_config , auth_provider = auth_provider , protocol_version = 4 ) session = cluster . connect () def lambda_handler ( event , context ): row = session . execute ( \"SELECT cql_version FROM system.local WHERE key = 'local';\" ) . one () cql_version = row [ 0 ] print ( cql_version ) print ( 'Success' ) return cql_version You can learn more about the code above by reading the cassandra-driver documentation. Install the cassandra-driver library: pip install --target . cassandra-driver Download the Secure Connect Bundle for your database and copy it into the project directory. Create a deployment package with lambda_function.py , cassandra-driver , and secure connect bundle: zip -r lambda-astra-db-deployment-package.zip . \u2705 2. Create a function. \u00b6 Go to the Functions page of the Lambda console and click Create function . Choose Author from scratch . Under the Basic information section, specify preferred Function name , Runtime , and Architecture . Click Create function . Under the Code tab and the Code source section, select Upload from and upload the deployment package created in the previous steps. Since the deployment package exceeds 3 MBs, the Console Editor may not be available to view the source code: Under the Configuration tab, select and create these Environment variables : ASTRA_DB_CLIENT_ID : A Client ID is generated together with an application token (see the Prerequisites section above). ASTRA_DB_CLIENT_SECRET : A Client secret is generated together with an application token (see the Prerequisites section above). Note that, for better security, you can alternatively use the AWS Secret Manager service to store and manage client id and secret, and then retrieve them programmatically. \u2705 3. Test the function. \u00b6 Under the Test tab, click the Test button and observe the output. Notice the CQL version output and return value of 3.4.5 . D - Using Python SDK \u00b6 \u2705 1. Create a deployment package. \u00b6 A deployment package is a .zip file with a function source code and dependencies. To access Astra DB from a function using REST API, we must add AstraPy , a Pythonic SDK for DataStax Astra and Stargate, as a dependency. Open a command prompt and create a project directory: mkdir lambda-astra-db-project cd lambda-astra-db-project Create file lambda_function.py with the function source code: from astrapy.rest import create_client , http_methods import os ASTRA_DB_ID = os . environ . get ( 'ASTRA_DB_ID' ) ASTRA_DB_REGION = os . environ . get ( 'ASTRA_DB_REGION' ) ASTRA_DB_APPLICATION_TOKEN = os . environ . get ( 'ASTRA_DB_APPLICATION_TOKEN' ) astra_http_client = create_client ( astra_database_id = ASTRA_DB_ID , astra_database_region = ASTRA_DB_REGION , astra_application_token = ASTRA_DB_APPLICATION_TOKEN ) def lambda_handler ( event , context ): res = astra_http_client . request ( method = http_methods . GET , path = f \"/api/rest/v2/keyspaces/system/local/local\" ) cql_version = res [ \"data\" ][ 0 ][ 'cql_version' ] print ( cql_version ) print ( 'Success' ) return cql_version You can learn more about the code above by reading the AstraPy documentation. Install the AstraPy library: pip install --target . astrapy Create a deployment package with lambda_function.py and astrapy : zip -r lambda-astra-db-deployment-package.zip . \u2705 2. Create a function. \u00b6 Go to the Functions page of the Lambda console and click Create function . Choose Author from scratch . Under the Basic information section, specify preferred Function name , Runtime , and Architecture . Click Create function . Under the Code tab and the Code source section, select Upload from and upload the deployment package created in the previous steps. Verify that the uploaded function has the correct lambda_function.py and dependencies: You can learn more about the code above by reading the AstraPy documentation. Click Deploy to deploy the function. Under the Configuration tab, select and create these Environment variables : ASTRA_DB_ID : A Database ID value can be found on the Astra DB dashboard. ASTRA_DB_REGION : A Region name can be found on the overview page for a specific Astra DB database. ASTRA_DB_APPLICATION_TOKEN : An Application Token can be generated for a specific Astra DB database (see the Prerequisites section above). Note that, for better security, you can alternatively use the AWS Secret Manager service to store and manage an application token as a secret. A secret can then be retrieved programmatically. \u2705 3. Test the function. \u00b6 Under the Test tab, click the Test button and observe the output. Notice the CQL version output and return value of 3.4.5 . E - Using Java Driver \u00b6 \u2705 1. Create a deployment package. \u00b6 A deployment package is a .zip or .jar file archive with compiled function code and dependencies. In this tutorial, we use Apache Maven\u2122 to create, compile and package a function into a .jar file. We need to include the following pieces into a deployment package to access Astra DB from an AWS Lambda function: a) aws-lambda-java-core that defines necessary interfaces and classes to create functions; b) java-driver that enables connectivity to Apache Cassandra, DataStax Astra DB and DataStax Enterprise; and c) secure connect bundle for a database in Astra DB that we want to query. Open a command prompt and create a new project using Apache Maven\u2122 : mvn archetype:generate -DgroupId = com.example -DartifactId = AstraDBFunction -DinteractiveMode = false Rename file App.java to AstraDBFunction.java and replace its content with the function source code: package com.example ; import com.amazonaws.services.lambda.runtime.Context ; import com.amazonaws.services.lambda.runtime.RequestHandler ; import com.amazonaws.services.lambda.runtime.LambdaLogger ; import com.datastax.oss.driver.api.core.CqlSession ; import com.datastax.oss.driver.api.core.cql.ResultSet ; import com.datastax.oss.driver.api.core.cql.Row ; import java.nio.file.Paths ; import java.util.Map ; public class AstraDBFunction implements RequestHandler < Map < String , String > , String > { private static final String ASTRA_DB_CLIENT_ID = System . getenv ( \"ASTRA_DB_CLIENT_ID\" ); private static final String ASTRA_DB_CLIENT_SECRET = System . getenv ( \"ASTRA_DB_CLIENT_SECRET\" ); private static CqlSession session = CqlSession . builder () . withCloudSecureConnectBundle ( Paths . get ( \"secure-connect-bundle-for-your-database.zip\" )) . withAuthCredentials ( ASTRA_DB_CLIENT_ID , ASTRA_DB_CLIENT_SECRET ) . build (); public String handleRequest ( Map < String , String > event , Context context ) { LambdaLogger logger = context . getLogger (); ResultSet rs = session . execute ( \"SELECT cql_version FROM system.local WHERE key = 'local';\" ); Row row = rs . one (); String response = row . getString ( \"cql_version\" ); logger . log ( response + \" Success \\n\" ); return response ; } } You can learn more about the code above by reading the java-driver documentation. In the project directory, under /src/main , create directory resources . Download the Secure Connect Bundle for your database and copy it into the resources directory. The project directory structure should look like this: Add AWS Lambda and Java Driver dependencies to the pom.xml file: <dependency> <groupId> com.amazonaws </groupId> <artifactId> aws-lambda-java-core </artifactId> <version> 1.2.1 </version> </dependency> <dependency> <groupId> com.amazonaws </groupId> <artifactId> aws-lambda-java-events </artifactId> <version> 3.11.0 </version> </dependency> <dependency> <groupId> com.amazonaws </groupId> <artifactId> aws-lambda-java-log4j2 </artifactId> <version> 1.5.1 </version> </dependency> <dependency> <groupId> com.datastax.oss </groupId> <artifactId> java-driver-core </artifactId> <version> 4.14.1 </version> </dependency> Add or replace an existing build section in the pom.xml file with the following: <build> <plugins> <plugin> <artifactId> maven-surefire-plugin </artifactId> <version> 2.22.2 </version> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-shade-plugin </artifactId> <version> 3.2.2 </version> <configuration> <createDependencyReducedPom> false </createDependencyReducedPom> <filters> <filter> <artifact> *:* </artifact> <excludes> <exclude> **/Log4j2Plugins.dat </exclude> </excludes> </filter> </filters> </configuration> <executions> <execution> <phase> package </phase> <goals> <goal> shade </goal> </goals> </execution> </executions> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <version> 3.8.1 </version> <configuration> <source> 1.8 </source> <target> 1.8 </target> </configuration> </plugin> </plugins> </build> Run the Maven command in the project directory to compile code and create a .jar file: mvn clean compile package Find the deployment package file AstraDBFunction-1.0-SNAPSHOT.jar under the target directory: \u2705 2. Create a function. \u00b6 Go to the Functions page of the Lambda console and click Create function . Choose Author from scratch . Under the Basic information section, specify preferred Function name , Runtime , and Architecture . Click Create function . Under the Code tab and the Code source section, select Upload from and upload the deployment package created in the previous steps. Since the deployment package exceeds 3 MBs, the Console Editor may not be available to view the source code: Under the Code tab, change Handler in section Runtime settings to com.example.AstraDBFunction::handleRequest : Under the Configuration tab, select and create these Environment variables : ASTRA_DB_CLIENT_ID : A Client ID is generated together with an application token (see the Prerequisites section above). ASTRA_DB_CLIENT_SECRET : A Client secret is generated together with an application token (see the Prerequisites section above). Note that, for better security, you can alternatively use the AWS Secret Manager service to store and manage client id and secret, and then retrieve them programmatically. \u2705 3. Test the function. \u00b6 Under the Test tab, click the Test button and observe the output. Notice the CQL version output and return value of 3.4.5 . F - Using Java gRPC \u00b6 \u2705 1. Create a deployment package. \u00b6 A deployment package is a .zip or .jar file archive with compiled function code and dependencies. In this tutorial, we use Apache Maven\u2122 to create, compile and package a function into a .jar file. We need to include the following pieces into a deployment package to access Astra DB from an AWS Lambda function: a) aws-lambda-java-core that defines necessary interfaces and classes to create functions; b) Stargate that enables connectivity to Apache Cassandra, DataStax Astra DB and DataStax Enterprise; and c) gRPC that works as a high performance Remote Procedure Call (RPC) framework. Open a command prompt and create a new project using Apache Maven\u2122 : mvn archetype:generate -DgroupId = com.example -DartifactId = AstraDBFunction -DinteractiveMode = false The project directory structure should look like this: Rename file App.java to AstraDBFunction.java and replace its content with the function source code: package com.example ; import com.amazonaws.services.lambda.runtime.Context ; import com.amazonaws.services.lambda.runtime.RequestHandler ; import com.amazonaws.services.lambda.runtime.LambdaLogger ; import java.util.Map ; import io.grpc.ManagedChannel ; import io.grpc.ManagedChannelBuilder ; import io.stargate.grpc.StargateBearerToken ; import io.stargate.proto.QueryOuterClass ; import io.stargate.proto.QueryOuterClass.Row ; import io.stargate.proto.StargateGrpc ; public class AstraDBFunction implements RequestHandler < Map < String , String > , String > { private static final String ASTRA_DB_TOKEN = System . getenv ( \"ASTRA_DB_APPLICATION_TOKEN\" ); private static final String ASTRA_DB_ID = System . getenv ( \"ASTRA_DB_ID\" ); private static final String ASTRA_DB_REGION = System . getenv ( \"ASTRA_DB_REGION\" ); public static ManagedChannel channel = ManagedChannelBuilder . forAddress ( ASTRA_DB_ID + \"-\" + ASTRA_DB_REGION + \".apps.astra.datastax.com\" , 443 ) . useTransportSecurity () . build (); public static StargateGrpc . StargateBlockingStub blockingStub = StargateGrpc . newBlockingStub ( channel ). withCallCredentials ( new StargateBearerToken ( ASTRA_DB_TOKEN )); public String handleRequest ( Map < String , String > event , Context context ) { LambdaLogger logger = context . getLogger (); QueryOuterClass . Response queryString = blockingStub . executeQuery ( QueryOuterClass . Query . newBuilder () . setCql ( \"SELECT cql_version FROM system.local WHERE key = 'local';\" ) . build ()); QueryOuterClass . ResultSet rs = queryString . getResultSet (); String response = rs . getRows ( 0 ). getValues ( 0 ). getString (); logger . log ( response + \" Success \\n\" ); return response ; } } You can learn more about the code above by reading the Stargate documentation. Add AWS Lambda, Stargate and gRPC dependencies to the pom.xml file: <dependency> <groupId> com.amazonaws </groupId> <artifactId> aws-lambda-java-core </artifactId> <version> 1.2.1 </version> </dependency> <dependency> <groupId> com.amazonaws </groupId> <artifactId> aws-lambda-java-events </artifactId> <version> 3.11.0 </version> </dependency> <dependency> <groupId> com.amazonaws </groupId> <artifactId> aws-lambda-java-log4j2 </artifactId> <version> 1.5.1 </version> </dependency> <dependency> <groupId> io.stargate.grpc </groupId> <artifactId> grpc-proto </artifactId> <version> 1.0.41 </version> </dependency> <dependency> <groupId> io.grpc </groupId> <artifactId> grpc-netty-shaded </artifactId> <version> 1.41.0 </version> </dependency> Add or replace an existing build section in the pom.xml file with the following: <build> <plugins> <plugin> <artifactId> maven-surefire-plugin </artifactId> <version> 2.22.2 </version> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-shade-plugin </artifactId> <version> 3.2.2 </version> <configuration> <createDependencyReducedPom> false </createDependencyReducedPom> </configuration> <executions> <execution> <phase> package </phase> <goals> <goal> shade </goal> </goals> </execution> </executions> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <version> 3.8.1 </version> <configuration> <source> 1.8 </source> <target> 1.8 </target> </configuration> </plugin> </plugins> </build> Run the Maven command in the project directory to compile code and create a .jar file: mvn clean compile package Find the deployment package file AstraDBFunction-1.0-SNAPSHOT.jar under the target directory: \u2705 2. Create a function. \u00b6 Go to the Functions page of the Lambda console and click Create function . Choose Author from scratch . Under the Basic information section, specify preferred Function name , Runtime , and Architecture . Click Create function . Under the Code tab and the Code source section, select Upload from and upload the deployment package created in the previous steps. Since the deployment package exceeds 3 MBs, the Console Editor may not be available to view the source code: Under the Code tab, change Handler in section Runtime settings to com.example.AstraDBFunction::handleRequest : Under the Configuration tab, select and create these Environment variables : ASTRA_DB_ID : A Database ID value can be found on the Astra DB dashboard. ASTRA_DB_REGION : A Region name can be found on the overview page for a specific Astra DB database. ASTRA_DB_APPLICATION_TOKEN : An Application Token can be generated for a specific Astra DB database (see the Prerequisites section above). Note that, for better security, you can alternatively use the AWS Secret Manager service to store and manage an application token as a secret. A secret can then be retrieved programmatically. \u2705 3. Test the function. \u00b6 Under the Test tab, click the Test button and observe the output. Notice the CQL version output and return value of 3.4.5 .","title":"\u2022 AWS Lambda Functions"},{"location":"pages/develop/platform/aws-lambda-function/#aws-lambda-functions","text":"","title":"AWS Lambda Functions"},{"location":"pages/develop/platform/aws-lambda-function/#a-overview","text":"AWS Lambda is AWS' function-as-a-service offering that provides a serverless execution environment for your code. AWS Lambda functions are commonly used to: Extend Astra DB with additional data processing capabilities, such as aggregating, summarizing and validating data periodically; Connect Astra DB with other cloud services into data pipelines that move, process and analyze data.","title":"A - Overview"},{"location":"pages/develop/platform/aws-lambda-function/#b-prerequisites","text":"Create an Astra Database Create an Astra Token Download a Secure Connect Bundle Optionally, if you are new to AWS Lambda, practice creating a simpler function first","title":"B - Prerequisites"},{"location":"pages/develop/platform/aws-lambda-function/#c-using-python-driver","text":"","title":"C - Using Python Driver"},{"location":"pages/develop/platform/aws-lambda-function/#1-create-a-deployment-package","text":"A deployment package is a .zip file with a function source code and dependencies. To access Astra DB from a function using Python Driver, we must add cassandra-driver , a Python client library for Apache Cassandra, DataStax Astra DB and DataStax Enterprise, as a dependency. In addition, as part of the deployment package, we need to include a secure connect bundle for a database in Astra DB that we want to query. Open a command prompt and create a project directory: mkdir lambda-astra-db-project cd lambda-astra-db-project Create file lambda_function.py with the function source code: from cassandra.cluster import Cluster from cassandra.auth import PlainTextAuthProvider import os ASTRA_DB_CLIENT_ID = os . environ . get ( 'ASTRA_DB_CLIENT_ID' ) ASTRA_DB_CLIENT_SECRET = os . environ . get ( 'ASTRA_DB_CLIENT_SECRET' ) cloud_config = { 'secure_connect_bundle' : 'secure-connect-bundle-for-your-database.zip' , 'use_default_tempdir' : True } auth_provider = PlainTextAuthProvider ( ASTRA_DB_CLIENT_ID , ASTRA_DB_CLIENT_SECRET ) cluster = Cluster ( cloud = cloud_config , auth_provider = auth_provider , protocol_version = 4 ) session = cluster . connect () def lambda_handler ( event , context ): row = session . execute ( \"SELECT cql_version FROM system.local WHERE key = 'local';\" ) . one () cql_version = row [ 0 ] print ( cql_version ) print ( 'Success' ) return cql_version You can learn more about the code above by reading the cassandra-driver documentation. Install the cassandra-driver library: pip install --target . cassandra-driver Download the Secure Connect Bundle for your database and copy it into the project directory. Create a deployment package with lambda_function.py , cassandra-driver , and secure connect bundle: zip -r lambda-astra-db-deployment-package.zip .","title":"\u2705 1. Create a deployment package."},{"location":"pages/develop/platform/aws-lambda-function/#2-create-a-function","text":"Go to the Functions page of the Lambda console and click Create function . Choose Author from scratch . Under the Basic information section, specify preferred Function name , Runtime , and Architecture . Click Create function . Under the Code tab and the Code source section, select Upload from and upload the deployment package created in the previous steps. Since the deployment package exceeds 3 MBs, the Console Editor may not be available to view the source code: Under the Configuration tab, select and create these Environment variables : ASTRA_DB_CLIENT_ID : A Client ID is generated together with an application token (see the Prerequisites section above). ASTRA_DB_CLIENT_SECRET : A Client secret is generated together with an application token (see the Prerequisites section above). Note that, for better security, you can alternatively use the AWS Secret Manager service to store and manage client id and secret, and then retrieve them programmatically.","title":"\u2705 2. Create a function."},{"location":"pages/develop/platform/aws-lambda-function/#3-test-the-function","text":"Under the Test tab, click the Test button and observe the output. Notice the CQL version output and return value of 3.4.5 .","title":"\u2705 3. Test the function."},{"location":"pages/develop/platform/aws-lambda-function/#d-using-python-sdk","text":"","title":"D - Using Python SDK"},{"location":"pages/develop/platform/aws-lambda-function/#1-create-a-deployment-package_1","text":"A deployment package is a .zip file with a function source code and dependencies. To access Astra DB from a function using REST API, we must add AstraPy , a Pythonic SDK for DataStax Astra and Stargate, as a dependency. Open a command prompt and create a project directory: mkdir lambda-astra-db-project cd lambda-astra-db-project Create file lambda_function.py with the function source code: from astrapy.rest import create_client , http_methods import os ASTRA_DB_ID = os . environ . get ( 'ASTRA_DB_ID' ) ASTRA_DB_REGION = os . environ . get ( 'ASTRA_DB_REGION' ) ASTRA_DB_APPLICATION_TOKEN = os . environ . get ( 'ASTRA_DB_APPLICATION_TOKEN' ) astra_http_client = create_client ( astra_database_id = ASTRA_DB_ID , astra_database_region = ASTRA_DB_REGION , astra_application_token = ASTRA_DB_APPLICATION_TOKEN ) def lambda_handler ( event , context ): res = astra_http_client . request ( method = http_methods . GET , path = f \"/api/rest/v2/keyspaces/system/local/local\" ) cql_version = res [ \"data\" ][ 0 ][ 'cql_version' ] print ( cql_version ) print ( 'Success' ) return cql_version You can learn more about the code above by reading the AstraPy documentation. Install the AstraPy library: pip install --target . astrapy Create a deployment package with lambda_function.py and astrapy : zip -r lambda-astra-db-deployment-package.zip .","title":"\u2705 1. Create a deployment package."},{"location":"pages/develop/platform/aws-lambda-function/#2-create-a-function_1","text":"Go to the Functions page of the Lambda console and click Create function . Choose Author from scratch . Under the Basic information section, specify preferred Function name , Runtime , and Architecture . Click Create function . Under the Code tab and the Code source section, select Upload from and upload the deployment package created in the previous steps. Verify that the uploaded function has the correct lambda_function.py and dependencies: You can learn more about the code above by reading the AstraPy documentation. Click Deploy to deploy the function. Under the Configuration tab, select and create these Environment variables : ASTRA_DB_ID : A Database ID value can be found on the Astra DB dashboard. ASTRA_DB_REGION : A Region name can be found on the overview page for a specific Astra DB database. ASTRA_DB_APPLICATION_TOKEN : An Application Token can be generated for a specific Astra DB database (see the Prerequisites section above). Note that, for better security, you can alternatively use the AWS Secret Manager service to store and manage an application token as a secret. A secret can then be retrieved programmatically.","title":"\u2705 2. Create a function."},{"location":"pages/develop/platform/aws-lambda-function/#3-test-the-function_1","text":"Under the Test tab, click the Test button and observe the output. Notice the CQL version output and return value of 3.4.5 .","title":"\u2705 3. Test the function."},{"location":"pages/develop/platform/aws-lambda-function/#e-using-java-driver","text":"","title":"E - Using Java Driver"},{"location":"pages/develop/platform/aws-lambda-function/#1-create-a-deployment-package_2","text":"A deployment package is a .zip or .jar file archive with compiled function code and dependencies. In this tutorial, we use Apache Maven\u2122 to create, compile and package a function into a .jar file. We need to include the following pieces into a deployment package to access Astra DB from an AWS Lambda function: a) aws-lambda-java-core that defines necessary interfaces and classes to create functions; b) java-driver that enables connectivity to Apache Cassandra, DataStax Astra DB and DataStax Enterprise; and c) secure connect bundle for a database in Astra DB that we want to query. Open a command prompt and create a new project using Apache Maven\u2122 : mvn archetype:generate -DgroupId = com.example -DartifactId = AstraDBFunction -DinteractiveMode = false Rename file App.java to AstraDBFunction.java and replace its content with the function source code: package com.example ; import com.amazonaws.services.lambda.runtime.Context ; import com.amazonaws.services.lambda.runtime.RequestHandler ; import com.amazonaws.services.lambda.runtime.LambdaLogger ; import com.datastax.oss.driver.api.core.CqlSession ; import com.datastax.oss.driver.api.core.cql.ResultSet ; import com.datastax.oss.driver.api.core.cql.Row ; import java.nio.file.Paths ; import java.util.Map ; public class AstraDBFunction implements RequestHandler < Map < String , String > , String > { private static final String ASTRA_DB_CLIENT_ID = System . getenv ( \"ASTRA_DB_CLIENT_ID\" ); private static final String ASTRA_DB_CLIENT_SECRET = System . getenv ( \"ASTRA_DB_CLIENT_SECRET\" ); private static CqlSession session = CqlSession . builder () . withCloudSecureConnectBundle ( Paths . get ( \"secure-connect-bundle-for-your-database.zip\" )) . withAuthCredentials ( ASTRA_DB_CLIENT_ID , ASTRA_DB_CLIENT_SECRET ) . build (); public String handleRequest ( Map < String , String > event , Context context ) { LambdaLogger logger = context . getLogger (); ResultSet rs = session . execute ( \"SELECT cql_version FROM system.local WHERE key = 'local';\" ); Row row = rs . one (); String response = row . getString ( \"cql_version\" ); logger . log ( response + \" Success \\n\" ); return response ; } } You can learn more about the code above by reading the java-driver documentation. In the project directory, under /src/main , create directory resources . Download the Secure Connect Bundle for your database and copy it into the resources directory. The project directory structure should look like this: Add AWS Lambda and Java Driver dependencies to the pom.xml file: <dependency> <groupId> com.amazonaws </groupId> <artifactId> aws-lambda-java-core </artifactId> <version> 1.2.1 </version> </dependency> <dependency> <groupId> com.amazonaws </groupId> <artifactId> aws-lambda-java-events </artifactId> <version> 3.11.0 </version> </dependency> <dependency> <groupId> com.amazonaws </groupId> <artifactId> aws-lambda-java-log4j2 </artifactId> <version> 1.5.1 </version> </dependency> <dependency> <groupId> com.datastax.oss </groupId> <artifactId> java-driver-core </artifactId> <version> 4.14.1 </version> </dependency> Add or replace an existing build section in the pom.xml file with the following: <build> <plugins> <plugin> <artifactId> maven-surefire-plugin </artifactId> <version> 2.22.2 </version> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-shade-plugin </artifactId> <version> 3.2.2 </version> <configuration> <createDependencyReducedPom> false </createDependencyReducedPom> <filters> <filter> <artifact> *:* </artifact> <excludes> <exclude> **/Log4j2Plugins.dat </exclude> </excludes> </filter> </filters> </configuration> <executions> <execution> <phase> package </phase> <goals> <goal> shade </goal> </goals> </execution> </executions> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <version> 3.8.1 </version> <configuration> <source> 1.8 </source> <target> 1.8 </target> </configuration> </plugin> </plugins> </build> Run the Maven command in the project directory to compile code and create a .jar file: mvn clean compile package Find the deployment package file AstraDBFunction-1.0-SNAPSHOT.jar under the target directory:","title":"\u2705 1. Create a deployment package."},{"location":"pages/develop/platform/aws-lambda-function/#2-create-a-function_2","text":"Go to the Functions page of the Lambda console and click Create function . Choose Author from scratch . Under the Basic information section, specify preferred Function name , Runtime , and Architecture . Click Create function . Under the Code tab and the Code source section, select Upload from and upload the deployment package created in the previous steps. Since the deployment package exceeds 3 MBs, the Console Editor may not be available to view the source code: Under the Code tab, change Handler in section Runtime settings to com.example.AstraDBFunction::handleRequest : Under the Configuration tab, select and create these Environment variables : ASTRA_DB_CLIENT_ID : A Client ID is generated together with an application token (see the Prerequisites section above). ASTRA_DB_CLIENT_SECRET : A Client secret is generated together with an application token (see the Prerequisites section above). Note that, for better security, you can alternatively use the AWS Secret Manager service to store and manage client id and secret, and then retrieve them programmatically.","title":"\u2705 2. Create a function."},{"location":"pages/develop/platform/aws-lambda-function/#3-test-the-function_2","text":"Under the Test tab, click the Test button and observe the output. Notice the CQL version output and return value of 3.4.5 .","title":"\u2705 3. Test the function."},{"location":"pages/develop/platform/aws-lambda-function/#f-using-java-grpc","text":"","title":"F - Using Java gRPC"},{"location":"pages/develop/platform/aws-lambda-function/#1-create-a-deployment-package_3","text":"A deployment package is a .zip or .jar file archive with compiled function code and dependencies. In this tutorial, we use Apache Maven\u2122 to create, compile and package a function into a .jar file. We need to include the following pieces into a deployment package to access Astra DB from an AWS Lambda function: a) aws-lambda-java-core that defines necessary interfaces and classes to create functions; b) Stargate that enables connectivity to Apache Cassandra, DataStax Astra DB and DataStax Enterprise; and c) gRPC that works as a high performance Remote Procedure Call (RPC) framework. Open a command prompt and create a new project using Apache Maven\u2122 : mvn archetype:generate -DgroupId = com.example -DartifactId = AstraDBFunction -DinteractiveMode = false The project directory structure should look like this: Rename file App.java to AstraDBFunction.java and replace its content with the function source code: package com.example ; import com.amazonaws.services.lambda.runtime.Context ; import com.amazonaws.services.lambda.runtime.RequestHandler ; import com.amazonaws.services.lambda.runtime.LambdaLogger ; import java.util.Map ; import io.grpc.ManagedChannel ; import io.grpc.ManagedChannelBuilder ; import io.stargate.grpc.StargateBearerToken ; import io.stargate.proto.QueryOuterClass ; import io.stargate.proto.QueryOuterClass.Row ; import io.stargate.proto.StargateGrpc ; public class AstraDBFunction implements RequestHandler < Map < String , String > , String > { private static final String ASTRA_DB_TOKEN = System . getenv ( \"ASTRA_DB_APPLICATION_TOKEN\" ); private static final String ASTRA_DB_ID = System . getenv ( \"ASTRA_DB_ID\" ); private static final String ASTRA_DB_REGION = System . getenv ( \"ASTRA_DB_REGION\" ); public static ManagedChannel channel = ManagedChannelBuilder . forAddress ( ASTRA_DB_ID + \"-\" + ASTRA_DB_REGION + \".apps.astra.datastax.com\" , 443 ) . useTransportSecurity () . build (); public static StargateGrpc . StargateBlockingStub blockingStub = StargateGrpc . newBlockingStub ( channel ). withCallCredentials ( new StargateBearerToken ( ASTRA_DB_TOKEN )); public String handleRequest ( Map < String , String > event , Context context ) { LambdaLogger logger = context . getLogger (); QueryOuterClass . Response queryString = blockingStub . executeQuery ( QueryOuterClass . Query . newBuilder () . setCql ( \"SELECT cql_version FROM system.local WHERE key = 'local';\" ) . build ()); QueryOuterClass . ResultSet rs = queryString . getResultSet (); String response = rs . getRows ( 0 ). getValues ( 0 ). getString (); logger . log ( response + \" Success \\n\" ); return response ; } } You can learn more about the code above by reading the Stargate documentation. Add AWS Lambda, Stargate and gRPC dependencies to the pom.xml file: <dependency> <groupId> com.amazonaws </groupId> <artifactId> aws-lambda-java-core </artifactId> <version> 1.2.1 </version> </dependency> <dependency> <groupId> com.amazonaws </groupId> <artifactId> aws-lambda-java-events </artifactId> <version> 3.11.0 </version> </dependency> <dependency> <groupId> com.amazonaws </groupId> <artifactId> aws-lambda-java-log4j2 </artifactId> <version> 1.5.1 </version> </dependency> <dependency> <groupId> io.stargate.grpc </groupId> <artifactId> grpc-proto </artifactId> <version> 1.0.41 </version> </dependency> <dependency> <groupId> io.grpc </groupId> <artifactId> grpc-netty-shaded </artifactId> <version> 1.41.0 </version> </dependency> Add or replace an existing build section in the pom.xml file with the following: <build> <plugins> <plugin> <artifactId> maven-surefire-plugin </artifactId> <version> 2.22.2 </version> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-shade-plugin </artifactId> <version> 3.2.2 </version> <configuration> <createDependencyReducedPom> false </createDependencyReducedPom> </configuration> <executions> <execution> <phase> package </phase> <goals> <goal> shade </goal> </goals> </execution> </executions> </plugin> <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <version> 3.8.1 </version> <configuration> <source> 1.8 </source> <target> 1.8 </target> </configuration> </plugin> </plugins> </build> Run the Maven command in the project directory to compile code and create a .jar file: mvn clean compile package Find the deployment package file AstraDBFunction-1.0-SNAPSHOT.jar under the target directory:","title":"\u2705 1. Create a deployment package."},{"location":"pages/develop/platform/aws-lambda-function/#2-create-a-function_3","text":"Go to the Functions page of the Lambda console and click Create function . Choose Author from scratch . Under the Basic information section, specify preferred Function name , Runtime , and Architecture . Click Create function . Under the Code tab and the Code source section, select Upload from and upload the deployment package created in the previous steps. Since the deployment package exceeds 3 MBs, the Console Editor may not be available to view the source code: Under the Code tab, change Handler in section Runtime settings to com.example.AstraDBFunction::handleRequest : Under the Configuration tab, select and create these Environment variables : ASTRA_DB_ID : A Database ID value can be found on the Astra DB dashboard. ASTRA_DB_REGION : A Region name can be found on the overview page for a specific Astra DB database. ASTRA_DB_APPLICATION_TOKEN : An Application Token can be generated for a specific Astra DB database (see the Prerequisites section above). Note that, for better security, you can alternatively use the AWS Secret Manager service to store and manage an application token as a secret. A secret can then be retrieved programmatically.","title":"\u2705 2. Create a function."},{"location":"pages/develop/platform/aws-lambda-function/#3-test-the-function_3","text":"Under the Test tab, click the Test button and observe the output. Notice the CQL version output and return value of 3.4.5 .","title":"\u2705 3. Test the function."},{"location":"pages/develop/platform/google-cloud-function/","text":"Google Cloud Functions \u00b6 A - Overview \u00b6 Cloud Functions is Google's function-as-a-service offering that provides a serverless execution environment for your code. Cloud Functions are commonly used to: Extend Astra DB with additional data processing capabilities, such as aggregating, summarizing and validating data periodically; Connect Astra DB with other cloud services into data pipelines that move, process and analyze data. B - Prerequisites \u00b6 Create an Astra Database Create an Astra Token Download a Secure Connect Bundle Optionally, if you are new to Cloud Functions, practice creating a simpler function first C - Using Python Driver \u00b6 \u2705 1. Create a secret with the secure connect bundle file. \u00b6 Go to the Secret Manager page , select a project that has Secret Manager and Cloud Functions enabled, and click Create secret . Give a Name to the secret and upload the secure connect bundle file as a Secret value . (See the Prerequisites section above if you need to download your secure connect bundle.) Optionally, customize other secret management settings. Click Create secret . On the Secret Manager page , find the newly created secret. \u2705 2. Create a function. \u00b6 Go to the Functions Overview page , select the same project that has Secret Manager and Cloud Functions enabled, and click Create function . Under the Basics section, specify preferred Function name and Region . Under the Trigger section, select HTTP , Allow unauthenticated invocations , and Require HTTPS . Click Save . Under the Runtime, build, connections and security settings section, customize additional settings and create these Runtime environment variables : ASTRA_DB_CLIENT_ID : A Client ID is generated together with an application token (see the Prerequisites section above). ASTRA_DB_CLIENT_SECRET : A Client secret is generated together with an application token (see the Prerequisites section above). Note that, for better security, you can alternatively use the Secret Manager service to store and manage a client secret. A secret can then be similarly exposed as an environment variable. The settings can be found under the Runtime, build, connections and security settings section, the Security tab, and the Secrets field. Under the Runtime, build, connections and security settings section and the Security , click Reference a secret . Select the previously created Secret with the secure connect bundle file, Grant the service account access to the secret, if needed, use Mounted as volume in the Reference method field, and enter secrets in the Mount path field. Notice the final Path that should be used to access the secure connect bundle in the function code. Click Done and Next . Select Python 3.7 or your preferred version in the Runtime field. Select Inline Editor in the Source code field. Enter query_astra_db in the Entry point field. Add cassandra-driver , a Python client library for Apache Cassandra, DataStax Astra DB and DataStax Enterprise, to the requirements.txt file. Replace the main.py content with: from cassandra.cluster import Cluster from cassandra.auth import PlainTextAuthProvider import os ASTRA_DB_CLIENT_ID = os . environ . get ( 'ASTRA_DB_CLIENT_ID' ) ASTRA_DB_CLIENT_SECRET = os . environ . get ( 'ASTRA_DB_CLIENT_SECRET' ) cloud_config = { 'secure_connect_bundle' : '/secrets/secure-connect-secret' , 'use_default_tempdir' : True } auth_provider = PlainTextAuthProvider ( ASTRA_DB_CLIENT_ID , ASTRA_DB_CLIENT_SECRET ) cluster = Cluster ( cloud = cloud_config , auth_provider = auth_provider , protocol_version = 4 ) def query_astra_db ( request ): session = cluster . connect () row = session . execute ( \"SELECT cql_version FROM system.local WHERE key = 'local';\" ) . one () print ( row [ 0 ]) print ( 'Success' ) You can learn more about the code above by reading the python-driver documentation. \u2705 3. Deploy the function. \u00b6 Click Deploy . On the Cloud Functions Overview page, find the newly deployed function. \u2705 4. Test the function. \u00b6 Under Actions , select Test function . On the testing page, click Test the function and observe the output. Notice the CQL version output 3.4.5 and status code 200 . \u2705 5. View logs. \u00b6 You can further explore the log history by either clicking on the Logs tab or the View all logs link that opens Logs Explorer . D - Using Python SDK \u00b6 \u2705 1. Create a function. \u00b6 Go to the Functions Overview page , select a project that has Cloud Functions enabled, and click Create function . Under the Basics section, specify preferred Function name and Region . Under the Trigger section, select HTTP , Allow unauthenticated invocations , and Require HTTPS . Click Save . Under the Runtime, build, connections and security settings section, customize additional settings and create these Runtime environment variables : ASTRA_DB_ID : A Database ID value can be found on the Astra DB dashboard. ASTRA_DB_REGION : A Region name can be found on the overview page for a specific Astra DB database. ASTRA_DB_APPLICATION_TOKEN : An Application Token can be generated for a specific Astra DB database (see the Prerequisites section above). Note that, for better security, you can alternatively use the Secret Manager service to store and manage an application token as a secret. A secret can then be similarly exposed as an environment variable. The settings can be found under the Runtime, build, connections and security settings section, the Security tab, and the Secrets field. Click Next . Select Python 3.7 or your preferred version in the Runtime field. Select Inline Editor in the Source code field. Enter query_astra_db in the Entry point field. Add AstraPy , a Pythonic SDK for DataStax Astra and Stargate, and its preferred version to the requirements.txt file. Replace the main.py content with: from astrapy.rest import create_client , http_methods import os ASTRA_DB_ID = os . environ . get ( 'ASTRA_DB_ID' ) ASTRA_DB_REGION = os . environ . get ( 'ASTRA_DB_REGION' ) ASTRA_DB_APPLICATION_TOKEN = os . environ . get ( 'ASTRA_DB_APPLICATION_TOKEN' ) astra_http_client = create_client ( astra_database_id = ASTRA_DB_ID , astra_database_region = ASTRA_DB_REGION , astra_application_token = ASTRA_DB_APPLICATION_TOKEN ) def query_astra_db ( request ): # Retrieve a row with primary key value 'local' # from table 'local' in keyspace 'system' res = astra_http_client . request ( method = http_methods . GET , path = f \"/api/rest/v2/keyspaces/system/local/local\" ) # Print the 'cql_version' field value of the row print ( res [ \"data\" ][ 0 ][ 'cql_version' ]) print ( 'Success' ) You can learn more about the code above by reading the AstraPy documentation. \u2705 2. Deploy the function. \u00b6 Click Deploy . On the Cloud Functions Overview page, find the newly deployed function. \u2705 3. Test the function. \u00b6 Under Actions , select Test function . On the testing page, click Test the function and observe the output. Notice the CQL version output 3.4.5 and status code 200 . \u2705 4. View logs. \u00b6 You can further explore the log history by either clicking on the Logs tab or the View all logs link that opens Logs Explorer . E - Using Java Driver \u00b6 \u2705 1. Create a secret with the secure connect bundle file. \u00b6 Go to the Secret Manager page , select a project that has Secret Manager and Cloud Functions enabled, and click Create secret . Give a Name to the secret and upload the secure connect bundle file as a Secret value . (See the Prerequisites section above if you need to download your secure connect bundle.) Optionally, customize other secret management settings. Click Create secret . On the Secret Manager page , find the newly created secret. \u2705 2. Create a function. \u00b6 Go to the Functions Overview page , select the same project that has Secret Manager and Cloud Functions enabled, and click Create function . Under the Basics section, specify preferred Function name and Region . Under the Trigger section, select HTTP , Allow unauthenticated invocations , and Require HTTPS . Click Save . Under the Runtime, build, connections and security settings section, customize additional settings and create these Runtime environment variables : ASTRA_DB_CLIENT_ID : A Client ID is generated together with an application token (see the Prerequisites section above). ASTRA_DB_CLIENT_SECRET : A Client secret is generated together with an application token (see the Prerequisites section above). Note that, for better security, you can alternatively use the Secret Manager service to store and manage a client secret. A secret can then be similarly exposed as an environment variable. The settings can be found under the Runtime, build, connections and security settings section, the Security tab, and the Secrets field. Under the Runtime, build, connections and security settings section and the Security , click Reference a secret . Select the previously created Secret with the secure connect bundle file, Grant the service account access to the secret, if needed, use Mounted as volume in the Reference method field, and enter secrets in the Mount path field. Notice the final Path that should be used to access the secure connect bundle in the function code. Click Done and Next . Select Java 11 or your preferred version in the Runtime field. Select Inline Editor in the Source code field. Enter com.example.AstraDBFunction in the Entry point field. Add java-driver , a Java client library for Apache Cassandra, DataStax Astra DB and DataStax Enterprise, to the pom.xml file: <dependency> <groupId> com.datastax.oss </groupId> <artifactId> java-driver-core </artifactId> <version> 4.13.0 </version> </dependency> Rename the Example.java file to AstraDBFunction.java and replace its content with: package com.example ; import com.google.cloud.functions.HttpFunction ; import com.google.cloud.functions.HttpRequest ; import com.google.cloud.functions.HttpResponse ; import java.io.BufferedWriter ; import com.datastax.oss.driver.api.core.CqlSession ; import com.datastax.oss.driver.api.core.cql.ResultSet ; import com.datastax.oss.driver.api.core.cql.Row ; import java.nio.file.Paths ; public class AstraDBFunction implements HttpFunction { public static final String ASTRA_DB_CLIENT_ID = System . getenv ( \"ASTRA_DB_CLIENT_ID\" ); public static final String ASTRA_DB_CLIENT_SECRET = System . getenv ( \"ASTRA_DB_CLIENT_SECRET\" ); public static CqlSession session = CqlSession . builder () . withCloudSecureConnectBundle ( Paths . get ( \"/secrets/secure-connect-secret\" )) . withAuthCredentials ( ASTRA_DB_CLIENT_ID , ASTRA_DB_CLIENT_SECRET ) . build (); public void service ( HttpRequest request , HttpResponse response ) throws Exception { BufferedWriter writer = response . getWriter (); ResultSet rs = session . execute ( \"SELECT cql_version FROM system.local WHERE key = 'local';\" ); Row row = rs . one (); writer . write ( row . getString ( \"cql_version\" ) ); writer . newLine (); writer . write ( \"Success\" ); } } You can learn more about the code above by reading the java-driver documentation. \u2705 3. Deploy the function. \u00b6 Click Deploy . On the Cloud Functions Overview page, find the newly deployed function. \u2705 4. Test the function. \u00b6 Under Actions , select Test function . On the testing page, click Test the function and observe the output. Notice the CQL version output 3.4.5 and status code 200 . \u2705 5. View logs. \u00b6 You can further explore the log history by either clicking on the Logs tab or the View all logs link that opens Logs Explorer . F - Using Java gRPC \u00b6 \u2705 1. Create a function. \u00b6 Go to the Functions Overview page , select a project that has Cloud Functions enabled, and click Create function . Under the Basics section, specify preferred Function name and Region . Under the Trigger section, select HTTP , Allow unauthenticated invocations , and Require HTTPS . Click Save . Under the Runtime, build, connections and security settings section, customize additional settings and create these Runtime environment variables : ASTRA_DB_ID : A Database ID value can be found on the Astra DB dashboard. ASTRA_DB_REGION : A Region name can be found on the overview page for a specific Astra DB database. ASTRA_DB_APPLICATION_TOKEN : An Application Token can be generated for a specific Astra DB database (see the Prerequisites section above). Note that, for better security, you can alternatively use the Secret Manager service to store and manage an application token as a secret. A secret can then be similarly exposed as an environment variable. The settings can be found under the Runtime, build, connections and security settings section, the Security tab, and the Secrets field. Click Next . Select Java 11 or your preferred version in the Runtime field. Select Inline Editor in the Source code field. Enter com.example.AstraDBFunction in the Entry point field. Add gRPC dependencies to the pom.xml file: <dependency> <groupId> io.stargate.grpc </groupId> <artifactId> grpc-proto </artifactId> <version> 1.0.41 </version> </dependency> <dependency> <groupId> io.grpc </groupId> <artifactId> grpc-netty-shaded </artifactId> <version> 1.41.0 </version> </dependency> Rename the Example.java file to AstraDBFunction.java and replace its content with: package com.example ; import com.google.cloud.functions.HttpFunction ; import com.google.cloud.functions.HttpRequest ; import com.google.cloud.functions.HttpResponse ; import java.io.BufferedWriter ; import java.util.concurrent.TimeUnit ; import io.grpc.ManagedChannel ; import io.grpc.ManagedChannelBuilder ; import io.stargate.grpc.StargateBearerToken ; import io.stargate.proto.QueryOuterClass ; import io.stargate.proto.QueryOuterClass.Row ; import io.stargate.proto.StargateGrpc ; public class AstraDBFunction implements HttpFunction { public static final String ASTRA_DB_TOKEN = System . getenv ( \"ASTRA_DB_APPLICATION_TOKEN\" ); public static final String ASTRA_DB_ID = System . getenv ( \"ASTRA_DB_ID\" ); public static final String ASTRA_DB_REGION = System . getenv ( \"ASTRA_DB_REGION\" ); public static ManagedChannel channel = ManagedChannelBuilder . forAddress ( ASTRA_DB_ID + \"-\" + ASTRA_DB_REGION + \".apps.astra.datastax.com\" , 443 ) . useTransportSecurity () . build (); public static StargateGrpc . StargateBlockingStub blockingStub = StargateGrpc . newBlockingStub ( channel ). withCallCredentials ( new StargateBearerToken ( ASTRA_DB_TOKEN )); public void service ( HttpRequest request , HttpResponse response ) throws Exception { QueryOuterClass . Response queryString = blockingStub . executeQuery ( QueryOuterClass . Query . newBuilder () . setCql ( \"SELECT cql_version FROM system.local WHERE key = 'local';\" ) . build ()); QueryOuterClass . ResultSet rs = queryString . getResultSet (); BufferedWriter writer = response . getWriter (); writer . write ( rs . getRows ( 0 ). getValues ( 0 ). getString () ); writer . newLine (); writer . write ( \"Success\" ); } } You can learn more about the code above by reading the Stargate documentation. \u2705 2. Deploy the function. \u00b6 Click Deploy . On the Cloud Functions Overview page, find the newly deployed function. \u2705 3. Test the function. \u00b6 Under Actions , select Test function . On the testing page, click Test the function and observe the output. Notice the CQL version output 3.4.5 and status code 200 . \u2705 4. View logs. \u00b6 You can further explore the log history by either clicking on the Logs tab or the View all logs link that opens Logs Explorer .","title":"\u2022 Google Cloud Functions"},{"location":"pages/develop/platform/google-cloud-function/#google-cloud-functions","text":"","title":"Google Cloud Functions"},{"location":"pages/develop/platform/google-cloud-function/#a-overview","text":"Cloud Functions is Google's function-as-a-service offering that provides a serverless execution environment for your code. Cloud Functions are commonly used to: Extend Astra DB with additional data processing capabilities, such as aggregating, summarizing and validating data periodically; Connect Astra DB with other cloud services into data pipelines that move, process and analyze data.","title":"A - Overview"},{"location":"pages/develop/platform/google-cloud-function/#b-prerequisites","text":"Create an Astra Database Create an Astra Token Download a Secure Connect Bundle Optionally, if you are new to Cloud Functions, practice creating a simpler function first","title":"B - Prerequisites"},{"location":"pages/develop/platform/google-cloud-function/#c-using-python-driver","text":"","title":"C - Using Python Driver"},{"location":"pages/develop/platform/google-cloud-function/#1-create-a-secret-with-the-secure-connect-bundle-file","text":"Go to the Secret Manager page , select a project that has Secret Manager and Cloud Functions enabled, and click Create secret . Give a Name to the secret and upload the secure connect bundle file as a Secret value . (See the Prerequisites section above if you need to download your secure connect bundle.) Optionally, customize other secret management settings. Click Create secret . On the Secret Manager page , find the newly created secret.","title":"\u2705 1. Create a secret with the secure connect bundle file."},{"location":"pages/develop/platform/google-cloud-function/#2-create-a-function","text":"Go to the Functions Overview page , select the same project that has Secret Manager and Cloud Functions enabled, and click Create function . Under the Basics section, specify preferred Function name and Region . Under the Trigger section, select HTTP , Allow unauthenticated invocations , and Require HTTPS . Click Save . Under the Runtime, build, connections and security settings section, customize additional settings and create these Runtime environment variables : ASTRA_DB_CLIENT_ID : A Client ID is generated together with an application token (see the Prerequisites section above). ASTRA_DB_CLIENT_SECRET : A Client secret is generated together with an application token (see the Prerequisites section above). Note that, for better security, you can alternatively use the Secret Manager service to store and manage a client secret. A secret can then be similarly exposed as an environment variable. The settings can be found under the Runtime, build, connections and security settings section, the Security tab, and the Secrets field. Under the Runtime, build, connections and security settings section and the Security , click Reference a secret . Select the previously created Secret with the secure connect bundle file, Grant the service account access to the secret, if needed, use Mounted as volume in the Reference method field, and enter secrets in the Mount path field. Notice the final Path that should be used to access the secure connect bundle in the function code. Click Done and Next . Select Python 3.7 or your preferred version in the Runtime field. Select Inline Editor in the Source code field. Enter query_astra_db in the Entry point field. Add cassandra-driver , a Python client library for Apache Cassandra, DataStax Astra DB and DataStax Enterprise, to the requirements.txt file. Replace the main.py content with: from cassandra.cluster import Cluster from cassandra.auth import PlainTextAuthProvider import os ASTRA_DB_CLIENT_ID = os . environ . get ( 'ASTRA_DB_CLIENT_ID' ) ASTRA_DB_CLIENT_SECRET = os . environ . get ( 'ASTRA_DB_CLIENT_SECRET' ) cloud_config = { 'secure_connect_bundle' : '/secrets/secure-connect-secret' , 'use_default_tempdir' : True } auth_provider = PlainTextAuthProvider ( ASTRA_DB_CLIENT_ID , ASTRA_DB_CLIENT_SECRET ) cluster = Cluster ( cloud = cloud_config , auth_provider = auth_provider , protocol_version = 4 ) def query_astra_db ( request ): session = cluster . connect () row = session . execute ( \"SELECT cql_version FROM system.local WHERE key = 'local';\" ) . one () print ( row [ 0 ]) print ( 'Success' ) You can learn more about the code above by reading the python-driver documentation.","title":"\u2705 2. Create a function."},{"location":"pages/develop/platform/google-cloud-function/#3-deploy-the-function","text":"Click Deploy . On the Cloud Functions Overview page, find the newly deployed function.","title":"\u2705 3. Deploy the function."},{"location":"pages/develop/platform/google-cloud-function/#4-test-the-function","text":"Under Actions , select Test function . On the testing page, click Test the function and observe the output. Notice the CQL version output 3.4.5 and status code 200 .","title":"\u2705 4. Test the function."},{"location":"pages/develop/platform/google-cloud-function/#5-view-logs","text":"You can further explore the log history by either clicking on the Logs tab or the View all logs link that opens Logs Explorer .","title":"\u2705 5. View logs."},{"location":"pages/develop/platform/google-cloud-function/#d-using-python-sdk","text":"","title":"D - Using Python SDK"},{"location":"pages/develop/platform/google-cloud-function/#1-create-a-function","text":"Go to the Functions Overview page , select a project that has Cloud Functions enabled, and click Create function . Under the Basics section, specify preferred Function name and Region . Under the Trigger section, select HTTP , Allow unauthenticated invocations , and Require HTTPS . Click Save . Under the Runtime, build, connections and security settings section, customize additional settings and create these Runtime environment variables : ASTRA_DB_ID : A Database ID value can be found on the Astra DB dashboard. ASTRA_DB_REGION : A Region name can be found on the overview page for a specific Astra DB database. ASTRA_DB_APPLICATION_TOKEN : An Application Token can be generated for a specific Astra DB database (see the Prerequisites section above). Note that, for better security, you can alternatively use the Secret Manager service to store and manage an application token as a secret. A secret can then be similarly exposed as an environment variable. The settings can be found under the Runtime, build, connections and security settings section, the Security tab, and the Secrets field. Click Next . Select Python 3.7 or your preferred version in the Runtime field. Select Inline Editor in the Source code field. Enter query_astra_db in the Entry point field. Add AstraPy , a Pythonic SDK for DataStax Astra and Stargate, and its preferred version to the requirements.txt file. Replace the main.py content with: from astrapy.rest import create_client , http_methods import os ASTRA_DB_ID = os . environ . get ( 'ASTRA_DB_ID' ) ASTRA_DB_REGION = os . environ . get ( 'ASTRA_DB_REGION' ) ASTRA_DB_APPLICATION_TOKEN = os . environ . get ( 'ASTRA_DB_APPLICATION_TOKEN' ) astra_http_client = create_client ( astra_database_id = ASTRA_DB_ID , astra_database_region = ASTRA_DB_REGION , astra_application_token = ASTRA_DB_APPLICATION_TOKEN ) def query_astra_db ( request ): # Retrieve a row with primary key value 'local' # from table 'local' in keyspace 'system' res = astra_http_client . request ( method = http_methods . GET , path = f \"/api/rest/v2/keyspaces/system/local/local\" ) # Print the 'cql_version' field value of the row print ( res [ \"data\" ][ 0 ][ 'cql_version' ]) print ( 'Success' ) You can learn more about the code above by reading the AstraPy documentation.","title":"\u2705 1. Create a function."},{"location":"pages/develop/platform/google-cloud-function/#2-deploy-the-function","text":"Click Deploy . On the Cloud Functions Overview page, find the newly deployed function.","title":"\u2705 2. Deploy the function."},{"location":"pages/develop/platform/google-cloud-function/#3-test-the-function","text":"Under Actions , select Test function . On the testing page, click Test the function and observe the output. Notice the CQL version output 3.4.5 and status code 200 .","title":"\u2705 3. Test the function."},{"location":"pages/develop/platform/google-cloud-function/#4-view-logs","text":"You can further explore the log history by either clicking on the Logs tab or the View all logs link that opens Logs Explorer .","title":"\u2705 4. View logs."},{"location":"pages/develop/platform/google-cloud-function/#e-using-java-driver","text":"","title":"E - Using Java Driver"},{"location":"pages/develop/platform/google-cloud-function/#1-create-a-secret-with-the-secure-connect-bundle-file_1","text":"Go to the Secret Manager page , select a project that has Secret Manager and Cloud Functions enabled, and click Create secret . Give a Name to the secret and upload the secure connect bundle file as a Secret value . (See the Prerequisites section above if you need to download your secure connect bundle.) Optionally, customize other secret management settings. Click Create secret . On the Secret Manager page , find the newly created secret.","title":"\u2705 1. Create a secret with the secure connect bundle file."},{"location":"pages/develop/platform/google-cloud-function/#2-create-a-function_1","text":"Go to the Functions Overview page , select the same project that has Secret Manager and Cloud Functions enabled, and click Create function . Under the Basics section, specify preferred Function name and Region . Under the Trigger section, select HTTP , Allow unauthenticated invocations , and Require HTTPS . Click Save . Under the Runtime, build, connections and security settings section, customize additional settings and create these Runtime environment variables : ASTRA_DB_CLIENT_ID : A Client ID is generated together with an application token (see the Prerequisites section above). ASTRA_DB_CLIENT_SECRET : A Client secret is generated together with an application token (see the Prerequisites section above). Note that, for better security, you can alternatively use the Secret Manager service to store and manage a client secret. A secret can then be similarly exposed as an environment variable. The settings can be found under the Runtime, build, connections and security settings section, the Security tab, and the Secrets field. Under the Runtime, build, connections and security settings section and the Security , click Reference a secret . Select the previously created Secret with the secure connect bundle file, Grant the service account access to the secret, if needed, use Mounted as volume in the Reference method field, and enter secrets in the Mount path field. Notice the final Path that should be used to access the secure connect bundle in the function code. Click Done and Next . Select Java 11 or your preferred version in the Runtime field. Select Inline Editor in the Source code field. Enter com.example.AstraDBFunction in the Entry point field. Add java-driver , a Java client library for Apache Cassandra, DataStax Astra DB and DataStax Enterprise, to the pom.xml file: <dependency> <groupId> com.datastax.oss </groupId> <artifactId> java-driver-core </artifactId> <version> 4.13.0 </version> </dependency> Rename the Example.java file to AstraDBFunction.java and replace its content with: package com.example ; import com.google.cloud.functions.HttpFunction ; import com.google.cloud.functions.HttpRequest ; import com.google.cloud.functions.HttpResponse ; import java.io.BufferedWriter ; import com.datastax.oss.driver.api.core.CqlSession ; import com.datastax.oss.driver.api.core.cql.ResultSet ; import com.datastax.oss.driver.api.core.cql.Row ; import java.nio.file.Paths ; public class AstraDBFunction implements HttpFunction { public static final String ASTRA_DB_CLIENT_ID = System . getenv ( \"ASTRA_DB_CLIENT_ID\" ); public static final String ASTRA_DB_CLIENT_SECRET = System . getenv ( \"ASTRA_DB_CLIENT_SECRET\" ); public static CqlSession session = CqlSession . builder () . withCloudSecureConnectBundle ( Paths . get ( \"/secrets/secure-connect-secret\" )) . withAuthCredentials ( ASTRA_DB_CLIENT_ID , ASTRA_DB_CLIENT_SECRET ) . build (); public void service ( HttpRequest request , HttpResponse response ) throws Exception { BufferedWriter writer = response . getWriter (); ResultSet rs = session . execute ( \"SELECT cql_version FROM system.local WHERE key = 'local';\" ); Row row = rs . one (); writer . write ( row . getString ( \"cql_version\" ) ); writer . newLine (); writer . write ( \"Success\" ); } } You can learn more about the code above by reading the java-driver documentation.","title":"\u2705 2. Create a function."},{"location":"pages/develop/platform/google-cloud-function/#3-deploy-the-function_1","text":"Click Deploy . On the Cloud Functions Overview page, find the newly deployed function.","title":"\u2705 3. Deploy the function."},{"location":"pages/develop/platform/google-cloud-function/#4-test-the-function_1","text":"Under Actions , select Test function . On the testing page, click Test the function and observe the output. Notice the CQL version output 3.4.5 and status code 200 .","title":"\u2705 4. Test the function."},{"location":"pages/develop/platform/google-cloud-function/#5-view-logs_1","text":"You can further explore the log history by either clicking on the Logs tab or the View all logs link that opens Logs Explorer .","title":"\u2705 5. View logs."},{"location":"pages/develop/platform/google-cloud-function/#f-using-java-grpc","text":"","title":"F - Using Java gRPC"},{"location":"pages/develop/platform/google-cloud-function/#1-create-a-function_1","text":"Go to the Functions Overview page , select a project that has Cloud Functions enabled, and click Create function . Under the Basics section, specify preferred Function name and Region . Under the Trigger section, select HTTP , Allow unauthenticated invocations , and Require HTTPS . Click Save . Under the Runtime, build, connections and security settings section, customize additional settings and create these Runtime environment variables : ASTRA_DB_ID : A Database ID value can be found on the Astra DB dashboard. ASTRA_DB_REGION : A Region name can be found on the overview page for a specific Astra DB database. ASTRA_DB_APPLICATION_TOKEN : An Application Token can be generated for a specific Astra DB database (see the Prerequisites section above). Note that, for better security, you can alternatively use the Secret Manager service to store and manage an application token as a secret. A secret can then be similarly exposed as an environment variable. The settings can be found under the Runtime, build, connections and security settings section, the Security tab, and the Secrets field. Click Next . Select Java 11 or your preferred version in the Runtime field. Select Inline Editor in the Source code field. Enter com.example.AstraDBFunction in the Entry point field. Add gRPC dependencies to the pom.xml file: <dependency> <groupId> io.stargate.grpc </groupId> <artifactId> grpc-proto </artifactId> <version> 1.0.41 </version> </dependency> <dependency> <groupId> io.grpc </groupId> <artifactId> grpc-netty-shaded </artifactId> <version> 1.41.0 </version> </dependency> Rename the Example.java file to AstraDBFunction.java and replace its content with: package com.example ; import com.google.cloud.functions.HttpFunction ; import com.google.cloud.functions.HttpRequest ; import com.google.cloud.functions.HttpResponse ; import java.io.BufferedWriter ; import java.util.concurrent.TimeUnit ; import io.grpc.ManagedChannel ; import io.grpc.ManagedChannelBuilder ; import io.stargate.grpc.StargateBearerToken ; import io.stargate.proto.QueryOuterClass ; import io.stargate.proto.QueryOuterClass.Row ; import io.stargate.proto.StargateGrpc ; public class AstraDBFunction implements HttpFunction { public static final String ASTRA_DB_TOKEN = System . getenv ( \"ASTRA_DB_APPLICATION_TOKEN\" ); public static final String ASTRA_DB_ID = System . getenv ( \"ASTRA_DB_ID\" ); public static final String ASTRA_DB_REGION = System . getenv ( \"ASTRA_DB_REGION\" ); public static ManagedChannel channel = ManagedChannelBuilder . forAddress ( ASTRA_DB_ID + \"-\" + ASTRA_DB_REGION + \".apps.astra.datastax.com\" , 443 ) . useTransportSecurity () . build (); public static StargateGrpc . StargateBlockingStub blockingStub = StargateGrpc . newBlockingStub ( channel ). withCallCredentials ( new StargateBearerToken ( ASTRA_DB_TOKEN )); public void service ( HttpRequest request , HttpResponse response ) throws Exception { QueryOuterClass . Response queryString = blockingStub . executeQuery ( QueryOuterClass . Query . newBuilder () . setCql ( \"SELECT cql_version FROM system.local WHERE key = 'local';\" ) . build ()); QueryOuterClass . ResultSet rs = queryString . getResultSet (); BufferedWriter writer = response . getWriter (); writer . write ( rs . getRows ( 0 ). getValues ( 0 ). getString () ); writer . newLine (); writer . write ( \"Success\" ); } } You can learn more about the code above by reading the Stargate documentation.","title":"\u2705 1. Create a function."},{"location":"pages/develop/platform/google-cloud-function/#2-deploy-the-function_1","text":"Click Deploy . On the Cloud Functions Overview page, find the newly deployed function.","title":"\u2705 2. Deploy the function."},{"location":"pages/develop/platform/google-cloud-function/#3-test-the-function_1","text":"Under Actions , select Test function . On the testing page, click Test the function and observe the output. Notice the CQL version output 3.4.5 and status code 200 .","title":"\u2705 3. Test the function."},{"location":"pages/develop/platform/google-cloud-function/#4-view-logs_1","text":"You can further explore the log history by either clicking on the Logs tab or the View all logs link that opens Logs Explorer .","title":"\u2705 4. View logs."},{"location":"pages/tools/","text":"Apache Airflow : Apache Airflow is an open source workflow management system. It provides components which allow engineers to build data pipelines between different systems. Apache Flink : Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Apache Nifi : NiFi was built to automate the flow of data between systems. While the term 'dataflow' is used in a variety of contexts, we use it here to mean the automated and managed flow of information between systems. Apache Spark : Apache Spark is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size. Use Apache Spark to connect to your database and begin accessing your Astra DB tables using Scala in spark-shell. Authorizer : Authorizer is an open source auth solution for application. It works with many different databases, allowing the developers to use a single datastore for the entire application stack and have complete control over all user data. Celery : Celery is an open-source, distributed task queue written in Python. With Celery you can run tasks (e.g. processing of messages) in an asynchronous fashion. Celery supports a variety of message buses and backends; among the supported backends are Cassandra and Astra DB. Cloud Functions (Python Driver) : Google's function-as-a-service offering that provides a serverless execution environment for your code. Cloud Functions are commonly used to extend Astra DB with additional data processing capabilities and connect Astra DB with other cloud services into data pipelines. Cloud Functions (Python SDK) : Google's function-as-a-service offering that provides a serverless execution environment for your code. Cloud Functions are commonly used to extend Astra DB with additional data processing capabilities and connect Astra DB with other cloud services into data pipelines. CQL Proxy : cql-proxy is designed to forward your application's CQL traffic to an appropriate database service. It listens on a local address and securely forwards that traffic. CQL Shell : the standalone CQLSH client is a separate, lightweight tool you can use to interact with your database. Datagrip Jetbrains : DataGrip is a database management environment for developers. It is designed to query, create, and manage databases. Databases can work locally, on a server, or in the cloud. Supports MySQL, PostgreSQL, Microsoft SQL Server, Oracle, and more. If you have a JDBC driver, add it to DataGrip, connect to your DBMS, and start working. DataStation : DataStation is an open-source data IDE for developers. DataStax Bulk : The DataStax Bulk Loader tool (DSBulk) is a unified tool for loading into and unloading from Cassandra-compatible storage engines, such as OSS Apache Cassandra\u00ae, DataStax Astra and DataStax Enterprise (DSE). DBeaver : DBeaver is a universal database management tool for everyone who needs to work with data in a professional way. With DBeaver you are able to manipulate with your data like in a regular spreadsheet, create analytical reports based on records from different data storages, export information in an appropriate format. Feast : Feast is a feature store for machine learning whose goal is to provide a (mostly cloud-based) infrastructure for managing, versioning and sharing features for training and serving ML models. Grafana : Grafana is an industry standard tool for data visualisation. With Grafana, you can explore your time-series data using different visualisations: charts, plots, diagrams and even configure alerting if a value exceeds some desired range. HashiCorp Vault : Vault is an identity-based secrets and encryption management system. A secret is anything that you want to tightly control access to, such as API encryption keys, passwords, or certificates. Vault provides encryption services that are gated by authentication and authorization methods. Astra DB Plugin IntelliJ IDEA : The Capable & Ergonomic Java IDE by JetBrains JanusGraph : JanusGraph is designed to support the processing of graphs so large that they require storage and computational capacities beyond what a single machine can provide. Scaling graph data processing for real time traversals and analytical queries is JanusGraph\u2019s foundational benefit. This section will discuss the various specific benefits of JanusGraph and its underlying, supported persistence solutions. Liquibase : Liquibase is a database schema change management solution that enables you to revise and release database changes faster and safer from development to production. Micronaut : Micronaut is a modern, JVM-based, full stack Java framework designed for building modular, easily testable JVM applications with support for Java, Kotlin, and Groovy. Micronaut is developed by the creators of the Grails framework and takes inspiration from lessons learnt over the years building real-world applications from monoliths to microservices using Spring, Spring Boot and Grails. MindsDB :MindsDB enables you to use ML predictions in your database using SQL. Pentaho Data Integration : Pentaho Data Integration (PDI) provides the Extract, Transform, and Load (ETL) capabilities that facilitate the process of capturing, cleansing, and storing data using a uniform and consistent format that is accessible and relevant to end users and IoT technologies. Quine : Quine.io is a streaming graph capable of building high-volumes of data into a stateful graph. It allows for real-time traversals on a graph, as well as for the data to be streamed-out for event processing. StepZen : StepZen helps developers build GraphQL faster, deploy in seconds, and run on StepZen. It simplifies how you access the data you need, and with zero infrastructure to build or manage, you can focus on crafting modern data-driven experiences. TablePlus : TablePlus is a modern, native tool with elegant UI that allows you to simultaneously manage multiple databases such as MySQL, PostgreSQL, SQLite, Microsoft SQL Server and more. Temporal : Temporal.io is an open source microservice orchestration platform that assists in tracking workflows in your application development. It provides the user with a plug-and-play persistence layer that lets the user choose and configure their Temporal Server with their preferred backend.","title":"Tools List"},{"location":"pages/tools/databases/janusgraph/","text":"This article includes information that was originally written by Erick Ramirez on DataStax Community Documented on JanusGraph official documentation A - Overview \u00b6 JanusGraph is designed to support the processing of graphs so large that they require storage and computational capacities beyond what a single machine can provide. Scaling graph data processing for real time traversals and analytical queries is JanusGraph\u2019s foundational benefit. This section will discuss the various specific benefits of JanusGraph and its underlying, supported persistence solutions. \u2139\ufe0f Introduction to JanusGraph \ud83d\udce5 JanusGraph Installation JanusGraph uses the Java driver to connect to Cassandra as the storage backend. The Java driver itself supports connections to Astra DB natively. For example: CqlSession session = CqlSession.builder() .withCloudSecureConnectBundle(Paths.get(\"/path/to/secure-connect-db_name.zip\")) .withAuthCredentials(\"token\", ASTRA_APP_TOKEN) .withKeyspace(\"keyspace_name\") .build(); However, JanusGraph does not expose this functionality so you will need to manually unpack the secure connect bundle and use its contents to configure JanusGraph which you will obtain in the Prerequisites . B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle This article assumes you have a running installation of JanusGraph server. This was written and tested on JanusGraph v0.6.0. It has not been tested on older versions of JanusGraph. You will need to choose which keyspace to use to store your graph. If it doesn't exist, you will need to create the keyspace on the Astra UI C - Installation and Setup \u00b6 Note: For simplicity, the secure connect bundle has been placed in /path/to/scb \u2705 Step 1: DB Information \u00b6 On the JanusGraph server, unpack your secure bundle. For example: $ cd /path/to/scb $ unzip secure-connect-janusgraph.zip Here is an example file listing after unpacking the bundle: / path/ to/ scb/ ca.crt cert cert.pfx config.json cqlshrc identity.jks key trustStore.jks Obtain information about your database from the config.json file. Here is an example: { \"host\": \"70bf8560-105f-11ec-a3ea-0800200c9a66-us-west1.db.astra.datastax.com\", \"port\": 98765, \"cql_port\": 34567, \"keyspace\": \"janusgraph\", \"localDC\": \"us-west1\", \"caCertLocation\": \"./ca.crt\", \"keyLocation\": \"./key\", \"certLocation\": \"./cert\", \"keyStoreLocation\": \"./identity.jks\", \"keyStorePassword\": \"Kga1OJ83EF2oBQYR5\", \"trustStoreLocation\": \"./trustStore.jks\", \"trustStorePassword\": \"n8F9ptJO3H7YRxTW1\", \"csvLocation\": \"./data\", \"pfxCertPassword\": \"9b3HgFChtY60m4nfJ\" } We will use this information to configure Astra DB as the storage backend for JanusGraph. \u2705 Step 2: Graph Storage \u00b6 On the JanusGraph server, modify the CQL storage configuration file: $ cd janusgraph-0.6.0 $ vi conf/janusgraph-cql.properties Make the necessary changes using this template: # basic CQL settings gremlin.graph=org.janusgraph.core.JanusGraphFactory storage.backend=cql storage.hostname=CONFIG-JSON-HOST storage.port=CONFIG-JSON-CQL-PORT storage.username=token <----- do NOT change this storage.password=ASTRA_APP_TOKEN storage.cql.keyspace=GRAPH_KEYSPACE storage.cql.local-datacenter=CONFIG-JSON-LOCALDC # SSL related settings storage.cql.ssl.enabled=true storage.cql.ssl.truststore.location=/path/to/scb/trustStore.jks storage.cql.ssl.truststore.password=CONFIG-JSON-TRUSTSTOREPASSWORD storage.cql.ssl.keystore.location=/path/to/scb/identity.jks storage.cql.ssl.keystore.keypassword=CONFIG-JSON-KEYSTOREPASSWORD storage.cql.ssl.keystore.storepassword=CONFIG-JSON-KEYSTOREPASSWORD storage.cql.ssl.client-authentication-enabled=true # consistency settings storage.cql.read-consistency-level=LOCAL_QUORUM storage.cql.write-consistency-level=LOCAL_QUORUM WARNING The username to connect to Astra is the literal string token . Do NOT set this value to your DB's client ID. IMPORTANT The ASTRA_APP_TOKEN is from the token you generated in the Prerequisites section above. Using the example values in the config.json above, my conf/janusgraph-cql.properties would contain: # basic CQL settings gremlin.graph=org.janusgraph.core.JanusGraphFactory storage.backend=cql storage.hostname=70bf8560-105f-11ec-a3ea-0800200c9a66-us-west1.db.astra.datastax.com storage.port=34567 storage.username=token storage.password=AstraCS:AbCwZYOKqvXHZWRvpbvHqXYz:47820923e5be3b7b9e689bc18614c631d5fdd8b435e68613433651fd20fexyz0 storage.cql.keyspace=janusgraph storage.cql.local-datacenter=us-west1 # SSL related settings storage.cql.ssl.enabled=true storage.cql.ssl.truststore.location=/path/to/scb/trustStore.jks storage.cql.ssl.truststore.password=n8F9ptJO3H7YRxTW1 storage.cql.ssl.keystore.location=/path/to/scb/identity.jks storage.cql.ssl.keystore.keypassword=Kga1OJ83EF2oBQYR5 storage.cql.ssl.keystore.storepassword=Kga1OJ83EF2oBQYR5 storage.cql.ssl.client-authentication-enabled=true # consistency settings storage.cql.read-consistency-level=LOCAL_QUORUM storage.cql.write-consistency-level=LOCAL_QUORUM \u2705 Step 3: Final Test \u00b6 Start a Gremlin console: $ bin/gremlin.sh \\,,,/ (o o) -----oOOo-(3)-oOOo----- gremlin> Load a graph using Astra as the storage backend with: gremlin> graph = JanusGraphFactory.open('conf/janusgraph-cql.properties') ==>standardjanusgraph[cql:[70bf8560-105f-11ec-a3ea-0800200c9a66-us-west1.db.astra.datastax.com]] Note It is normal to see some warnings on the Gremlin console. I have attached a text file with a sample output so you know what to expect. In the Astra CQL Console , I can see JanusGraph created the following tables in the janusgraph keyspace: token@cqlsh> USE janusgraph; token@cqlsh:janusgraph> DESCRIBE TABLES; edgestore_lock_ graphindex_lock_ janusgraph_ids txlog systemlog graphindex edgestore system_properties_lock_ system_properties \ud83c\udfe0 Back to home","title":"\u2022 JanusGraph"},{"location":"pages/tools/databases/janusgraph/#a-overview","text":"JanusGraph is designed to support the processing of graphs so large that they require storage and computational capacities beyond what a single machine can provide. Scaling graph data processing for real time traversals and analytical queries is JanusGraph\u2019s foundational benefit. This section will discuss the various specific benefits of JanusGraph and its underlying, supported persistence solutions. \u2139\ufe0f Introduction to JanusGraph \ud83d\udce5 JanusGraph Installation JanusGraph uses the Java driver to connect to Cassandra as the storage backend. The Java driver itself supports connections to Astra DB natively. For example: CqlSession session = CqlSession.builder() .withCloudSecureConnectBundle(Paths.get(\"/path/to/secure-connect-db_name.zip\")) .withAuthCredentials(\"token\", ASTRA_APP_TOKEN) .withKeyspace(\"keyspace_name\") .build(); However, JanusGraph does not expose this functionality so you will need to manually unpack the secure connect bundle and use its contents to configure JanusGraph which you will obtain in the Prerequisites .","title":"A - Overview"},{"location":"pages/tools/databases/janusgraph/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle This article assumes you have a running installation of JanusGraph server. This was written and tested on JanusGraph v0.6.0. It has not been tested on older versions of JanusGraph. You will need to choose which keyspace to use to store your graph. If it doesn't exist, you will need to create the keyspace on the Astra UI","title":"B - Prerequisites"},{"location":"pages/tools/databases/janusgraph/#c-installation-and-setup","text":"Note: For simplicity, the secure connect bundle has been placed in /path/to/scb","title":"C - Installation and Setup"},{"location":"pages/tools/databases/janusgraph/#step-1-db-information","text":"On the JanusGraph server, unpack your secure bundle. For example: $ cd /path/to/scb $ unzip secure-connect-janusgraph.zip Here is an example file listing after unpacking the bundle: / path/ to/ scb/ ca.crt cert cert.pfx config.json cqlshrc identity.jks key trustStore.jks Obtain information about your database from the config.json file. Here is an example: { \"host\": \"70bf8560-105f-11ec-a3ea-0800200c9a66-us-west1.db.astra.datastax.com\", \"port\": 98765, \"cql_port\": 34567, \"keyspace\": \"janusgraph\", \"localDC\": \"us-west1\", \"caCertLocation\": \"./ca.crt\", \"keyLocation\": \"./key\", \"certLocation\": \"./cert\", \"keyStoreLocation\": \"./identity.jks\", \"keyStorePassword\": \"Kga1OJ83EF2oBQYR5\", \"trustStoreLocation\": \"./trustStore.jks\", \"trustStorePassword\": \"n8F9ptJO3H7YRxTW1\", \"csvLocation\": \"./data\", \"pfxCertPassword\": \"9b3HgFChtY60m4nfJ\" } We will use this information to configure Astra DB as the storage backend for JanusGraph.","title":"\u2705 Step 1: DB Information"},{"location":"pages/tools/databases/janusgraph/#step-2-graph-storage","text":"On the JanusGraph server, modify the CQL storage configuration file: $ cd janusgraph-0.6.0 $ vi conf/janusgraph-cql.properties Make the necessary changes using this template: # basic CQL settings gremlin.graph=org.janusgraph.core.JanusGraphFactory storage.backend=cql storage.hostname=CONFIG-JSON-HOST storage.port=CONFIG-JSON-CQL-PORT storage.username=token <----- do NOT change this storage.password=ASTRA_APP_TOKEN storage.cql.keyspace=GRAPH_KEYSPACE storage.cql.local-datacenter=CONFIG-JSON-LOCALDC # SSL related settings storage.cql.ssl.enabled=true storage.cql.ssl.truststore.location=/path/to/scb/trustStore.jks storage.cql.ssl.truststore.password=CONFIG-JSON-TRUSTSTOREPASSWORD storage.cql.ssl.keystore.location=/path/to/scb/identity.jks storage.cql.ssl.keystore.keypassword=CONFIG-JSON-KEYSTOREPASSWORD storage.cql.ssl.keystore.storepassword=CONFIG-JSON-KEYSTOREPASSWORD storage.cql.ssl.client-authentication-enabled=true # consistency settings storage.cql.read-consistency-level=LOCAL_QUORUM storage.cql.write-consistency-level=LOCAL_QUORUM WARNING The username to connect to Astra is the literal string token . Do NOT set this value to your DB's client ID. IMPORTANT The ASTRA_APP_TOKEN is from the token you generated in the Prerequisites section above. Using the example values in the config.json above, my conf/janusgraph-cql.properties would contain: # basic CQL settings gremlin.graph=org.janusgraph.core.JanusGraphFactory storage.backend=cql storage.hostname=70bf8560-105f-11ec-a3ea-0800200c9a66-us-west1.db.astra.datastax.com storage.port=34567 storage.username=token storage.password=AstraCS:AbCwZYOKqvXHZWRvpbvHqXYz:47820923e5be3b7b9e689bc18614c631d5fdd8b435e68613433651fd20fexyz0 storage.cql.keyspace=janusgraph storage.cql.local-datacenter=us-west1 # SSL related settings storage.cql.ssl.enabled=true storage.cql.ssl.truststore.location=/path/to/scb/trustStore.jks storage.cql.ssl.truststore.password=n8F9ptJO3H7YRxTW1 storage.cql.ssl.keystore.location=/path/to/scb/identity.jks storage.cql.ssl.keystore.keypassword=Kga1OJ83EF2oBQYR5 storage.cql.ssl.keystore.storepassword=Kga1OJ83EF2oBQYR5 storage.cql.ssl.client-authentication-enabled=true # consistency settings storage.cql.read-consistency-level=LOCAL_QUORUM storage.cql.write-consistency-level=LOCAL_QUORUM","title":"\u2705 Step 2: Graph Storage"},{"location":"pages/tools/databases/janusgraph/#step-3-final-test","text":"Start a Gremlin console: $ bin/gremlin.sh \\,,,/ (o o) -----oOOo-(3)-oOOo----- gremlin> Load a graph using Astra as the storage backend with: gremlin> graph = JanusGraphFactory.open('conf/janusgraph-cql.properties') ==>standardjanusgraph[cql:[70bf8560-105f-11ec-a3ea-0800200c9a66-us-west1.db.astra.datastax.com]] Note It is normal to see some warnings on the Gremlin console. I have attached a text file with a sample output so you know what to expect. In the Astra CQL Console , I can see JanusGraph created the following tables in the janusgraph keyspace: token@cqlsh> USE janusgraph; token@cqlsh:janusgraph> DESCRIBE TABLES; edgestore_lock_ graphindex_lock_ janusgraph_ids txlog systemlog graphindex edgestore system_properties_lock_ system_properties \ud83c\udfe0 Back to home","title":"\u2705 Step 3: Final Test"},{"location":"pages/tools/ide/datastation/","text":"Overview \u00b6 DataStation is an open-source data IDE for developers. It allows you to easily build graphs and tables with data pulled from SQL databases, logging databases, metrics databases, HTTP servers, and all kinds of text and binary files. Need to join or munge data? Write embedded scripts as needed in languages like Python, JavaScript, R or SQL. All in one application. This tutorial will show you step-by-step how to connect your Astra DB with DataStation. \u2139\ufe0f Introduction to DataStation \ud83d\udce5 DataStation Quick Install Prerequisites \u00b6 You should Install DataStation You should have an Astra account You should Create an Astra Database You should Have an Astra Token Clone this repository to use to set up CQL-Proxy which is a sidecar that enables unsupported CQL drivers to work with DataStax Astra You need your Astra Token and Astra Database ID to use CQL-Proxy Follow the steps in the repo to spin up CQL-Proxy using Terminal/Command Line. Once successfully running, you should see the following output: {\"level\":\"info\",\"ts\":1651012815.176512,\"caller\":\"proxy/proxy.go:222\",\"msg\":\"proxy is listening\",\"address\":\"[::]:9042\"} C - Installation and Setup \u00b6 Once you have completed all of the Prerequisites and confirmed CQL Proxy is running, you are now able to move on to setting up your Astra DB with DataStation IDE. First, launch your DataStation IDE. Click Add Data Source and select Cassandra A dialog will appear that will prompt you to name this connection (in this example I used Test ) and your Cassandra credentials: Host - Use localhost:9042 or 127.0.0.1:9042 Keyspace - Enter the name of the keyspace that you want to use from your Astra DB. Username - Use token Password - From your Astra Token creation in the Prerequisites, find your Token. Ex. AstraCS:BWsdjhdf... Once you've entered your credentials, click Add Panel and select Database under IMPORT FROM . This will allow DataStation to connect with the database you are trying to view through the credentials you had just entered in the previous step. Note DataStation IDE currently doesn't show a success message after the credentials have been entered. The Password field will also show up blank once you've minimized the credentials panel on the sidebar, but this does not necessarily mean you have to re-enter your password. You will know your database has been added once your display looks like this. You can name this Panel what you'd like. In this example, it is titled Test Panel . To test and validate, you can run a quick query to one of the tables that are present in the Keyspace that you provided in Step 3. ...and you're done! This tutorial quickly shows you how you can easily integrate your Astra DB with the DataStation IDE to run queries, build tables, and further enhance how you interact with your data. \ud83c\udfe0 Back to home","title":"\u2022 DataStation"},{"location":"pages/tools/ide/datastation/#overview","text":"DataStation is an open-source data IDE for developers. It allows you to easily build graphs and tables with data pulled from SQL databases, logging databases, metrics databases, HTTP servers, and all kinds of text and binary files. Need to join or munge data? Write embedded scripts as needed in languages like Python, JavaScript, R or SQL. All in one application. This tutorial will show you step-by-step how to connect your Astra DB with DataStation. \u2139\ufe0f Introduction to DataStation \ud83d\udce5 DataStation Quick Install","title":"Overview"},{"location":"pages/tools/ide/datastation/#prerequisites","text":"You should Install DataStation You should have an Astra account You should Create an Astra Database You should Have an Astra Token Clone this repository to use to set up CQL-Proxy which is a sidecar that enables unsupported CQL drivers to work with DataStax Astra You need your Astra Token and Astra Database ID to use CQL-Proxy Follow the steps in the repo to spin up CQL-Proxy using Terminal/Command Line. Once successfully running, you should see the following output: {\"level\":\"info\",\"ts\":1651012815.176512,\"caller\":\"proxy/proxy.go:222\",\"msg\":\"proxy is listening\",\"address\":\"[::]:9042\"}","title":"Prerequisites"},{"location":"pages/tools/ide/datastation/#c-installation-and-setup","text":"Once you have completed all of the Prerequisites and confirmed CQL Proxy is running, you are now able to move on to setting up your Astra DB with DataStation IDE. First, launch your DataStation IDE. Click Add Data Source and select Cassandra A dialog will appear that will prompt you to name this connection (in this example I used Test ) and your Cassandra credentials: Host - Use localhost:9042 or 127.0.0.1:9042 Keyspace - Enter the name of the keyspace that you want to use from your Astra DB. Username - Use token Password - From your Astra Token creation in the Prerequisites, find your Token. Ex. AstraCS:BWsdjhdf... Once you've entered your credentials, click Add Panel and select Database under IMPORT FROM . This will allow DataStation to connect with the database you are trying to view through the credentials you had just entered in the previous step. Note DataStation IDE currently doesn't show a success message after the credentials have been entered. The Password field will also show up blank once you've minimized the credentials panel on the sidebar, but this does not necessarily mean you have to re-enter your password. You will know your database has been added once your display looks like this. You can name this Panel what you'd like. In this example, it is titled Test Panel . To test and validate, you can run a quick query to one of the tables that are present in the Keyspace that you provided in Step 3. ...and you're done! This tutorial quickly shows you how you can easily integrate your Astra DB with the DataStation IDE to run queries, build tables, and further enhance how you interact with your data. \ud83c\udfe0 Back to home","title":"C - Installation and Setup"},{"location":"pages/tools/ide/eclipse/","text":"Check back soon Nothing to see here yet! Check back for updates!","title":"\u2022 Eclipse"},{"location":"pages/tools/ide/gitpod/","text":"Check back soon Nothing to see here yet! Check back for updates!","title":"\u2022 Gitpod"},{"location":"pages/tools/ide/intellij/","text":"This content has been built using Reference Documentation A - Overview \u00b6 Astra DB is a serverless NoSQL database as a service, built on Apache Cassandra \u2122. Navigate, insert and edit data in your Astra DB without coding, directly in your favorite JetBrains IDE using this plugin from DataStax. This plugin also works with open source Apache Cassandra 4.0 once a Stargate Data API gateway has been configured. B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should download either Community or ultimate edition of intelliJ from the Download Page C - Installation Guide \u00b6 \u2705 1. Download Plugin \u00b6 Astra DB Explorer Installation Page Open the plugin panel and search for astra File > Preferences > Plugins Click the [INSTALL] button Once the plugin is downloaded and installed you will be asked to restart \u2705 2. Setup Plugin \u00b6 During the first restart you will got an IDE error occured message it is expected we will now configure the plugin The plugin configuration is defined in a file on disk located at ${user.home}/.astra/config . Fortunately you can do it directly in the IDE In the bottom left hand corner locate the panel astra.explorer and open it \u2705 3. Edit Profiles \u00b6 In the drop down menu select Edit Profiles the configuration file is referred as a profile You will be asked if you want to create the file, click [CREATE] Also pick the first option in the radio button Edit this file anyway The file open and the content should look like. Not that the value used for the bearerToken is the one starting by AstraCS:.... . Save the file [astraProfileFile.profiles] default = \"AstraCS:XXXX\" \u2705 4. Reload Profiles \u00b6 Now on the drop down menu select Reload Profiles Et voila you can now list databases on your Astra organization and for each you can see the different keyspaces \ud83c\udfe0 Back to home","title":"\u2022 IntelliJ"},{"location":"pages/tools/ide/intellij/#a-overview","text":"Astra DB is a serverless NoSQL database as a service, built on Apache Cassandra \u2122. Navigate, insert and edit data in your Astra DB without coding, directly in your favorite JetBrains IDE using this plugin from DataStax. This plugin also works with open source Apache Cassandra 4.0 once a Stargate Data API gateway has been configured.","title":"A - Overview"},{"location":"pages/tools/ide/intellij/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should download either Community or ultimate edition of intelliJ from the Download Page","title":"B - Prerequisites"},{"location":"pages/tools/ide/intellij/#c-installation-guide","text":"","title":"C - Installation Guide"},{"location":"pages/tools/ide/intellij/#1-download-plugin","text":"Astra DB Explorer Installation Page Open the plugin panel and search for astra File > Preferences > Plugins Click the [INSTALL] button Once the plugin is downloaded and installed you will be asked to restart","title":"\u2705 1. Download Plugin"},{"location":"pages/tools/ide/intellij/#2-setup-plugin","text":"During the first restart you will got an IDE error occured message it is expected we will now configure the plugin The plugin configuration is defined in a file on disk located at ${user.home}/.astra/config . Fortunately you can do it directly in the IDE In the bottom left hand corner locate the panel astra.explorer and open it","title":"\u2705 2. Setup Plugin"},{"location":"pages/tools/ide/intellij/#3-edit-profiles","text":"In the drop down menu select Edit Profiles the configuration file is referred as a profile You will be asked if you want to create the file, click [CREATE] Also pick the first option in the radio button Edit this file anyway The file open and the content should look like. Not that the value used for the bearerToken is the one starting by AstraCS:.... . Save the file [astraProfileFile.profiles] default = \"AstraCS:XXXX\"","title":"\u2705 3. Edit Profiles"},{"location":"pages/tools/ide/intellij/#4-reload-profiles","text":"Now on the drop down menu select Reload Profiles Et voila you can now list databases on your Astra organization and for each you can see the different keyspaces \ud83c\udfe0 Back to home","title":"\u2705 4. Reload Profiles"},{"location":"pages/tools/ide/vscode/","text":"Check back soon Nothing to see here yet! Check back for updates!","title":"\u2022 VSCode"},{"location":"pages/tools/integration/apache-airflow/","text":"Overview \u00b6 Apache Airflow is an open source workflow management system. It provides components which allow engineers to build data pipelines between different systems. These instructions will step through tasks/adjustments to be done in each product (Astra DB, cql-proxy, Apache Airflow), ultimately resulting in Airflow being able to work with AstraDB in its directed acyclic graphs (DAG). \u2139\ufe0f Apache Airflow Documentation Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle You should install `python3` and `pip3` (local deployment of Airflow) or Docker (docker) This article was written for Apache Airflow version 2.2.3 on MacOS with Python 3.9 . Installation \u00b6 \u2705 Download and install \u00b6 Following the Apache Airflow reference documentation download and install the software. \u2705 Create the keyspace airflow \u00b6 From the Astra DB dashboard , click on your database name. Scroll down to where the keyspaces are listed, and click the Add Keyspace button to create a new keyspace. Name this keyspace airflow . \u2705 Start Cql Proxy \u00b6 DataStax\u2019s cql-proxy is designed to function as an intermediate connection point to allow legacy Apache Cassandra applications to connect to DataStax Astra DB using its new Secure Connect Bundle. There are a few ways to install and run DataStax\u2019s cql-proxy, as outlined in cql-proxy . Be sure to start cql-proxy with the following settings: Using the Secure Connect Bundle downloaded in the previous section Binding it to the listen IP of the server instance Specifying the username of \u201ctoken\u201d Specifying the Astra Token created for the user in Astra DB as the password You can run cql-proxy (in the foreground) from the command line in this way, like this: ./cql-proxy --bundle ~/local/astraCreds/secure-connect.zip \\ --bind 127 .0.0.1 \\ --username token \\ --password AstraCS:rtFckUZblahblahblahblahblahblaha3953d799a525 Important to note that the command shown above binds cql-proxy to localhost (127.0.0.1), meaning it is not reachable (by Airflow) from outside the server instance. \u2705 Create a new connection in Apache Airflow \u00b6 Inside Apache Airflow, click Connections from underneath the Admin drop-down menu. Then click on the blue button labeled with the plus sign ( + ) to add a new connection. Fill out the form as shown in Figure 2: Connection Id: A unique identifier for the connection in Apache Airflow, which will be referenced inside the DAG code. We will use \u201ccassandra_cqlproxy.\u201d Connection Type: Select \u201cCassandra\u201d from the drop-down. If it is not present, you will have to install Airflow\u2019s Cassandra provider. Host: The listen address that cql-proxy is bound to. In this case, that is \u201c127.0.0.1.\u201d Schema: The Cassandra keyspace which we created in Astra DB. We\u2019ll set that to \u201cairflow\u201d in this case. Login: Your Astra DB client id. Password: Your Astra DB client secret. Port: The port that cql-proxy is listening on for the CQL native binary protocol, most likely 9042. Figure 2 - Create a new Cassandra connection for Apache Airflow. Click the blue Save button to persist the new connection. \u2705 Create a new DAG in Apache Airflow \u00b6 A directed acyclic graph (DAG) is essentially a Python script which imports one or more libraries specific to Airflow. To create a new DAG, first locate your DAG directory. By default, Airflow looks for custom DAGs in the ~/airflow/dags/ directory. For testing, there is a sample DAG out in the following GitHub repository: https://github.com/aar0np/DS_Python_stuff/blob/main/cassandra_test_dag.py This DAG uses the following line to reference the Cassandra connection we created in the above step: hook = CassandraHook('cassandra_cqlproxy') The other important aspect is that this DAG sets its unique identifier as cass_hooks_tutorial : with DAG( 'cass_hooks_tutorial', It also specifically creates two unique tasks: check_table_exists query_system_local \u2705 Final Test \u00b6 To test the connection, copy the DAG mentioned above into the /dags/ directory. Then we will invoke Airflow\u2019s task testing functionality, by running airflow tasks test and specifying: The DAG\u2019s unique identifier The name of the task to be run The execution date If today\u2019s date is 2022-02-08, the command looks like this: airflow tasks test cass_hooks_tutorial check_table_exists 2022-02-08 Many messages will go by quickly. If it worked, the final messages should look something like this: INFO - Done. Returned value was: True INFO - Marking task as SUCCESS. dag_id=cass_hooks_tutorial, task_id=check_table_exists, execution_date=20220208T000000, start_date=20220208T195333, end_date=20220208T195334 Acknowledgements \u00b6 Special thanks goes out to Obioma Anomnachi of Anant. Obi\u2019s video and GitHub repo proved quite helpful in building out this tutorial. \ud83c\udfe0 Back to home","title":"\u2022 Apache Airflow"},{"location":"pages/tools/integration/apache-airflow/#overview","text":"Apache Airflow is an open source workflow management system. It provides components which allow engineers to build data pipelines between different systems. These instructions will step through tasks/adjustments to be done in each product (Astra DB, cql-proxy, Apache Airflow), ultimately resulting in Airflow being able to work with AstraDB in its directed acyclic graphs (DAG). \u2139\ufe0f Apache Airflow Documentation","title":"Overview"},{"location":"pages/tools/integration/apache-airflow/#prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle You should install `python3` and `pip3` (local deployment of Airflow) or Docker (docker) This article was written for Apache Airflow version 2.2.3 on MacOS with Python 3.9 .","title":"Prerequisites"},{"location":"pages/tools/integration/apache-airflow/#installation","text":"","title":"Installation"},{"location":"pages/tools/integration/apache-airflow/#download-and-install","text":"Following the Apache Airflow reference documentation download and install the software.","title":"\u2705 Download and install"},{"location":"pages/tools/integration/apache-airflow/#create-the-keyspace-airflow","text":"From the Astra DB dashboard , click on your database name. Scroll down to where the keyspaces are listed, and click the Add Keyspace button to create a new keyspace. Name this keyspace airflow .","title":"\u2705 Create the keyspace airflow"},{"location":"pages/tools/integration/apache-airflow/#start-cql-proxy","text":"DataStax\u2019s cql-proxy is designed to function as an intermediate connection point to allow legacy Apache Cassandra applications to connect to DataStax Astra DB using its new Secure Connect Bundle. There are a few ways to install and run DataStax\u2019s cql-proxy, as outlined in cql-proxy . Be sure to start cql-proxy with the following settings: Using the Secure Connect Bundle downloaded in the previous section Binding it to the listen IP of the server instance Specifying the username of \u201ctoken\u201d Specifying the Astra Token created for the user in Astra DB as the password You can run cql-proxy (in the foreground) from the command line in this way, like this: ./cql-proxy --bundle ~/local/astraCreds/secure-connect.zip \\ --bind 127 .0.0.1 \\ --username token \\ --password AstraCS:rtFckUZblahblahblahblahblahblaha3953d799a525 Important to note that the command shown above binds cql-proxy to localhost (127.0.0.1), meaning it is not reachable (by Airflow) from outside the server instance.","title":"\u2705 Start Cql Proxy"},{"location":"pages/tools/integration/apache-airflow/#create-a-new-connection-in-apache-airflow","text":"Inside Apache Airflow, click Connections from underneath the Admin drop-down menu. Then click on the blue button labeled with the plus sign ( + ) to add a new connection. Fill out the form as shown in Figure 2: Connection Id: A unique identifier for the connection in Apache Airflow, which will be referenced inside the DAG code. We will use \u201ccassandra_cqlproxy.\u201d Connection Type: Select \u201cCassandra\u201d from the drop-down. If it is not present, you will have to install Airflow\u2019s Cassandra provider. Host: The listen address that cql-proxy is bound to. In this case, that is \u201c127.0.0.1.\u201d Schema: The Cassandra keyspace which we created in Astra DB. We\u2019ll set that to \u201cairflow\u201d in this case. Login: Your Astra DB client id. Password: Your Astra DB client secret. Port: The port that cql-proxy is listening on for the CQL native binary protocol, most likely 9042. Figure 2 - Create a new Cassandra connection for Apache Airflow. Click the blue Save button to persist the new connection.","title":"\u2705 Create a new connection in Apache Airflow"},{"location":"pages/tools/integration/apache-airflow/#create-a-new-dag-in-apache-airflow","text":"A directed acyclic graph (DAG) is essentially a Python script which imports one or more libraries specific to Airflow. To create a new DAG, first locate your DAG directory. By default, Airflow looks for custom DAGs in the ~/airflow/dags/ directory. For testing, there is a sample DAG out in the following GitHub repository: https://github.com/aar0np/DS_Python_stuff/blob/main/cassandra_test_dag.py This DAG uses the following line to reference the Cassandra connection we created in the above step: hook = CassandraHook('cassandra_cqlproxy') The other important aspect is that this DAG sets its unique identifier as cass_hooks_tutorial : with DAG( 'cass_hooks_tutorial', It also specifically creates two unique tasks: check_table_exists query_system_local","title":"\u2705 Create a new DAG in Apache Airflow"},{"location":"pages/tools/integration/apache-airflow/#final-test","text":"To test the connection, copy the DAG mentioned above into the /dags/ directory. Then we will invoke Airflow\u2019s task testing functionality, by running airflow tasks test and specifying: The DAG\u2019s unique identifier The name of the task to be run The execution date If today\u2019s date is 2022-02-08, the command looks like this: airflow tasks test cass_hooks_tutorial check_table_exists 2022-02-08 Many messages will go by quickly. If it worked, the final messages should look something like this: INFO - Done. Returned value was: True INFO - Marking task as SUCCESS. dag_id=cass_hooks_tutorial, task_id=check_table_exists, execution_date=20220208T000000, start_date=20220208T195333, end_date=20220208T195334","title":"\u2705 Final Test"},{"location":"pages/tools/integration/apache-airflow/#acknowledgements","text":"Special thanks goes out to Obioma Anomnachi of Anant. Obi\u2019s video and GitHub repo proved quite helpful in building out this tutorial. \ud83c\udfe0 Back to home","title":"Acknowledgements"},{"location":"pages/tools/integration/apache-nifi/","text":"This is an adaptation of the Steven Matison Blogpost \ud83d\udccb On this page A - Overview B - Prerequisites C - Log Ingestion to Astra with Stargate Document Api A - Overview \u00b6 \ud83d\udcd8 What is NiFi? \u00b6 Apache NiFi is a software project from the Apache Software Foundation designed to automate the flow of data between software systems. It is super powerful tool I have been using for a few years to develop data flows and data pipelines. With NiFi I can do just about anything without writing a single line of code. You can use NiFi\u2019s invokeHttp processor for any Astra API calls. You can also use native NiFi Cassandra Processors : QueryCassandraRecord PutCassandraRecord and PutCassandraQL against Astra. \ud83d\udcd8 My Astra NiFi Templates \u00b6 You can find my official NiFi Astra Cassandra templates here You can also find templates from my previous life here B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should install a `Java JDK 1.8+` and Apache Maven Download and install Apache Nifi You should add `invokeHttp` and `Cassandra` [processors](https://nifi.apache.org/docs/nifi-docs/html/getting-started.html#adding-a-processor) C - Log Ingestion to Astra with NiFi \u00b6 In this blog I am going to show you how to ingest raw log data into cassandra using NiFi and Astra. With NiFi ingesting data from any source is super easy. With Astra and Cassandra ingesting raw data can be a challenge due to data model constraints (primary keys and clustering columns). In this demo I am going to remove that constraint and ingest all raw data using Astra & Stargate Document API which is accepting of schemaless JSON data. Although not a focus off this blog, it is fully possible to build a cassandra data model and do this log ingestion using NiFi Cassandra Processors or Astra REST API against standard cassandra database tables. In this demo we are going to communicate with Astra via Stargate\u2019s Documement APIs. \u2705 Step 1 : Get NiFi Authorized for Astra Calls \u00b6 GetAuthToken \u00b6 Upload and add Get Astra Get Auth Token Template to your canvas. Record the Process Group Id for later. Collect Astra details needed: astra databaseid, region, api url, username, password. Update Process Group variables with API url, process Group Id, Username and Password. Configure and Enable SSL Context Services. For simple demo purposes we use java cacerts and in my environment I have copied ca certs to local path /nifi/ssl/cacerts. You will need to locate your path to cacerts and adjust. The cacerts password is \u201cchangeit\u201d. You can also use Astra Secure Bundle and keystore/trustore found within that bundled zip file. Confirm NiFi host:port in the Blue InvokeHTTP Processors. Play the data flow and confirm variable astraToken is filled with authorization token. \u2139\ufe0f Things to Note: Top of flow (GenerateFlowFile) will kick off the auth process every 30 minutes. For sake of this demo, all variables are included in GetAuthToken Process Group. In production or in your data flow you will want those variables in the parent location. Adjust your own flow accordingly. For demo purposes failure routes are visible. In production, these may be auto terminated or routed to exception handling. \u2705 Step 2 : Create Data Flow for Log Ingestion \u00b6 In this first example, we are going to ingest apache log data from a custom log file. Reference the template Astra Apache Logs to Cassandra with Stargate for this data flow . This log data happens to be on the same NiFi host in the normal /var/log/httpd/ location. The custom apache log file is in the format of: <IfModule log_config_module > LogFormat \"%>s %U %h %{%Y-%m-%d %H:%M:%S}t\" urlsdetails CustomLog \"/var/log/httpd/access-file-details.log\" urlsdetails </IfModule> The CSVReader Schema used in QueryRecord is as follows: { \"name\" : \"apache_logs\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"http_status\" , \"type\" : \"string\" }, { \"name\" : \"access_url\" , \"type\" : \"string\" }, { \"name\" : \"ip\" , \"type\" : \"string\" }, { \"name\" : \"apachetime\" , \"type\" : \"string\" } ] } And the output of the JSON Writer is as follows: { \"http_status\" : \"200\" , \"access_url\" : \"/INTROV8.mp3\" , \"ip\" : \"115.164.45.55\" , \"apachetime\" : \"2021-01-26 14:16:58\" } \u26a0\ufe0f Notice this JSON structure is exactly what we need to insert into Astra. We do not have to create the collection or schema ahead of time. This collection creation will automatically happen with the delivery of the first document. \ud83d\udca1 \u2139\ufe0f Things to Note: For portability of the log data flow template, the SSL Context Service is duplicated. You can adjust your flow to use a single context service at the root canvas level. Some of the NiFi Variables from above template are referenced in this template. Adjust your flow accordingly with root level variables or import this template into same Process Group above. \u2705 Step 3 : Verify Log Data With Cql Console \u00b6 Login to the Astra and navigate to your Cql Consoe and execute the following query: select count ( * ) FROM apache_log ; \u26a0\ufe0f For this demo it is not important to look at the data, only important to verify results are in Astra. In future updates I will go into Postman and show how to access the data in a meaningful manner. For now, let us just bask in the glory of being able to ingest log data to cassandra without data modeling. What\u2019s Next \u00b6 We can now use Stargate Document API to query this data source and even search into the JSON Object. We can have conversations about the raw data, build cassandra data models, and investigate how this log data can be used downtream from cassandra. Stay tuned as I add other Log Ingestion Use Cases, a UUID Generator, and more Astra NiFi content here. \ud83c\udfe0 Back to home","title":"\u2022 Apache Nifi"},{"location":"pages/tools/integration/apache-nifi/#a-overview","text":"","title":"A - Overview"},{"location":"pages/tools/integration/apache-nifi/#what-is-nifi","text":"Apache NiFi is a software project from the Apache Software Foundation designed to automate the flow of data between software systems. It is super powerful tool I have been using for a few years to develop data flows and data pipelines. With NiFi I can do just about anything without writing a single line of code. You can use NiFi\u2019s invokeHttp processor for any Astra API calls. You can also use native NiFi Cassandra Processors : QueryCassandraRecord PutCassandraRecord and PutCassandraQL against Astra.","title":"\ud83d\udcd8 What is NiFi?"},{"location":"pages/tools/integration/apache-nifi/#my-astra-nifi-templates","text":"You can find my official NiFi Astra Cassandra templates here You can also find templates from my previous life here","title":"\ud83d\udcd8 My Astra NiFi Templates"},{"location":"pages/tools/integration/apache-nifi/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should install a `Java JDK 1.8+` and Apache Maven Download and install Apache Nifi You should add `invokeHttp` and `Cassandra` [processors](https://nifi.apache.org/docs/nifi-docs/html/getting-started.html#adding-a-processor)","title":"B - Prerequisites"},{"location":"pages/tools/integration/apache-nifi/#c-log-ingestion-to-astra-with-nifi","text":"In this blog I am going to show you how to ingest raw log data into cassandra using NiFi and Astra. With NiFi ingesting data from any source is super easy. With Astra and Cassandra ingesting raw data can be a challenge due to data model constraints (primary keys and clustering columns). In this demo I am going to remove that constraint and ingest all raw data using Astra & Stargate Document API which is accepting of schemaless JSON data. Although not a focus off this blog, it is fully possible to build a cassandra data model and do this log ingestion using NiFi Cassandra Processors or Astra REST API against standard cassandra database tables. In this demo we are going to communicate with Astra via Stargate\u2019s Documement APIs.","title":"C - Log Ingestion to Astra with NiFi"},{"location":"pages/tools/integration/apache-nifi/#step-1-get-nifi-authorized-for-astra-calls","text":"","title":"\u2705 Step 1 : Get NiFi Authorized for Astra Calls"},{"location":"pages/tools/integration/apache-nifi/#getauthtoken","text":"Upload and add Get Astra Get Auth Token Template to your canvas. Record the Process Group Id for later. Collect Astra details needed: astra databaseid, region, api url, username, password. Update Process Group variables with API url, process Group Id, Username and Password. Configure and Enable SSL Context Services. For simple demo purposes we use java cacerts and in my environment I have copied ca certs to local path /nifi/ssl/cacerts. You will need to locate your path to cacerts and adjust. The cacerts password is \u201cchangeit\u201d. You can also use Astra Secure Bundle and keystore/trustore found within that bundled zip file. Confirm NiFi host:port in the Blue InvokeHTTP Processors. Play the data flow and confirm variable astraToken is filled with authorization token. \u2139\ufe0f Things to Note: Top of flow (GenerateFlowFile) will kick off the auth process every 30 minutes. For sake of this demo, all variables are included in GetAuthToken Process Group. In production or in your data flow you will want those variables in the parent location. Adjust your own flow accordingly. For demo purposes failure routes are visible. In production, these may be auto terminated or routed to exception handling.","title":"GetAuthToken"},{"location":"pages/tools/integration/apache-nifi/#step-2-create-data-flow-for-log-ingestion","text":"In this first example, we are going to ingest apache log data from a custom log file. Reference the template Astra Apache Logs to Cassandra with Stargate for this data flow . This log data happens to be on the same NiFi host in the normal /var/log/httpd/ location. The custom apache log file is in the format of: <IfModule log_config_module > LogFormat \"%>s %U %h %{%Y-%m-%d %H:%M:%S}t\" urlsdetails CustomLog \"/var/log/httpd/access-file-details.log\" urlsdetails </IfModule> The CSVReader Schema used in QueryRecord is as follows: { \"name\" : \"apache_logs\" , \"type\" : \"record\" , \"fields\" : [ { \"name\" : \"http_status\" , \"type\" : \"string\" }, { \"name\" : \"access_url\" , \"type\" : \"string\" }, { \"name\" : \"ip\" , \"type\" : \"string\" }, { \"name\" : \"apachetime\" , \"type\" : \"string\" } ] } And the output of the JSON Writer is as follows: { \"http_status\" : \"200\" , \"access_url\" : \"/INTROV8.mp3\" , \"ip\" : \"115.164.45.55\" , \"apachetime\" : \"2021-01-26 14:16:58\" } \u26a0\ufe0f Notice this JSON structure is exactly what we need to insert into Astra. We do not have to create the collection or schema ahead of time. This collection creation will automatically happen with the delivery of the first document. \ud83d\udca1 \u2139\ufe0f Things to Note: For portability of the log data flow template, the SSL Context Service is duplicated. You can adjust your flow to use a single context service at the root canvas level. Some of the NiFi Variables from above template are referenced in this template. Adjust your flow accordingly with root level variables or import this template into same Process Group above.","title":"\u2705 Step 2 : Create Data Flow for Log Ingestion"},{"location":"pages/tools/integration/apache-nifi/#step-3-verify-log-data-with-cql-console","text":"Login to the Astra and navigate to your Cql Consoe and execute the following query: select count ( * ) FROM apache_log ; \u26a0\ufe0f For this demo it is not important to look at the data, only important to verify results are in Astra. In future updates I will go into Postman and show how to access the data in a meaningful manner. For now, let us just bask in the glory of being able to ingest log data to cassandra without data modeling.","title":"\u2705 Step 3 : Verify Log Data With Cql Console"},{"location":"pages/tools/integration/apache-nifi/#whats-next","text":"We can now use Stargate Document API to query this data source and even search into the JSON Object. We can have conversations about the raw data, build cassandra data models, and investigate how this log data can be used downtream from cassandra. Stay tuned as I add other Log Ingestion Use Cases, a UUID Generator, and more Astra NiFi content here. \ud83c\udfe0 Back to home","title":"What\u2019s Next"},{"location":"pages/tools/integration/apache-spark/","text":"This article includes information that was originally written by Arpan Patel on Anant Github and Astra DataStax A - Overview \u00b6 Apache Spark is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size. Use Apache Spark to connect to your database and begin accessing your Astra DB tables using Scala in spark-shell. \u2139\ufe0f Introduction to Apache Spark \ud83d\udce5 Apache Spark Download Link B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle and unpack it. Download and install the latest version of Spark Cassandra Connector that matches with your Apache Spark and Scala version from the maven central repository. To find the right version of SCC, please check SCC compatibility here. C - Installation and Setup \u00b6 These steps assume you will be using Apache Spark in local mode. For help using Spark cluster mode click the chat button on the bottom of the screen. \u2705 Steps: \u00b6 Expand the downloaded Apache Spark package into a directory, and assign the directory name to $SPARK_HOME . Navigate to this directory using cd $SPARK_HOME Append the following lines at the end of a file called $SPARK_HOME/conf/spark-defaults.conf (you may be able to find a template under $SPARK_HOME/conf directory), and replace the second column (value) with the first four lines: spark.files $SECURE_CONNECT_BUNDLE_FILE_PATH/secure-connect-astraiscool.zip spark.cassandra.connection.config.cloud.path secure-connect-astraiscool.zip spark.cassandra.auth.username <<CLIENT ID>> spark.cassandra.auth.password <<CLIENT SECRET>> spark.dse.continuousPagingEnabled false Launch spark-shell and enter the following scala commands: import com . datastax . spark . connector . _ import org . apache . spark . sql . cassandra . _ spark . read . cassandraFormat ( \"tables\" , \"system_schema\" ). load (). count () You should expect to see the following output: $ bin/spark-shell Using Spark 's default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Spark context Web UI available at http://localhost:4040 Spark context available as ' sc ' (master = local[*], app id = local-1608781805157). Spark session available as ' spark '. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ ' _/ /___/ .__/ \\_ ,_/_/ /_/ \\_\\ version 3 .0.1 /_/ Using Scala version 2 .12.10 ( OpenJDK 64 -Bit Server VM, Java 11 .0.9.1 ) Type in expressions to have them evaluated. Type :help for more information. scala> import com.datastax.spark.connector._ import com.datastax.spark.connector._ scala> import org.apache.spark.sql.cassandra._ import org.apache.spark.sql.cassandra._ scala> spark.read.cassandraFormat ( \"tables\" , \"system_schema\" ) .load () .count () res0: Long = 25 scala> :quit \ud83c\udfe0 Back to home","title":"\u2022 Apache Spark"},{"location":"pages/tools/integration/apache-spark/#a-overview","text":"Apache Spark is an open-source, distributed processing system used for big data workloads. It utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size. Use Apache Spark to connect to your database and begin accessing your Astra DB tables using Scala in spark-shell. \u2139\ufe0f Introduction to Apache Spark \ud83d\udce5 Apache Spark Download Link","title":"A - Overview"},{"location":"pages/tools/integration/apache-spark/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle and unpack it. Download and install the latest version of Spark Cassandra Connector that matches with your Apache Spark and Scala version from the maven central repository. To find the right version of SCC, please check SCC compatibility here.","title":"B - Prerequisites"},{"location":"pages/tools/integration/apache-spark/#c-installation-and-setup","text":"These steps assume you will be using Apache Spark in local mode. For help using Spark cluster mode click the chat button on the bottom of the screen.","title":"C - Installation and Setup"},{"location":"pages/tools/integration/apache-spark/#steps","text":"Expand the downloaded Apache Spark package into a directory, and assign the directory name to $SPARK_HOME . Navigate to this directory using cd $SPARK_HOME Append the following lines at the end of a file called $SPARK_HOME/conf/spark-defaults.conf (you may be able to find a template under $SPARK_HOME/conf directory), and replace the second column (value) with the first four lines: spark.files $SECURE_CONNECT_BUNDLE_FILE_PATH/secure-connect-astraiscool.zip spark.cassandra.connection.config.cloud.path secure-connect-astraiscool.zip spark.cassandra.auth.username <<CLIENT ID>> spark.cassandra.auth.password <<CLIENT SECRET>> spark.dse.continuousPagingEnabled false Launch spark-shell and enter the following scala commands: import com . datastax . spark . connector . _ import org . apache . spark . sql . cassandra . _ spark . read . cassandraFormat ( \"tables\" , \"system_schema\" ). load (). count () You should expect to see the following output: $ bin/spark-shell Using Spark 's default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Spark context Web UI available at http://localhost:4040 Spark context available as ' sc ' (master = local[*], app id = local-1608781805157). Spark session available as ' spark '. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ ' _/ /___/ .__/ \\_ ,_/_/ /_/ \\_\\ version 3 .0.1 /_/ Using Scala version 2 .12.10 ( OpenJDK 64 -Bit Server VM, Java 11 .0.9.1 ) Type in expressions to have them evaluated. Type :help for more information. scala> import com.datastax.spark.connector._ import com.datastax.spark.connector._ scala> import org.apache.spark.sql.cassandra._ import org.apache.spark.sql.cassandra._ scala> spark.read.cassandraFormat ( \"tables\" , \"system_schema\" ) .load () .count () res0: Long = 25 scala> :quit \ud83c\udfe0 Back to home","title":"\u2705 Steps:"},{"location":"pages/tools/integration/authorizer/","text":"A - Overview \u00b6 Authorizer is an open source auth solution for application. It works with many different databases, allowing the developers to use a single datastore for the entire application stack and have complete control over all user data. \u2139\ufe0f Authorizer Documentation B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle C - Installation \u00b6 \u2705 Step 0 Download and install Following the Authorizer documentation download and untar the software where you would like to install it. \u2705 Step 1 Create the keyspace authorizer From the Astra DB dashboard , click on your database name. Scroll down to where the keyspaces are listed, and click the Add Keyspace button to create a new keyspace. Name this keyspace authorizer . \u2705 Step 2 Create configuration file Use the delivered .env.sample file to create a new .env file for your configuration. Edit this file with Atom, Vi, or whichever editor you choose. cd authorizer cp .env.sample .env atom .env \u2705 Step 3 Create base64 encoded strings from your cert, ca.crt, and key files To successfully connect with Astra DB, you will need to open the secure bundle and convert the following files into base64 encoded strings: cert ca.crt key You can accomplish this with the base64 command: base64 cert cert_base64_file base64 ca.crt ca_base64_file base64 key key_base64_file Note that you can omit the file parameter and output the base64 encoded string to STDOUT for easy copy/paste accessibility. \u2705 Step 4 Connect to Astra DB To connect to Astra DB, you will need to specify the following variables in the .env file: DATABASE_HOST=\"ASTRA_DB_ID-ASTRA_DB_REGION.db.astra.datastax.com\" DATABASE_TYPE=\"cassandradb\" DATABASE_PORT=29042 DATABASE_USERNAME=\"token\" DATABASE_PASSWORD=\"AstraCS:yourAstraT0ken\" DATABASE_CERT=\"LS0tLS1CRUdJTiBDblahblahblahnotrealRVJUSUZJQ0FURS0tLS0\" DATABASE_CERT_KEY=\"RXNRNVcKYXkwblahblahblahnotrealkt4b1FnL2s4K29IaD\" DATABASE_CA_CERT=\"WVhneERqQU1CZblahblahblahnotrealWQkFzVEJVTnNiM1Z\" \u2705 Step 5 Start Authorizer From the authorizer directory, run the server binary from the build directory. It will run in the foreground. build/server Verify that it is running by bringing up the Authorizer dashboard in a browser: http://127.0.0.1:8080/dashboard/ D - Acknowledgements \u00b6 Special thanks goes out to Lakhan Samani of Authorizer. YouTube channel GitHub repo \ud83c\udfe0 Back to home","title":"\u2022 Authorizer"},{"location":"pages/tools/integration/authorizer/#a-overview","text":"Authorizer is an open source auth solution for application. It works with many different databases, allowing the developers to use a single datastore for the entire application stack and have complete control over all user data. \u2139\ufe0f Authorizer Documentation","title":"A - Overview"},{"location":"pages/tools/integration/authorizer/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle","title":"B - Prerequisites"},{"location":"pages/tools/integration/authorizer/#c-installation","text":"\u2705 Step 0 Download and install Following the Authorizer documentation download and untar the software where you would like to install it. \u2705 Step 1 Create the keyspace authorizer From the Astra DB dashboard , click on your database name. Scroll down to where the keyspaces are listed, and click the Add Keyspace button to create a new keyspace. Name this keyspace authorizer . \u2705 Step 2 Create configuration file Use the delivered .env.sample file to create a new .env file for your configuration. Edit this file with Atom, Vi, or whichever editor you choose. cd authorizer cp .env.sample .env atom .env \u2705 Step 3 Create base64 encoded strings from your cert, ca.crt, and key files To successfully connect with Astra DB, you will need to open the secure bundle and convert the following files into base64 encoded strings: cert ca.crt key You can accomplish this with the base64 command: base64 cert cert_base64_file base64 ca.crt ca_base64_file base64 key key_base64_file Note that you can omit the file parameter and output the base64 encoded string to STDOUT for easy copy/paste accessibility. \u2705 Step 4 Connect to Astra DB To connect to Astra DB, you will need to specify the following variables in the .env file: DATABASE_HOST=\"ASTRA_DB_ID-ASTRA_DB_REGION.db.astra.datastax.com\" DATABASE_TYPE=\"cassandradb\" DATABASE_PORT=29042 DATABASE_USERNAME=\"token\" DATABASE_PASSWORD=\"AstraCS:yourAstraT0ken\" DATABASE_CERT=\"LS0tLS1CRUdJTiBDblahblahblahnotrealRVJUSUZJQ0FURS0tLS0\" DATABASE_CERT_KEY=\"RXNRNVcKYXkwblahblahblahnotrealkt4b1FnL2s4K29IaD\" DATABASE_CA_CERT=\"WVhneERqQU1CZblahblahblahnotrealWQkFzVEJVTnNiM1Z\" \u2705 Step 5 Start Authorizer From the authorizer directory, run the server binary from the build directory. It will run in the foreground. build/server Verify that it is running by bringing up the Authorizer dashboard in a browser: http://127.0.0.1:8080/dashboard/","title":"C - Installation"},{"location":"pages/tools/integration/authorizer/#d-acknowledgements","text":"Special thanks goes out to Lakhan Samani of Authorizer. YouTube channel GitHub repo \ud83c\udfe0 Back to home","title":"D - Acknowledgements"},{"location":"pages/tools/integration/cadence/","text":"Overview \u00b6 Cadence is a multi-tenant orchestration framework that helps with managing workflows. It scales horizontally to handle millions of concurrent executions from various customers. Cadence Open Sources uses docker compose to run their server, and uses Apache Cassandra\u24c7 as its default backend dependency. Using docker compose, users are able to also use Cadence with MySQL, PostgreSQL, Statsd+Graphite, and Elasticsearch. \u2139\ufe0f Introduction to Cadence \ud83d\udce5 Cadence Quick Install Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token Note This runbook was written using Mac OS Monterey but it will also work with Windows. Any Windows-specific instructions will be noted as such. Installation and Setup \u00b6 \u2705 1. Setup Astra \u00b6 In your Astra database, create two new keyspaces called \"cadence\" and \"cadence_visibility\". You will be using both of these in the next steps. Make sure to create an Astra token with Admin Role Get your Database ID Find your Database ID in one of two ways Navigate to your your database and get the last ID in the URL: https://astra.datastax.com/org/.../database/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx Copy and paste the Datacenter ID without the trailing -1 from the Regions section of your Astra Dashboard. \u2705 2. Cadence Pre-setup \u00b6 Clone this GitHub repository Navigate to your cloned repository and using your preferred text editor (e.g. VisualStudio or Sublime), update the .env file with your Astra Token and Astra Database ID that you obtained above. ASTRA_TOKEN = <your Astra token> ASTRA_DATABASE_ID = <your DB ID> \u2705 3. Cadence Schema Migration to Astra DB \u00b6 For this step, you will set up the keyspaces you created earlier in the Astra prerequisites ( cadence and cadence_visibility ). You will be using cadence-cassandra-tool which is part of the Temporal repo and it relies on schema definition. Navigate to your cloned cadence-astra-cql-proxy directory Run the following commands to initialize the keyspaces that we created through Astra. Note that there are two sets of commands, one for cadence keyspace and one for cadence_visibility keyspace: docker-compose -f docker-compose-schema.yaml run cadence \\ -ep cqlproxy-cadence -k cadence setup-schema -v 0 .0 docker-compose -f docker-compose-schema.yaml run cadence \\ -ep cql-proxy -k cadence update-schema -d schema/cassandra/cadence/versioned/ docker-compose -f docker-compose-schema.yaml run cadence \\ -ep cql-proxy -k cadence_visibility setup-schema -v 0 .0 docker-compose -f docker-compose-schema.yaml run cadence \\ -ep cql-proxy -k cadence_visibility update-schema -d schema/cassandra/visibility/versioned/ Once the process is completed, you should see a message similar to this: 2022 /04/05 21 :50:24 Starting schema setup, config = & { SchemaFilePath: InitialVersion:0.0 Overwrite:false DisableVersioning:false } 2022 /04/05 21 :50:24 Setting up version tables 2022 /04/05 21 :50:25 Setting initial schema version to 0 .0 2022 /04/05 21 :50:25 Updating schema update log 2022 /04/05 21 :50:26 Schema setup complete ... 2022 /04/05 22 :13:16 ---- Done ---- 2022 /04/05 22 :13:16 Schema updated from 0 .32 to 0 .33, elapsed 1 .4960138s 2022 /04/05 22 :13:16 All schema changes completed in 32 .5941245s 2022 /04/05 22 :13:16 UpdateSchemeTask done Great! Your schemas have been migrated with Astra DB. Confirm your tables exist in Astra You can double-check to make sure the correct tables have been created by querying your database in Astra DB\u2019s CQL Console. Run DESC tables; in both your cadence and cadence_visibility keyspaces. You should see there are tables loaded in that were created by the schema migration with cadence-cassandra-tool . token@cqlsh> use cadence ; token@cqlsh:cadence> desc tables ; history_node schema_version tasks history_tree domains_by_name_v2 executions domains events cluster_config queue queue_metadata schema_update_history token@cqlsh:cadence> use cadence_visibility ; token@cqlsh:cadence_visibility> desc tables ; open_executions closed_executions_v2 closed_executions schema_update_history schema_version \u2705 4. Run Docker Compose \u00b6 In this step, the docker-compose.yaml file is already provided for you in the cadence-astra-cql-proxy repo. This file creates different docker containers to run Temporal server. The persistence layer is configured for you to connect with cql-proxy, and it should pull your Astra credentials from when you set it earlier. services: cql-proxy: container_name: cqlproxy image: datastax/cql-proxy:v ${ CQL_PROXY_VERSION } ... environment: - ASTRA_TOKEN = ${ ASTRA_TOKEN } - ASTRA_DATABASE_ID = ${ ASTRA_DATABASE_ID } - HEALTH_CHECK = true Now you can run the docker-compose command to start up Cadence: docker-compose up \u2705 5. Test and Validate \u00b6 You can test your connection and play with your Cadence cluster with these instructions. Using Cadence\u2019s Command Line tool, you will be able to interact with your local Temporal server. Create a domain samples-domain by running the following command. You should see the success message once the domain is created: % cadence --do samples-domain d re Domain samples-domain successfully registered. Clone the sample project repository to your machine. Navigate to this project and run make to build all the projects. Once this is complete, you can start by running the sample Hello World project by following the instructions in that repository. Once you have this all running, you should be able to see your workflows reflect on both the Cadence UI and Astra UI. You can see the domain on the top left is samples-domain, the domain we created, as well as the Status of each workflow as \u201cCompleted\u201d. \ud83c\udfe0 Back to HOME","title":"\u2022 Cadence"},{"location":"pages/tools/integration/cadence/#overview","text":"Cadence is a multi-tenant orchestration framework that helps with managing workflows. It scales horizontally to handle millions of concurrent executions from various customers. Cadence Open Sources uses docker compose to run their server, and uses Apache Cassandra\u24c7 as its default backend dependency. Using docker compose, users are able to also use Cadence with MySQL, PostgreSQL, Statsd+Graphite, and Elasticsearch. \u2139\ufe0f Introduction to Cadence \ud83d\udce5 Cadence Quick Install","title":"Overview"},{"location":"pages/tools/integration/cadence/#prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token Note This runbook was written using Mac OS Monterey but it will also work with Windows. Any Windows-specific instructions will be noted as such.","title":"Prerequisites"},{"location":"pages/tools/integration/cadence/#installation-and-setup","text":"","title":"Installation and Setup"},{"location":"pages/tools/integration/cadence/#1-setup-astra","text":"In your Astra database, create two new keyspaces called \"cadence\" and \"cadence_visibility\". You will be using both of these in the next steps. Make sure to create an Astra token with Admin Role Get your Database ID Find your Database ID in one of two ways Navigate to your your database and get the last ID in the URL: https://astra.datastax.com/org/.../database/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx Copy and paste the Datacenter ID without the trailing -1 from the Regions section of your Astra Dashboard.","title":"\u2705  1. Setup Astra"},{"location":"pages/tools/integration/cadence/#2-cadence-pre-setup","text":"Clone this GitHub repository Navigate to your cloned repository and using your preferred text editor (e.g. VisualStudio or Sublime), update the .env file with your Astra Token and Astra Database ID that you obtained above. ASTRA_TOKEN = <your Astra token> ASTRA_DATABASE_ID = <your DB ID>","title":"\u2705  2. Cadence Pre-setup"},{"location":"pages/tools/integration/cadence/#3-cadence-schema-migration-to-astra-db","text":"For this step, you will set up the keyspaces you created earlier in the Astra prerequisites ( cadence and cadence_visibility ). You will be using cadence-cassandra-tool which is part of the Temporal repo and it relies on schema definition. Navigate to your cloned cadence-astra-cql-proxy directory Run the following commands to initialize the keyspaces that we created through Astra. Note that there are two sets of commands, one for cadence keyspace and one for cadence_visibility keyspace: docker-compose -f docker-compose-schema.yaml run cadence \\ -ep cqlproxy-cadence -k cadence setup-schema -v 0 .0 docker-compose -f docker-compose-schema.yaml run cadence \\ -ep cql-proxy -k cadence update-schema -d schema/cassandra/cadence/versioned/ docker-compose -f docker-compose-schema.yaml run cadence \\ -ep cql-proxy -k cadence_visibility setup-schema -v 0 .0 docker-compose -f docker-compose-schema.yaml run cadence \\ -ep cql-proxy -k cadence_visibility update-schema -d schema/cassandra/visibility/versioned/ Once the process is completed, you should see a message similar to this: 2022 /04/05 21 :50:24 Starting schema setup, config = & { SchemaFilePath: InitialVersion:0.0 Overwrite:false DisableVersioning:false } 2022 /04/05 21 :50:24 Setting up version tables 2022 /04/05 21 :50:25 Setting initial schema version to 0 .0 2022 /04/05 21 :50:25 Updating schema update log 2022 /04/05 21 :50:26 Schema setup complete ... 2022 /04/05 22 :13:16 ---- Done ---- 2022 /04/05 22 :13:16 Schema updated from 0 .32 to 0 .33, elapsed 1 .4960138s 2022 /04/05 22 :13:16 All schema changes completed in 32 .5941245s 2022 /04/05 22 :13:16 UpdateSchemeTask done Great! Your schemas have been migrated with Astra DB. Confirm your tables exist in Astra You can double-check to make sure the correct tables have been created by querying your database in Astra DB\u2019s CQL Console. Run DESC tables; in both your cadence and cadence_visibility keyspaces. You should see there are tables loaded in that were created by the schema migration with cadence-cassandra-tool . token@cqlsh> use cadence ; token@cqlsh:cadence> desc tables ; history_node schema_version tasks history_tree domains_by_name_v2 executions domains events cluster_config queue queue_metadata schema_update_history token@cqlsh:cadence> use cadence_visibility ; token@cqlsh:cadence_visibility> desc tables ; open_executions closed_executions_v2 closed_executions schema_update_history schema_version","title":"\u2705  3. Cadence Schema Migration to Astra DB"},{"location":"pages/tools/integration/cadence/#4-run-docker-compose","text":"In this step, the docker-compose.yaml file is already provided for you in the cadence-astra-cql-proxy repo. This file creates different docker containers to run Temporal server. The persistence layer is configured for you to connect with cql-proxy, and it should pull your Astra credentials from when you set it earlier. services: cql-proxy: container_name: cqlproxy image: datastax/cql-proxy:v ${ CQL_PROXY_VERSION } ... environment: - ASTRA_TOKEN = ${ ASTRA_TOKEN } - ASTRA_DATABASE_ID = ${ ASTRA_DATABASE_ID } - HEALTH_CHECK = true Now you can run the docker-compose command to start up Cadence: docker-compose up","title":"\u2705  4. Run Docker Compose"},{"location":"pages/tools/integration/cadence/#5-test-and-validate","text":"You can test your connection and play with your Cadence cluster with these instructions. Using Cadence\u2019s Command Line tool, you will be able to interact with your local Temporal server. Create a domain samples-domain by running the following command. You should see the success message once the domain is created: % cadence --do samples-domain d re Domain samples-domain successfully registered. Clone the sample project repository to your machine. Navigate to this project and run make to build all the projects. Once this is complete, you can start by running the sample Hello World project by following the instructions in that repository. Once you have this all running, you should be able to see your workflows reflect on both the Cadence UI and Astra UI. You can see the domain on the top left is samples-domain, the domain we created, as well as the Status of each workflow as \u201cCompleted\u201d. \ud83c\udfe0 Back to HOME","title":"\u2705  5. Test and Validate"},{"location":"pages/tools/integration/celery/","text":"A - Overview \u00b6 Celery is a (BSD-licensed) open source, simple and flexible distributed task queue for asynchronous processing of messages. With Celery one can define units of work called \"tasks\" and dispatch them for execution, in a distributed way if desired. Celery is a Python package and as such is easily integrated in any Python project. Typical use cases might be: a queue of uploaded images to resize in the background, long-running tasks initiated by a Web application's API, a batch of email scheduled to be sent, ... Celery is composed of two parts: on one side, one or more clients define the tasks to be run and enqueue/schedule them for execution; on the other side, one or more workers pick up these tasks, execute them and optionally store the resulting values. Communication between these two parts happens through a message bus (such as RabbitMQ) acting as broker, while the return value of a task is made available back to the caller through a backend (de/serialization is transparently handled by the Celery infrastructure). Celery supports several backends for storing and exposing task results. Among the supported backends are Cassandra and (starting with v5.2 ) Astra DB. In the following we assume familiarity with the celeryconfig configuration object for Celery and with the usage of Cassandra as backend. See the Celery documentation for more details: \u2139\ufe0f Celery documentation \u2139\ufe0f The celeryconfig object \u2139\ufe0f Cassandra/AstraDB backend configuration guide (which covers the instructions on this page as well) \ud83d\udce5 Celery installation instructions B - Prerequisites \u00b6 Create an Astra Database . In the following example, a keyspace called celeryks is created in the database. Create an Astra Token with the role \"Database Administrator\" (it is desirable to leave table creation to Celery). You should have received your token while creating the database in the previous step. Download your secure connect bundle ZIP . Install Celery with the Cassandra backend in your local Python environment, e.g. pip install celery[cassandra] . See the backend-settings page for additional info. Keep the token information and the bundle file location ready: these will be soon provided in the Celery configuration. C - Installation and Setup \u00b6 Here a minimal Celery setup that makes use of the Astra DB backend is described start-to-end. A task will be defined and executed through Celery: afterwards, its return value will be retrieved on the client side. For this example to work, a message bus is needed - here, in line with a quickstart on Celery's documentation, a dockerized RabbitMQ is used. Steps: \u00b6 1. Start a message broker \u00b6 Make sure you have a RabbitMQ instance running in Docker with docker run -d -p 5672:5672 rabbitmq (it might take a while for the image to be downloaded and complete startup). 2. Define a task \u00b6 Create a tasks.py module with the definition of a task, to be later executed through Celery: from celery import Celery app = Celery ( 'tasks' ) app . config_from_object ( 'celeryconfig' ) @app . task def sortWords ( text , capitalize ): # Rearrange the text so that words are in alphabetical order. words = text . split ( ' ' ) sortedWords = sorted ( words , key = str . upper ) return ' ' . join ([ w if not capitalize else w . upper () for w in sortedWords ]) 3. Configure Celery \u00b6 Create a module celeryconfig.py in the same directory, providing (among other things) the broker and backend configuration: broker_url = 'pyamqp://guest@localhost//' broker_connection_retry_on_startup = True task_serializer = 'json' result_serializer = 'json' accept_content = [ 'json' ] enable_utc = True result_backend = 'cassandra://' cassandra_keyspace = 'celeryks' # REPLACE_ME cassandra_table = 'celery_tasks' # REPLACE_ME cassandra_read_consistency = 'quorum' cassandra_write_consistency = 'quorum' cassandra_auth_provider = 'PlainTextAuthProvider' cassandra_auth_kwargs = { 'username' : 'client-id-from-astra-token' , # REPLACE_ME 'password' : 'client-secret-from-astra-token' , # REPLACE_ME } cassandra_secure_bundle_path = '/path/to/secure-connect-database.zip' # REPLACE_ME In the above, take care of inserting your values for: the keyspace name you created earlier in Astra DB; the table name you want Celery to store results in (no need to create it beforehand); the Client ID and Client Secret generated in your Astra DB token earlier (resp. as username and password in cassandra_auth_kwargs ); the path to the Secure Connect Bundle you downloaded earlier. 4. Start the worker \u00b6 Start a Celery worker with: celery -A tasks worker --loglevel = INFO 5. Run and check a task \u00b6 In a different shell, open a Python REPL and type the following commands to run a couple of tasks and retrieve their result: from tasks import sortWords sorted1 = sortWords . delay ( 'storage yay my DB is powerful results Astra' , False ) sorted1 . ready () # Returns: True # (as soon as the function completes, which here is almost immediately) sorted1 . get () # Returns: 'Astra DB is my powerful results storage yay' sorted2 = sortWords . delay ( 'In the land of another wizards day' , capitalize = True ) sorted2 . get () # Returns: 'ANOTHER DAY IN LAND OF THE WIZARDS' 6. (optional) Look at the database \u00b6 Check the corresponding data stored on Astra DB. Navigate to the CQL Console for the database you created and enter the following commands: USE celeryks; // <== enter your keyspace name here DESCRIBE TABLES; // the output, e.g. \"celery_tasks\", lists the tables SELECT * FROM celery_tasks; // <== enter your table name here D - Additional configuration \u00b6 Celery uses the DataStax Python driver for Cassandra; hence, the choice of connection parameters is that for the generic driver-based usage of Cassandra in Python. In particular, one may want to specify additional parameters through the celeryconfig such as protocol level, load-balancing policy and so on. Refer to the \"Additional configuration\" section in the Celery documentation for a more comprehensive setup.","title":"\u2022 Celery"},{"location":"pages/tools/integration/celery/#a-overview","text":"Celery is a (BSD-licensed) open source, simple and flexible distributed task queue for asynchronous processing of messages. With Celery one can define units of work called \"tasks\" and dispatch them for execution, in a distributed way if desired. Celery is a Python package and as such is easily integrated in any Python project. Typical use cases might be: a queue of uploaded images to resize in the background, long-running tasks initiated by a Web application's API, a batch of email scheduled to be sent, ... Celery is composed of two parts: on one side, one or more clients define the tasks to be run and enqueue/schedule them for execution; on the other side, one or more workers pick up these tasks, execute them and optionally store the resulting values. Communication between these two parts happens through a message bus (such as RabbitMQ) acting as broker, while the return value of a task is made available back to the caller through a backend (de/serialization is transparently handled by the Celery infrastructure). Celery supports several backends for storing and exposing task results. Among the supported backends are Cassandra and (starting with v5.2 ) Astra DB. In the following we assume familiarity with the celeryconfig configuration object for Celery and with the usage of Cassandra as backend. See the Celery documentation for more details: \u2139\ufe0f Celery documentation \u2139\ufe0f The celeryconfig object \u2139\ufe0f Cassandra/AstraDB backend configuration guide (which covers the instructions on this page as well) \ud83d\udce5 Celery installation instructions","title":"A - Overview"},{"location":"pages/tools/integration/celery/#b-prerequisites","text":"Create an Astra Database . In the following example, a keyspace called celeryks is created in the database. Create an Astra Token with the role \"Database Administrator\" (it is desirable to leave table creation to Celery). You should have received your token while creating the database in the previous step. Download your secure connect bundle ZIP . Install Celery with the Cassandra backend in your local Python environment, e.g. pip install celery[cassandra] . See the backend-settings page for additional info. Keep the token information and the bundle file location ready: these will be soon provided in the Celery configuration.","title":"B - Prerequisites"},{"location":"pages/tools/integration/celery/#c-installation-and-setup","text":"Here a minimal Celery setup that makes use of the Astra DB backend is described start-to-end. A task will be defined and executed through Celery: afterwards, its return value will be retrieved on the client side. For this example to work, a message bus is needed - here, in line with a quickstart on Celery's documentation, a dockerized RabbitMQ is used.","title":"C - Installation and Setup"},{"location":"pages/tools/integration/celery/#steps","text":"","title":"Steps:"},{"location":"pages/tools/integration/celery/#1-start-a-message-broker","text":"Make sure you have a RabbitMQ instance running in Docker with docker run -d -p 5672:5672 rabbitmq (it might take a while for the image to be downloaded and complete startup).","title":"1. Start a message broker"},{"location":"pages/tools/integration/celery/#2-define-a-task","text":"Create a tasks.py module with the definition of a task, to be later executed through Celery: from celery import Celery app = Celery ( 'tasks' ) app . config_from_object ( 'celeryconfig' ) @app . task def sortWords ( text , capitalize ): # Rearrange the text so that words are in alphabetical order. words = text . split ( ' ' ) sortedWords = sorted ( words , key = str . upper ) return ' ' . join ([ w if not capitalize else w . upper () for w in sortedWords ])","title":"2. Define a task"},{"location":"pages/tools/integration/celery/#3-configure-celery","text":"Create a module celeryconfig.py in the same directory, providing (among other things) the broker and backend configuration: broker_url = 'pyamqp://guest@localhost//' broker_connection_retry_on_startup = True task_serializer = 'json' result_serializer = 'json' accept_content = [ 'json' ] enable_utc = True result_backend = 'cassandra://' cassandra_keyspace = 'celeryks' # REPLACE_ME cassandra_table = 'celery_tasks' # REPLACE_ME cassandra_read_consistency = 'quorum' cassandra_write_consistency = 'quorum' cassandra_auth_provider = 'PlainTextAuthProvider' cassandra_auth_kwargs = { 'username' : 'client-id-from-astra-token' , # REPLACE_ME 'password' : 'client-secret-from-astra-token' , # REPLACE_ME } cassandra_secure_bundle_path = '/path/to/secure-connect-database.zip' # REPLACE_ME In the above, take care of inserting your values for: the keyspace name you created earlier in Astra DB; the table name you want Celery to store results in (no need to create it beforehand); the Client ID and Client Secret generated in your Astra DB token earlier (resp. as username and password in cassandra_auth_kwargs ); the path to the Secure Connect Bundle you downloaded earlier.","title":"3. Configure Celery"},{"location":"pages/tools/integration/celery/#4-start-the-worker","text":"Start a Celery worker with: celery -A tasks worker --loglevel = INFO","title":"4. Start the worker"},{"location":"pages/tools/integration/celery/#5-run-and-check-a-task","text":"In a different shell, open a Python REPL and type the following commands to run a couple of tasks and retrieve their result: from tasks import sortWords sorted1 = sortWords . delay ( 'storage yay my DB is powerful results Astra' , False ) sorted1 . ready () # Returns: True # (as soon as the function completes, which here is almost immediately) sorted1 . get () # Returns: 'Astra DB is my powerful results storage yay' sorted2 = sortWords . delay ( 'In the land of another wizards day' , capitalize = True ) sorted2 . get () # Returns: 'ANOTHER DAY IN LAND OF THE WIZARDS'","title":"5. Run and check a task"},{"location":"pages/tools/integration/celery/#6-optional-look-at-the-database","text":"Check the corresponding data stored on Astra DB. Navigate to the CQL Console for the database you created and enter the following commands: USE celeryks; // <== enter your keyspace name here DESCRIBE TABLES; // the output, e.g. \"celery_tasks\", lists the tables SELECT * FROM celery_tasks; // <== enter your table name here","title":"6. (optional) Look at the database"},{"location":"pages/tools/integration/celery/#d-additional-configuration","text":"Celery uses the DataStax Python driver for Cassandra; hence, the choice of connection parameters is that for the generic driver-based usage of Cassandra in Python. In particular, one may want to specify additional parameters through the celeryconfig such as protocol level, load-balancing policy and so on. Refer to the \"Additional configuration\" section in the Celery documentation for a more comprehensive setup.","title":"D - Additional configuration"},{"location":"pages/tools/integration/feast/","text":"A - Overview \u00b6 Feast is a (Apache-licensed) open-source feature store for machine learning. Feast aims at providing a fast solution to the typical MLOps needs one encounters when bringing ML applications to production. Feast offers a solution to the problem of training/serving skew, provides tools to standardize the data engineering workflows (thus avoiding having to \"re-invent the features\" every time), and ensures reproducible feature sets with point-in-time historical retrievals. This feature store supports several backends, both as offline store (for historical time-series data) and online store (with the latest features, synced from the former by Feast itself). Some of the backends are native in Feast, but several more are available as external plugins. Feast is built with the cloud in mind: one of its goals is to free MLOps practitioners and data engineers from having to manage their own infrastructure. In this spirit, the Feast online store plugin for Cassandra flexibly supports both Cassandra and Astra DB, as will be explained below. Reference documentation: \u2139\ufe0f Feast documentation \u2139\ufe0f Minimal quickstart with Feast \u2139\ufe0f The feast-cassandra plugin B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database In the following example, a keyspace called `feastks` is created in the database. You should Create an Astra Token with the role \"Database Administrator\" (Feast will have to dynamically create and delete tables in the keyspace). You should Download your Secure Connect Bundle . Install Feast and the Cassandra/Astra DB plugin in your local Python environment, i.e. pip install feast feast-cassandra . See the specific pages ( Feast , Cassandra plugin ) for additional installation info. Keep the token information and the bundle file location ready: these will be soon provided in the Feast configuration. C - Quickstart \u00b6 Note: this quickstart is modeled after the one found in the Feast documentation . The numbering of the steps is chosen to be consistent with it. All credits for the sample code given here goes to the Feast documentation. A new feature store is created and configured to use Astra DB as online store; next, it will be materialized to database using sample feature definitions and sample data; finally, historical/online feature retrieval is demonstrated. \u2705 Steps: \u00b6 1. Install Feast and the plugin \u00b6 See last item in the \"Prerequisites\" above. 2. Create a feature repository \u00b6 In a directory of your choice, create a new repository and cd to the corresponding directory: feast init astra_feature_repo cd astra_feature_repo As you can see, the new feature store already contains sample data and a sample feature definition. These will be used in this walkthrough, so don't delete them. 2B. Configure Astra DB as online store \u00b6 Locate and open the store configuration file, feature_store.yaml . Replace the online_store portion of the file with something like ( use your values for the bundle path and the token authentication info ): online_store: type: feast_cassandra_online_store.cassandra_online_store.CassandraOnlineStore secure_bundle_path: /path/to/secure/bundle.zip username: Client_ID password: Client_Secret keyspace: feastks Settings in \"feature_store.yaml\" for usage with Cassandra If using regular Cassandra as opposed to Astra DB, the \"online_store\" portion might look like: online_store: type: feast_cassandra_online_store.cassandra_online_store.CassandraOnlineStore hosts: - 192.168.1.1 - 192.168.1.2 - 192.168.1.3 keyspace: feastks port: 9042 # optional username: user # optional password: secret # optional 3. Register feature definitions and deploy the store \u00b6 With the apply command, features defined in Python modules (in this case, example.py ) are scanned and used for actual deployment of the infrastructure. Run the command feast apply This is the step that actually accesses the database. After running it, you may want to check directly the presence of a new table in the Astra DB keyspace. 4. Generate training data \u00b6 This illustrates the get_historical_features store method, which directly scans the offline source data and performs a point-in-time join to construct the features requested up to a certain provided timestamp. Create a file generate.py and run it with python generate.py : Show \"generate.py\" from datetime import datetime , timedelta import pandas as pd from feast import FeatureStore # The entity dataframe is the dataframe we want to enrich with feature values entity_df = pd . DataFrame . from_dict ( { # entity's join key -> entity values \"driver_id\" : [ 1001 , 1002 , 1003 ], # label name -> label values \"label_driver_reported_satisfaction\" : [ 1 , 5 , 3 ], # \"event_timestamp\" (reserved key) -> timestamps \"event_timestamp\" : [ datetime . now () - timedelta ( minutes = 11 ), datetime . now () - timedelta ( minutes = 36 ), datetime . now () - timedelta ( minutes = 73 ), ], } ) store = FeatureStore ( repo_path = \".\" ) training_df = store . get_historical_features ( entity_df = entity_df , features = [ \"driver_hourly_stats:conv_rate\" , \"driver_hourly_stats:acc_rate\" , \"driver_hourly_stats:avg_daily_trips\" , ], ) . to_df () print ( \"----- Feature schema ----- \\n \" ) print ( training_df . info ()) print () print ( \"----- Example features ----- \\n \" ) print ( training_df . head ()) 5. Load features in the online store \u00b6 With the materialize-incremental command, Feast is instructed to carry the latest feature values over to the online store, for quick access during feature serving: CURRENT_TIME=$(date -u +\"%Y-%m-%dT%H:%M:%S\") feast materialize-incremental $CURRENT_TIME At this point, inspection of the Astra DB table will show the presence of newly-inserted rows. 6. Fetch feature vectors from the online store \u00b6 The get_online_features store method will query the online store and return the required features, as resulting from the last \"materialize\" operation. Create a fetch_online.py script and run it with python fetch_online.py : Show \"fetch_online.py\" from pprint import pprint from feast import FeatureStore store = FeatureStore ( repo_path = \".\" ) feature_vector = store . get_online_features ( features = [ \"driver_hourly_stats:conv_rate\" , \"driver_hourly_stats:acc_rate\" , \"driver_hourly_stats:avg_daily_trips\" , ], entity_rows = [ # {join_key: entity_value} { \"driver_id\" : 1004 }, { \"driver_id\" : 1005 }, ], ) . to_dict () pprint ( feature_vector )","title":"\u2022 Feast"},{"location":"pages/tools/integration/feast/#a-overview","text":"Feast is a (Apache-licensed) open-source feature store for machine learning. Feast aims at providing a fast solution to the typical MLOps needs one encounters when bringing ML applications to production. Feast offers a solution to the problem of training/serving skew, provides tools to standardize the data engineering workflows (thus avoiding having to \"re-invent the features\" every time), and ensures reproducible feature sets with point-in-time historical retrievals. This feature store supports several backends, both as offline store (for historical time-series data) and online store (with the latest features, synced from the former by Feast itself). Some of the backends are native in Feast, but several more are available as external plugins. Feast is built with the cloud in mind: one of its goals is to free MLOps practitioners and data engineers from having to manage their own infrastructure. In this spirit, the Feast online store plugin for Cassandra flexibly supports both Cassandra and Astra DB, as will be explained below. Reference documentation: \u2139\ufe0f Feast documentation \u2139\ufe0f Minimal quickstart with Feast \u2139\ufe0f The feast-cassandra plugin","title":"A - Overview"},{"location":"pages/tools/integration/feast/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database In the following example, a keyspace called `feastks` is created in the database. You should Create an Astra Token with the role \"Database Administrator\" (Feast will have to dynamically create and delete tables in the keyspace). You should Download your Secure Connect Bundle . Install Feast and the Cassandra/Astra DB plugin in your local Python environment, i.e. pip install feast feast-cassandra . See the specific pages ( Feast , Cassandra plugin ) for additional installation info. Keep the token information and the bundle file location ready: these will be soon provided in the Feast configuration.","title":"B - Prerequisites"},{"location":"pages/tools/integration/feast/#c-quickstart","text":"Note: this quickstart is modeled after the one found in the Feast documentation . The numbering of the steps is chosen to be consistent with it. All credits for the sample code given here goes to the Feast documentation. A new feature store is created and configured to use Astra DB as online store; next, it will be materialized to database using sample feature definitions and sample data; finally, historical/online feature retrieval is demonstrated.","title":"C - Quickstart"},{"location":"pages/tools/integration/feast/#steps","text":"","title":"\u2705 Steps:"},{"location":"pages/tools/integration/feast/#1-install-feast-and-the-plugin","text":"See last item in the \"Prerequisites\" above.","title":"1. Install Feast and the plugin"},{"location":"pages/tools/integration/feast/#2-create-a-feature-repository","text":"In a directory of your choice, create a new repository and cd to the corresponding directory: feast init astra_feature_repo cd astra_feature_repo As you can see, the new feature store already contains sample data and a sample feature definition. These will be used in this walkthrough, so don't delete them.","title":"2. Create a feature repository"},{"location":"pages/tools/integration/feast/#2b-configure-astra-db-as-online-store","text":"Locate and open the store configuration file, feature_store.yaml . Replace the online_store portion of the file with something like ( use your values for the bundle path and the token authentication info ): online_store: type: feast_cassandra_online_store.cassandra_online_store.CassandraOnlineStore secure_bundle_path: /path/to/secure/bundle.zip username: Client_ID password: Client_Secret keyspace: feastks Settings in \"feature_store.yaml\" for usage with Cassandra If using regular Cassandra as opposed to Astra DB, the \"online_store\" portion might look like: online_store: type: feast_cassandra_online_store.cassandra_online_store.CassandraOnlineStore hosts: - 192.168.1.1 - 192.168.1.2 - 192.168.1.3 keyspace: feastks port: 9042 # optional username: user # optional password: secret # optional","title":"2B. Configure Astra DB as online store"},{"location":"pages/tools/integration/feast/#3-register-feature-definitions-and-deploy-the-store","text":"With the apply command, features defined in Python modules (in this case, example.py ) are scanned and used for actual deployment of the infrastructure. Run the command feast apply This is the step that actually accesses the database. After running it, you may want to check directly the presence of a new table in the Astra DB keyspace.","title":"3. Register feature definitions and deploy the store"},{"location":"pages/tools/integration/feast/#4-generate-training-data","text":"This illustrates the get_historical_features store method, which directly scans the offline source data and performs a point-in-time join to construct the features requested up to a certain provided timestamp. Create a file generate.py and run it with python generate.py : Show \"generate.py\" from datetime import datetime , timedelta import pandas as pd from feast import FeatureStore # The entity dataframe is the dataframe we want to enrich with feature values entity_df = pd . DataFrame . from_dict ( { # entity's join key -> entity values \"driver_id\" : [ 1001 , 1002 , 1003 ], # label name -> label values \"label_driver_reported_satisfaction\" : [ 1 , 5 , 3 ], # \"event_timestamp\" (reserved key) -> timestamps \"event_timestamp\" : [ datetime . now () - timedelta ( minutes = 11 ), datetime . now () - timedelta ( minutes = 36 ), datetime . now () - timedelta ( minutes = 73 ), ], } ) store = FeatureStore ( repo_path = \".\" ) training_df = store . get_historical_features ( entity_df = entity_df , features = [ \"driver_hourly_stats:conv_rate\" , \"driver_hourly_stats:acc_rate\" , \"driver_hourly_stats:avg_daily_trips\" , ], ) . to_df () print ( \"----- Feature schema ----- \\n \" ) print ( training_df . info ()) print () print ( \"----- Example features ----- \\n \" ) print ( training_df . head ())","title":"4. Generate training data"},{"location":"pages/tools/integration/feast/#5-load-features-in-the-online-store","text":"With the materialize-incremental command, Feast is instructed to carry the latest feature values over to the online store, for quick access during feature serving: CURRENT_TIME=$(date -u +\"%Y-%m-%dT%H:%M:%S\") feast materialize-incremental $CURRENT_TIME At this point, inspection of the Astra DB table will show the presence of newly-inserted rows.","title":"5. Load features in the online store"},{"location":"pages/tools/integration/feast/#6-fetch-feature-vectors-from-the-online-store","text":"The get_online_features store method will query the online store and return the required features, as resulting from the last \"materialize\" operation. Create a fetch_online.py script and run it with python fetch_online.py : Show \"fetch_online.py\" from pprint import pprint from feast import FeatureStore store = FeatureStore ( repo_path = \".\" ) feature_vector = store . get_online_features ( features = [ \"driver_hourly_stats:conv_rate\" , \"driver_hourly_stats:acc_rate\" , \"driver_hourly_stats:avg_daily_trips\" , ], entity_rows = [ # {join_key: entity_value} { \"driver_id\" : 1004 }, { \"driver_id\" : 1005 }, ], ) . to_dict () pprint ( feature_vector )","title":"6. Fetch feature vectors from the online store"},{"location":"pages/tools/integration/flink/","text":"This article includes information that was originally written by Bret McGuire on GitHub Overview \u00b6 Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. This tutorial will show you step-by-step how to use Astra as a sink for results computed by Flink. These instructions are intended to demonstrate how to enable such support when using a Flink DataStream. This code is intended as a fairly simple demonstration of how to enable an Apache Flink job to interact with DataStax Astra. There is certainly room for optimization here. A simple example: Flink's CassandraSink will open a new Session on each open() call even though these Session objects are thread-safe. A more robust implementation would be more aggressive about memoizing Sessions, encouraging a minimal number of open sessions for multiple operations on the same JVM. This work may be undertaken in the future, but for the moment it is beyond the scope of what we're aiming for here. \u2139\ufe0f Introduction to Apache Flink \ud83d\udce5 Download Apache Flink Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should clone this GitHub Repository You should have Apache Flink , Gradle , and Java installed in your system. Note For this tutorial, you will need either Java 8 or Java 11 to run it. Any other version might run into an exception and cause build failure. Installation and Setup \u00b6 Now that you have gathered all of your prerequisites, you are ready to configure and setup for this example. Create a keyspace named example in your Astra database. At the moment, this name will be hard-coded. Download the secure connect bundle (SCB) for your database. You can find this under the \"Connect\" tab in the UI. Once you have downloaded your secure connect bundle, place it in app/src/main/resources in your GitHub directory (You do not have to unzip the file). Create a properties file titled app.properties , and place it in app/src/main/resources/ . Add properties specifying your Astra client ID, Astra secret, and SCB file name . These should map to the \"astra.clientid\", \"astra.secret\", and \"astra.scb\" properties respectively. Your app.properties file should look something like this: astra.clientid = Bwy... astra.secret = E4dfE... astra.scb = secure-connect-test.zip Test and Validate \u00b6 Once you have completed all of the prerequisites along with the section above, you can move on to this section to run the sample app and validate the connection between Flink and Astra. In your flink-astra cloned GitHub directory, run ./gradlew run Verify that the application runs and exits normally. If this completed successfully you should see the following message: BUILD SUCCESSFUL in 31s 3 actionable tasks: 2 executed, 1 up-to-date Navigate back to the Astra UI to use the CQL Console. You can run this sample query to confirm that the defined data from the sample app has been loaded properly: token@cqlsh:example> select * from wordcount ; word | count --------+------- dogs | 1 lazier | 1 least | 1 foxes | 1 jumped | 1 at | 1 are | 1 just | 1 quick | 1 than | 1 fox | 1 our | 1 dog | 2 or | 1 over | 1 brown | 1 lazy | 1 the | 2 (18 rows) token@cqlsh:example> \ud83c\udfe0 Back to home","title":"\u2022 Apache Flink"},{"location":"pages/tools/integration/flink/#overview","text":"Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. This tutorial will show you step-by-step how to use Astra as a sink for results computed by Flink. These instructions are intended to demonstrate how to enable such support when using a Flink DataStream. This code is intended as a fairly simple demonstration of how to enable an Apache Flink job to interact with DataStax Astra. There is certainly room for optimization here. A simple example: Flink's CassandraSink will open a new Session on each open() call even though these Session objects are thread-safe. A more robust implementation would be more aggressive about memoizing Sessions, encouraging a minimal number of open sessions for multiple operations on the same JVM. This work may be undertaken in the future, but for the moment it is beyond the scope of what we're aiming for here. \u2139\ufe0f Introduction to Apache Flink \ud83d\udce5 Download Apache Flink","title":"Overview"},{"location":"pages/tools/integration/flink/#prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should clone this GitHub Repository You should have Apache Flink , Gradle , and Java installed in your system. Note For this tutorial, you will need either Java 8 or Java 11 to run it. Any other version might run into an exception and cause build failure.","title":"Prerequisites"},{"location":"pages/tools/integration/flink/#installation-and-setup","text":"Now that you have gathered all of your prerequisites, you are ready to configure and setup for this example. Create a keyspace named example in your Astra database. At the moment, this name will be hard-coded. Download the secure connect bundle (SCB) for your database. You can find this under the \"Connect\" tab in the UI. Once you have downloaded your secure connect bundle, place it in app/src/main/resources in your GitHub directory (You do not have to unzip the file). Create a properties file titled app.properties , and place it in app/src/main/resources/ . Add properties specifying your Astra client ID, Astra secret, and SCB file name . These should map to the \"astra.clientid\", \"astra.secret\", and \"astra.scb\" properties respectively. Your app.properties file should look something like this: astra.clientid = Bwy... astra.secret = E4dfE... astra.scb = secure-connect-test.zip","title":"Installation and Setup"},{"location":"pages/tools/integration/flink/#test-and-validate","text":"Once you have completed all of the prerequisites along with the section above, you can move on to this section to run the sample app and validate the connection between Flink and Astra. In your flink-astra cloned GitHub directory, run ./gradlew run Verify that the application runs and exits normally. If this completed successfully you should see the following message: BUILD SUCCESSFUL in 31s 3 actionable tasks: 2 executed, 1 up-to-date Navigate back to the Astra UI to use the CQL Console. You can run this sample query to confirm that the defined data from the sample app has been loaded properly: token@cqlsh:example> select * from wordcount ; word | count --------+------- dogs | 1 lazier | 1 least | 1 foxes | 1 jumped | 1 at | 1 are | 1 just | 1 quick | 1 than | 1 fox | 1 our | 1 dog | 2 or | 1 over | 1 brown | 1 lazy | 1 the | 2 (18 rows) token@cqlsh:example> \ud83c\udfe0 Back to home","title":"Test and Validate"},{"location":"pages/tools/integration/grafana/","text":"A - Overview \u00b6 Grafana is a multi-platform open source analytics and interactive visualization web application. It provides charts, graphs, and alerts for the web when connected to supported data sources. A licensed Grafana Enterprise version with additional capabilities is also available as a self-hosted installation or an account on the Grafana Labs cloud service. It is expandable through a plug-in system. End users can create complex dashboards using interactive query builders. Community-developed Cassandra Datasource for Grafana supports both Apache Cassandra as well as DataStax AstraDB, allowing to use Cassandra as a data backend for Grafana. Data can be pulled using simple Query Configurator or more advanced but powerful Query Editor. (On the picture: Query Editor at work) B - Prerequisites \u00b6 To use Grafana, you will need a running Grafana instance deployed locally or in a cloud. Locally launched Grafana in Docker works well too. You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle and unpack it. Keep the token information and the bundle file location ready: these will be soon provided in the datasource configuration. C - Quickstart \u00b6 Install the plugin using CLI or using web-interface \u00b6 Install the plugin using grafana console tool: grafana-cli plugins install hadesarchitect-cassandra-datasource It will be installed into your grafana plugins directory; the default is /var/lib/grafana/plugins. Alternatively, enable it using Grafana Web UI. Create a Datasource \u00b6 Add the Apache Cassandra Data Source as a data source at the datasource configuration page. Enable Custom TLS Settings button and configure the datasource using following details: Host : specify the host:cql_port values from the config.json file from the SecureConnectBundle. It should look like 1234567890qwerty-eu-central-1.db.astra.datastax.com:29402 IMPORTANT Notice, it has to be the cql_port value, not just port User : Client ID of the API Token Password : Client Secret of the API Token Certificate Path : /path/to/cert (use cert file from SecureConnectBundle) Root Certificate Path : /path/to/key (use key file from SecureConnectBundle) RootCA Certificate Path : /path/to/ca.crt (use ca.crt file from SecureConnectBundle) Push the Save and Test button, if everything is right, you will see a Database Connection OK message. If the database cannot be connected, check the following known common issues: Known issues: \u00b6 Misconfigured Port (Using port instead of cql-port ) \u00b6 Sometimes users specify the wrong port and a connection cannot be established. If you can't connect to your Astra instance, please check if the correct port specified in the datasource config (See step 3 above) Unavailable TLS files \u00b6 if you have an error message like [ERROR] cassandra-backend-datasource: Unable create tls config, open /cert: permission denied , it means that Grafana cannot open TLS certificate files. Set the proper permission f.e. using chown command. If you copied the files using docker cp command, they'll be copied by a root user and grafana will have no access to them. Usage \u00b6 First, to visualize the data, you have to create a panel. Choose or create a dashboard and create a panel. In the panel setup, choose the correct datasource from the previous steps. There are two ways to query data from Cassandra: Query Configurator and Query Editor . Configurator is easier to use but has limited capabilities, Editor is more powerful but requires an understanding of CQL . Query Configurator \u00b6 Query Configurator is the easiest way to query data. At first, enter the keyspace and table name, then pick proper columns. If keyspace and table names are given correctly, the datasource will suggest the column names automatically. Time Column - the column storing the timestamp value, it's used to answer \"when\" question. Value Column - the column storing the value you'd like to show. It can be the value , temperature or whatever property you need. ID Column - the column to uniquely identify the source of the data, e.g. sensor_id , shop_id , or whatever allows you to identify the origin of data. After that, you have to specify the ID Value , the particular ID of the data origin you want to show. You may need to enable \"ALLOW FILTERING\" although we recommend avoiding it. Example Imagine you want to visualise reports of a temperature sensor installed in your smart home. Given the sensor reports its ID, time, location and temperature every minute, we create a table to store the data and put some values there: CREATE TABLE IF NOT EXISTS temperature ( sensor_id uuid, registered_at timestamp, temperature int, location text, PRIMARY KEY ((sensor_id), registered_at) ); insert into temperature (sensor_id, registered_at, temperature, location) values (99051fe9-6a9c-46c2-b949-38ef78858dd0, '2020-04-01T11:21:59.001+0000', 18, 'kitchen'); insert into temperature (sensor_id, registered_at, temperature, location) values (99051fe9-6a9c-46c2-b949-38ef78858dd0, '2020-04-01T11:22:59.001+0000', 19, 'kitchen'); insert into temperature (sensor_id, registered_at, temperature, location) values (99051fe9-6a9c-46c2-b949-38ef78858dd0, '2020-04-01T11:23:59.001+0000', 20, 'kitchen'); In this case, we have to fill the configurator fields the following way to get the results: Keyspace - smarthome (keyspace name) Table - temperature (table name) Time Column - registered_at (occurence) Value Column - temperature (value to show) ID Column - sensor_id (ID of the data origin) ID Value - 99051fe9-6a9c-46c2-b949-38ef78858dd0 ID of the sensor ALLOW FILTERING - FALSE (not required, so we are happy to avoid) In the case of a few origins (multiple sensors), you will need to add more rows. If your case is as simple as that, a query configurator will be a good choice, otherwise please proceed to the query editor. Query Editor \u00b6 Query Editor is a more powerful way to query data. To enable query editor, press the \"toggle text edit mode\" button. Query Editor unlocks all possibilities of CQL including aggregations, etc. SELECT sensor_id, CAST(temperature as double), registered_at FROM test.test WHERE id IN (99051fe9-6a9c-46c2-b949-38ef78858dd1, 99051fe9-6a9c-46c2-b949-38ef78858dd0) AND created_at > $__timeFrom and created_at < $__timeTo Follow the order of the SELECT expressions, it's important! Identifier - the first property in the SELECT expression must be the ID, something that uniquely identifies the data (e.g. sensor_id ) Value - The second property must be the value that you are going to show Timestamp - The third value must be a timestamp of the value. All other properties will be ignored To filter data by time, use $__timeFrom and $__timeTo placeholders as in the example. The datasource will replace them with time values from the panel. Notice It's important to add the placeholders otherwise query will try to fetch data for the whole period of time. Don't try to specify the timeframe on your own, just put the placeholders. It's grafana's job to specify time limits. Contacts \u00b6 We hope it works well for you! In case of any questions please contact developers using GitHub Discussions . \ud83c\udfe0 Back to home","title":"\u2022 Grafana"},{"location":"pages/tools/integration/grafana/#a-overview","text":"Grafana is a multi-platform open source analytics and interactive visualization web application. It provides charts, graphs, and alerts for the web when connected to supported data sources. A licensed Grafana Enterprise version with additional capabilities is also available as a self-hosted installation or an account on the Grafana Labs cloud service. It is expandable through a plug-in system. End users can create complex dashboards using interactive query builders. Community-developed Cassandra Datasource for Grafana supports both Apache Cassandra as well as DataStax AstraDB, allowing to use Cassandra as a data backend for Grafana. Data can be pulled using simple Query Configurator or more advanced but powerful Query Editor. (On the picture: Query Editor at work)","title":"A - Overview"},{"location":"pages/tools/integration/grafana/#b-prerequisites","text":"To use Grafana, you will need a running Grafana instance deployed locally or in a cloud. Locally launched Grafana in Docker works well too. You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle and unpack it. Keep the token information and the bundle file location ready: these will be soon provided in the datasource configuration.","title":"B - Prerequisites"},{"location":"pages/tools/integration/grafana/#c-quickstart","text":"","title":"C - Quickstart"},{"location":"pages/tools/integration/grafana/#install-the-plugin-using-cli-or-using-web-interface","text":"Install the plugin using grafana console tool: grafana-cli plugins install hadesarchitect-cassandra-datasource It will be installed into your grafana plugins directory; the default is /var/lib/grafana/plugins. Alternatively, enable it using Grafana Web UI.","title":"Install the plugin using CLI or using web-interface"},{"location":"pages/tools/integration/grafana/#create-a-datasource","text":"Add the Apache Cassandra Data Source as a data source at the datasource configuration page. Enable Custom TLS Settings button and configure the datasource using following details: Host : specify the host:cql_port values from the config.json file from the SecureConnectBundle. It should look like 1234567890qwerty-eu-central-1.db.astra.datastax.com:29402 IMPORTANT Notice, it has to be the cql_port value, not just port User : Client ID of the API Token Password : Client Secret of the API Token Certificate Path : /path/to/cert (use cert file from SecureConnectBundle) Root Certificate Path : /path/to/key (use key file from SecureConnectBundle) RootCA Certificate Path : /path/to/ca.crt (use ca.crt file from SecureConnectBundle) Push the Save and Test button, if everything is right, you will see a Database Connection OK message. If the database cannot be connected, check the following known common issues:","title":"Create a Datasource"},{"location":"pages/tools/integration/grafana/#known-issues","text":"","title":"Known issues:"},{"location":"pages/tools/integration/grafana/#misconfigured-port-using-port-instead-of-cql-port","text":"Sometimes users specify the wrong port and a connection cannot be established. If you can't connect to your Astra instance, please check if the correct port specified in the datasource config (See step 3 above)","title":"Misconfigured Port (Using port instead of cql-port)"},{"location":"pages/tools/integration/grafana/#unavailable-tls-files","text":"if you have an error message like [ERROR] cassandra-backend-datasource: Unable create tls config, open /cert: permission denied , it means that Grafana cannot open TLS certificate files. Set the proper permission f.e. using chown command. If you copied the files using docker cp command, they'll be copied by a root user and grafana will have no access to them.","title":"Unavailable TLS files"},{"location":"pages/tools/integration/grafana/#usage","text":"First, to visualize the data, you have to create a panel. Choose or create a dashboard and create a panel. In the panel setup, choose the correct datasource from the previous steps. There are two ways to query data from Cassandra: Query Configurator and Query Editor . Configurator is easier to use but has limited capabilities, Editor is more powerful but requires an understanding of CQL .","title":"Usage"},{"location":"pages/tools/integration/grafana/#query-configurator","text":"Query Configurator is the easiest way to query data. At first, enter the keyspace and table name, then pick proper columns. If keyspace and table names are given correctly, the datasource will suggest the column names automatically. Time Column - the column storing the timestamp value, it's used to answer \"when\" question. Value Column - the column storing the value you'd like to show. It can be the value , temperature or whatever property you need. ID Column - the column to uniquely identify the source of the data, e.g. sensor_id , shop_id , or whatever allows you to identify the origin of data. After that, you have to specify the ID Value , the particular ID of the data origin you want to show. You may need to enable \"ALLOW FILTERING\" although we recommend avoiding it. Example Imagine you want to visualise reports of a temperature sensor installed in your smart home. Given the sensor reports its ID, time, location and temperature every minute, we create a table to store the data and put some values there: CREATE TABLE IF NOT EXISTS temperature ( sensor_id uuid, registered_at timestamp, temperature int, location text, PRIMARY KEY ((sensor_id), registered_at) ); insert into temperature (sensor_id, registered_at, temperature, location) values (99051fe9-6a9c-46c2-b949-38ef78858dd0, '2020-04-01T11:21:59.001+0000', 18, 'kitchen'); insert into temperature (sensor_id, registered_at, temperature, location) values (99051fe9-6a9c-46c2-b949-38ef78858dd0, '2020-04-01T11:22:59.001+0000', 19, 'kitchen'); insert into temperature (sensor_id, registered_at, temperature, location) values (99051fe9-6a9c-46c2-b949-38ef78858dd0, '2020-04-01T11:23:59.001+0000', 20, 'kitchen'); In this case, we have to fill the configurator fields the following way to get the results: Keyspace - smarthome (keyspace name) Table - temperature (table name) Time Column - registered_at (occurence) Value Column - temperature (value to show) ID Column - sensor_id (ID of the data origin) ID Value - 99051fe9-6a9c-46c2-b949-38ef78858dd0 ID of the sensor ALLOW FILTERING - FALSE (not required, so we are happy to avoid) In the case of a few origins (multiple sensors), you will need to add more rows. If your case is as simple as that, a query configurator will be a good choice, otherwise please proceed to the query editor.","title":"Query Configurator"},{"location":"pages/tools/integration/grafana/#query-editor","text":"Query Editor is a more powerful way to query data. To enable query editor, press the \"toggle text edit mode\" button. Query Editor unlocks all possibilities of CQL including aggregations, etc. SELECT sensor_id, CAST(temperature as double), registered_at FROM test.test WHERE id IN (99051fe9-6a9c-46c2-b949-38ef78858dd1, 99051fe9-6a9c-46c2-b949-38ef78858dd0) AND created_at > $__timeFrom and created_at < $__timeTo Follow the order of the SELECT expressions, it's important! Identifier - the first property in the SELECT expression must be the ID, something that uniquely identifies the data (e.g. sensor_id ) Value - The second property must be the value that you are going to show Timestamp - The third value must be a timestamp of the value. All other properties will be ignored To filter data by time, use $__timeFrom and $__timeTo placeholders as in the example. The datasource will replace them with time values from the panel. Notice It's important to add the placeholders otherwise query will try to fetch data for the whole period of time. Don't try to specify the timeframe on your own, just put the placeholders. It's grafana's job to specify time limits.","title":"Query Editor"},{"location":"pages/tools/integration/grafana/#contacts","text":"We hope it works well for you! In case of any questions please contact developers using GitHub Discussions . \ud83c\udfe0 Back to home","title":"Contacts"},{"location":"pages/tools/integration/liquibase/","text":"Overview \u00b6 The purpose of this document is to guide you through the process of creating a new Liquibase project with Cassandra. In this tutorial, you will generate an example project and follow the instructions to apply and learn concepts associated with creating new Liquibase projects with Cassandra on DataStax Astra. \u2139\ufe0f Introduction to Liquibase \ud83d\udce5 Liquibase Quick Install Prerequisites \u00b6 Liquibase Prerequisites \u00b6 Install the latest version of Liquibase Ensure the Liquibase install directory path is set to a location in the PATH System variable Download the liquibase-cassandra- .jar latest release extension jar file and place this file in the `liquibase/lib` install directory Astra Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token Download the Simba JDBC Jar driver file for Apache Cassandra and place this file in the `liquibase/lib` install directory Clone this repository to use to set up CQL-Proxy which is a sidecar that enables unsupported CQL drivers to work with DataStax Astra You need your Astra Token and Astra Database ID to use CQL-Proxy Follow the steps in the repo to spin up CQL-Proxy using Terminal/Command Line. Once successfully running, you should see the following output: {\"level\":\"info\",\"ts\":1651012815.176512,\"caller\":\"proxy/proxy.go:222\",\"msg\":\"proxy is listening\",\"address\":\"[::]:9042\"} Installation and Setup \u00b6 To create a Liquibase project with Cassandra on DataStax Astra on your machine, begin with the following steps: Create a new project folder and name it LiquibaseProj. In your LiquibaseProj folder, create a new text file and name it dbchangelog.sql. Open the dbchangelog.sql file and update the changelog file with the following code snippet: --liquibase formatted sql In your LiquibaseProj folder, create a new text file and name it liquibase.properties. Edit the liquibase.properties file to add the following properties: changelog-file: dbchangelog.sql url: jdbc:cassandra://localhost:9042/test;DefaultKeyspace=test;TunableConsistency=6 driver: com.simba.cassandra.jdbc42.Driver defaultSchemaName: test liquibase.hub.mode=off In liquibase.properties above, replace test with the name of your own keyspace. Add a changeset to the changelog \u2013 changeset are uniquely identified by author and id attributes. Liquibase attempts to execute each changeset in a transaction that is committed at the end. In the dbchangelog.sql file, add a new changeset with a create table statement. We will create a new table department using a changeset as follows: --liquibase formatted sql --changeset bob:1 CREATE TABLE test.DEPARTMENT (id int PRIMARY KEY, NAME text, ACTIVE BOOLEAN); Open the command prompt. Navigate to the LiquibaseProj directory. Run the following command: liquibase update From a SQL Client User Interface, check your database changes. You should see a new department table added to the database. For example: SELECT * FROM \"keyspace\".\"department\"; ID NAME ACTIVE NULL NULL NULL After your first update, your database will contain the table you added along with the DATABASECHANGELOG and DATABASECHANGELOGLOCK tables: DATABASECHANGELOG table. This table keeps a record of all the changesets that were deployed. When you deploy, the changesets in the changelog are compared with the DATABASECHANGELOG tracking table, and only the new changesets that were not found in the DATABASECHANGELOG will be deployed. DATABASECHANGELOGLOCK table. This table is used internally by Liquibase to manage access to the DATABASECHANGELOG table during deployment and ensure only one instance of Liquibase is updating the database at a time, whether that is creating, updating, or deleting changes.","title":"\u2022 Liquibase"},{"location":"pages/tools/integration/liquibase/#overview","text":"The purpose of this document is to guide you through the process of creating a new Liquibase project with Cassandra. In this tutorial, you will generate an example project and follow the instructions to apply and learn concepts associated with creating new Liquibase projects with Cassandra on DataStax Astra. \u2139\ufe0f Introduction to Liquibase \ud83d\udce5 Liquibase Quick Install","title":"Overview"},{"location":"pages/tools/integration/liquibase/#prerequisites","text":"","title":"Prerequisites"},{"location":"pages/tools/integration/liquibase/#liquibase-prerequisites","text":"Install the latest version of Liquibase Ensure the Liquibase install directory path is set to a location in the PATH System variable Download the liquibase-cassandra- .jar latest release extension jar file and place this file in the `liquibase/lib` install directory","title":"Liquibase Prerequisites"},{"location":"pages/tools/integration/liquibase/#astra-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token Download the Simba JDBC Jar driver file for Apache Cassandra and place this file in the `liquibase/lib` install directory Clone this repository to use to set up CQL-Proxy which is a sidecar that enables unsupported CQL drivers to work with DataStax Astra You need your Astra Token and Astra Database ID to use CQL-Proxy Follow the steps in the repo to spin up CQL-Proxy using Terminal/Command Line. Once successfully running, you should see the following output: {\"level\":\"info\",\"ts\":1651012815.176512,\"caller\":\"proxy/proxy.go:222\",\"msg\":\"proxy is listening\",\"address\":\"[::]:9042\"}","title":"Astra Prerequisites"},{"location":"pages/tools/integration/liquibase/#installation-and-setup","text":"To create a Liquibase project with Cassandra on DataStax Astra on your machine, begin with the following steps: Create a new project folder and name it LiquibaseProj. In your LiquibaseProj folder, create a new text file and name it dbchangelog.sql. Open the dbchangelog.sql file and update the changelog file with the following code snippet: --liquibase formatted sql In your LiquibaseProj folder, create a new text file and name it liquibase.properties. Edit the liquibase.properties file to add the following properties: changelog-file: dbchangelog.sql url: jdbc:cassandra://localhost:9042/test;DefaultKeyspace=test;TunableConsistency=6 driver: com.simba.cassandra.jdbc42.Driver defaultSchemaName: test liquibase.hub.mode=off In liquibase.properties above, replace test with the name of your own keyspace. Add a changeset to the changelog \u2013 changeset are uniquely identified by author and id attributes. Liquibase attempts to execute each changeset in a transaction that is committed at the end. In the dbchangelog.sql file, add a new changeset with a create table statement. We will create a new table department using a changeset as follows: --liquibase formatted sql --changeset bob:1 CREATE TABLE test.DEPARTMENT (id int PRIMARY KEY, NAME text, ACTIVE BOOLEAN); Open the command prompt. Navigate to the LiquibaseProj directory. Run the following command: liquibase update From a SQL Client User Interface, check your database changes. You should see a new department table added to the database. For example: SELECT * FROM \"keyspace\".\"department\"; ID NAME ACTIVE NULL NULL NULL After your first update, your database will contain the table you added along with the DATABASECHANGELOG and DATABASECHANGELOGLOCK tables: DATABASECHANGELOG table. This table keeps a record of all the changesets that were deployed. When you deploy, the changesets in the changelog are compared with the DATABASECHANGELOG tracking table, and only the new changesets that were not found in the DATABASECHANGELOG will be deployed. DATABASECHANGELOGLOCK table. This table is used internally by Liquibase to manage access to the DATABASECHANGELOG table during deployment and ensure only one instance of Liquibase is updating the database at a time, whether that is creating, updating, or deleting changes.","title":"Installation and Setup"},{"location":"pages/tools/integration/pentaho/","text":"This article was originally written by Erick Ramirez on community.datastax.com Overview \u00b6 Pentaho Data Integration (PDI) provides the Extract, Transform, and Load (ETL) capabilities that facilitate the process of capturing, cleansing, and storing data using a uniform and consistent format that is accessible and relevant to end users and IoT technologies. \u2139\ufe0f Introduction to PDI \ud83d\udce5 PDI Download Link \ud83d\udcd8 Installation Guide on Linux \ud83d\udcd8 Installation Guide on Windows Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle You should Download and install PDI This article was written for version 9.1 on MacOS but it should also work for the Windows version. Installation and Setup \u00b6 \u2705 Step 1: Download JDBC Driver \u00b6 Download the JDBC driver from the DataStax website: Go to https://downloads.datastax.com/#odbc-jdbc-drivers. Select Simba JDBC Driver for Apache Cassandra. Select JDBC 4.2. Read the license terms and accept it (click the checkbox). Hit the blue Download button. Once the download completes, unzip the downloaded file. \u2705 Step 2: Import Driver JAR in Pentaho \u00b6 Deploy the Simba driver to Pentaho servers using the distribution tool: On your laptop or PC, copy the Simba JAR to the JDBC distribution directory: $ cp CassandraJDBC42.jar pentaho/jdbc-distribution/ Run the distribution tool ( distribute-files.bat on Windows) $ cd /Applications/Pentaho/jdbc-distribution $ ./distribute-files.sh CassandraJDBC42.jar Verify that the JAR has been copied to the PDI library: $ cd /Applications/Pentaho $ ls -lh design-tools/data-integration/lib/CassandraJDBC42.jar Expected output: -rw-r--r-- 1 erick vaxxed 16M 14 Sep 22 :18 design-tools/data-integration/lib/CassandraJDBC42.jar $ file design-tools/data-integration/lib/CassandraJDBC42.jar Expected output: design-tools/data-integration/lib/CassandraJDBC42.jar: Java archive data ( JAR ) Restart Pentaho on your workstation for the Simba driver to be loaded. \u2705 Step 3: Define a connection in Pentaho \u00b6 In this section we assume that your database in Astra is called pentaho and as such the download secure bundle is called secure-connect-pentaho.zip Create a new Transformation. Open a new Database Connection dialog box. In the Connection name field, give your DB connection a name. Under Connection type, select Generic database. Set the Custom connection URL . (Note that you will need to specify the full path to your secure bundle and adapt to your database name) jdbc:cassandra://;AuthMech=2;TunableConsistency=6;SecureConnectionBundlePath=/path/to/secure-connect-pentaho.zip In the Username field, enter the string token . In the Password field, paste the value of the token you created in the Prerequisites section above. The token looks like AstraCS:AbC...XYz:123...edf0 Click on the Test Connection button to confirm that the driver configuration is working: Click on the OK button to save the connection settings. \u2705 Step 4: Final Test \u00b6 Connect to your Astra DB by launching the SQL Editor in Pentaho and run a simple CQL statement. For example: Here's an example output: You should also be able to browse the keyspaces in your Astra DB using the DataBase Explorer. Here's an example output: \ud83c\udfe0 Back to HOME |","title":"\u2022 Pentaho Data Integration"},{"location":"pages/tools/integration/pentaho/#overview","text":"Pentaho Data Integration (PDI) provides the Extract, Transform, and Load (ETL) capabilities that facilitate the process of capturing, cleansing, and storing data using a uniform and consistent format that is accessible and relevant to end users and IoT technologies. \u2139\ufe0f Introduction to PDI \ud83d\udce5 PDI Download Link \ud83d\udcd8 Installation Guide on Linux \ud83d\udcd8 Installation Guide on Windows","title":"Overview"},{"location":"pages/tools/integration/pentaho/#prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure bundle You should Download and install PDI This article was written for version 9.1 on MacOS but it should also work for the Windows version.","title":"Prerequisites"},{"location":"pages/tools/integration/pentaho/#installation-and-setup","text":"","title":"Installation and Setup"},{"location":"pages/tools/integration/pentaho/#step-1-download-jdbc-driver","text":"Download the JDBC driver from the DataStax website: Go to https://downloads.datastax.com/#odbc-jdbc-drivers. Select Simba JDBC Driver for Apache Cassandra. Select JDBC 4.2. Read the license terms and accept it (click the checkbox). Hit the blue Download button. Once the download completes, unzip the downloaded file.","title":"\u2705 Step 1: Download JDBC Driver"},{"location":"pages/tools/integration/pentaho/#step-2-import-driver-jar-in-pentaho","text":"Deploy the Simba driver to Pentaho servers using the distribution tool: On your laptop or PC, copy the Simba JAR to the JDBC distribution directory: $ cp CassandraJDBC42.jar pentaho/jdbc-distribution/ Run the distribution tool ( distribute-files.bat on Windows) $ cd /Applications/Pentaho/jdbc-distribution $ ./distribute-files.sh CassandraJDBC42.jar Verify that the JAR has been copied to the PDI library: $ cd /Applications/Pentaho $ ls -lh design-tools/data-integration/lib/CassandraJDBC42.jar Expected output: -rw-r--r-- 1 erick vaxxed 16M 14 Sep 22 :18 design-tools/data-integration/lib/CassandraJDBC42.jar $ file design-tools/data-integration/lib/CassandraJDBC42.jar Expected output: design-tools/data-integration/lib/CassandraJDBC42.jar: Java archive data ( JAR ) Restart Pentaho on your workstation for the Simba driver to be loaded.","title":"\u2705 Step 2: Import Driver JAR in Pentaho"},{"location":"pages/tools/integration/pentaho/#step-3-define-a-connection-in-pentaho","text":"In this section we assume that your database in Astra is called pentaho and as such the download secure bundle is called secure-connect-pentaho.zip Create a new Transformation. Open a new Database Connection dialog box. In the Connection name field, give your DB connection a name. Under Connection type, select Generic database. Set the Custom connection URL . (Note that you will need to specify the full path to your secure bundle and adapt to your database name) jdbc:cassandra://;AuthMech=2;TunableConsistency=6;SecureConnectionBundlePath=/path/to/secure-connect-pentaho.zip In the Username field, enter the string token . In the Password field, paste the value of the token you created in the Prerequisites section above. The token looks like AstraCS:AbC...XYz:123...edf0 Click on the Test Connection button to confirm that the driver configuration is working: Click on the OK button to save the connection settings.","title":"\u2705 Step 3: Define a connection in Pentaho"},{"location":"pages/tools/integration/pentaho/#step-4-final-test","text":"Connect to your Astra DB by launching the SQL Editor in Pentaho and run a simple CQL statement. For example: Here's an example output: You should also be able to browse the keyspaces in your Astra DB using the DataBase Explorer. Here's an example output: \ud83c\udfe0 Back to HOME |","title":"\u2705 Step 4: Final Test"},{"location":"pages/tools/integration/quine.io/","text":"A - Overview \u00b6 Quine is a streaming graph capable of building high-volumes of data into a stateful graph. It allows for real-time traversals on a graph, as well as for the data to be streamed-out for event processing. \u2139\ufe0f Quine Documentation - Core Concepts B - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle You should install a JDK (version 11 or higher). This article was written for Quine version 1.2.1 on MacOS with Java 11.10 . C - Installation \u00b6 \u2705 Step 1 Download and install Follow the Download Quine page to download the JAR. Choose/create a directory for Quine, and copy the JAR to this location: mkdir ~/local/quine cp ~/Downloads/quine-1.2.1.jar ~/local/quine \u2705 Step 2 Create the keyspace quine From the Astra DB console , click on your database name, or create a new one called quine (or another name of your preference). Scroll down to where the keyspaces are listed, and click the Add Keyspace button to create a new keyspace. Name this keyspace quine . \u2705 Step 3 Configuration Create a quine.conf file inside the quine directory: cd ~/local/quine touch quine.conf Edit the quine.conf file to look like the following: quine.store { # store data in an Apache Cassandra instance type = cassandra # the keyspace to use keyspace = quine should-create-keyspace = false should-create-tables = true replication-factor = 3 write-consistency = LOCAL_QUORUM read-consistency = LOCAL_QUORUM local-datacenter = \"us-east1\" write-timeout = \"10s\" read-timeout = \"10s\" } datastax-java-driver { advanced { auth-provider { class = PlainTextAuthProvider username = \"token\" password = \"AstraCS:qFDPGZEgBlahBlahYourTokenGoesHerecff15fc\" } } basic { cloud { secure-connect-bundle = \"/Users/aaronploetz/local/secure-connect-quine.zip\" } } } Astra-Specific Settings: type = cassandra - If the type is not specified, Quine defaults to use RocksDB. should-create-keyspace = false - Remember keyspaces can only be created in Astra via the dashboard. replication-factor = 3 - Defaults to 1 if not set, which will not work with Astra DB. write-consistency = LOCAL_QUORUM - Minimum consistency level required by Astra. read-consistency = LOCAL_QUORUM - Minimum consistency level required by Astra. local-datacenter = \"us-east1\" - Set Astra DB's cloud region as the local DC. username = \"token\" - No need to mess with this. Just leave it as the literal word \"token.\" password - A valid token for an Astra DB cluster. secure-connect-bundle - A valid, local file location of a downloaded secure connect bundle. Also, the driver gets the Astra DB hostname and Cloud provider from the secure bundle, so there is no need to specify endpoints separately. \u2705 Step 4 Download Secure Connect Bundle (SCB) In your Astra DB console navigate to your database in the dashboard, then the connect tab. In the 'Connect using a Driver' , and then the click 'Java' Java section. Then click the 'download bundle' on the right. Without unzipping it, move the downloaded file to the directory you created in step 1, that contains the quine-1.2.1.jar. The file will be named secure-connect-[your databasename].zip , so in this example secure-connect-quine.zip . You will reference this file directly in the previous configation file step above. \u2705 Step 5 Run Quine To run Quine, invoke the JAR with Java, while passing the quine.conf in the config.file parameter: $ java -Dconfig.file=quine.conf -jar quine-1.2.1.jar 2022-06-15 15:11:52,666 WARN [NotFromActor] [s0-io-4] com.datastax.oss.driver.internal.core.cql.CqlRequestHandler - Query '[0 values] CREATE TABLE IF NOT EXISTS journals (quine_id blob,timestamp bigint,data blob,PRIMARY KEY(quine_id,timestamp)) WITH CLUSTERING ORDER BY (timestamp ASC) AND compaction={'class':'TimeWindowCompactionStrategy'}' generated server side warning(s): Ignoring provided values [compaction] as they are not supported for Table Properties (ignored values are: [additional_write_policy, bloom_filter_fp_chance, caching, cdc, compaction, compression, crc_check_chance, dclocal_read_repair_chance, extensions, gc_grace_seconds, id, max_index_interval, memtable_flush_period_in_ms, min_index_interval, nodesync, read_repair, read_repair_chance, speculative_retry]) Graph is ready! Application state loaded. Quine app web server available at http://localhost:8080 As shown above, Astra DB will return a warning about table valid options which it will ignore. You can now use Quine's visual graph explorer in a web browser, and create/traverse data with either Gremlin or Cypher: http://localhost:8080/ The Swagger spec for the Quine API can also be found locally at: http://localhost:8080/docs \u2705 Optional Step 6: Loading some sample data Download attempts.json (74.MB) from the Quine Password Spraying example and locate it in the root of your Quine directory alongside the quine-1.2.1.jar file. Make sure the Quine server is not running -- it's requires graceful shutdown. Simply issue a curl -X \"POST\" \"http://127.0.0.1:8080/api/v1/admin/shutdown\" in a seperate terminal or command prompt window to do so. Then execute java -Dconfig.file=quine.conf -jar quine-1.2.1.jar -r passwordspraying Note that it will take a few minutes to load! When it is completed successfully you will see: INGEST-1 status is completed and ingested 55000 \u2705 Troubleshooting If the output does not read: Graph is ready! Application state loaded. Quine app web server available at http://locahost:8080 Then look for exceptions. If you see an error: com.datastax.oss.driver.api.core.servererrors.InvalidQueryException: Clustering key columns must exactly match columns in CLUSTERING ORDER BY directive Check to ensure the snapshots table exists: cqlsh> use quine; cqlsh> desc quine; If not, execute this command in CQLSH to create it: CREATE TABLE quine.snapshots ( quine_id blob, timestamp bigint, multipart_index int, data blob, multipart_count int, PRIMARY KEY (quine_id, timestamp, multipart_index) ) WITH CLUSTERING ORDER BY (timestamp DESC, multipart_index ASC) AND additional_write_policy = '99PERCENTILE' AND bloom_filter_fp_chance = 0.01 AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'} AND comment = '' AND compaction = {'class': 'org.apache.cassandra.db.compaction.UnifiedCompactionStrategy'} AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'} AND crc_check_chance = 1.0 AND default_time_to_live = 0 AND gc_grace_seconds = 864000 AND max_index_interval = 2048 AND memtable_flush_period_in_ms = 0 AND min_index_interval = 128 AND read_repair = 'BLOCKING' AND speculative_retry = '99PERCENTILE'; D - Acknowledgements \u00b6 Special thanks goes out to Ryan Wright and Leif Warner of thatDot for their help with getting Quine running and connected. \ud83c\udfe0 Back to home","title":"\u2022 Quine"},{"location":"pages/tools/integration/quine.io/#a-overview","text":"Quine is a streaming graph capable of building high-volumes of data into a stateful graph. It allows for real-time traversals on a graph, as well as for the data to be streamed-out for event processing. \u2139\ufe0f Quine Documentation - Core Concepts","title":"A - Overview"},{"location":"pages/tools/integration/quine.io/#b-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Connect Bundle You should install a JDK (version 11 or higher). This article was written for Quine version 1.2.1 on MacOS with Java 11.10 .","title":"B - Prerequisites"},{"location":"pages/tools/integration/quine.io/#c-installation","text":"\u2705 Step 1 Download and install Follow the Download Quine page to download the JAR. Choose/create a directory for Quine, and copy the JAR to this location: mkdir ~/local/quine cp ~/Downloads/quine-1.2.1.jar ~/local/quine \u2705 Step 2 Create the keyspace quine From the Astra DB console , click on your database name, or create a new one called quine (or another name of your preference). Scroll down to where the keyspaces are listed, and click the Add Keyspace button to create a new keyspace. Name this keyspace quine . \u2705 Step 3 Configuration Create a quine.conf file inside the quine directory: cd ~/local/quine touch quine.conf Edit the quine.conf file to look like the following: quine.store { # store data in an Apache Cassandra instance type = cassandra # the keyspace to use keyspace = quine should-create-keyspace = false should-create-tables = true replication-factor = 3 write-consistency = LOCAL_QUORUM read-consistency = LOCAL_QUORUM local-datacenter = \"us-east1\" write-timeout = \"10s\" read-timeout = \"10s\" } datastax-java-driver { advanced { auth-provider { class = PlainTextAuthProvider username = \"token\" password = \"AstraCS:qFDPGZEgBlahBlahYourTokenGoesHerecff15fc\" } } basic { cloud { secure-connect-bundle = \"/Users/aaronploetz/local/secure-connect-quine.zip\" } } } Astra-Specific Settings: type = cassandra - If the type is not specified, Quine defaults to use RocksDB. should-create-keyspace = false - Remember keyspaces can only be created in Astra via the dashboard. replication-factor = 3 - Defaults to 1 if not set, which will not work with Astra DB. write-consistency = LOCAL_QUORUM - Minimum consistency level required by Astra. read-consistency = LOCAL_QUORUM - Minimum consistency level required by Astra. local-datacenter = \"us-east1\" - Set Astra DB's cloud region as the local DC. username = \"token\" - No need to mess with this. Just leave it as the literal word \"token.\" password - A valid token for an Astra DB cluster. secure-connect-bundle - A valid, local file location of a downloaded secure connect bundle. Also, the driver gets the Astra DB hostname and Cloud provider from the secure bundle, so there is no need to specify endpoints separately. \u2705 Step 4 Download Secure Connect Bundle (SCB) In your Astra DB console navigate to your database in the dashboard, then the connect tab. In the 'Connect using a Driver' , and then the click 'Java' Java section. Then click the 'download bundle' on the right. Without unzipping it, move the downloaded file to the directory you created in step 1, that contains the quine-1.2.1.jar. The file will be named secure-connect-[your databasename].zip , so in this example secure-connect-quine.zip . You will reference this file directly in the previous configation file step above. \u2705 Step 5 Run Quine To run Quine, invoke the JAR with Java, while passing the quine.conf in the config.file parameter: $ java -Dconfig.file=quine.conf -jar quine-1.2.1.jar 2022-06-15 15:11:52,666 WARN [NotFromActor] [s0-io-4] com.datastax.oss.driver.internal.core.cql.CqlRequestHandler - Query '[0 values] CREATE TABLE IF NOT EXISTS journals (quine_id blob,timestamp bigint,data blob,PRIMARY KEY(quine_id,timestamp)) WITH CLUSTERING ORDER BY (timestamp ASC) AND compaction={'class':'TimeWindowCompactionStrategy'}' generated server side warning(s): Ignoring provided values [compaction] as they are not supported for Table Properties (ignored values are: [additional_write_policy, bloom_filter_fp_chance, caching, cdc, compaction, compression, crc_check_chance, dclocal_read_repair_chance, extensions, gc_grace_seconds, id, max_index_interval, memtable_flush_period_in_ms, min_index_interval, nodesync, read_repair, read_repair_chance, speculative_retry]) Graph is ready! Application state loaded. Quine app web server available at http://localhost:8080 As shown above, Astra DB will return a warning about table valid options which it will ignore. You can now use Quine's visual graph explorer in a web browser, and create/traverse data with either Gremlin or Cypher: http://localhost:8080/ The Swagger spec for the Quine API can also be found locally at: http://localhost:8080/docs \u2705 Optional Step 6: Loading some sample data Download attempts.json (74.MB) from the Quine Password Spraying example and locate it in the root of your Quine directory alongside the quine-1.2.1.jar file. Make sure the Quine server is not running -- it's requires graceful shutdown. Simply issue a curl -X \"POST\" \"http://127.0.0.1:8080/api/v1/admin/shutdown\" in a seperate terminal or command prompt window to do so. Then execute java -Dconfig.file=quine.conf -jar quine-1.2.1.jar -r passwordspraying Note that it will take a few minutes to load! When it is completed successfully you will see: INGEST-1 status is completed and ingested 55000 \u2705 Troubleshooting If the output does not read: Graph is ready! Application state loaded. Quine app web server available at http://locahost:8080 Then look for exceptions. If you see an error: com.datastax.oss.driver.api.core.servererrors.InvalidQueryException: Clustering key columns must exactly match columns in CLUSTERING ORDER BY directive Check to ensure the snapshots table exists: cqlsh> use quine; cqlsh> desc quine; If not, execute this command in CQLSH to create it: CREATE TABLE quine.snapshots ( quine_id blob, timestamp bigint, multipart_index int, data blob, multipart_count int, PRIMARY KEY (quine_id, timestamp, multipart_index) ) WITH CLUSTERING ORDER BY (timestamp DESC, multipart_index ASC) AND additional_write_policy = '99PERCENTILE' AND bloom_filter_fp_chance = 0.01 AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'} AND comment = '' AND compaction = {'class': 'org.apache.cassandra.db.compaction.UnifiedCompactionStrategy'} AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'} AND crc_check_chance = 1.0 AND default_time_to_live = 0 AND gc_grace_seconds = 864000 AND max_index_interval = 2048 AND memtable_flush_period_in_ms = 0 AND min_index_interval = 128 AND read_repair = 'BLOCKING' AND speculative_retry = '99PERCENTILE';","title":"C - Installation"},{"location":"pages/tools/integration/quine.io/#d-acknowledgements","text":"Special thanks goes out to Ryan Wright and Leif Warner of thatDot for their help with getting Quine running and connected. \ud83c\udfe0 Back to home","title":"D - Acknowledgements"},{"location":"pages/tools/integration/stepzen/","text":"Overview \u00b6 StepZen helps developers build GraphQL faster, deploy in seconds, and run on StepZen. It simplifies how you access the data you need, and with zero infrastructure to build or manage, you can focus on crafting modern data-driven experiences. \u2139\ufe0f Introduction to StepZen \ud83d\udce5 StepZen Quick Install - Prerequisites \u00b6 You should have an Astra account You should create a StepZen account You should Create an Astra Database You should Have an Astra Token You should retrieve your **Database ID** and **Region** from your Astra DB dashboard Installation and Setup \u00b6 After logging into your StepZen account and have all of your credentials ready, you can navigate to this page or follow the steps below to setup StepZen. First, install the StepZen CLI npm install -g stepzen Log in with your StepZen account stepzen login -a YOUR_ACCOUNT Enter your Admin Key when prompted YOUR_ADMIN_KEY Note: For steps #2 and #3, if it does not autopopulate, you can find this information in your StepZen account under \"My Stepzen\". Import a DataStax Astra DB GraphQL API from your terminal stepzen import graphql When prompted, enter your GraphQL API details: What is the GraphQL endpoint URL? https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com/api/graphql/<KEYSPACE_NAME> Prefix to add to all generated type names (leave blank for none) Optional. Adviced to use when you're importing multiple data sources. Add an HTTP header, e.g. Header-Name: header value (leave blank for none) X-Cassandra-Token: <APPLICATION_TOKEN> Once successful, you should see the following output: Generating schemas...... done Successfully imported schema graphql from StepZen 6. Type stepzen start in your terminal StepZen introspects your DataStax Astra DB GraphQL API and builds your endpoint. You should receive something similar to this output: File changed: /your/path/.DS_Store Deploying api/coy-aardwolf to StepZen... done in 4.8s Your API url is https://<YOUR_ACCOUNT>.stepzen.net/api/<YOUR_ENDPOINT_NAME>/__graphql You can test your hosted API with cURL: curl https://<YOUR_ACCOUNT>.stepzen.net/api/<YOUR_ENDPOINT_NAME>/__graphql \\ --header \"Authorization: Apikey $(stepzen whoami --apikey)\" \\ --header \"Content-Type: application/json\" \\ --data '{\"query\": \"your graphql query\"}' or explore it with GraphiQL at http://localhost:5001/api/<YOUR_ENDPOINT_NAME> Watching ~/your/path/here for GraphQL changes... Test and Validate \u00b6 To quickly validate that the previous steps went smoothly, navigate to your local host to view the StepZen UI. http://localhost:5001/api/<YOUR_ENDPOINT_NAME> Using the Explorer you can visualize what tables are in your keyspace, and by selecting each box, you are building your GraphQL query which shows up in the middle console. ...and you're done! You can now use StepZen to build your GraphQL queries with ease to use with your applications.","title":"\u2022 StepZen"},{"location":"pages/tools/integration/stepzen/#overview","text":"StepZen helps developers build GraphQL faster, deploy in seconds, and run on StepZen. It simplifies how you access the data you need, and with zero infrastructure to build or manage, you can focus on crafting modern data-driven experiences. \u2139\ufe0f Introduction to StepZen \ud83d\udce5 StepZen Quick Install","title":"Overview"},{"location":"pages/tools/integration/stepzen/#-prerequisites","text":"You should have an Astra account You should create a StepZen account You should Create an Astra Database You should Have an Astra Token You should retrieve your **Database ID** and **Region** from your Astra DB dashboard","title":"- Prerequisites"},{"location":"pages/tools/integration/stepzen/#installation-and-setup","text":"After logging into your StepZen account and have all of your credentials ready, you can navigate to this page or follow the steps below to setup StepZen. First, install the StepZen CLI npm install -g stepzen Log in with your StepZen account stepzen login -a YOUR_ACCOUNT Enter your Admin Key when prompted YOUR_ADMIN_KEY Note: For steps #2 and #3, if it does not autopopulate, you can find this information in your StepZen account under \"My Stepzen\". Import a DataStax Astra DB GraphQL API from your terminal stepzen import graphql When prompted, enter your GraphQL API details: What is the GraphQL endpoint URL? https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com/api/graphql/<KEYSPACE_NAME> Prefix to add to all generated type names (leave blank for none) Optional. Adviced to use when you're importing multiple data sources. Add an HTTP header, e.g. Header-Name: header value (leave blank for none) X-Cassandra-Token: <APPLICATION_TOKEN> Once successful, you should see the following output: Generating schemas...... done Successfully imported schema graphql from StepZen 6. Type stepzen start in your terminal StepZen introspects your DataStax Astra DB GraphQL API and builds your endpoint. You should receive something similar to this output: File changed: /your/path/.DS_Store Deploying api/coy-aardwolf to StepZen... done in 4.8s Your API url is https://<YOUR_ACCOUNT>.stepzen.net/api/<YOUR_ENDPOINT_NAME>/__graphql You can test your hosted API with cURL: curl https://<YOUR_ACCOUNT>.stepzen.net/api/<YOUR_ENDPOINT_NAME>/__graphql \\ --header \"Authorization: Apikey $(stepzen whoami --apikey)\" \\ --header \"Content-Type: application/json\" \\ --data '{\"query\": \"your graphql query\"}' or explore it with GraphiQL at http://localhost:5001/api/<YOUR_ENDPOINT_NAME> Watching ~/your/path/here for GraphQL changes...","title":"Installation and Setup"},{"location":"pages/tools/integration/stepzen/#test-and-validate","text":"To quickly validate that the previous steps went smoothly, navigate to your local host to view the StepZen UI. http://localhost:5001/api/<YOUR_ENDPOINT_NAME> Using the Explorer you can visualize what tables are in your keyspace, and by selecting each box, you are building your GraphQL query which shows up in the middle console. ...and you're done! You can now use StepZen to build your GraphQL queries with ease to use with your applications.","title":"Test and Validate"},{"location":"pages/tools/integration/temporal/","text":"Overview \u00b6 Temporal.io is an open source microservice orchestration platform that assists in tracking workflows in your application development. It provides the user with a plug-and-play persistence layer that lets the user choose and configure their Temporal Server with their preferred backend. Currently, Temporal is compatible with Postgres, MySQL, and Apache Cassandra\u24c7 as backend dependencies. \ud83d\udce5 Temporal Quick Install \u2139\ufe0f Introduction to Temporal \u2139\ufe0f Part 1: Introduction to Temporal and Cassandra, Astra DB \u2139\ufe0f Part 2: Connect Temporalio to Astra DB in 5 Easy Steps \u2139\ufe0f Part 3: Connect Temporalio to Astra DB With Kubernetes \ud83c\udfa5 Workflow with Temporal and Astra DB in 5 minutes - Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token Note This runbook was written using Mac OS Monterey but it will also work with Windows. Any Windows-specific instructions will be noted as such. Installation and Setup \u00b6 \u2705 Step 1: Setup Astra \u00b6 In your Astra database, create two new keyspaces called \"temporal\" and \"temporal_visibility\". You will be using both of these in the next steps. Make sure to create an Astra token with Admin Role Get your Database ID Find your Database ID in one of two ways: Navigate to your your database and get the last ID in the URL: https://astra.datastax.com/org/.../database/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx Copy and paste the Datacenter ID without the trailing -1 from the Regions section of your Astra Dashboard. \u2705 Step 2: Temporal Pre-setup \u00b6 Clone this GitHub repository Navigate to your cloned repository and using your preferred text editor (e.g. VisualStudio or Sublime), update the .env file with your Astra Token and Astra Database ID that you obtained above. ASTRA_TOKEN = <your Astra token> ASTRA_DATABASE_ID = <your DB ID> \u2705 Step 3: Temporal Schema Migration to Astra DB \u00b6 For this step, you will set up the keyspaces you created earlier in the Astra prerequisites ( temporal and temporal_visibility ). You will be using temporal-cassandra-tool which is part of the Temporal repo and it relies on schema definition. Navigate to your cloned temporal-astra-cql-proxy directory Run the following commands to initialize the keyspaces that we created through Astra. Note that there are two sets of commands, one for temporal keyspace and one for temporal_visibility keyspace: docker-compose -f docker-compose-schema.yaml run temporal-admin-tools \\ -ep cql-proxy -k temporal setup-schema -v 0 .0 docker-compose -f docker-compose-schema.yaml run temporal-admin-tools \\ -ep cql-proxy -k temporal update-schema -d schema/cassandra/temporal/versioned/ docker-compose -f docker-compose-schema.yaml run temporal-admin-tools \\ -ep cql-proxy -k temporal_visibility setup-schema -v 0 .0 docker-compose -f docker-compose-schema.yaml run temporal-admin-tools \\ -ep cql-proxy -k temporal_visibility update-schema -d schema/cassandra/visibility/versioned/ Once the process is completed, you should see a message similar to this: 2022 -03-02T22:23:27.618Z INFO Validating connection to cassandra cluster. { \"logging-call-at\" : \"cqlclient.go:112\" } 2022 -03-02T22:42:53.526Z INFO Connection validation succeeded. { \"logging-call-at\" : \"cqlclient.go:118\" } 2022 -03-02T22:42:53.526Z INFO Starting schema setup { \"config\" : { \"SchemaFilePath\" : \"\" , \"InitialVersion\" : \"0.0\" , \"Overwrite\" :false, \"DisableVersioning\" :false } , \"logging-call-at\" : \"setuptask.go:57\" } 2022 -03-02T22:42:53.526Z DEBUG Setting up version tables { \"logging-call-at\" : \"setuptask.go:67\" } 2022 -03-02T22:42:54.120Z DEBUG Current database schema version 1 .6 is greater than initial schema version 0 .0. Skip version upgrade { \"logging-call-at\" : \"setuptask.go:116\" } 2022 -03-02T22:42:54.120Z INFO Schema setup complete { \"logging-call-at\" : \"setuptask.go:131\" } Great! Your schemas have been migrated with Astra DB. Confirm your tables exist in Astra You can double-check to make sure the correct tables have been created by querying your database in Astra DB\u2019s CQL Console. Run DESC tables; in both your temporal and temporal_visibility keyspaces. You should see there are tables loaded in that were created by the schema migration with temporal-cassandra-tool . token@cqlsh> use temporal ; token@cqlsh:temporal> desc tables ; history_node tasks cluster_metadata_info cluster_membership namespaces cluster_metadata schema_version namespaces_by_id schema_update_history executions queue_metadata queue history_tree token@cqlsh:temporal> use temporal_visibility ; token@cqlsh:temporal_visibility> desc tables ; open_executions schema_update_history schema_version closed_executions \u2705 Step 4: Run Docker Compose \u00b6 In this step, the docker-compose.yaml file is already provided for you in the temporal-astra-cql-proxy repo. This file creates different docker containers to run Temporal server. The persistence layer is configured for you to connect with cql-proxy , and it should pull your Astra credentials from when you set it earlier: services: cql-proxy: container_name: cqlproxy image: datastax/cql-proxy:v ${ CQL_PROXY_VERSION } ... environment: - ASTRA_TOKEN = ${ ASTRA_TOKEN } - ASTRA_DATABASE_ID = ${ ASTRA_DATABASE_ID } - HEALTH_CHECK = true Now you can run the docker-compose command to start up Temporal: docker-compose up \u2705 Step 5: Test and Validate \u00b6 You can test your connection and play with your Temporal cluster with these instructions. Make sure to use tctl to create namespaces dedicated to certain workflows: bash-5.0# tctl --namespace test namespace re Namespace test successfully registered. When using the sample apps, keep in mind that you want to modify the starter and worker code so that it points to this specific Temporal deployment. For example: c, err : = client.NewClient ( client.Options { HostPort: \"127.0.0.1:7233\" , Namespace: \"test\" }) Once you have this all running, you should be able to see your workflows reflect on both the Temporal UI and Astra UI. \ud83c\udfe0 Back to HOME","title":"\u2022 Temporal"},{"location":"pages/tools/integration/temporal/#overview","text":"Temporal.io is an open source microservice orchestration platform that assists in tracking workflows in your application development. It provides the user with a plug-and-play persistence layer that lets the user choose and configure their Temporal Server with their preferred backend. Currently, Temporal is compatible with Postgres, MySQL, and Apache Cassandra\u24c7 as backend dependencies. \ud83d\udce5 Temporal Quick Install \u2139\ufe0f Introduction to Temporal \u2139\ufe0f Part 1: Introduction to Temporal and Cassandra, Astra DB \u2139\ufe0f Part 2: Connect Temporalio to Astra DB in 5 Easy Steps \u2139\ufe0f Part 3: Connect Temporalio to Astra DB With Kubernetes \ud83c\udfa5 Workflow with Temporal and Astra DB in 5 minutes","title":"Overview"},{"location":"pages/tools/integration/temporal/#-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token Note This runbook was written using Mac OS Monterey but it will also work with Windows. Any Windows-specific instructions will be noted as such.","title":"- Prerequisites"},{"location":"pages/tools/integration/temporal/#installation-and-setup","text":"","title":"Installation and Setup"},{"location":"pages/tools/integration/temporal/#step-1-setup-astra","text":"In your Astra database, create two new keyspaces called \"temporal\" and \"temporal_visibility\". You will be using both of these in the next steps. Make sure to create an Astra token with Admin Role Get your Database ID Find your Database ID in one of two ways: Navigate to your your database and get the last ID in the URL: https://astra.datastax.com/org/.../database/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx Copy and paste the Datacenter ID without the trailing -1 from the Regions section of your Astra Dashboard.","title":"\u2705 Step 1: Setup Astra"},{"location":"pages/tools/integration/temporal/#step-2-temporal-pre-setup","text":"Clone this GitHub repository Navigate to your cloned repository and using your preferred text editor (e.g. VisualStudio or Sublime), update the .env file with your Astra Token and Astra Database ID that you obtained above. ASTRA_TOKEN = <your Astra token> ASTRA_DATABASE_ID = <your DB ID>","title":"\u2705 Step 2: Temporal Pre-setup"},{"location":"pages/tools/integration/temporal/#step-3-temporal-schema-migration-to-astra-db","text":"For this step, you will set up the keyspaces you created earlier in the Astra prerequisites ( temporal and temporal_visibility ). You will be using temporal-cassandra-tool which is part of the Temporal repo and it relies on schema definition. Navigate to your cloned temporal-astra-cql-proxy directory Run the following commands to initialize the keyspaces that we created through Astra. Note that there are two sets of commands, one for temporal keyspace and one for temporal_visibility keyspace: docker-compose -f docker-compose-schema.yaml run temporal-admin-tools \\ -ep cql-proxy -k temporal setup-schema -v 0 .0 docker-compose -f docker-compose-schema.yaml run temporal-admin-tools \\ -ep cql-proxy -k temporal update-schema -d schema/cassandra/temporal/versioned/ docker-compose -f docker-compose-schema.yaml run temporal-admin-tools \\ -ep cql-proxy -k temporal_visibility setup-schema -v 0 .0 docker-compose -f docker-compose-schema.yaml run temporal-admin-tools \\ -ep cql-proxy -k temporal_visibility update-schema -d schema/cassandra/visibility/versioned/ Once the process is completed, you should see a message similar to this: 2022 -03-02T22:23:27.618Z INFO Validating connection to cassandra cluster. { \"logging-call-at\" : \"cqlclient.go:112\" } 2022 -03-02T22:42:53.526Z INFO Connection validation succeeded. { \"logging-call-at\" : \"cqlclient.go:118\" } 2022 -03-02T22:42:53.526Z INFO Starting schema setup { \"config\" : { \"SchemaFilePath\" : \"\" , \"InitialVersion\" : \"0.0\" , \"Overwrite\" :false, \"DisableVersioning\" :false } , \"logging-call-at\" : \"setuptask.go:57\" } 2022 -03-02T22:42:53.526Z DEBUG Setting up version tables { \"logging-call-at\" : \"setuptask.go:67\" } 2022 -03-02T22:42:54.120Z DEBUG Current database schema version 1 .6 is greater than initial schema version 0 .0. Skip version upgrade { \"logging-call-at\" : \"setuptask.go:116\" } 2022 -03-02T22:42:54.120Z INFO Schema setup complete { \"logging-call-at\" : \"setuptask.go:131\" } Great! Your schemas have been migrated with Astra DB. Confirm your tables exist in Astra You can double-check to make sure the correct tables have been created by querying your database in Astra DB\u2019s CQL Console. Run DESC tables; in both your temporal and temporal_visibility keyspaces. You should see there are tables loaded in that were created by the schema migration with temporal-cassandra-tool . token@cqlsh> use temporal ; token@cqlsh:temporal> desc tables ; history_node tasks cluster_metadata_info cluster_membership namespaces cluster_metadata schema_version namespaces_by_id schema_update_history executions queue_metadata queue history_tree token@cqlsh:temporal> use temporal_visibility ; token@cqlsh:temporal_visibility> desc tables ; open_executions schema_update_history schema_version closed_executions","title":"\u2705 Step 3: Temporal Schema Migration to Astra DB"},{"location":"pages/tools/integration/temporal/#step-4-run-docker-compose","text":"In this step, the docker-compose.yaml file is already provided for you in the temporal-astra-cql-proxy repo. This file creates different docker containers to run Temporal server. The persistence layer is configured for you to connect with cql-proxy , and it should pull your Astra credentials from when you set it earlier: services: cql-proxy: container_name: cqlproxy image: datastax/cql-proxy:v ${ CQL_PROXY_VERSION } ... environment: - ASTRA_TOKEN = ${ ASTRA_TOKEN } - ASTRA_DATABASE_ID = ${ ASTRA_DATABASE_ID } - HEALTH_CHECK = true Now you can run the docker-compose command to start up Temporal: docker-compose up","title":"\u2705 Step 4: Run Docker Compose"},{"location":"pages/tools/integration/temporal/#step-5-test-and-validate","text":"You can test your connection and play with your Temporal cluster with these instructions. Make sure to use tctl to create namespaces dedicated to certain workflows: bash-5.0# tctl --namespace test namespace re Namespace test successfully registered. When using the sample apps, keep in mind that you want to modify the starter and worker code so that it points to this specific Temporal deployment. For example: c, err : = client.NewClient ( client.Options { HostPort: \"127.0.0.1:7233\" , Namespace: \"test\" }) Once you have this all running, you should be able to see your workflows reflect on both the Temporal UI and Astra UI. \ud83c\udfe0 Back to HOME","title":"\u2705  Step 5: Test and Validate"},{"location":"pages/tools/integration/vault/","text":"Overview \u00b6 The purpose of this document is to guide you through the process using Astra DB as the storage configuration for your HashiCorp Vault instance. In this tutorial, you will install Vault and edit the configuration file to point to Astra DB. \u2139\ufe0f Introduction to Vault \ud83d\udce5 Vault Quick Install Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Bundle You should Install Vault Clone this repository to use to set up CQL-Proxy which is a sidecar that enables unsupported CQL drivers to work with DataStax Astra You need your Astra Token and Astra Database ID to use CQL-Proxy Follow the steps in the repo to spin up CQL-Proxy using Terminal/Command Line. Once successfully running, you should see the following output: {\"level\":\"info\",\"ts\":1651012815.176512,\"caller\":\"proxy/proxy.go:222\",\"msg\":\"proxy is listening\",\"address\":\"[::]:9042\"} Installation and Setup \u00b6 In the Astra UI, create a keyspace called vault . Navigate to your CQL Console in the Astra UI. Issue the following statement to create a table called entries CREATE TABLE vault. \"entries\" ( bucket text, key text, value blob, PRIMARY KEY ( bucket, key ) ) WITH CLUSTERING ORDER BY ( key ASC ) ; Navigate to your terminal. Create a Vault configuration file config.hcl in your local directory. Edit your config.hcl file. Copy and paste the following to your configuration file: storage \"cassandra\" { hosts = \"localhost\" consistency = \"LOCAL_QUORUM\" protocol_version = 3 } listener \"tcp\" { address = \"127.0.0.1:8200\" tls_disable = \"true\" } api_addr = \"http://127.0.0.1:8200\" cluster_addr = \"https://127.0.0.1:8201\" ui = true 4. Run Vault from your terminal with the following command: vault server -config=config.hcl Successful output should look like this: == > Vault server configuration: Api Address: http://127.0.0.1:8200 Cgo: disabled Cluster Address: https://127.0.0.1:8201 Go Version: go1.17.9 Listener 1 : tcp ( addr: \"127.0.0.1:8200\" , cluster address: \"127.0.0.1:8201\" , max_request_duration: \"1m30s\" , max_request_size: \"33554432\" , tls: \"disabled\" ) Log Level: info Mlock: supported: false, enabled: false Recovery Mode: false Storage: cassandra Version: Vault v1.10.2 Version Sha: 94325865b12662cb72efa3003d6aaa4f5ae57f3a == > Vault server started! Log data will stream in below: Note If you get a warning message about mlock not being supported, that is okay. However, for maximum security you should run Vault on a system that supports mlock. Test and Validate \u00b6 Once you see the above message that you successfully started Vault server, open a new terminal window. Run vault operator init . This will give you 5 Unseal Keys and a Root Token. Vault needs 3 Unseal Keys to properly unseal. Note You may get an error that looks like this: Error initializing: Put \"https://127.0.0.1:8200/v1/sys/init\": http: server gave HTTP response to HTTPS client This is because Vault runs on localhost, but the default address is HTTPS. Instead, you might need to specify the explicit address with the follow command: vault operator init -address=http://127.0.0.1:8200 Once Vault is initialized, it should give you an output of your Unseal Keys: % vault operator init Unseal Key 1 : rVRPym... Unseal Key 2 : 71tY5X... Unseal Key 3 : ETYWDf... Unseal Key 4 : 4mDtrr... Unseal Key 5 : o9X46m... Initial Root Token: hvs.gF14F... Vault initialized with 5 key shares and a key threshold of 3 . Please securely distribute the key shares printed above. When the Vault is re-sealed, restarted, or stopped, you must supply at least 3 of these keys to unseal it before it can start servicing requests. Vault does not store the generated root key. Without at least 3 keys to reconstruct the root key, Vault will remain permanently sealed! Note Make sure to save these keys somewhere safe. This is the only time that Vault will generate these keys. Run the Vault UI at http://127.0.0.1:8200 Enter your Unseal Keys and Root Token You should now be able to access the Vault UI as well as cross-reference your CQL Console to make sure the requests are properly being written to your entries table! Note: When querying from the entries table, you must use double-quotes as entries is a reserved word for CQL. token@cqlsh:vault> use vault ; //Switches to Vault keyspace token@cqlsh:vault> expand on ; //Prints output in readable format token@cqlsh:vault> select * from \"entries\" limit 1 ; //Select statement from \"entries\" table @ Row 1 --------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- bucket | logical/59ca7834-32f6-a70a-8a61-53dce6dd9c18/oidc_provider/provider key | logical/59ca7834-32f6-a70a-8a61-53dce6dd9c18/oidc_provider/provider/default value | 0x0000000102002d31867373d44b1ae2412b4a1a2bd895c3eec2b2db671ec6a8e323e69539cf6d5e1b43e2e11fabc9cc76ad3c77a722caac47cc3f877013df200e4e6d268e6dbff10ba4007cef042643721101e669ae35ff08842e2d1f70e19de2 ( 1 rows ) \ud83c\udfe0 Back to home","title":"\u2022 HashiCorp Vault"},{"location":"pages/tools/integration/vault/#overview","text":"The purpose of this document is to guide you through the process using Astra DB as the storage configuration for your HashiCorp Vault instance. In this tutorial, you will install Vault and edit the configuration file to point to Astra DB. \u2139\ufe0f Introduction to Vault \ud83d\udce5 Vault Quick Install","title":"Overview"},{"location":"pages/tools/integration/vault/#prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token You should Download your Secure Bundle You should Install Vault Clone this repository to use to set up CQL-Proxy which is a sidecar that enables unsupported CQL drivers to work with DataStax Astra You need your Astra Token and Astra Database ID to use CQL-Proxy Follow the steps in the repo to spin up CQL-Proxy using Terminal/Command Line. Once successfully running, you should see the following output: {\"level\":\"info\",\"ts\":1651012815.176512,\"caller\":\"proxy/proxy.go:222\",\"msg\":\"proxy is listening\",\"address\":\"[::]:9042\"}","title":"Prerequisites"},{"location":"pages/tools/integration/vault/#installation-and-setup","text":"In the Astra UI, create a keyspace called vault . Navigate to your CQL Console in the Astra UI. Issue the following statement to create a table called entries CREATE TABLE vault. \"entries\" ( bucket text, key text, value blob, PRIMARY KEY ( bucket, key ) ) WITH CLUSTERING ORDER BY ( key ASC ) ; Navigate to your terminal. Create a Vault configuration file config.hcl in your local directory. Edit your config.hcl file. Copy and paste the following to your configuration file: storage \"cassandra\" { hosts = \"localhost\" consistency = \"LOCAL_QUORUM\" protocol_version = 3 } listener \"tcp\" { address = \"127.0.0.1:8200\" tls_disable = \"true\" } api_addr = \"http://127.0.0.1:8200\" cluster_addr = \"https://127.0.0.1:8201\" ui = true 4. Run Vault from your terminal with the following command: vault server -config=config.hcl Successful output should look like this: == > Vault server configuration: Api Address: http://127.0.0.1:8200 Cgo: disabled Cluster Address: https://127.0.0.1:8201 Go Version: go1.17.9 Listener 1 : tcp ( addr: \"127.0.0.1:8200\" , cluster address: \"127.0.0.1:8201\" , max_request_duration: \"1m30s\" , max_request_size: \"33554432\" , tls: \"disabled\" ) Log Level: info Mlock: supported: false, enabled: false Recovery Mode: false Storage: cassandra Version: Vault v1.10.2 Version Sha: 94325865b12662cb72efa3003d6aaa4f5ae57f3a == > Vault server started! Log data will stream in below: Note If you get a warning message about mlock not being supported, that is okay. However, for maximum security you should run Vault on a system that supports mlock.","title":"Installation and Setup"},{"location":"pages/tools/integration/vault/#test-and-validate","text":"Once you see the above message that you successfully started Vault server, open a new terminal window. Run vault operator init . This will give you 5 Unseal Keys and a Root Token. Vault needs 3 Unseal Keys to properly unseal. Note You may get an error that looks like this: Error initializing: Put \"https://127.0.0.1:8200/v1/sys/init\": http: server gave HTTP response to HTTPS client This is because Vault runs on localhost, but the default address is HTTPS. Instead, you might need to specify the explicit address with the follow command: vault operator init -address=http://127.0.0.1:8200 Once Vault is initialized, it should give you an output of your Unseal Keys: % vault operator init Unseal Key 1 : rVRPym... Unseal Key 2 : 71tY5X... Unseal Key 3 : ETYWDf... Unseal Key 4 : 4mDtrr... Unseal Key 5 : o9X46m... Initial Root Token: hvs.gF14F... Vault initialized with 5 key shares and a key threshold of 3 . Please securely distribute the key shares printed above. When the Vault is re-sealed, restarted, or stopped, you must supply at least 3 of these keys to unseal it before it can start servicing requests. Vault does not store the generated root key. Without at least 3 keys to reconstruct the root key, Vault will remain permanently sealed! Note Make sure to save these keys somewhere safe. This is the only time that Vault will generate these keys. Run the Vault UI at http://127.0.0.1:8200 Enter your Unseal Keys and Root Token You should now be able to access the Vault UI as well as cross-reference your CQL Console to make sure the requests are properly being written to your entries table! Note: When querying from the entries table, you must use double-quotes as entries is a reserved word for CQL. token@cqlsh:vault> use vault ; //Switches to Vault keyspace token@cqlsh:vault> expand on ; //Prints output in readable format token@cqlsh:vault> select * from \"entries\" limit 1 ; //Select statement from \"entries\" table @ Row 1 --------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- bucket | logical/59ca7834-32f6-a70a-8a61-53dce6dd9c18/oidc_provider/provider key | logical/59ca7834-32f6-a70a-8a61-53dce6dd9c18/oidc_provider/provider/default value | 0x0000000102002d31867373d44b1ae2412b4a1a2bd895c3eec2b2db671ec6a8e323e69539cf6d5e1b43e2e11fabc9cc76ad3c77a722caac47cc3f877013df200e4e6d268e6dbff10ba4007cef042643721101e669ae35ff08842e2d1f70e19de2 ( 1 rows ) \ud83c\udfe0 Back to home","title":"Test and Validate"},{"location":"pages/tools/notebooks/datastax-studio/","text":"Overview \u00b6 DataStax Studio is an interactive developer tool for CQL (Cassandra Query Language), Spark SQL, and DSE Graph. Developers and analysts collaborate by mixing code, documentation, query results, and visualizations in self-documenting notebooks. \u2139\ufe0f Introduction to DataStax Studio \ud83d\udce5 DataStax Studio Quick Install Prerequisites \u00b6 DataStax Studio Prerequisites \u00b6 You should have a supported browser You should have a supported version of Java Recommended: OpenJDK 8 Supported: Oracle Java SE 8 (JRE or JDK) Astra Prerequisites \u00b6 You should have an Astra account You should Create an Astra Database You should Have an Astra Token Installation and Setup \u00b6 As mentioned in the Prerequisites above, you must have DataStax Studio already installed. You can follow the quick installation steps here . Once you have successfully installed DataStax Studio, you may proceed to the following steps. Start up DataStax Studio by running the Studio Server shell script: Linux: cd installation_location/datastax-studio-6.8.0 ./bin/server.sh Windows: C:/> cd installation_location \\d atastax-studio-6.8.0 \\b in \\ C:/> server.bat Once Studio is running, your output should look something similar to this: Studio is now running at: http://127.0.0.1:9091 You may now use the localhost URL provided in your terminal or command line to navigate to the DataStax Studio UI. This should look something like this: For this example, we will use the Getting Started with Astra notebook. A notebook is essentially a workspace used to visualize queries from your database, test and run different commands, and more. On the top right corner of the notebook, click default localhost and then Add Connection to configure a new connection for the notebook. A screen should appear with the options Standard Connection and Astra Connection . For this example, you will select Astra Connection . Here, you will need the credentials that you gathered in the Astra Prerequisites . Name: <Your Database Name> Secure Connection Bundle path: <The path to your SCB locally> Client ID: <Your Client ID> Client Secret: <Your Client Secret> Once you have filled this information out, you can select Test in the bottom right corner. If this is successful, you should see a message that says CQL connected successfully . Once this is completed, click Save . In the upper right hand corner, you should be able to switch the connection to the name of the database you just configured. Test and Validate \u00b6 Finally, we will test and validate once more that the connection is validated by submitting a couple test queries. Click the + symbol in the top-middle of the screen to add a new cell. In the cell, you can select which Keyspace that you want to query from. Run the following queries to confirm that the connection to your Astra Database is successful. describe tables; select * from <YOUR_TABLE>; Once you have received the correct results back, that's it! You have successfully connected DataStax Studio to Astra DB and can use this as a tool to help model your queries. You may also scroll down within the Getting Started with Astra notebook for more examples and recommendations. \ud83c\udfe0 Back to HOME","title":"\u2022 DataStax Studio"},{"location":"pages/tools/notebooks/datastax-studio/#overview","text":"DataStax Studio is an interactive developer tool for CQL (Cassandra Query Language), Spark SQL, and DSE Graph. Developers and analysts collaborate by mixing code, documentation, query results, and visualizations in self-documenting notebooks. \u2139\ufe0f Introduction to DataStax Studio \ud83d\udce5 DataStax Studio Quick Install","title":"Overview"},{"location":"pages/tools/notebooks/datastax-studio/#prerequisites","text":"","title":"Prerequisites"},{"location":"pages/tools/notebooks/datastax-studio/#datastax-studio-prerequisites","text":"You should have a supported browser You should have a supported version of Java Recommended: OpenJDK 8 Supported: Oracle Java SE 8 (JRE or JDK)","title":"DataStax Studio Prerequisites"},{"location":"pages/tools/notebooks/datastax-studio/#astra-prerequisites","text":"You should have an Astra account You should Create an Astra Database You should Have an Astra Token","title":"Astra Prerequisites"},{"location":"pages/tools/notebooks/datastax-studio/#installation-and-setup","text":"As mentioned in the Prerequisites above, you must have DataStax Studio already installed. You can follow the quick installation steps here . Once you have successfully installed DataStax Studio, you may proceed to the following steps. Start up DataStax Studio by running the Studio Server shell script: Linux: cd installation_location/datastax-studio-6.8.0 ./bin/server.sh Windows: C:/> cd installation_location \\d atastax-studio-6.8.0 \\b in \\ C:/> server.bat Once Studio is running, your output should look something similar to this: Studio is now running at: http://127.0.0.1:9091 You may now use the localhost URL provided in your terminal or command line to navigate to the DataStax Studio UI. This should look something like this: For this example, we will use the Getting Started with Astra notebook. A notebook is essentially a workspace used to visualize queries from your database, test and run different commands, and more. On the top right corner of the notebook, click default localhost and then Add Connection to configure a new connection for the notebook. A screen should appear with the options Standard Connection and Astra Connection . For this example, you will select Astra Connection . Here, you will need the credentials that you gathered in the Astra Prerequisites . Name: <Your Database Name> Secure Connection Bundle path: <The path to your SCB locally> Client ID: <Your Client ID> Client Secret: <Your Client Secret> Once you have filled this information out, you can select Test in the bottom right corner. If this is successful, you should see a message that says CQL connected successfully . Once this is completed, click Save . In the upper right hand corner, you should be able to switch the connection to the name of the database you just configured.","title":"Installation and Setup"},{"location":"pages/tools/notebooks/datastax-studio/#test-and-validate","text":"Finally, we will test and validate once more that the connection is validated by submitting a couple test queries. Click the + symbol in the top-middle of the screen to add a new cell. In the cell, you can select which Keyspace that you want to query from. Run the following queries to confirm that the connection to your Astra Database is successful. describe tables; select * from <YOUR_TABLE>; Once you have received the correct results back, that's it! You have successfully connected DataStax Studio to Astra DB and can use this as a tool to help model your queries. You may also scroll down within the Getting Started with Astra notebook for more examples and recommendations. \ud83c\udfe0 Back to HOME","title":"Test and Validate"},{"location":"pages/tools/notebooks/jupyter/","text":"Check back soon Nothing to see here yet! Check back for updates!","title":"\u2022 Jupyter Notebooks"},{"location":"pages/tools/notebooks/zeppelin/","text":"Check back soon Nothing to see here yet! Check back for updates!","title":"\u2022 Apache Zeppelin"},{"location":"pages/tools/plugins/astradb-vault-plugin/","text":"Overview \u00b6 DataStax Astra DB Plugin for HashiCorp Vault is an open-source project that adds robust token lifecycle management features for Astra DB. Due to the nature of the Astra DB object hierarchy, by default, API tokens are not associated with specific users and currently the tokens do not have metadata descriptions. For more details, see the full Astra DB Plugin for HashiCorp Vault documentation in the plugin\u2019s open-source GitHub repo. Without the plugin, it\u2019s easy to lose track of: Who created tokens The purpose of each token Which tokens are being used actively Consequently, there\u2019s no audit trail of who has downloaded and used tokens, and there\u2019s no tracking regarding who may have manually shared tokens with others. Astra DB Plugin for HashiCorp Vault solves these security management issues. To ensure that your token ownership and usage are well understood, the plugin gives you the ability to associate metadata with tokens\u2014such as the user who created each token, and what it is being used for. The plugin also logs who has accessed the tokens. What is Hashi Vault? \u00b6 HashiCorp Vault is a widely-used solution across the tech industry. It\u2019s an identity-based secrets and encryption management system. HashiCorp Vault from HashiCorp provides key-value encryption services that are gated by authentication and authorization methods. Access to tokens, secrets, and other sensitive data are securely stored, managed, and tightly controlled. Audit trails are provided. HashiCorp Vault is also extensible via a variety of interfaces, allowing plugins (including Astra DB Plugin for HashiCorp Vault) to contribute to this ecosystem. What's next? \u00b6 See the full Astra DB Plugin for HashiCorp Vault documentation in the plugin\u2019s open-source GitHub repo.","title":"\u2022 HashiCorp Vault"},{"location":"pages/tools/plugins/astradb-vault-plugin/#overview","text":"DataStax Astra DB Plugin for HashiCorp Vault is an open-source project that adds robust token lifecycle management features for Astra DB. Due to the nature of the Astra DB object hierarchy, by default, API tokens are not associated with specific users and currently the tokens do not have metadata descriptions. For more details, see the full Astra DB Plugin for HashiCorp Vault documentation in the plugin\u2019s open-source GitHub repo. Without the plugin, it\u2019s easy to lose track of: Who created tokens The purpose of each token Which tokens are being used actively Consequently, there\u2019s no audit trail of who has downloaded and used tokens, and there\u2019s no tracking regarding who may have manually shared tokens with others. Astra DB Plugin for HashiCorp Vault solves these security management issues. To ensure that your token ownership and usage are well understood, the plugin gives you the ability to associate metadata with tokens\u2014such as the user who created each token, and what it is being used for. The plugin also logs who has accessed the tokens.","title":"Overview"},{"location":"pages/tools/plugins/astradb-vault-plugin/#what-is-hashi-vault","text":"HashiCorp Vault is a widely-used solution across the tech industry. It\u2019s an identity-based secrets and encryption management system. HashiCorp Vault from HashiCorp provides key-value encryption services that are gated by authentication and authorization methods. Access to tokens, secrets, and other sensitive data are securely stored, managed, and tightly controlled. Audit trails are provided. HashiCorp Vault is also extensible via a variety of interfaces, allowing plugins (including Astra DB Plugin for HashiCorp Vault) to contribute to this ecosystem.","title":"What is Hashi Vault?"},{"location":"pages/tools/plugins/astradb-vault-plugin/#whats-next","text":"See the full Astra DB Plugin for HashiCorp Vault documentation in the plugin\u2019s open-source GitHub repo.","title":"What's next?"}]}