{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbae6dd2-2d8e-48cf-bebc-fc6e49868e91",
   "metadata": {},
   "source": [
    "# SageMaker + Astra DB, integration example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661f23b6-9a44-4787-bd82-0416aeed9a52",
   "metadata": {},
   "source": [
    "Use an LLM and an Embedding model from Amazon SageMaker and a Vector Store from [Astra DB](https://astra.datastax.com) to run a simple RAG-based application.\n",
    "\n",
    "In this notebook, you will:\n",
    "- either deploy Embedding model and LLM, or connect to existing ones in SageMaker, and see them in action;\n",
    "- Connect with Astra DB and create a Vector Store in it;\n",
    "- populate it with example \"pretend entomology\" information;\n",
    "- run an AI-powered entomology assistant to help identification of field insect observations.\n",
    "\n",
    "> Note: this notebook is designed to run within Amazon SageMaker Studio. See [this page](https://awesome-astra.github.io/docs/pages/aiml/aws/aws-sagemaker/) for more information and references."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c0fe8-998d-4594-b010-95cf0c7f3dd1",
   "metadata": {},
   "source": [
    "## General setup\n",
    "\n",
    "_Note: you may see some dependency-resolution error in the output from `pip` here. Do not pay too much attention: the rest of this notebook will work just fine._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd118138-60cc-406f-af74-a8dca0a5868d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --quiet \\\n",
    "    \"sagemaker==2.193.0\" \\\n",
    "    \"langchain==0.0.317\" \\\n",
    "    \"cassio>=0.1.3\" \\\n",
    "    \"datasets==2.14.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7db92cd9-3999-4a6f-ba8d-b3a7d8f17093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Optional, Any\n",
    "import json\n",
    "\n",
    "\n",
    "import boto3\n",
    "import cassio\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris, model_uris\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.base_serializers import JSONSerializer\n",
    "from sagemaker.base_deserializers import JSONDeserializer\n",
    "\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.vectorstores import Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "024ac83b-dfa4-4f6a-994d-eb45ed2d1fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "boto3_sm_client = boto3.client('runtime.sagemaker')\n",
    "region_name = boto3.Session().region_name\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc084f2-480d-4953-9dc3-ffd36e305d45",
   "metadata": {},
   "source": [
    "#### Prepare a function that specializes the default SageMaker \"Predictor\": it will be later supplied when creating the `Model` objects.\n",
    "\n",
    "In some cases one can pass a `Model` out of the box, but for our models you want to specify to use JSON serialization/deserialization when interacting with the model endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a3e589-e11e-473b-9a15-3c9cc3773658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_json_predictor(*pargs, **kwargs):\n",
    "    return Predictor(*pargs, **kwargs,\n",
    "                     serializer=JSONSerializer(),\n",
    "                     deserializer=JSONDeserializer(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92fb7b-1f89-4ae0-94db-3df2ea337f87",
   "metadata": {},
   "source": [
    "## Embedding model, setup\n",
    "\n",
    "Here you can choose between a model already deployed in the UI and a programmatic deploy throug the SageMaker SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ada861aa-c567-41cf-ae04-d11e0e6a78cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the *embedding model* endpoint name if already deployed (leave empty if deploying with SDK): jumpstart-dft-yo-hf-textembedding-gpt-j-6b\n"
     ]
    }
   ],
   "source": [
    "emb_endpoint_supplied = False\n",
    "\n",
    "emb_endpoint_name = input(\"Enter the *embedding model* endpoint name if already deployed (leave empty if deploying with SDK):\").strip()\n",
    "\n",
    "if emb_endpoint_name == \"\":\n",
    "    print(f\"\\n{'*' * 101}\")\n",
    "    print(\"*** INFO: the embedding model will be deployed programmatically, as no endpoint name was provided. **\")\n",
    "    print(\"***       Re-run this cell and supply the endpoint name if this is incorrect.                      **\")\n",
    "    print(f\"{'*' * 101}\")\n",
    "else:\n",
    "    emb_endpoint_supplied = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec4fbc2-a3d1-48ff-b5cd-45ddda2d78a3",
   "metadata": {},
   "source": [
    "The following cells will go through the steps required for programmatic deployment of a JumpStart model through the SageMaker SDK.\n",
    "\n",
    "Note that they will do nothing else than print a message, instead, if the embedding model endpoint has been given already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3155e93-eb44-423e-883b-6e0702099348",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nothing to do in this case)\n"
     ]
    }
   ],
   "source": [
    "if not emb_endpoint_supplied:\n",
    "    emb_model_id = \"huggingface-textembedding-gpt-j-6b\"\n",
    "    # Visit https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html for the model IDs\n",
    "    emb_endpoint_name = name_from_base(emb_model_id)\n",
    "    print(f\"[INFO] Embedding endpoint name = '{emb_endpoint_name}'\")\n",
    "    emb_instance_type = \"ml.g5.24xlarge\"\n",
    "    emb_model_version = \"*\"\n",
    "    emb_model_env = {}\n",
    "\n",
    "    emb_deploy_image_uri = image_uris.retrieve(\n",
    "        region=None,\n",
    "        framework=None,\n",
    "        image_scope=\"inference\",\n",
    "        model_id=emb_model_id,\n",
    "        model_version=emb_model_version,\n",
    "        instance_type=emb_instance_type,\n",
    "    )\n",
    "else:\n",
    "    print(\"(nothing to do in this case)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55109846-7c80-43d6-a6e0-65c4eca9a8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nothing to do in this case)\n"
     ]
    }
   ],
   "source": [
    "if not emb_endpoint_supplied:\n",
    "    emb_model_uri = model_uris.retrieve(\n",
    "        model_id=emb_model_id,\n",
    "        model_version=emb_model_version,\n",
    "        model_scope=\"inference\",\n",
    "    )\n",
    "    emb_model_inference = Model(\n",
    "        image_uri=emb_deploy_image_uri,\n",
    "        model_data=emb_model_uri,\n",
    "        role=aws_role,\n",
    "        predictor_cls=my_json_predictor,\n",
    "        name=emb_endpoint_name,\n",
    "        env=emb_model_env,\n",
    "    )\n",
    "else:\n",
    "    print(\"(nothing to do in this case)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c00c54-e78f-4203-8a95-3e1b86d7bacf",
   "metadata": {},
   "source": [
    "#### This is the actual deploy step.\n",
    "\n",
    "> _Note: this cell may take even **ten minutes** to complete. You may check the SageMaker Studio 'endpoints' tab while this is running._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44bb6ace-174f-47d1-b866-8e036cdc2464",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nothing to do in this case)\n"
     ]
    }
   ],
   "source": [
    "if not emb_endpoint_supplied:\n",
    "    print(\"*** About to start the embedding model deploy ...\\n\")\n",
    "    emb_predictor = emb_model_inference.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=emb_instance_type,\n",
    "        predictor_cls=my_json_predictor,\n",
    "        endpoint_name=emb_endpoint_name,\n",
    "    )\n",
    "    print(\"\\n*** Embedding model deploy completed.\")\n",
    "else:\n",
    "    print(\"(nothing to do in this case)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75f8f8-2fd5-434d-ad8d-6fcaa2d792b1",
   "metadata": {},
   "source": [
    "## Embedding model, LangChain setup\n",
    "\n",
    "To be able to work with the shape of the input and output specific to _this_ embedding model, we need to create and supply a suitable `EmbeddingsContentHandler` when instantiating the LangChain abstraction for the SageMaker embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f48e98a-90b0-4550-8f72-28d7e9ad2879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SageMakerGPTJ6BContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        input_encoded = json.dumps({\n",
    "            \"text_inputs\": inputs,\n",
    "            **model_kwargs,\n",
    "        }).encode(\"utf-8\")\n",
    "        return input_encoded\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        `output` is actually a botocore.response.StreamingBody object in our case\n",
    "        \"\"\"\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"embedding\"]\n",
    "\n",
    "\n",
    "emb_content_handler = SageMakerGPTJ6BContentHandler()\n",
    "\n",
    "embeddings = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=emb_endpoint_name,\n",
    "    region_name=region_name,\n",
    "    content_handler=emb_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76691705-8794-480c-8fce-807adfc65bf7",
   "metadata": {},
   "source": [
    "### Embedding model, test invocation through LangChain\n",
    "\n",
    "As a simple test, we check that the model returns vectors normalized to having unit norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c851524f-8e62-42b5-897b-68bbeb5b5333",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of 'vector1': 1.0000\n",
      "Norms of 'vectors'\n",
      "    [0] norm = 1.0000\n",
      "    [1] norm = 1.0000\n"
     ]
    }
   ],
   "source": [
    "vector1 = embeddings.embed_query(\"Hello, SageMaker\")\n",
    "vectors = embeddings.embed_documents([\"Can you embed multiple sentences at once?\", \"Sure, you can.\"])\n",
    "\n",
    "print(f\"Norm of 'vector1': {sum(x*x for x in vector1):.4f}\")\n",
    "\n",
    "print(\"Norms of 'vectors'\")\n",
    "for i, v in enumerate(vectors):\n",
    "    print(f\"    [{i}] norm = {sum(x*x for x in v):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b6900-8e84-48d2-9a4d-1f1112ca1e33",
   "metadata": {},
   "source": [
    "## LLM, setup\n",
    "\n",
    "Here you can choose between a model already deployed in the UI and a programmatic deploy throug the SageMaker SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f35f11c9-3d4f-4f90-917d-b067ea431fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the *LLM* endpoint name if already deployed (leave empty if deploying with SDK): jumpstart-dft-my2-meta-textgeneration-llama-2-70b-f\n"
     ]
    }
   ],
   "source": [
    "llm_endpoint_supplied = False\n",
    "\n",
    "llm_endpoint_name = input(\"Enter the *LLM* endpoint name if already deployed (leave empty if deploying with SDK):\").strip()\n",
    "\n",
    "if llm_endpoint_name == \"\":\n",
    "    print(f\"\\n{'*' * 89}\")\n",
    "    print(\"*** INFO: the LLM will be deployed programmatically, as no endpoint name was provided. **\")\n",
    "    print(\"***       Re-run this cell and supply the endpoint name if this is incorrect.          **\")\n",
    "    print(f\"{'*' * 89}\")\n",
    "else:\n",
    "    llm_endpoint_supplied = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc628aad-ba67-41d8-84a9-0ad026ecbdab",
   "metadata": {},
   "source": [
    "The following cells work similarly to the embedding model deployment seen earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d41d49d-dabb-47b9-bdfb-ed112af97465",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nothing to do in this case)\n"
     ]
    }
   ],
   "source": [
    "if not llm_endpoint_supplied:\n",
    "    llm_model_id = \"meta-textgeneration-llama-2-70b-f\"\n",
    "    # Visit https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html for the model IDs\n",
    "    llm_endpoint_name = name_from_base(llm_model_id)\n",
    "    print(f\"[INFO] LLM endpoint name = '{llm_endpoint_name}'\")\n",
    "    llm_instance_type = \"ml.g5.48xlarge\"\n",
    "    llm_model_version = \"*\"\n",
    "    llm_model_env = {}\n",
    "    # llm_model_env = {\"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\", \"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\"} # TODO: check if relevant\n",
    "\n",
    "    llm_deploy_image_uri = image_uris.retrieve(\n",
    "        region=None,\n",
    "        framework=None,\n",
    "        image_scope=\"inference\",\n",
    "        model_id=llm_model_id,\n",
    "        model_version=llm_model_version,\n",
    "        instance_type=llm_instance_type,\n",
    "    )\n",
    "else:\n",
    "    print(\"(nothing to do in this case)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d7292c5-a38d-4c29-8bc9-456860559b87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nothing to do in this case)\n"
     ]
    }
   ],
   "source": [
    "if not llm_endpoint_supplied:\n",
    "    llm_model_uri = model_uris.retrieve(\n",
    "        model_id=llm_model_id,\n",
    "        model_version=llm_model_version,\n",
    "        model_scope=\"inference\",\n",
    "    )\n",
    "\n",
    "    llm_model_inference = Model(\n",
    "        image_uri=llm_deploy_image_uri,\n",
    "        model_data=llm_model_uri,\n",
    "        role=aws_role,\n",
    "        predictor_cls=my_json_predictor,\n",
    "        name=llm_endpoint_name,\n",
    "        env=llm_model_env,\n",
    "    )\n",
    "else:\n",
    "    print(\"(nothing to do in this case)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d0ee7e-1d19-4b74-b393-785f1fc5c27a",
   "metadata": {},
   "source": [
    "#### This is the actual deploy step.\n",
    "\n",
    "> _Note: this cell may take even **twenty minutes or so** to complete. You may check the SageMaker Studio 'endpoints' tab while this is running._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b5ba4ff-8aa8-4e59-acb8-fd5e4ca2e3f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nothing to do in this case)\n"
     ]
    }
   ],
   "source": [
    "if not llm_endpoint_supplied:\n",
    "    print(\"*** About to start the LLM deploy ...\\n\")\n",
    "    llm_predictor = llm_model_inference.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=llm_instance_type,\n",
    "        predictor_cls=my_json_predictor,\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "    )\n",
    "    print(\"\\n*** LLM deploy completed.\")\n",
    "else:\n",
    "    print(\"(nothing to do in this case)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49314b60-596d-4ecd-9337-d66efef2dbb7",
   "metadata": {},
   "source": [
    "## LLM, LangChain setup\n",
    "\n",
    "Similarly as what was done for the embedding model, we need to provide a \"Content Handler\" tailored to the specific signature of this LLM.\n",
    "\n",
    "While we are at it, we allow for custom \"system role\" instructions to be coded in the actual payload to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a24cf8a-6c53-4a78-ad94-97ddb9769953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_ROLE_INSTRUCTIONS = \"You are a helpful chat assistant.\"\n",
    "\n",
    "class Llama2_70BChatContentHandler(LLMContentHandler):\n",
    "\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    system_role_instructions: str\n",
    "\n",
    "    def __init__(self, *pargs, system_role_instructions: str = DEFAULT_SYSTEM_ROLE_INSTRUCTIONS, **kwargs):\n",
    "        self.system_role_instructions = system_role_instructions\n",
    "        LLMContentHandler.__init__(self, *pargs, **kwargs)\n",
    "    \n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_dict = {\n",
    "            \"inputs\": [\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": self.system_role_instructions},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            ],\n",
    "            \"parameters\": model_kwargs,\n",
    "        }\n",
    "        return json.dumps(input_dict).encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generation\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "llm_content_handler = Llama2_70BChatContentHandler(\n",
    "    system_role_instructions=(\n",
    "        \"You are a helpful biology chat assistant; your task is to fulfill the incoming requests \"\n",
    "        \"truthfully and with scientific accuracy, also cutting the clutter and pleasantries.\"\n",
    "    ),\n",
    ")\n",
    "    \n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=llm_endpoint_name,\n",
    "    region_name=region_name,\n",
    "    model_kwargs= {\"max_new_tokens\": 1024, \"top_p\": 0.4, \"temperature\": 0.8},\n",
    "    content_handler=llm_content_handler,\n",
    "    # model-specific (Llama requires acceptance of the EULA)\n",
    "    endpoint_kwargs={\n",
    "        'CustomAttributes': 'accept_eula=true',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e083aab-14b6-4272-83b2-cf9d18bf7395",
   "metadata": {},
   "source": [
    "_A note about the `endpoint_kwargs` parameter._\n",
    "\n",
    "As mentioned earlier, for this model each LLM call must carry a special header to signal acceptance of the EULA. This is accomplished,\n",
    "at the LangChain level, by passing this parameter when creating the `SagemakerEndpoint` instance. For reference, you can check how this paramter\n",
    "is used within the LangChain code ([check the code](https://github.com/langchain-ai/langchain/blob/7db6aabf65e70811e40ee6f2e1ba8e0425ba81c9/libs/langchain/langchain/llms/sagemaker_endpoint.py#L359C23-L359C39)).\n",
    "Essentially the EULA acceptance flag is passed down to the underlying `boto3` library, whose `invoke_endpoint` method accepts the `CustomAttributes` parameter\n",
    "([check the docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint.html#invoke-endpoint))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee60c6-b47a-4fd0-b665-0b87974a9ca9",
   "metadata": {},
   "source": [
    "### LLM, test invocation through LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0db63a9f-3309-4c16-af5c-bf703c2043d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolf eats mouse.\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"Would a wolf eat a mouse, or would the mouse eat the wolf? Keep the answer as short as possible.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b69f6f-6bc8-460a-90ee-e00e93fe4d2a",
   "metadata": {},
   "source": [
    "## Vector store on Astra DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c948036-b0f3-4005-859c-3ddda4b3b15f",
   "metadata": {},
   "source": [
    "In this section, first initialize `cassio` globally so that it establishes a connection to your database (your connection secrets must be provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "928c948b-65a6-4935-9055-e21269013144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ASTRA_DB_ID = input(\"Enter your Astra DB ID ('0123abcd-'):\")\n",
    "ASTRA_DB_APPLICATION_TOKEN = getpass(\"Enter your Astra DB Token ('AstraCS:...'):\")\n",
    "ASTRA_DB_KEYSPACE = input(\"Enter your keyspace name (optional, default keyspace used if not provided):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8724b8f6-1818-4b8d-bb16-ef89c9d27a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cassio.init(\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "    database_id=ASTRA_DB_ID,\n",
    "    keyspace=ASTRA_DB_KEYSPACE if ASTRA_DB_KEYSPACE else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f98d5-a4e6-4b98-946b-c60417a98bd5",
   "metadata": {},
   "source": [
    "Now a vector store is created, ready for use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "833b848b-65bb-45f6-be81-4c5fcdb76056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "astra_v_store = Cassandra(\n",
    "    session=None,   # <-- meaning: fall back to globally-set \"cassio.init()\" connection\n",
    "    keyspace=None,  # <-- meaning: fall back to globally-set \"cassio.init()\" connection\n",
    "    table_name=\"sagemaker_demo_v_store\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb2229a-936c-4904-83e4-665571bdc999",
   "metadata": {},
   "source": [
    "A small example dataset is loaded through HuggingFace. You can print a sample item to get an idea of its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98a48730-7bf3-4104-9175-957856e529c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 32 entries\n",
      "Example entry:\n",
      "    {\n",
      "        \"id\": \"psilodermis_spectrus\",\n",
      "        \"name\": \"Psilodermis spectrus\",\n",
      "        \"order\": \"Coleoptera\",\n",
      "        \"description\": \"Beetle with transparent wings and a spec...\"\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "sample_dataset = load_dataset(\"datastax/entomology\")[\"train\"]\n",
    "\n",
    "def _shorten(dct): return {k: v if len(v) < 40 else v[:40]+\"...\" for k, v in dct.items()}\n",
    "\n",
    "print(f\"Loaded {len(sample_dataset)} entries\")\n",
    "print(\"Example entry:\")\n",
    "print(\"\\n\".join(\n",
    "    f\"    {l}\" for l in json.dumps(_shorten(sample_dataset[19]), indent=4).split(\"\\n\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3f28e-7748-451c-b01b-e11b76148f2b",
   "metadata": {},
   "source": [
    "The dataset is prepared for insertion in the vector store:\n",
    "\n",
    "_(Note: Care is taken of calculating IDs deterministically to avoid accidental creation of duplicates in case the `add_texts` cell is run repeatedly.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2addd01-b3ae-4375-b6a3-14f569f402db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from `texts`:\n",
      "    \"Beetle with transparent wings and a spec...\"\n",
      "Example from `metadatas`:\n",
      "    {'name': 'Psilodermis spectrus', 'order': 'Coleoptera'}\n",
      "Example from `ids`:\n",
      "    \"psilodermis_spectrus\"\n"
     ]
    }
   ],
   "source": [
    "texts = [entry[\"description\"] for entry in sample_dataset]\n",
    "metadatas = [\n",
    "    {\n",
    "        \"name\": entry[\"name\"],\n",
    "        \"order\": entry[\"order\"],\n",
    "    }\n",
    "    for entry in sample_dataset\n",
    "]\n",
    "ids = [entry[\"name\"].lower().replace(\" \", \"_\") for entry in sample_dataset]\n",
    "\n",
    "print(f\"Example from `texts`:\\n    \\\"{texts[19][:40]}...\\\"\")\n",
    "print(f\"Example from `metadatas`:\\n    {metadatas[19]}\")\n",
    "print(f\"Example from `ids`:\\n    \\\"{ids[19]}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d65238-89ad-4008-926e-d169342ca474",
   "metadata": {},
   "source": [
    "This is where the writes take place (and the embedding vectors are calculated for each item in `texts`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c0982a0-ae91-4e98-a6a3-5690f0026e12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted: glitterus_aurorae, luminastrum_nocturnalis, prismaticus_geminus, cryptocicada_my... (32 items)\n"
     ]
    }
   ],
   "source": [
    "inserted_ids = astra_v_store.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
    "\n",
    "print(f\"Inserted: {', '.join(inserted_ids)[:80]}... ({len(inserted_ids)} items)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d46e04-e5f4-4056-8250-95db5315f3af",
   "metadata": {},
   "source": [
    "## Set up the full pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0a7d4-1af2-44b4-8d77-306de3678700",
   "metadata": {},
   "source": [
    "### Retrieval part\n",
    "\n",
    "Package the search part of the flow in a handy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2cc4fee-db34-42db-9555-dec8d619fbf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_similar_entries(description, k=3, order=None):\n",
    "    if order:\n",
    "        md = {\"order\": order}\n",
    "    else:\n",
    "        md = {}\n",
    "    documents = astra_v_store.similarity_search(description, k=k, filter=md)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe8156ed-84cc-44f9-925c-0b19973ac022",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Bright blue body, wings edged with a golden hue. Often seen hovering above ponds and lakes during springtime. Characteristic double-wing beat.', metadata={'name': 'Hexaplectra azurea', 'order': 'Odonata'}), Document(page_content='Ruby-red dragonfly with black-tipped wings. Fast flyer, and can often be seen darting between flowers on sunny days.', metadata={'name': 'Rubroptera rosetta', 'order': 'Odonata'})]\n"
     ]
    }
   ],
   "source": [
    "print(find_similar_entries(\"Long wings with brown spots, flies erratically, thin legs\", k=2, order=\"Odonata\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e573b4b9-8c04-45be-a26c-b7e71bc1b9b6",
   "metadata": {},
   "source": [
    "### Generation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b09987f9-d37f-4959-adf1-f0eec4867304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"You are an expert entomologist tasked with helping specimen identification on the field.\n",
    "You are given relevant excerpts from an invertebrate textbook along with my field observation.\n",
    "Your task is to compare my observation with the textbook excerpts and come to an identification, explaining why you came to that conclusion and giving the degree of certainity.\n",
    "Only use the information provided in the user observation to come to your conclusion!\n",
    "Be sure to provide, in your verdict, the species' Order together with the full Latin name.\n",
    "\n",
    "USER OBSERVATION: {description}\n",
    "\n",
    "TEXTBOOK CANDIDATE MATCHES:\n",
    "{candidates}\n",
    "\n",
    "YOUR EXPLAINED IDENTIFICATION:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130f9f1-5657-4a51-9f23-92dd4b95c15b",
   "metadata": {},
   "source": [
    "The above is the prompt that will be used in the full RAG pipeline. Another handy tool is a utility function to turn the returned items from the vector store into a single-string description, ready for insertion in the prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8120062-8b27-4b1b-83e5-c03f0d30dcb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def describe_candidates(matches):\n",
    "    return \"\\n\".join([\n",
    "        f\"Candidate species {i+1}: '{doc.metadata['name']}' (order: {doc.metadata['order']})\\nDescription: {doc.page_content}\\n\"\n",
    "        for i, doc in enumerate(matches)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f213c03c-4019-48b9-8d30-432a4cae3d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate species 1: 'Hexaplectra azurea' (order: Odonata)\n",
      "Description: Bright blue body, wings edged with a golden hue. Often seen hovering above ponds and lakes during springtime. Characteristic double-wing beat.\n",
      "\n",
      "Candidate species 2: 'Rubroptera rosetta' (order: Odonata)\n",
      "Description: Ruby-red dragonfly with black-tipped wings. Fast flyer, and can often be seen darting between flowers on sunny days.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(describe_candidates(find_similar_entries(\"Long wings with brown spots, flies erratically, thin legs\", k=2, order=\"Odonata\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9eeed-505c-4694-bfb8-d89e64cda56e",
   "metadata": {},
   "source": [
    "This is the main function implementing the complete RAG pipeline: search, construction of the prompt, and invocation of the LLM to get the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cfb02e0-86ec-4d4f-9213-cfb0d343e44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def identify_and_suggest(description, order=None):\n",
    "    matches = find_similar_entries(description, k=3, order=order)\n",
    "    candidates_text = describe_candidates(matches)\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        description=description,\n",
    "        candidates=candidates_text,\n",
    "    )\n",
    "    return llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f573031-731d-4482-97fb-52f242f138e7",
   "metadata": {},
   "source": [
    "### Putting it all to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5ca2e2a-4dbf-48a1-a25b-6bdf30874449",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your observation of a large butterfly with elongated wing tips and a yellow spot in the middle of each wing, I would identify the species as 'Diamantis glittoris' (order: Lepidoptera).\n",
      "\n",
      "The description of 'Diamantis glittoris' matches your observation in several ways. Firstly, the butterfly is described as having sparkling wings that shine like diamonds in sunlight, which aligns with your observation of a yellow spot in the middle of each wing. Additionally, the species is known to have a slender body and short, clubbed antennae, which is consistent with your description of a large butterfly with elongated wing tips.\n",
      "\n",
      "Furthermore, 'Diamantis glittoris' is known to inhabit flowery gardens and coastal habitats, which could explain why you spotted the butterfly in the field.\n",
      "\n",
      "My degree of certainty for this identification is moderate, as there could be other species that match your observation. However, based on the information provided, 'Diamantis glittoris' seems to be the most likely candidate.\n",
      "\n",
      "Full Latin name: Diamantis glittoris (order: Lepidoptera)\n"
     ]
    }
   ],
   "source": [
    "print(identify_and_suggest(\"A large butterfly with elongated wing tips and a yellow spot in the middle of each wing.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e486732b-2c7b-49bf-b472-bda4802dbcc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your observation of an elongated brown body with small wings, dark elytra, and sturdy antennae, I would identify the insect you found as a member of the order Coleoptera, specifically the species Psilodermis spectrus.\n",
      "\n",
      "The description of Psilodermis spectrus matches your observation in several ways. Firstly, the beetle has transparent wings, which fits with your observation of small wings. Secondly, the body of the beetle is slender and colorless, which aligns with your description of an elongated brown body. Additionally, the species is known to have faint, shifting colors on its back, which could explain the dark elytra you observed. Finally, the fact that the insect was found in a meadow suggests that it may have been attracted to the vegetation, which is consistent with the species' known habitat of haunted forests and eerie swamps.\n",
      "\n",
      "While the other two candidate species, Anthroptila punctatus and Zephyrella albis, do share some similarities with your observation, they do not match as closely as Psilodermis spectrus. Anthroptila punctatus has delicate wings with a wingspan of 3 inches, which is not consistent with your observation of small wings. Zephyrella albis has a small, pale white body and translucent wings, but it is not known to have dark elytra or sturdy antennae.\n",
      "\n",
      "Therefore, based on the information provided, I would identify the insect you found as Psilodermis spectrus, order Coleoptera, with a degree of certainty of 80%.\n"
     ]
    }
   ],
   "source": [
    "print(identify_and_suggest(\"I found an elongated brown but with small wings, dark elitra and sturdy antennae in a meadow.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0e32bbd-bc24-4f32-8095-35980b633b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear User,\n",
      "\n",
      "Thank you for your observation. Based on your description of a moving, leaf-like entity without antennae, I would identify the organism you encountered as Neonymphalis radiatus, a species of butterfly in the order Lepidoptera.\n",
      "\n",
      "The absence of antennae is not unusual for a butterfly, as many species have reduced or absent antennae. The description of the wings as \"neon-bright, radiant patterns\" matches the characteristic appearance of Neonymphalis radiatus, which has vibrant, iridescent markings on its wings. The robust body and glowing tips on the antennae also align with the description of this species.\n",
      "\n",
      "While Cryptoclytra mirabilis and Psilodermis spectrus are both intriguing species, they do not match the details of your observation as closely. Cryptoclytra mirabilis has a more elongated body and long, slender antennae, and is typically found in historic buildings and libraries, whereas Psilodermis spectrus has a slender, colorless body and is found in haunted forests and eerie swamps.\n",
      "\n",
      "Therefore, based on the information provided, I would conclude that the organism you encountered was most likely Neonymphalis radiatus, a species of butterfly in the order Lepidoptera.\n",
      "\n",
      "Degree of certainty: 80%\n",
      "\n",
      "I hope this helps! If you have any further questions or observations, please feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "print(identify_and_suggest(\"What looked like a leaf was in fact moving! It startled me greatly. But I'm not sure it's an insect, I did not see antennae. What was it?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8234b34-7cae-4185-bf39-79b4e65c17ac",
   "metadata": {},
   "source": [
    "### The \"final app\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e7ba2-8e8b-42d1-af05-9355a4cc6e73",
   "metadata": {},
   "source": [
    "The loop below is a simple \"app\" to repeatedly interact with the entomology assistant:\n",
    "\n",
    "- Try it with simple observations such as _I found a strange bug in the library, whose appearance was that of an old piece of paper. What was it?_\n",
    "- Enter an empty input to end the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bb8adbe-6dfc-4b05-aa18-eb662bb98343",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "Enter your field observation:  I found a strange bug in the library, whose appearance was that of an old piece of paper. What was it?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Result ==> Based on your observation of the bug's appearance, which resembles an old piece of paper, and its habitat in a library, I would identify it as Cryptoclytra mirabilis (order: Hemiptera).\n",
      "\n",
      "The description of Cryptoclytra mirabilis matches your observation, with its wings resembling ancient parchment scrolls and its elongated body. The fact that it inhabits old libraries and historic buildings also fits with your finding the bug in a library.\n",
      "\n",
      "While Phantasma mirus (order: Phasmida) is a good candidate due to its ability to mimic small, dry twigs, its habitat in deciduous forests does not match the library environment. Psilodermis spectrus (order: Coleoptera) is also a good candidate due to its ghostly appearance, but its habitat in haunted forests and eerie swamps does not match the library setting.\n",
      "\n",
      "Therefore, with a high degree of certainty, I identify the bug you found in the library as Cryptoclytra mirabilis (order: Hemiptera).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "Enter your field observation:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(no input)\n",
      "\n",
      "========\n",
      "Goodbye.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    observation = input(\"\\n=============================\\nEnter your field observation: \").strip()\n",
    "    if observation:\n",
    "        print(\"-----------------------------\")\n",
    "        result = identify_and_suggest(observation)\n",
    "        print(f\"Result ==> {result}\")\n",
    "    else:\n",
    "        print(\"(no input)\")\n",
    "        break\n",
    "        \n",
    "print(\"\\n========\\nGoodbye.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9cb4e-b00f-46ae-8076-e310e6398531",
   "metadata": {},
   "source": [
    "## Appendix: non-LangChain model tests\n",
    "\n",
    "The code below is not part of the main LangChain-based application, but shows how you can use the SageMaker endpoints at lower abstraction layers than LangChain, namely by calling directly the boto3 or the SageMaker SDK primitives. Note that in the latter case, if you have deployed the model in the SageMaker UI, you will have to construct a `Predictor` object manually.\n",
    "\n",
    "_These non-LangChain idioms are important in themselves, as they open the way to a richer set of possibilities for integrating Astra DB with Amazon SageMaker._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5bf686-bc78-4bb1-96c4-89ce71200bd9",
   "metadata": {},
   "source": [
    "### Embedding model, test invocation through boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4df5ac27-0ff8-4110-ad1d-222255dc49ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned 2 embedding vectors.\n",
      "Each is made of 4096 float values.\n",
      "  The first one starts with: [0.016896691173315048, -1.7813106751418673e-05, -0.007678704336285591, 0.0056925...\n"
     ]
    }
   ],
   "source": [
    "encoded_body = json.dumps(\n",
    "    {\n",
    "        \"text_inputs\": [\n",
    "            \"Can you invoke a SageMaker embedding model from boto3 directly?\",\n",
    "            \"Wait and see...\"\n",
    "        ]\n",
    "    }\n",
    ").encode(\"utf-8\")\n",
    "\n",
    "response = boto3_sm_client.invoke_endpoint(\n",
    "    EndpointName=emb_endpoint_name,\n",
    "    Body=encoded_body,\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    ")\n",
    "\n",
    "response_body = response['Body']\n",
    "read_body = response_body.read()\n",
    "response_json = json.loads(read_body.decode())\n",
    "\n",
    "# This is a list 2 lists, each made of 4096 floats:\n",
    "embedding_vectors = response_json['embedding']\n",
    "\n",
    "print(f\"Returned {len(embedding_vectors)} embedding vectors.\")\n",
    "print(f\"Each is made of {len(embedding_vectors[0])} float values.\")\n",
    "print(f\"  The first one starts with: {str(embedding_vectors[0])[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985f22e-5874-4692-8149-6eadb59121d6",
   "metadata": {},
   "source": [
    "### Embedding model, test invocation through SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4896578-466a-4856-a22d-bf705255291c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Returned 2 embedding vectors.\n",
      "Each is made of 4096 float values.\n",
      "  The first one starts with: [0.010399903170764446, -0.014787523075938225, 0.005106857977807522, -0.008509762...\n"
     ]
    }
   ],
   "source": [
    "if emb_endpoint_supplied:\n",
    "    emb_predictor = Predictor(\n",
    "        emb_endpoint_name,\n",
    "        serializer=JSONSerializer(),\n",
    "        deserializer=JSONDeserializer(),\n",
    "    )\n",
    "else:\n",
    "    # `emb_predictor` was already created as part of the deploy-from-code procedure\n",
    "    pass\n",
    "\n",
    "response_json = emb_predictor.predict(\n",
    "    {\"text_inputs\": [\n",
    "            \"Can you show me how to use the SageMaker SDK directly for embeddings?\",\n",
    "            \"Let me look at the docs...\"\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# This is a list 2 lists, each made of 4096 floats:\n",
    "embedding_vectors = response_json[\"embedding\"]\n",
    "\n",
    "print(f\"Returned {len(embedding_vectors)} embedding vectors.\")\n",
    "print(f\"Each is made of {len(embedding_vectors[0])} float values.\")\n",
    "print(f\"  The first one starts with: {str(embedding_vectors[0])[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e0476-8418-4804-9d2c-5bed720bb5ee",
   "metadata": {},
   "source": [
    "### LLM, test invocation through boto3\n",
    "\n",
    "For this particular model, the exact shape of the input data can be found in [this blog post](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34d2f9c7-3fc1-4841-ac17-9690eedccd89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"generation\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \" Spiders have eight legs.\"\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "encoded_body = json.dumps({\n",
    "    \"inputs\":\n",
    "        [\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Always answer with scientific accuracy\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"How many legs do spiders have?\",\n",
    "                },\n",
    "            ],\n",
    "        ],\n",
    "       \"parameters\": {\n",
    "           \"max_new_tokens\": 256,\n",
    "           \"top_p\": 0.9,\n",
    "           \"temperature\": 0.6\n",
    "       },\n",
    "}).encode(\"utf-8\")\n",
    "\n",
    "response = boto3_sm_client.invoke_endpoint(\n",
    "    EndpointName=llm_endpoint_name,\n",
    "    Body=encoded_body,\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    "    # This is required for each invocation of this model:\n",
    "    CustomAttributes='accept_eula=true',\n",
    ")\n",
    "response_body = response['Body']\n",
    "read_body = response_body.read()\n",
    "response_json = json.loads(read_body.decode())\n",
    "\n",
    "print(f\"Full response:\\n\")\n",
    "print(json.dumps(response_json, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669341f5-835e-4959-afcb-66b6aa51708c",
   "metadata": {},
   "source": [
    "### LLM, test invocation through SageMaker SDK\n",
    "\n",
    "Note how the EULA acceptance is passed in this case ([reference](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bb12751-debb-402e-b1b7-8618dc5b2bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "Full response:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"generation\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \" A proton is composed of three quarks: two up quarks and one down quark. Therefore, there are two up quarks in a proton.\"\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "if llm_endpoint_supplied:\n",
    "    llm_predictor = Predictor(\n",
    "        llm_endpoint_name,\n",
    "        serializer=JSONSerializer(),\n",
    "        deserializer=JSONDeserializer(),\n",
    "    )\n",
    "else:\n",
    "    # `llm_predictor` was already created as part of the deploy-from-code procedure\n",
    "    pass\n",
    "\n",
    "\n",
    "response_json = llm_predictor.predict(\n",
    "    {\n",
    "        \"inputs\":\n",
    "            [\n",
    "                [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"Always answer with scientific accuracy\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": \"How many up quarks are in a proton?\",\n",
    "                    },\n",
    "                ],\n",
    "            ],\n",
    "           \"parameters\": {\n",
    "               \"max_new_tokens\": 256,\n",
    "               \"top_p\": 0.9,\n",
    "               \"temperature\": 0.6\n",
    "           },\n",
    "    },\n",
    "    custom_attributes = 'accept_eula=true',\n",
    ")\n",
    "\n",
    "print(f\"Full response:\\n\")\n",
    "print(json.dumps(response_json, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
